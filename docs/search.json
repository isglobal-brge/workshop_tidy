[
  {
    "objectID": "16-ensemble-methods.html",
    "href": "16-ensemble-methods.html",
    "title": "Chapter 16: Ensemble Methods - The Power of Many",
    "section": "",
    "text": "By the end of this chapter, you will master:\n\nThe theory behind ensemble methods\nBagging and random forests\nBoosting algorithms (AdaBoost, GBM, XGBoost)\nStacking and blending models\nVoting classifiers\nModel diversity and ensemble selection\nPractical implementation with tidymodels\nWhen and why ensembles work"
  },
  {
    "objectID": "16-ensemble-methods.html#learning-objectives",
    "href": "16-ensemble-methods.html#learning-objectives",
    "title": "Chapter 16: Ensemble Methods - The Power of Many",
    "section": "",
    "text": "By the end of this chapter, you will master:\n\nThe theory behind ensemble methods\nBagging and random forests\nBoosting algorithms (AdaBoost, GBM, XGBoost)\nStacking and blending models\nVoting classifiers\nModel diversity and ensemble selection\nPractical implementation with tidymodels\nWhen and why ensembles work"
  },
  {
    "objectID": "16-ensemble-methods.html#the-wisdom-of-crowds-in-machine-learning",
    "href": "16-ensemble-methods.html#the-wisdom-of-crowds-in-machine-learning",
    "title": "Chapter 16: Ensemble Methods - The Power of Many",
    "section": "The Wisdom of Crowds in Machine Learning",
    "text": "The Wisdom of Crowds in Machine Learning\nImagine you’re trying to guess the number of jellybeans in a jar. One person might overestimate, another might underestimate, but the average of many guesses often comes remarkably close to the truth. This is the fundamental principle behind ensemble methods: combining multiple models can produce predictions that are more accurate than any individual model.\nThe mathematics behind this are elegant. If we have multiple independent models with uncorrelated errors, combining them reduces the overall error. This is why ensemble methods consistently win machine learning competitions.\n\nlibrary(tidymodels)\n\n-- Attaching packages -------------------------------------- tidymodels 1.4.1 --\n\n\nv broom        1.0.10     v recipes      1.3.1 \nv dials        1.4.2      v rsample      1.3.1 \nv dplyr        1.1.4      v tailor       0.1.0 \nv ggplot2      4.0.0      v tidyr        1.3.1 \nv infer        1.0.9      v tune         2.0.0 \nv modeldata    1.5.1      v workflows    1.3.0 \nv parsnip      1.3.3      v workflowsets 1.1.1 \nv purrr        1.1.0      v yardstick    1.3.2 \n\n\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx purrr::discard() masks scales::discard()\nx dplyr::filter()  masks stats::filter()\nx dplyr::lag()     masks stats::lag()\nx recipes::step()  masks stats::step()\n\nlibrary(tidyverse)\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv forcats   1.0.0     v stringr   1.5.2\nv lubridate 1.9.4     v tibble    3.3.0\nv readr     2.1.5     \n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx readr::col_factor() masks scales::col_factor()\nx purrr::discard()    masks scales::discard()\nx dplyr::filter()     masks stats::filter()\nx stringr::fixed()    masks recipes::fixed()\nx dplyr::lag()        masks stats::lag()\nx readr::spec()       masks yardstick::spec()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(modeldata)\nlibrary(vip)\n\n\nAdjuntando el paquete: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(patchwork)\nlibrary(stacks)\n\nRegistered S3 method overwritten by 'butcher':\n  method                 from    \n  as.character.dev_topic generics\n\nlibrary(baguette)\nlibrary(rules)\n\n\nAdjuntando el paquete: 'rules'\n\nThe following object is masked from 'package:dials':\n\n    max_rules\n\nlibrary(ranger)\nlibrary(xgboost)\n\n\nAdjuntando el paquete: 'xgboost'\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\n# Set theme and seed\ntheme_set(theme_minimal())\nset.seed(123)\n\n# Load and prepare data\ndata(ames)\names_split &lt;- initial_split(ames, prop = 0.75, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test &lt;- testing(ames_split)\n\n# Create resamples for evaluation\names_folds &lt;- vfold_cv(ames_train, v = 5, strata = Sale_Price)\n\n# Prepare a simpler dataset for visualization\names_simple &lt;- ames_train %&gt;%\n  select(Sale_Price, Gr_Liv_Area, Overall_Cond, Year_Built, Neighborhood) %&gt;%\n  slice_sample(n = 500)"
  },
  {
    "objectID": "16-ensemble-methods.html#why-ensembles-work-the-mathematical-foundation",
    "href": "16-ensemble-methods.html#why-ensembles-work-the-mathematical-foundation",
    "title": "Chapter 16: Ensemble Methods - The Power of Many",
    "section": "Why Ensembles Work: The Mathematical Foundation",
    "text": "Why Ensembles Work: The Mathematical Foundation\nLet’s understand why combining models reduces error:\n\n# Simulate the power of ensembles\nset.seed(456)\nn_models &lt;- 50\nn_points &lt;- 100\n\n# True function\nx &lt;- seq(0, 10, length.out = n_points)\ny_true &lt;- sin(x) + 0.5 * x\ny_observed &lt;- y_true + rnorm(n_points, sd = 0.5)\n\n# Create multiple \"weak\" models (with random variations)\nweak_predictions &lt;- map(1:n_models, function(i) {\n  # Each model sees slightly different data (bootstrap)\n  sample_idx &lt;- sample(1:n_points, replace = TRUE)\n  \n  # Simple polynomial model with random degree\n  degree &lt;- sample(2:5, 1)\n  model &lt;- lm(y_observed[sample_idx] ~ poly(x[sample_idx], degree))\n  \n  # Predict on all points\n  predict(model, newdata = data.frame(x = x))\n})\n\n# Combine predictions\nensemble_pred &lt;- reduce(weak_predictions, `+`) / n_models\n\n# Calculate errors\nindividual_errors &lt;- map_dbl(weak_predictions, ~ mean((. - y_true)^2))\nensemble_error &lt;- mean((ensemble_pred - y_true)^2)\n\n# Visualize\nresults_df &lt;- tibble(\n  x = x,\n  y_true = y_true,\n  y_observed = y_observed,\n  ensemble = ensemble_pred\n) %&gt;%\n  bind_cols(\n    as_tibble(weak_predictions, .name_repair = \"minimal\") %&gt;%\n      set_names(paste0(\"model_\", 1:n_models))\n  )\n\n# Plot individual models vs ensemble\np1 &lt;- ggplot(results_df, aes(x = x)) +\n  geom_point(aes(y = y_observed), alpha = 0.3) +\n  geom_line(aes(y = y_true), color = \"black\", linewidth = 1.5) +\n  geom_line(aes(y = model_1), color = \"blue\", alpha = 0.5) +\n  geom_line(aes(y = model_2), color = \"blue\", alpha = 0.5) +\n  geom_line(aes(y = model_3), color = \"blue\", alpha = 0.5) +\n  labs(\n    title = \"Individual Weak Models\",\n    subtitle = \"Each model overfits in different ways\",\n    y = \"Value\"\n  )\n\np2 &lt;- ggplot(results_df, aes(x = x)) +\n  geom_point(aes(y = y_observed), alpha = 0.3) +\n  geom_line(aes(y = y_true), color = \"black\", linewidth = 1.5) +\n  geom_line(aes(y = ensemble), color = \"red\", linewidth = 1.5) +\n  labs(\n    title = \"Ensemble Prediction\",\n    subtitle = paste(\"Ensemble MSE:\", round(ensemble_error, 3), \n                    \"vs Avg Individual:\", round(mean(individual_errors), 3)),\n    y = \"Value\"\n  )\n\np1 + p2\n\n\n\n\n\n\n\n# Error reduction\ncat(\"Average individual model MSE:\", mean(individual_errors), \"\\n\")\n\nAverage individual model MSE: 4.698849 \n\ncat(\"Ensemble MSE:\", ensemble_error, \"\\n\")\n\nEnsemble MSE: 2.540965 \n\ncat(\"Error reduction:\", round((1 - ensemble_error/mean(individual_errors)) * 100, 1), \"%\\n\")\n\nError reduction: 45.9 %\n\n\nThe key insight: errors cancel out when models make different mistakes!"
  },
  {
    "objectID": "16-ensemble-methods.html#bagging-bootstrap-aggregating",
    "href": "16-ensemble-methods.html#bagging-bootstrap-aggregating",
    "title": "Chapter 16: Ensemble Methods - The Power of Many",
    "section": "Bagging: Bootstrap Aggregating",
    "text": "Bagging: Bootstrap Aggregating\nBagging creates diverse models by training on different bootstrap samples:\n\n# Implement bagging for decision trees\nbagging_spec &lt;- bag_tree(\n  cost_complexity = 0,\n  tree_depth = 10,\n  min_n = 2\n) %&gt;%\n  set_engine(\"rpart\", times = 25) %&gt;%  # 25 bootstrap samples\n  set_mode(\"regression\")\n\n# Create a simple recipe\nsimple_recipe &lt;- recipe(Sale_Price ~ ., data = ames_train) %&gt;%\n  step_rm(Street, Utilities, Condition_2, Roof_Matl, Heating, Pool_QC,\n          Misc_Feature, Pool_Area, Longitude, Latitude) %&gt;%\n  step_impute_median(all_numeric_predictors()) %&gt;%\n  step_impute_mode(all_nominal_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors())\n\n# Create workflow\nbagging_workflow &lt;- workflow() %&gt;%\n  add_recipe(simple_recipe) %&gt;%\n  add_model(bagging_spec)\n\n# Fit and evaluate\nbagging_fit &lt;- bagging_workflow %&gt;%\n  fit(ames_train)\n\n# Compare with single tree\nsingle_tree_spec &lt;- decision_tree(\n  cost_complexity = 0,\n  tree_depth = 10,\n  min_n = 2\n) %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"regression\")\n\nsingle_tree_workflow &lt;- workflow() %&gt;%\n  add_recipe(simple_recipe) %&gt;%\n  add_model(single_tree_spec)\n\nsingle_tree_fit &lt;- single_tree_workflow %&gt;%\n  fit(ames_train)\n\n# Evaluate both\ntest_predictions &lt;- tibble(\n  actual = ames_test$Sale_Price,\n  single_tree = predict(single_tree_fit, ames_test)$.pred,\n  bagging = predict(bagging_fit, ames_test)$.pred\n)\n\ncomparison_metrics &lt;- test_predictions %&gt;%\n  summarise(\n    single_tree_rmse = sqrt(mean((actual - single_tree)^2)),\n    bagging_rmse = sqrt(mean((actual - bagging)^2)),\n    improvement = (single_tree_rmse - bagging_rmse) / single_tree_rmse * 100\n  )\n\nknitr::kable(comparison_metrics, digits = 2)\n\n\n\n\nsingle_tree_rmse\nbagging_rmse\nimprovement\n\n\n\n\n35793.08\n26948.51\n24.71\n\n\n\n\n# Visualize predictions\ntest_predictions %&gt;%\n  pivot_longer(cols = c(single_tree, bagging), \n               names_to = \"model\", values_to = \"prediction\") %&gt;%\n  ggplot(aes(x = actual, y = prediction)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +\n  facet_wrap(~model) +\n  labs(\n    title = \"Single Tree vs Bagged Trees\",\n    subtitle = \"Bagging reduces overfitting and improves predictions\",\n    x = \"Actual Sale Price\",\n    y = \"Predicted Sale Price\"\n  ) +\n  coord_equal()\n\n\n\n\n\n\n\n\nBagging characteristics: - Reduces variance without increasing bias - Works best with high-variance, low-bias models (like deep trees) - Parallel training possible (each bootstrap independent) - Out-of-bag (OOB) error provides free validation"
  },
  {
    "objectID": "16-ensemble-methods.html#random-forests-bagging-with-a-twist",
    "href": "16-ensemble-methods.html#random-forests-bagging-with-a-twist",
    "title": "Chapter 16: Ensemble Methods - The Power of Many",
    "section": "Random Forests: Bagging with a Twist",
    "text": "Random Forests: Bagging with a Twist\nRandom forests add feature randomness to bagging:\n\n# Compare different mtry values\nrf_comparison &lt;- tibble(\n  mtry_prop = c(0.1, 0.33, 0.5, 0.75, 1.0),\n  mtry_desc = c(\"10% features\", \"33% features (sqrt)\", \"50% features\", \n                \"75% features\", \"100% features (bagging)\")\n) %&gt;%\n  mutate(\n    model = map(mtry_prop, function(prop) {\n      rand_forest(\n        trees = 100,\n        mtry = floor(prop * (ncol(ames_train) - 1)),\n        min_n = 5\n      ) %&gt;%\n        set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n        set_mode(\"regression\")\n    })\n  )\n\n# Fit all models\nrf_workflows &lt;- rf_comparison %&gt;%\n  mutate(\n    workflow = map(model, function(m) {\n      workflow() %&gt;%\n        add_recipe(simple_recipe) %&gt;%\n        add_model(m)\n    })\n  )\n\n# Evaluate with cross-validation (simplified for speed)\nrf_results &lt;- rf_workflows %&gt;%\n  mutate(\n    cv_results = map(workflow, ~ fit_resamples(\n      .,\n      resamples = vfold_cv(ames_train, v = 3),  # Reduced folds for speed\n      metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq)\n    ))\n  )\n\n# Extract metrics\nrf_metrics &lt;- rf_results %&gt;%\n  mutate(\n    metrics = map(cv_results, collect_metrics)\n  ) %&gt;%\n  unnest(metrics) %&gt;%\n  select(mtry_desc, .metric, mean, std_err)\n\n# Visualize mtry effect\nrf_metrics %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  ggplot(aes(x = mtry_desc, y = mean)) +\n  geom_col(fill = \"steelblue\") +\n  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), width = 0.2) +\n  labs(\n    title = \"Effect of Feature Randomness (mtry)\",\n    subtitle = \"Moderate mtry often performs best\",\n    x = \"Features Considered per Split\",\n    y = \"RMSE\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n# Feature importance from best model\nbest_rf &lt;- rand_forest(\n  trees = 500,\n  mtry = floor(sqrt(ncol(ames_train) - 1)),\n  min_n = 5\n) %&gt;%\n  set_engine(\"ranger\", importance = \"permutation\") %&gt;%\n  set_mode(\"regression\")\n\nbest_rf_fit &lt;- workflow() %&gt;%\n  add_recipe(simple_recipe) %&gt;%\n  add_model(best_rf) %&gt;%\n  fit(ames_train)\n\n# Extract and plot importance\nbest_rf_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  vip(num_features = 15) +\n  labs(title = \"Random Forest Feature Importance\")\n\n\n\n\n\n\n\n\nRandom forest advantages: - Further reduces overfitting compared to bagging - Decorrelates trees through feature sampling - Provides feature importance naturally - Robust to hyperparameters (often works well out-of-box)"
  },
  {
    "objectID": "16-ensemble-methods.html#boosting-learning-from-mistakes",
    "href": "16-ensemble-methods.html#boosting-learning-from-mistakes",
    "title": "Chapter 16: Ensemble Methods - The Power of Many",
    "section": "Boosting: Learning from Mistakes",
    "text": "Boosting: Learning from Mistakes\nBoosting sequentially builds models, each learning from previous errors:\n\nAdaBoost: The Original Boosting Algorithm\n\n# Demonstrate boosting concept with simple example\n# Create a difficult classification dataset\nset.seed(789)\nspiral_data &lt;- tibble(\n  angle = runif(300, 0, 4 * pi),\n  radius = runif(300, 0.5, 2)\n) %&gt;%\n  mutate(\n    x = radius * cos(angle) + rnorm(300, sd = 0.1),\n    y = radius * sin(angle) + rnorm(300, sd = 0.1),\n    class = factor(if_else(angle %% (2 * pi) &lt; pi, \"A\", \"B\"))\n  )\n\n# Visualize the problem\nggplot(spiral_data, aes(x = x, y = y, color = class)) +\n  geom_point(size = 2) +\n  labs(\n    title = \"Spiral Classification Problem\",\n    subtitle = \"Difficult for single linear boundary\"\n  ) +\n  coord_equal()\n\n\n\n\n\n\n\n# Boosting builds sequential models\n# Each focuses on misclassified points from previous models\n\n\n\nGradient Boosting Machines (GBM)\n\n# XGBoost - state-of-the-art gradient boosting\nxgb_spec &lt;- boost_tree(\n  trees = 100,\n  tree_depth = 4,\n  min_n = 10,\n  loss_reduction = 0.01,\n  sample_size = 0.8,\n  learn_rate = 0.1\n) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\nxgb_workflow &lt;- workflow() %&gt;%\n  add_recipe(simple_recipe) %&gt;%\n  add_model(xgb_spec)\n\n# Fit the model\nxgb_fit &lt;- xgb_workflow %&gt;%\n  fit(ames_train)\n\n# Compare learning rates\nlearning_rates &lt;- c(0.01, 0.05, 0.1, 0.3)\n\nlr_comparison &lt;- map_df(learning_rates, function(lr) {\n  spec &lt;- boost_tree(\n    trees = 100,\n    tree_depth = 4,\n    learn_rate = lr\n  ) %&gt;%\n    set_engine(\"xgboost\") %&gt;%\n    set_mode(\"regression\")\n  \n  wf &lt;- workflow() %&gt;%\n    add_recipe(simple_recipe) %&gt;%\n    add_model(spec)\n  \n  # Fit and evaluate\n  fit &lt;- wf %&gt;% fit(ames_train)\n  \n  # Get training history (if available)\n  train_pred &lt;- predict(fit, ames_train)\n  test_pred &lt;- predict(fit, ames_test)\n  \n  tibble(\n    learn_rate = lr,\n    train_rmse = sqrt(mean((ames_train$Sale_Price - train_pred$.pred)^2)),\n    test_rmse = sqrt(mean((ames_test$Sale_Price - test_pred$.pred)^2))\n  )\n})\n\n# Visualize learning rate effect\nlr_comparison %&gt;%\n  pivot_longer(cols = c(train_rmse, test_rmse), \n               names_to = \"set\", values_to = \"rmse\") %&gt;%\n  ggplot(aes(x = factor(learn_rate), y = rmse, fill = set)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Learning Rate Effect in Gradient Boosting\",\n    subtitle = \"Lower rates often generalize better but need more trees\",\n    x = \"Learning Rate\",\n    y = \"RMSE\"\n  )\n\n\n\n\n\n\n\n\nBoosting characteristics: - Sequential training (can’t parallelize easily) - Focuses on difficult cases progressively - Can overfit if not regularized properly - Often achieves best single-model performance"
  },
  {
    "objectID": "16-ensemble-methods.html#model-stacking-the-meta-learning-approach",
    "href": "16-ensemble-methods.html#model-stacking-the-meta-learning-approach",
    "title": "Chapter 16: Ensemble Methods - The Power of Many",
    "section": "Model Stacking: The Meta-Learning Approach",
    "text": "Model Stacking: The Meta-Learning Approach\nStacking uses a meta-model to combine predictions from base models:\n\n# Simple stacking example with manual blending\n# Create base models\nlm_spec &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\n\nrf_spec &lt;- rand_forest(trees = 200) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\nxgb_spec &lt;- boost_tree(trees = 100, tree_depth = 4) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n# Fit models\nlm_fit &lt;- workflow() %&gt;%\n  add_recipe(simple_recipe) %&gt;%\n  add_model(lm_spec) %&gt;%\n  fit(ames_train)\n\nrf_fit &lt;- workflow() %&gt;%\n  add_recipe(simple_recipe) %&gt;%\n  add_model(rf_spec) %&gt;%\n  fit(ames_train)\n\nxgb_fit &lt;- workflow() %&gt;%\n  add_recipe(simple_recipe) %&gt;%\n  add_model(xgb_spec) %&gt;%\n  fit(ames_train)\n\n# Get predictions from each model\nlm_pred &lt;- predict(lm_fit, ames_test)$.pred\nrf_pred &lt;- predict(rf_fit, ames_test)$.pred\nxgb_pred &lt;- predict(xgb_fit, ames_test)$.pred\n\n# Simple average ensemble\navg_pred &lt;- (lm_pred + rf_pred + xgb_pred) / 3\n\n# Weighted average (weights could be tuned)\nweighted_pred &lt;- 0.2 * lm_pred + 0.4 * rf_pred + 0.4 * xgb_pred\n\n# Calculate RMSE for each approach\nrmse_results &lt;- tibble(\n  Model = c(\"Linear\", \"Random Forest\", \"XGBoost\", \"Simple Average\", \"Weighted Average\"),\n  RMSE = c(\n    sqrt(mean((ames_test$Sale_Price - lm_pred)^2)),\n    sqrt(mean((ames_test$Sale_Price - rf_pred)^2)),\n    sqrt(mean((ames_test$Sale_Price - xgb_pred)^2)),\n    sqrt(mean((ames_test$Sale_Price - avg_pred)^2)),\n    sqrt(mean((ames_test$Sale_Price - weighted_pred)^2))\n  )\n)\n\n# Display results\nknitr::kable(rmse_results, digits = 0)\n\n\n\n\nModel\nRMSE\n\n\n\n\nLinear\n28724\n\n\nRandom Forest\n27934\n\n\nXGBoost\n24228\n\n\nSimple Average\n24186\n\n\nWeighted Average\n24076\n\n\n\n\n# Visualize ensemble effect\nggplot(rmse_results, aes(x = reorder(Model, RMSE), y = RMSE, fill = Model)) +\n  geom_col() +\n  coord_flip() +\n  labs(\n    title = \"Model Stacking Performance\",\n    subtitle = \"Ensemble methods typically outperform individual models\",\n    x = NULL\n  ) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "16-ensemble-methods.html#voting-ensembles",
    "href": "16-ensemble-methods.html#voting-ensembles",
    "title": "Chapter 16: Ensemble Methods - The Power of Many",
    "section": "Voting Ensembles",
    "text": "Voting Ensembles\nFor classification, voting combines predictions through majority vote or averaging:\n\n# Create classification problem\names_class &lt;- ames_train %&gt;%\n  mutate(expensive = factor(if_else(Sale_Price &gt; median(Sale_Price), \n                                    \"yes\", \"no\"))) %&gt;%\n  select(-Sale_Price)\n\nclass_split &lt;- initial_split(ames_class, strata = expensive)\nclass_train &lt;- training(class_split)\nclass_test &lt;- testing(class_split)\n\n# Create diverse classifiers\nlogistic_spec &lt;- logistic_reg(penalty = 0.01, mixture = 0.5) %&gt;%\n  set_engine(\"glmnet\")\n\ntree_spec &lt;- decision_tree(tree_depth = 10) %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\nrf_class_spec &lt;- rand_forest(trees = 100) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n# Simple recipe for classification\nclass_recipe &lt;- recipe(expensive ~ Gr_Liv_Area + Total_Bsmt_SF + Year_Built, \n                      data = class_train) %&gt;%\n  step_normalize(all_numeric_predictors())\n\n# Fit individual models\nmodels &lt;- list(\n  logistic = workflow() %&gt;%\n    add_recipe(class_recipe) %&gt;%\n    add_model(logistic_spec) %&gt;%\n    fit(class_train),\n  \n  tree = workflow() %&gt;%\n    add_recipe(class_recipe) %&gt;%\n    add_model(tree_spec) %&gt;%\n    fit(class_train),\n  \n  rf = workflow() %&gt;%\n    add_recipe(class_recipe) %&gt;%\n    add_model(rf_class_spec) %&gt;%\n    fit(class_train)\n)\n\n# Get predictions from each model\npredictions &lt;- map_dfc(models, function(model) {\n  predict(model, class_test, type = \"prob\") %&gt;%\n    select(.pred_yes) %&gt;%\n    pull()\n}) %&gt;%\n  set_names(paste0(names(models), \"_prob_yes\"))\n\n# Hard voting (majority vote)\nhard_vote &lt;- predictions %&gt;%\n  mutate(\n    vote_yes = rowSums(. &gt; 0.5),\n    prediction = factor(if_else(vote_yes &gt;= 2, \"yes\", \"no\"))\n  )\n\n# Soft voting (average probabilities)\nsoft_vote &lt;- predictions %&gt;%\n  mutate(\n    avg_prob = rowMeans(.),\n    prediction = factor(if_else(avg_prob &gt; 0.5, \"yes\", \"no\"))\n  )\n\n# Evaluate voting methods\nvoting_results &lt;- tibble(\n  method = c(\"Hard Voting\", \"Soft Voting\"),\n  accuracy = c(\n    mean(hard_vote$prediction == class_test$expensive),\n    mean(soft_vote$prediction == class_test$expensive)\n  )\n)\n\n# Add individual model accuracies\nindividual_acc &lt;- map_dbl(models, function(model) {\n  pred &lt;- predict(model, class_test)\n  mean(pred$.pred_class == class_test$expensive)\n})\n\nall_results &lt;- bind_rows(\n  tibble(method = names(individual_acc), accuracy = individual_acc),\n  voting_results\n)\n\n# Visualize\nggplot(all_results, aes(x = reorder(method, accuracy), y = accuracy)) +\n  geom_col(fill = c(rep(\"steelblue\", 3), rep(\"coral\", 2))) +\n  geom_text(aes(label = round(accuracy, 3)), vjust = -0.5) +\n  labs(\n    title = \"Voting Ensemble Performance\",\n    subtitle = \"Voting often outperforms individual models\",\n    x = \"Method\",\n    y = \"Accuracy\"\n  ) +\n  ylim(0, 1)"
  },
  {
    "objectID": "16-ensemble-methods.html#diversity-in-ensembles",
    "href": "16-ensemble-methods.html#diversity-in-ensembles",
    "title": "Chapter 16: Ensemble Methods - The Power of Many",
    "section": "Diversity in Ensembles",
    "text": "Diversity in Ensembles\nEnsemble success depends on model diversity:\n\n# Measure diversity through correlation\n# Get predictions from base models\nbase_predictions &lt;- map_dfc(models, ~ predict(., class_test, type = \"prob\")$.pred_yes) %&gt;%\n  set_names(names(models))\n\n# Calculate correlation matrix\ncor_matrix &lt;- cor(base_predictions)\n\n# Visualize\ncorrplot::corrplot(cor_matrix, method = \"circle\", type = \"upper\",\n                   title = \"Model Prediction Correlations\")\n\n\n\n\n\n\n\n# Diversity metrics\ndiversity_metrics &lt;- tibble(\n  metric = c(\"Average Pairwise Correlation\", \"Disagreement Rate\"),\n  value = c(\n    mean(cor_matrix[upper.tri(cor_matrix)]),\n    mean(apply(base_predictions &gt; 0.5, 1, function(x) length(unique(x))) &gt; 1)\n  )\n)\n\nknitr::kable(diversity_metrics, digits = 3)\n\n\n\n\nmetric\nvalue\n\n\n\n\nAverage Pairwise Correlation\n0.909\n\n\nDisagreement Rate\n0.133\n\n\n\n\n# Show how diversity affects ensemble performance\n# Create models with varying diversity\ndiverse_models &lt;- list(\n  # Similar models (low diversity)\n  rf1 = rand_forest(trees = 100, mtry = 5) %&gt;%\n    set_engine(\"ranger\") %&gt;%\n    set_mode(\"classification\"),\n  \n  rf2 = rand_forest(trees = 100, mtry = 6) %&gt;%\n    set_engine(\"ranger\") %&gt;%\n    set_mode(\"classification\"),\n  \n  # Different model types (high diversity)\n  linear = logistic_reg() %&gt;%\n    set_engine(\"glm\"),\n  \n  tree = decision_tree(tree_depth = 5) %&gt;%\n    set_engine(\"rpart\") %&gt;%\n    set_mode(\"classification\")\n)\n\n# Fit and evaluate\ndiverse_fits &lt;- map(diverse_models, function(model) {\n  workflow() %&gt;%\n    add_recipe(class_recipe) %&gt;%\n    add_model(model) %&gt;%\n    fit(class_train)\n})\n\n# Compare ensemble of similar vs diverse models\nsimilar_ensemble &lt;- map_dfc(diverse_fits[1:2], \n                           ~ predict(., class_test, type = \"prob\")$.pred_yes) %&gt;%\n  rowMeans()\n\ndiverse_ensemble &lt;- map_dfc(diverse_fits[3:4], \n                           ~ predict(., class_test, type = \"prob\")$.pred_yes) %&gt;%\n  rowMeans()\n\nensemble_comparison &lt;- tibble(\n  ensemble_type = c(\"Similar Models\", \"Diverse Models\"),\n  accuracy = c(\n    mean((similar_ensemble &gt; 0.5) == (class_test$expensive == \"yes\")),\n    mean((diverse_ensemble &gt; 0.5) == (class_test$expensive == \"yes\"))\n  )\n)\n\nknitr::kable(ensemble_comparison, digits = 3)\n\n\n\n\nensemble_type\naccuracy\n\n\n\n\nSimilar Models\n0.876\n\n\nDiverse Models\n0.858"
  },
  {
    "objectID": "16-ensemble-methods.html#advanced-ensemble-techniques",
    "href": "16-ensemble-methods.html#advanced-ensemble-techniques",
    "title": "Chapter 16: Ensemble Methods - The Power of Many",
    "section": "Advanced Ensemble Techniques",
    "text": "Advanced Ensemble Techniques\n\nDynamic Ensemble Selection\nChoose different models for different regions of the feature space:\n\n# Demonstrate region-based ensemble\n# Create a 2D problem for visualization\nset.seed(123)\nregion_data &lt;- tibble(\n  x1 = runif(500, -2, 2),\n  x2 = runif(500, -2, 2)\n) %&gt;%\n  mutate(\n    region = case_when(\n      x1 &lt; 0 & x2 &lt; 0 ~ \"A\",\n      x1 &gt;= 0 & x2 &lt; 0 ~ \"B\",\n      x1 &lt; 0 & x2 &gt;= 0 ~ \"C\",\n      TRUE ~ \"D\"\n    ),\n    y = case_when(\n      region == \"A\" ~ 2 * x1 + x2,        # Linear in region A\n      region == \"B\" ~ x1^2 + x2,          # Quadratic in region B\n      region == \"C\" ~ sin(2 * x1) + x2,   # Sinusoidal in region C\n      TRUE ~ exp(0.5 * x1) + x2           # Exponential in region D\n    ) + rnorm(500, sd = 0.3)\n  )\n\n# Visualize regions\nggplot(region_data, aes(x = x1, y = x2, color = y)) +\n  geom_point(size = 2) +\n  scale_color_viridis_c() +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(\n    title = \"Different Patterns in Different Regions\",\n    subtitle = \"Dynamic selection can use best model for each region\"\n  ) +\n  coord_equal()\n\n\n\n\n\n\n\n\n\n\nCascade Ensembles\nUse simple models for easy cases, complex models for hard cases:\n\n# Demonstrate cascade concept\n# First tier: Simple, fast model\nsimple_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\n\nsimple_fit &lt;- workflow() %&gt;%\n  add_recipe(simple_recipe) %&gt;%\n  add_model(simple_model) %&gt;%\n  fit(ames_train)\n\n# Get predictions and residuals\nsimple_pred &lt;- predict(simple_fit, ames_train)\ntrain_with_residuals &lt;- ames_train %&gt;%\n  mutate(\n    simple_pred = simple_pred$.pred,\n    residual = Sale_Price - simple_pred,\n    is_difficult = abs(residual) &gt; quantile(abs(residual), 0.75)\n  )\n\n# Second tier: Complex model for difficult cases\ncomplex_model &lt;- boost_tree(trees = 200, tree_depth = 6) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n# Train on difficult cases\ndifficult_cases &lt;- train_with_residuals %&gt;%\n  filter(is_difficult)\n\ncomplex_recipe &lt;- recipe(residual ~ Gr_Liv_Area + Year_Built + Total_Bsmt_SF, \n                        data = difficult_cases)\n\ncomplex_fit &lt;- workflow() %&gt;%\n  add_recipe(complex_recipe) %&gt;%\n  add_model(complex_model) %&gt;%\n  fit(difficult_cases)\n\ncat(\"Cascade ensemble:\\n\")\n\nCascade ensemble:\n\ncat(\"- Simple model handles\", sum(!train_with_residuals$is_difficult), \"cases\\n\")\n\n- Simple model handles 1648 cases\n\ncat(\"- Complex model handles\", sum(train_with_residuals$is_difficult), \"difficult cases\\n\")\n\n- Complex model handles 549 difficult cases"
  },
  {
    "objectID": "16-ensemble-methods.html#best-practices-for-ensembles",
    "href": "16-ensemble-methods.html#best-practices-for-ensembles",
    "title": "Chapter 16: Ensemble Methods - The Power of Many",
    "section": "Best Practices for Ensembles",
    "text": "Best Practices for Ensembles\n\n1. Ensure Model Diversity\n\n# Strategies for diversity\ndiversity_strategies &lt;- tibble(\n  Strategy = c(\n    \"Different algorithms\",\n    \"Different hyperparameters\",\n    \"Different features\",\n    \"Different training samples\",\n    \"Different target transformations\"\n  ),\n  Example = c(\n    \"Linear + Tree + Neural Network\",\n    \"Shallow trees + Deep trees\",\n    \"Subset 1 features + Subset 2 features\",\n    \"Bootstrap + Stratified + Random\",\n    \"Log(y) + sqrt(y) + y\"\n  ),\n  Benefit = c(\n    \"Captures different patterns\",\n    \"Varies complexity\",\n    \"Different perspectives\",\n    \"Reduces correlation\",\n    \"Different error distributions\"\n  )\n)\n\nknitr::kable(diversity_strategies)\n\n\n\n\n\n\n\n\n\nStrategy\nExample\nBenefit\n\n\n\n\nDifferent algorithms\nLinear + Tree + Neural Network\nCaptures different patterns\n\n\nDifferent hyperparameters\nShallow trees + Deep trees\nVaries complexity\n\n\nDifferent features\nSubset 1 features + Subset 2 features\nDifferent perspectives\n\n\nDifferent training samples\nBootstrap + Stratified + Random\nReduces correlation\n\n\nDifferent target transformations\nLog(y) + sqrt(y) + y\nDifferent error distributions\n\n\n\n\n\n\n\n2. Choose Appropriate Ensemble Method\n\n# Decision guide\nensemble_guide &lt;- tibble(\n  Scenario = c(\n    \"High-variance base models\",\n    \"Need interpretability\",\n    \"Limited computational budget\",\n    \"Maximizing performance\",\n    \"Imbalanced classes\",\n    \"Different expertise regions\"\n  ),\n  `Recommended Method` = c(\n    \"Bagging or Random Forest\",\n    \"Simple voting or linear stacking\",\n    \"Voting ensemble\",\n    \"Gradient boosting or stacking\",\n    \"Balanced bagging or cost-sensitive boosting\",\n    \"Dynamic selection\"\n  )\n)\n\nknitr::kable(ensemble_guide)\n\n\n\n\n\n\n\n\nScenario\nRecommended Method\n\n\n\n\nHigh-variance base models\nBagging or Random Forest\n\n\nNeed interpretability\nSimple voting or linear stacking\n\n\nLimited computational budget\nVoting ensemble\n\n\nMaximizing performance\nGradient boosting or stacking\n\n\nImbalanced classes\nBalanced bagging or cost-sensitive boosting\n\n\nDifferent expertise regions\nDynamic selection"
  },
  {
    "objectID": "16-ensemble-methods.html#exercises",
    "href": "16-ensemble-methods.html#exercises",
    "title": "Chapter 16: Ensemble Methods - The Power of Many",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Build a Custom Ensemble\nCreate your own ensemble combining different approaches:\n\n# Your solution\n# Create a custom ensemble for Ames housing\ncustom_recipe &lt;- recipe(Sale_Price ~ ., data = ames_train) %&gt;%\n  step_rm(Street, Utilities, Condition_2, Roof_Matl, Heating, Pool_QC,\n          Misc_Feature, Pool_Area, Longitude, Latitude) %&gt;%\n  step_impute_median(all_numeric_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors())\n\n# Base models\nbase_models &lt;- list(\n  # Regularized regression\n  elastic = linear_reg(penalty = 0.01, mixture = 0.5) %&gt;%\n    set_engine(\"glmnet\"),\n  \n  # Tree-based\n  rf = rand_forest(trees = 200, mtry = 10, min_n = 5) %&gt;%\n    set_engine(\"ranger\") %&gt;%\n    set_mode(\"regression\"),\n  \n  # Boosting\n  xgb = boost_tree(trees = 100, tree_depth = 5, learn_rate = 0.1) %&gt;%\n    set_engine(\"xgboost\") %&gt;%\n    set_mode(\"regression\"),\n  \n  # Local model\n  knn = nearest_neighbor(neighbors = 15) %&gt;%\n    set_engine(\"kknn\") %&gt;%\n    set_mode(\"regression\")\n)\n\n# Fit all base models\nbase_fits &lt;- map(base_models, function(model) {\n  workflow() %&gt;%\n    add_recipe(custom_recipe) %&gt;%\n    add_model(model) %&gt;%\n    fit(ames_train)\n})\n\n# Get out-of-sample predictions using cross-validation\n# (In practice, use proper validation set)\nval_split &lt;- initial_split(ames_train, prop = 0.8)\nval_train &lt;- training(val_split)\nval_test &lt;- testing(val_split)\n\n# Refit on validation training\nval_fits &lt;- map(base_models, function(model) {\n  workflow() %&gt;%\n    add_recipe(custom_recipe) %&gt;%\n    add_model(model) %&gt;%\n    fit(val_train)\n})\n\n# Get validation predictions for stacking\nval_predictions &lt;- map_dfc(val_fits, ~ predict(., val_test)$.pred) %&gt;%\n  set_names(names(base_models))\n\n# Train meta-learner\nmeta_data &lt;- val_predictions %&gt;%\n  mutate(target = val_test$Sale_Price)\n\nmeta_model &lt;- lm(target ~ ., data = meta_data)\n\n# Function to make ensemble predictions\nensemble_predict &lt;- function(new_data) {\n  # Get base predictions\n  base_preds &lt;- map_dfc(base_fits, ~ predict(., new_data)$.pred) %&gt;%\n    set_names(names(base_models))\n  \n  # Apply meta-model\n  predict(meta_model, base_preds)\n}\n\n# Evaluate custom ensemble\nensemble_pred &lt;- ensemble_predict(ames_test)\nensemble_rmse &lt;- sqrt(mean((ames_test$Sale_Price - ensemble_pred)^2))\n\n# Compare with individual models\nindividual_rmse &lt;- map_dbl(base_fits, function(fit) {\n  pred &lt;- predict(fit, ames_test)$.pred\n  sqrt(mean((ames_test$Sale_Price - pred)^2))\n})\n\nresults &lt;- c(individual_rmse, ensemble = ensemble_rmse)\nresults &lt;- sort(results)\n\nbarplot(results, main = \"Custom Ensemble Performance\",\n        ylab = \"RMSE\", col = c(rep(\"steelblue\", length(results)-1), \"coral\"))\n\n\n\n\n\n\n\n\n\n\nExercise 2: Optimize Ensemble Weights\nFind optimal weights for combining models:\n\n# Your solution\n# Get predictions from each model\ntest_preds &lt;- map_dfc(base_fits, ~ predict(., ames_test)$.pred) %&gt;%\n  set_names(names(base_models))\n\n# Optimization function\noptimize_weights &lt;- function(weights, predictions, target) {\n  # Normalize weights\n  weights &lt;- weights / sum(weights)\n  \n  # Weighted average\n  ensemble_pred &lt;- as.matrix(predictions) %*% weights\n  \n  # Return RMSE\n  sqrt(mean((target - ensemble_pred)^2))\n}\n\n# Initial equal weights\nn_models &lt;- length(base_models)\ninitial_weights &lt;- rep(1/n_models, n_models)\n\n# Optimize\nopt_result &lt;- optim(\n  par = initial_weights,\n  fn = optimize_weights,\n  predictions = test_preds,\n  target = ames_test$Sale_Price,\n  method = \"L-BFGS-B\",\n  lower = rep(0, n_models),\n  upper = rep(1, n_models)\n)\n\n# Optimal weights\noptimal_weights &lt;- opt_result$par / sum(opt_result$par)\nnames(optimal_weights) &lt;- names(base_models)\n\n# Compare equal vs optimal weights\nequal_pred &lt;- rowMeans(test_preds)\noptimal_pred &lt;- as.matrix(test_preds) %*% optimal_weights\n\nweight_comparison &lt;- tibble(\n  Method = c(\"Equal Weights\", \"Optimal Weights\"),\n  RMSE = c(\n    sqrt(mean((ames_test$Sale_Price - equal_pred)^2)),\n    sqrt(mean((ames_test$Sale_Price - optimal_pred)^2))\n  )\n)\n\nknitr::kable(weight_comparison, digits = 3)\n\n\n\n\nMethod\nRMSE\n\n\n\n\nEqual Weights\n26566.04\n\n\nOptimal Weights\n23026.81\n\n\n\n\n# Show optimal weights\nbarplot(optimal_weights, main = \"Optimal Ensemble Weights\",\n        ylab = \"Weight\", col = \"steelblue\")\n\n\n\n\n\n\n\n\n\n\nExercise 3: Implement Boosting from Scratch\nUnderstand boosting by implementing a simple version:\n\n# Your solution\n# Simple boosting implementation\nsimple_boosting &lt;- function(x, y, n_iterations = 10, learning_rate = 0.1) {\n  n &lt;- length(y)\n  \n  # Initialize with mean\n  predictions &lt;- rep(mean(y), n)\n  models &lt;- list()\n  \n  for (i in 1:n_iterations) {\n    # Calculate residuals\n    residuals &lt;- y - predictions\n    \n    # Fit a simple model to residuals (using a decision stump)\n    # For simplicity, we'll use a linear model here\n    model_data &lt;- data.frame(x = x, residual = residuals)\n    weak_model &lt;- lm(residual ~ x, data = model_data)\n    \n    # Store model\n    models[[i]] &lt;- weak_model\n    \n    # Update predictions\n    update &lt;- predict(weak_model, model_data)\n    predictions &lt;- predictions + learning_rate * update\n    \n    # Calculate current error\n    current_rmse &lt;- sqrt(mean((y - predictions)^2))\n    \n    cat(\"Iteration\", i, \"- RMSE:\", current_rmse, \"\\n\")\n  }\n  \n  return(list(models = models, final_predictions = predictions,\n              learning_rate = learning_rate))\n}\n\n# Test on simple data\ntest_x &lt;- ames_train$Gr_Liv_Area[1:100]\ntest_y &lt;- ames_train$Sale_Price[1:100]\n\nboost_result &lt;- simple_boosting(test_x, test_y, n_iterations = 10)\n\nIteration 1 - RMSE: 22607.51 \nIteration 2 - RMSE: 22598.13 \nIteration 3 - RMSE: 22590.53 \nIteration 4 - RMSE: 22584.37 \nIteration 5 - RMSE: 22579.38 \nIteration 6 - RMSE: 22575.34 \nIteration 7 - RMSE: 22572.07 \nIteration 8 - RMSE: 22569.41 \nIteration 9 - RMSE: 22567.27 \nIteration 10 - RMSE: 22565.52 \n\n# Visualize boosting progress\nplot(test_x, test_y, main = \"Simple Boosting Results\",\n     xlab = \"Gr_Liv_Area\", ylab = \"Sale_Price\")\npoints(test_x, boost_result$final_predictions, col = \"red\", pch = 16)\nlegend(\"topleft\", legend = c(\"Actual\", \"Predicted\"), \n       col = c(\"black\", \"red\"), pch = c(1, 16))"
  },
  {
    "objectID": "16-ensemble-methods.html#summary",
    "href": "16-ensemble-methods.html#summary",
    "title": "Chapter 16: Ensemble Methods - The Power of Many",
    "section": "Summary",
    "text": "Summary\nIn this comprehensive chapter, you’ve mastered:\n✅ Ensemble fundamentals - Why ensembles work mathematically - Bias-variance decomposition - The importance of diversity\n✅ Bagging methods - Bootstrap aggregating - Random forests - Out-of-bag error\n✅ Boosting algorithms - Sequential learning - AdaBoost and gradient boosting - XGBoost implementation\n✅ Stacking and blending - Meta-learning approaches - Cross-validation for stacking - Optimal weight finding\n✅ Advanced techniques - Dynamic selection - Cascade ensembles - Custom ensemble design\nKey takeaways: - Ensembles almost always outperform single models - Diversity is crucial for ensemble success - Different methods suit different problems - Boosting for accuracy, bagging for stability - Stacking combines strengths of different approaches - Computational cost vs performance trade-off"
  },
  {
    "objectID": "16-ensemble-methods.html#whats-next",
    "href": "16-ensemble-methods.html#whats-next",
    "title": "Chapter 16: Ensemble Methods - The Power of Many",
    "section": "What’s Next?",
    "text": "What’s Next?\nIn Chapter 17, we’ll explore unsupervised learning techniques for discovering patterns without labels."
  },
  {
    "objectID": "16-ensemble-methods.html#additional-resources",
    "href": "16-ensemble-methods.html#additional-resources",
    "title": "Chapter 16: Ensemble Methods - The Power of Many",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nIntroduction to Statistical Learning - Chapter 8\nElements of Statistical Learning - Chapters 10, 15, 16\nXGBoost Documentation\nRandom Forests Original Paper\nEnsemble Methods: Foundations and Algorithms"
  },
  {
    "objectID": "13-hyperparameter-tuning.html",
    "href": "13-hyperparameter-tuning.html",
    "title": "Chapter 13: Hyperparameter Tuning - Finding the Sweet Spot",
    "section": "",
    "text": "By the end of this chapter, you will master:\n\nUnderstanding hyperparameters vs parameters\nGrid search and its variations\nRandom search strategies\nBayesian optimization with Gaussian processes\nRacing methods for efficiency\nSimulated annealing\nNested resampling for unbiased evaluation\nParallel processing for faster tuning\nBest practices and common pitfalls"
  },
  {
    "objectID": "13-hyperparameter-tuning.html#learning-objectives",
    "href": "13-hyperparameter-tuning.html#learning-objectives",
    "title": "Chapter 13: Hyperparameter Tuning - Finding the Sweet Spot",
    "section": "",
    "text": "By the end of this chapter, you will master:\n\nUnderstanding hyperparameters vs parameters\nGrid search and its variations\nRandom search strategies\nBayesian optimization with Gaussian processes\nRacing methods for efficiency\nSimulated annealing\nNested resampling for unbiased evaluation\nParallel processing for faster tuning\nBest practices and common pitfalls"
  },
  {
    "objectID": "13-hyperparameter-tuning.html#the-art-and-science-of-hyperparameter-tuning",
    "href": "13-hyperparameter-tuning.html#the-art-and-science-of-hyperparameter-tuning",
    "title": "Chapter 13: Hyperparameter Tuning - Finding the Sweet Spot",
    "section": "The Art and Science of Hyperparameter Tuning",
    "text": "The Art and Science of Hyperparameter Tuning\nImagine you’re a chef perfecting a recipe. The ingredients are your features, the cooking method is your algorithm, but what about the temperature, timing, and seasoning amounts? These are like hyperparameters - they control the learning process but aren’t learned from the data directly.\nGetting hyperparameters right can mean the difference between a model that barely works and one that achieves state-of-the-art performance. Too conservative, and you underfit. Too aggressive, and you overfit. The sweet spot lies somewhere in between.\n\nlibrary(tidymodels)\n\n-- Attaching packages -------------------------------------- tidymodels 1.4.1 --\n\n\nv broom        1.0.10     v recipes      1.3.1 \nv dials        1.4.2      v rsample      1.3.1 \nv dplyr        1.1.4      v tailor       0.1.0 \nv ggplot2      4.0.0      v tidyr        1.3.1 \nv infer        1.0.9      v tune         2.0.0 \nv modeldata    1.5.1      v workflows    1.3.0 \nv parsnip      1.3.3      v workflowsets 1.1.1 \nv purrr        1.1.0      v yardstick    1.3.2 \n\n\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx purrr::discard() masks scales::discard()\nx dplyr::filter()  masks stats::filter()\nx dplyr::lag()     masks stats::lag()\nx recipes::step()  masks stats::step()\n\nlibrary(tidyverse)\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv forcats   1.0.0     v stringr   1.5.2\nv lubridate 1.9.4     v tibble    3.3.0\nv readr     2.1.5     \n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx readr::col_factor() masks scales::col_factor()\nx purrr::discard()    masks scales::discard()\nx dplyr::filter()     masks stats::filter()\nx stringr::fixed()    masks recipes::fixed()\nx dplyr::lag()        masks stats::lag()\nx readr::spec()       masks yardstick::spec()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(modeldata)\nlibrary(vip)\n\n\nAdjuntando el paquete: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(patchwork)\nlibrary(doParallel)\n\nCargando paquete requerido: foreach\n\nAdjuntando el paquete: 'foreach'\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n\nCargando paquete requerido: iterators\nCargando paquete requerido: parallel\n\nlibrary(finetune)\nlibrary(dials)\n\n# Set theme and seed\ntheme_set(theme_minimal())\nset.seed(123)\n\n# Load and prepare data\ndata(ames)\names_split &lt;- initial_split(ames, prop = 0.75, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test &lt;- testing(ames_split)\n\n# Create resamples for tuning\names_folds &lt;- vfold_cv(ames_train, v = 5, strata = Sale_Price)"
  },
  {
    "objectID": "13-hyperparameter-tuning.html#understanding-hyperparameters",
    "href": "13-hyperparameter-tuning.html#understanding-hyperparameters",
    "title": "Chapter 13: Hyperparameter Tuning - Finding the Sweet Spot",
    "section": "Understanding Hyperparameters",
    "text": "Understanding Hyperparameters\nFirst, let’s clarify the distinction:\n\nParameters: Learned from data (e.g., regression coefficients, neural network weights)\nHyperparameters: Set before training (e.g., learning rate, tree depth, penalty)\n\n\n# Example: Linear regression with regularization\n# Parameters: The coefficient values (beta_0, beta_1, ..., beta_p)\n# Hyperparameters: penalty (lambda) and mixture (alpha)\n\n# Without tuning - we guess the hyperparameters\nfixed_spec &lt;- linear_reg(\n  penalty = 0.01,  # Hyperparameter (guessed)\n  mixture = 0.5    # Hyperparameter (guessed)\n) %&gt;%\n  set_engine(\"glmnet\")\n\n# With tuning - we mark them for optimization\ntunable_spec &lt;- linear_reg(\n  penalty = tune(),  # To be optimized\n  mixture = tune()   # To be optimized\n) %&gt;%\n  set_engine(\"glmnet\")\n\n# The model will learn the coefficients (parameters) during fitting\n# But we need to find the best penalty and mixture (hyperparameters) through tuning\n\nDifferent models have different hyperparameters:\n\n# Decision tree hyperparameters\ntree_spec &lt;- decision_tree(\n  cost_complexity = tune(),  # Pruning parameter\n  tree_depth = tune(),       # Maximum depth\n  min_n = tune()            # Minimum observations in node\n) %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"regression\")\n\n# Random forest hyperparameters\nrf_spec &lt;- rand_forest(\n  mtry = tune(),    # Variables per split\n  trees = tune(),   # Number of trees\n  min_n = tune()    # Minimum node size\n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\n# XGBoost has many hyperparameters\nxgb_spec &lt;- boost_tree(\n  trees = tune(),\n  tree_depth = tune(),\n  min_n = tune(),\n  loss_reduction = tune(),\n  sample_size = tune(),\n  mtry = tune(),\n  learn_rate = tune()\n) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n# Each model type has its own set of tunable parameters"
  },
  {
    "objectID": "13-hyperparameter-tuning.html#grid-search-the-systematic-approach",
    "href": "13-hyperparameter-tuning.html#grid-search-the-systematic-approach",
    "title": "Chapter 13: Hyperparameter Tuning - Finding the Sweet Spot",
    "section": "Grid Search: The Systematic Approach",
    "text": "Grid Search: The Systematic Approach\nGrid search evaluates all combinations of hyperparameter values in a predefined grid:\n\n# Create a simple preprocessing recipe\nsimple_recipe &lt;- recipe(Sale_Price ~ Gr_Liv_Area + Overall_Cond + Year_Built + \n                       Neighborhood, data = ames_train) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\n# Create a tunable elastic net model\nelastic_spec &lt;- linear_reg(\n  penalty = tune(),\n  mixture = tune()\n) %&gt;%\n  set_engine(\"glmnet\")\n\n# Combine into workflow\nelastic_workflow &lt;- workflow() %&gt;%\n  add_recipe(simple_recipe) %&gt;%\n  add_model(elastic_spec)\n\n# Define the grid\nelastic_grid &lt;- grid_regular(\n  penalty(range = c(-3, 0), trans = log10_trans()),  # 10^-3 to 10^0\n  mixture(range = c(0, 1)),                          # 0 (ridge) to 1 (lasso)\n  levels = c(10, 5)  # 10 penalty values, 5 mixture values = 50 combinations\n)\n\nprint(elastic_grid)\n\n# A tibble: 50 x 2\n   penalty mixture\n     &lt;dbl&gt;   &lt;dbl&gt;\n 1 0.001         0\n 2 0.00215       0\n 3 0.00464       0\n 4 0.01          0\n 5 0.0215        0\n 6 0.0464        0\n 7 0.1           0\n 8 0.215         0\n 9 0.464         0\n10 1             0\n# i 40 more rows\n\n# Visualize the grid\nggplot(elastic_grid, aes(x = penalty, y = mixture)) +\n  geom_point(size = 3, color = \"steelblue\") +\n  scale_x_log10() +\n  labs(\n    title = \"Regular Grid for Elastic Net\",\n    subtitle = \"50 combinations to evaluate\",\n    x = \"Penalty (log scale)\",\n    y = \"Mixture (0=Ridge, 1=Lasso)\"\n  )\n\n\n\n\n\n\n\n\n\nPerforming Grid Search\n\n# Tune with grid search\ngrid_results &lt;- elastic_workflow %&gt;%\n  tune_grid(\n    resamples = ames_folds,\n    grid = elastic_grid,\n    metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq),\n    control = control_grid(save_pred = TRUE, verbose = FALSE)\n  )\n\n# Examine results\ngrid_results\n\n# Tuning results\n# 5-fold cross-validation using stratification \n# A tibble: 5 x 5\n  splits             id    .metrics           .notes           .predictions\n  &lt;list&gt;             &lt;chr&gt; &lt;list&gt;             &lt;list&gt;           &lt;list&gt;      \n1 &lt;split [1756/441]&gt; Fold1 &lt;tibble [100 x 6]&gt; &lt;tibble [1 x 4]&gt; &lt;tibble&gt;    \n2 &lt;split [1757/440]&gt; Fold2 &lt;tibble [100 x 6]&gt; &lt;tibble [1 x 4]&gt; &lt;tibble&gt;    \n3 &lt;split [1757/440]&gt; Fold3 &lt;tibble [100 x 6]&gt; &lt;tibble [1 x 4]&gt; &lt;tibble&gt;    \n4 &lt;split [1758/439]&gt; Fold4 &lt;tibble [100 x 6]&gt; &lt;tibble [1 x 4]&gt; &lt;tibble&gt;    \n5 &lt;split [1760/437]&gt; Fold5 &lt;tibble [100 x 6]&gt; &lt;tibble [1 x 4]&gt; &lt;tibble&gt;    \n\nThere were issues with some computations:\n\n  - Warning(s) x4: !  The following columns have zero variance so scaling cannot be ...\n  - Warning(s) x1: !  The following columns have zero variance so scaling cannot be ...\n\nRun `show_notes(.Last.tune.result)` for more information.\n\n# Best parameters\nbest_grid &lt;- grid_results %&gt;%\n  select_best(metric = \"rmse\")\n\nprint(best_grid)\n\n# A tibble: 1 x 3\n  penalty mixture .config         \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;           \n1   0.001    0.25 pre0_mod02_post0\n\n# Visualize tuning results\nautoplot(grid_results) +\n  labs(title = \"Grid Search Results\")\n\n\n\n\n\n\n\n# More detailed visualization\ngrid_results %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  ggplot(aes(x = penalty, y = mean, color = factor(mixture))) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), \n                width = 0.01, alpha = 0.5) +\n  scale_x_log10() +\n  scale_color_viridis_d(name = \"Mixture\") +\n  labs(\n    title = \"RMSE Across Penalty Values\",\n    subtitle = \"Different lines represent different mixture values\",\n    x = \"Penalty (log scale)\",\n    y = \"RMSE\"\n  )\n\n\n\n\n\n\n\n\nGrid search characteristics: - Pros: Systematic, reproducible, finds global optimum within grid - Cons: Computationally expensive, curse of dimensionality, may miss optimum between grid points"
  },
  {
    "objectID": "13-hyperparameter-tuning.html#random-search-the-efficient-explorer",
    "href": "13-hyperparameter-tuning.html#random-search-the-efficient-explorer",
    "title": "Chapter 13: Hyperparameter Tuning - Finding the Sweet Spot",
    "section": "Random Search: The Efficient Explorer",
    "text": "Random Search: The Efficient Explorer\nRandom search samples hyperparameter combinations randomly, often finding good solutions faster than grid search:\n\n# Random grid - same parameter space, random sampling\nrandom_grid &lt;- grid_random(\n  penalty(range = c(-3, 0), trans = log10_trans()),\n  mixture(range = c(0, 1)),\n  size = 30  # Only 30 random combinations instead of 50 in regular grid\n)\n\n# Visualize random vs regular grid\ngrid_comparison &lt;- bind_rows(\n  elastic_grid %&gt;% mutate(type = \"Regular Grid (50 points)\"),\n  random_grid %&gt;% mutate(type = \"Random Grid (30 points)\")\n)\n\nggplot(grid_comparison, aes(x = penalty, y = mixture, color = type)) +\n  geom_point(size = 2, alpha = 0.7) +\n  scale_x_log10() +\n  facet_wrap(~type) +\n  labs(\n    title = \"Grid Search vs Random Search\",\n    subtitle = \"Random search covers the space more efficiently\",\n    x = \"Penalty (log scale)\",\n    y = \"Mixture\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n# Perform random search\nrandom_results &lt;- elastic_workflow %&gt;%\n  tune_grid(\n    resamples = ames_folds,\n    grid = random_grid,\n    metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq),\n    control = control_grid(save_pred = FALSE, verbose = FALSE)\n  )\n\n# Compare efficiency\ncomparison &lt;- bind_rows(\n  grid_results %&gt;% \n    show_best(metric = \"rmse\", n = 1) %&gt;% \n    mutate(method = \"Grid (50 points)\"),\n  random_results %&gt;% \n    show_best(metric = \"rmse\", n = 1) %&gt;% \n    mutate(method = \"Random (30 points)\")\n) %&gt;%\n  select(method, penalty, mixture, mean, std_err)\n\nknitr::kable(comparison, digits = 4)\n\n\n\n\nmethod\npenalty\nmixture\nmean\nstd_err\n\n\n\n\nGrid (50 points)\n0.001\n0.2500\n38655.44\n1413.248\n\n\nRandom (30 points)\n0.009\n0.0377\n38646.32\n1412.018\n\n\n\n\n\nThe mathematics behind why random search works: - For important hyperparameters, random search explores more unique values - Less affected by unimportant hyperparameters - Better coverage of continuous spaces"
  },
  {
    "objectID": "13-hyperparameter-tuning.html#latin-hypercube-sampling",
    "href": "13-hyperparameter-tuning.html#latin-hypercube-sampling",
    "title": "Chapter 13: Hyperparameter Tuning - Finding the Sweet Spot",
    "section": "Latin Hypercube Sampling",
    "text": "Latin Hypercube Sampling\nLatin Hypercube provides space-filling designs that are more uniform than random:\n\n# Latin hypercube grid\nlhs_grid &lt;- grid_latin_hypercube(\n  penalty(range = c(-3, 0), trans = log10_trans()),\n  mixture(range = c(0, 1)),\n  size = 20  # Even fewer points\n)\n\n# Visualize all three approaches\nall_grids &lt;- bind_rows(\n  elastic_grid %&gt;% slice_sample(n = 20) %&gt;% mutate(type = \"Regular (20 points)\"),\n  random_grid %&gt;% slice_sample(n = 20) %&gt;% mutate(type = \"Random (20 points)\"),\n  lhs_grid %&gt;% mutate(type = \"Latin Hypercube (20 points)\")\n)\n\nggplot(all_grids, aes(x = penalty, y = mixture)) +\n  geom_point(size = 3, color = \"darkblue\", alpha = 0.7) +\n  scale_x_log10() +\n  facet_wrap(~type) +\n  labs(\n    title = \"Sampling Strategies Comparison\",\n    subtitle = \"Latin Hypercube provides better space coverage\",\n    x = \"Penalty (log scale)\",\n    y = \"Mixture\"\n  )"
  },
  {
    "objectID": "13-hyperparameter-tuning.html#bayesian-optimization-the-smart-search",
    "href": "13-hyperparameter-tuning.html#bayesian-optimization-the-smart-search",
    "title": "Chapter 13: Hyperparameter Tuning - Finding the Sweet Spot",
    "section": "Bayesian Optimization: The Smart Search",
    "text": "Bayesian Optimization: The Smart Search\nBayesian optimization uses past results to intelligently choose the next points to evaluate:\n\n# More complex model for Bayesian optimization demonstration\nrf_recipe &lt;- recipe(Sale_Price ~ ., data = ames_train) %&gt;%\n  step_impute_median(all_numeric_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors())\n\nrf_spec &lt;- rand_forest(\n  mtry = tune(),\n  min_n = tune(),\n  trees = 500  # Fix trees to reduce tuning time\n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\nrf_workflow &lt;- workflow() %&gt;%\n  add_recipe(rf_recipe) %&gt;%\n  add_model(rf_spec)\n\n# Set up parameter bounds\nrf_params &lt;- rf_workflow %&gt;%\n  extract_parameter_set_dials() %&gt;%\n  update(\n    mtry = mtry(c(5, 20)),      # Number of variables per split\n    min_n = min_n(c(5, 30))      # Minimum node size\n  )\n\n# Initial random search to seed Bayesian optimization\nset.seed(456)\ninitial_results &lt;- rf_workflow %&gt;%\n  tune_grid(\n    resamples = ames_folds,\n    grid = 5,  # Start with 5 random points\n    metrics = yardstick::metric_set(yardstick::rmse),\n    control = control_grid(verbose = FALSE)\n  )\n\n# Bayesian optimization\nbayes_results &lt;- rf_workflow %&gt;%\n  tune_bayes(\n    resamples = ames_folds,\n    initial = initial_results,  # Use initial results\n    iter = 10,                   # 10 more iterations\n    metrics = yardstick::metric_set(yardstick::rmse),\n    param_info = rf_params,\n    control = control_bayes(\n      verbose = FALSE,\n      no_improve = 5,  # Stop after 5 iterations without improvement\n      uncertain = 5    # Exploration vs exploitation trade-off\n    )\n  )\n\n# Visualize the optimization path\nautoplot(bayes_results, type = \"performance\") +\n  labs(\n    title = \"Bayesian Optimization Progress\",\n    subtitle = \"RMSE improves over iterations\"\n  )\n\n\n\n\n\n\n\n# Show acquisition function behavior\nautoplot(bayes_results, type = \"parameters\") +\n  labs(\n    title = \"Parameter Space Exploration\",\n    subtitle = \"Bayesian optimization focuses on promising regions\"\n  )\n\n\n\n\n\n\n\n\nHow Bayesian optimization works: 1. Surrogate model: Gaussian process models the objective function 2. Acquisition function: Balances exploration vs exploitation 3. Sequential design: Each point is chosen based on all previous results 4. Efficiency: Often finds good solutions with fewer evaluations"
  },
  {
    "objectID": "13-hyperparameter-tuning.html#racing-methods-survival-of-the-fittest",
    "href": "13-hyperparameter-tuning.html#racing-methods-survival-of-the-fittest",
    "title": "Chapter 13: Hyperparameter Tuning - Finding the Sweet Spot",
    "section": "Racing Methods: Survival of the Fittest",
    "text": "Racing Methods: Survival of the Fittest\nRacing methods eliminate poor performers early, focusing resources on promising candidates:\n\n# Create a larger initial grid\nrace_grid &lt;- grid_latin_hypercube(\n  penalty(range = c(-4, 0), trans = log10_trans()),\n  mixture(range = c(0, 1)),\n  size = 50  # Start with many candidates\n)\n\n# Racing with ANOVA (finetune package)\nrace_results &lt;- elastic_workflow %&gt;%\n  tune_race_anova(\n    resamples = ames_folds,\n    grid = race_grid,\n    metrics = yardstick::metric_set(yardstick::rmse),\n    control = control_race(\n      verbose_elim = TRUE,  # Show elimination progress\n      burn_in = 3,          # Evaluate all on first 3 folds\n      num_ties = 5,         # Number of ties to break\n      alpha = 0.05          # Significance level for elimination\n    )\n  )\n\n# Visualize racing progress\nplot_race(race_results) +\n  labs(\n    title = \"Racing Method: Progressive Elimination\",\n    subtitle = \"Poor performers are eliminated early\"\n  )\n\n\n\n\n\n\n\n# Compare efficiency\nracing_summary &lt;- tibble(\n  Method = c(\"Full Grid (50×5 folds)\", \"Racing\"),\n  `Total Evaluations` = c(50 * 5, \n                         sum(!is.na(collect_metrics(race_results, \n                                                   summarize = FALSE)$.estimate))),\n  `Best RMSE` = c(\n    show_best(grid_results, metric = \"rmse\", n = 1)$mean,\n    show_best(race_results, metric = \"rmse\", n = 1)$mean\n  )\n)\n\nknitr::kable(racing_summary, digits = 4)\n\n\n\n\nMethod\nTotal Evaluations\nBest RMSE\n\n\n\n\nFull Grid (50&lt;U+00D7&gt;5 folds)\n250\n38655.44\n\n\nRacing\n15\n38648.39\n\n\n\n\n\nRacing advantages: - Dramatically reduces computation time - Focuses on promising candidates - Statistical rigor in elimination decisions"
  },
  {
    "objectID": "13-hyperparameter-tuning.html#simulated-annealing-the-flexible-explorer",
    "href": "13-hyperparameter-tuning.html#simulated-annealing-the-flexible-explorer",
    "title": "Chapter 13: Hyperparameter Tuning - Finding the Sweet Spot",
    "section": "Simulated Annealing: The Flexible Explorer",
    "text": "Simulated Annealing: The Flexible Explorer\nSimulated annealing allows “bad” moves early on to escape local optima:\n\n# Simulated annealing for hyperparameter optimization\nsa_results &lt;- elastic_workflow %&gt;%\n  tune_sim_anneal(\n    resamples = ames_folds,\n    metrics = yardstick::metric_set(yardstick::rmse),\n    initial = 4,      # Start with 4 random points\n    iter = 25,        # 25 iterations\n    control = control_sim_anneal(\n      verbose = FALSE,\n      cooling_coef = 0.02,  # How fast temperature decreases\n      radius = c(0.05, 0.15)  # Search radius\n    )\n  )\n\n# Visualize annealing path\nautoplot(sa_results, type = \"performance\") +\n  labs(\n    title = \"Simulated Annealing Progress\",\n    subtitle = \"Allows temporary performance degradation to escape local optima\"\n  )\n\n\n\n\n\n\n\n# Show parameter exploration\nsa_results %&gt;%\n  collect_metrics(summarize = FALSE) %&gt;%\n  ggplot(aes(x = penalty, y = mixture, color = .estimate)) +\n  geom_point(size = 3) +\n  geom_path(alpha = 0.3) +\n  scale_x_log10() +\n  scale_color_viridis_c(name = \"RMSE\", direction = -1) +\n  labs(\n    title = \"Simulated Annealing Search Path\",\n    subtitle = \"Exploration of parameter space over iterations\",\n    x = \"Penalty (log scale)\",\n    y = \"Mixture\"\n  )"
  },
  {
    "objectID": "13-hyperparameter-tuning.html#nested-resampling-for-unbiased-evaluation",
    "href": "13-hyperparameter-tuning.html#nested-resampling-for-unbiased-evaluation",
    "title": "Chapter 13: Hyperparameter Tuning - Finding the Sweet Spot",
    "section": "Nested Resampling for Unbiased Evaluation",
    "text": "Nested Resampling for Unbiased Evaluation\nWhen tuning hyperparameters, we need nested resampling to get unbiased performance estimates:\n\n# Outer resampling for performance estimation\nouter_folds &lt;- vfold_cv(ames_train, v = 3)  # Reduced for computation time\n\n# Function to tune and evaluate on one outer fold\ntune_and_evaluate &lt;- function(split) {\n  # Training data for this outer fold\n  train_data &lt;- analysis(split)\n  # Test data for this outer fold\n  test_data &lt;- assessment(split)\n  \n  # Inner resampling for tuning\n  inner_folds &lt;- vfold_cv(train_data, v = 5)\n  \n  # Create workflow with data from this fold\n  fold_recipe &lt;- recipe(Sale_Price ~ Gr_Liv_Area + Overall_Cond + Year_Built, \n                       data = train_data) %&gt;%\n    step_normalize(all_numeric_predictors())\n  \n  fold_workflow &lt;- workflow() %&gt;%\n    add_recipe(fold_recipe) %&gt;%\n    add_model(elastic_spec)\n  \n  # Tune on inner folds\n  tune_results &lt;- fold_workflow %&gt;%\n    tune_grid(\n      resamples = inner_folds,\n      grid = 10,\n      metrics = yardstick::metric_set(yardstick::rmse)\n    )\n  \n  # Select best parameters\n  best_params &lt;- select_best(tune_results, metric = \"rmse\")\n  \n  # Finalize workflow\n  final_workflow &lt;- fold_workflow %&gt;%\n    finalize_workflow(best_params)\n  \n  # Fit on full training data for this fold\n  final_fit &lt;- final_workflow %&gt;%\n    fit(train_data)\n  \n  # Evaluate on test data for this fold\n  test_pred &lt;- predict(final_fit, test_data) %&gt;%\n    bind_cols(test_data %&gt;% select(Sale_Price))\n  \n  # Return metrics\n  test_pred %&gt;%\n    metrics(Sale_Price, .pred) %&gt;%\n    mutate(\n      penalty = best_params$penalty,\n      mixture = best_params$mixture\n    )\n}\n\n# Apply to all outer folds (this takes time!)\n# nested_results &lt;- map_df(outer_folds$splits, tune_and_evaluate, .id = \"fold\")\n\n# For demonstration, show the concept\ncat(\"Nested Resampling Structure:\nOuter Fold 1:\n  - Inner CV: Tune hyperparameters\n  - Select best hyperparameters\n  - Train final model\n  - Evaluate on outer test set\nOuter Fold 2:\n  - (Repeat process independently)\nOuter Fold 3:\n  - (Repeat process independently)\n  \nFinal estimate: Average of outer fold performances\nThis gives unbiased estimate of generalization performance\")\n\nNested Resampling Structure:\nOuter Fold 1:\n  - Inner CV: Tune hyperparameters\n  - Select best hyperparameters\n  - Train final model\n  - Evaluate on outer test set\nOuter Fold 2:\n  - (Repeat process independently)\nOuter Fold 3:\n  - (Repeat process independently)\n  \nFinal estimate: Average of outer fold performances\nThis gives unbiased estimate of generalization performance"
  },
  {
    "objectID": "13-hyperparameter-tuning.html#parallel-processing-for-speed",
    "href": "13-hyperparameter-tuning.html#parallel-processing-for-speed",
    "title": "Chapter 13: Hyperparameter Tuning - Finding the Sweet Spot",
    "section": "Parallel Processing for Speed",
    "text": "Parallel Processing for Speed\nTuning can be computationally intensive. Parallel processing helps:\n\n# Setup parallel backend\ncores &lt;- parallel::detectCores() - 1  # Leave one core free\ncl &lt;- makePSOCKcluster(cores)\nregisterDoParallel(cl)\n\n# Check parallel backend\ncat(\"Parallel backend registered with\", cores, \"cores\\n\")\n\nParallel backend registered with 7 cores\n\n# Time comparison (simplified example)\ntic &lt;- Sys.time()\nsmall_grid &lt;- grid_regular(\n  penalty(range = c(-2, 0)),\n  mixture(),\n  levels = c(5, 3)\n)\n\n# Parallel tuning\nparallel_results &lt;- elastic_workflow %&gt;%\n  tune_grid(\n    resamples = ames_folds,\n    grid = small_grid,\n    metrics = yardstick::metric_set(yardstick::rmse),\n    control = control_grid(parallel_over = \"resamples\")  # Parallelize over folds\n  )\n\nparallel_time &lt;- Sys.time() - tic\n\n# Clean up\nstopCluster(cl)\nregisterDoSEQ()  # Return to sequential\n\ncat(\"Tuning completed in\", round(parallel_time, 2), units(parallel_time), \"\\n\")\n\nTuning completed in 2.96 secs \n\ncat(\"With sequential processing, this would take approximately\", \n    round(parallel_time * cores, 2), units(parallel_time), \"\\n\")\n\nWith sequential processing, this would take approximately 20.7 secs \n\n\nParallelization strategies: - Over resamples: Each fold processed on different core - Over models: Each hyperparameter combination on different core - Hybrid: Both, depending on problem size"
  },
  {
    "objectID": "13-hyperparameter-tuning.html#advanced-tuning-strategies",
    "href": "13-hyperparameter-tuning.html#advanced-tuning-strategies",
    "title": "Chapter 13: Hyperparameter Tuning - Finding the Sweet Spot",
    "section": "Advanced Tuning Strategies",
    "text": "Advanced Tuning Strategies\n\nMulti-Metric Optimization\nSometimes we need to optimize multiple metrics:\n\n# Tune for multiple metrics\nmulti_metric_results &lt;- elastic_workflow %&gt;%\n  tune_grid(\n    resamples = ames_folds,\n    grid = 10,\n    metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq, yardstick::mae, yardstick::mape)\n  )\n\n# Different metrics might suggest different \"best\" parameters\nbest_rmse &lt;- select_best(multi_metric_results, metric = \"rmse\")\nbest_rsq &lt;- select_best(multi_metric_results, metric = \"rsq\")\nbest_mae &lt;- select_best(multi_metric_results, metric = \"mae\")\n\ncomparison &lt;- bind_rows(\n  best_rmse %&gt;% mutate(optimized_for = \"RMSE\"),\n  best_rsq %&gt;% mutate(optimized_for = \"R-squared\"),\n  best_mae %&gt;% mutate(optimized_for = \"MAE\")\n)\n\nknitr::kable(comparison, digits = 4)\n\n\n\n\npenalty\nmixture\n.config\noptimized_for\n\n\n\n\n0\n0.05\npre0_mod03_post0\nRMSE\n\n\n0\n0.05\npre0_mod03_post0\nR-squared\n\n\n0\n0.05\npre0_mod03_post0\nMAE\n\n\n\n\n# Pareto frontier for multi-objective optimization\nmulti_metric_results %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric %in% c(\"rmse\", \"rsq\")) %&gt;%\n  select(penalty, mixture, .metric, mean) %&gt;%\n  pivot_wider(names_from = .metric, values_from = mean) %&gt;%\n  ggplot(aes(x = rmse, y = rsq)) +\n  geom_point(size = 3, color = \"steelblue\") +\n  geom_point(data = . %&gt;% filter(rmse == min(rmse)), \n             color = \"red\", size = 5, shape = 17) +\n  geom_point(data = . %&gt;% filter(rsq == max(rsq)), \n             color = \"green\", size = 5, shape = 17) +\n  labs(\n    title = \"Multi-Metric Trade-offs\",\n    subtitle = \"Red: Best RMSE, Green: Best R²\",\n    x = \"RMSE (lower is better)\",\n    y = \"R² (higher is better)\"\n  )\n\n\n\n\n\n\n\n\n\n\nAdaptive Tuning Ranges\nAdjust parameter ranges based on initial results:\n\n# Initial coarse search\ncoarse_grid &lt;- grid_regular(\n  penalty(range = c(-4, 1)),\n  mixture(),\n  levels = c(6, 3)\n)\n\ncoarse_results &lt;- elastic_workflow %&gt;%\n  tune_grid(\n    resamples = ames_folds,\n    grid = coarse_grid,\n    metrics = yardstick::metric_set(yardstick::rmse)\n  )\n\n# Find promising region\nbest_coarse &lt;- select_best(coarse_results, metric = \"rmse\")\n\n# Refined search around best region\nfine_grid &lt;- grid_regular(\n  penalty(range = c(log10(best_coarse$penalty) - 0.5, \n                    log10(best_coarse$penalty) + 0.5)),\n  mixture(range = c(max(0, best_coarse$mixture - 0.2), \n                    min(1, best_coarse$mixture + 0.2))),\n  levels = c(10, 5)\n)\n\nfine_results &lt;- elastic_workflow %&gt;%\n  tune_grid(\n    resamples = ames_folds,\n    grid = fine_grid,\n    metrics = yardstick::metric_set(yardstick::rmse)\n  )\n\n# Visualize coarse vs fine search\nsearch_comparison &lt;- bind_rows(\n  coarse_results %&gt;% \n    collect_metrics() %&gt;% \n    mutate(search = \"Coarse\"),\n  fine_results %&gt;% \n    collect_metrics() %&gt;% \n    mutate(search = \"Fine\")\n)\n\nggplot(search_comparison, aes(x = penalty, y = mean, color = search)) +\n  geom_point(size = 2, alpha = 0.7) +\n  scale_x_log10() +\n  facet_wrap(~mixture, labeller = label_both) +\n  labs(\n    title = \"Adaptive Search Strategy\",\n    subtitle = \"Fine search focuses on promising region\",\n    x = \"Penalty (log scale)\",\n    y = \"RMSE\"\n  )"
  },
  {
    "objectID": "13-hyperparameter-tuning.html#finalizing-and-evaluating",
    "href": "13-hyperparameter-tuning.html#finalizing-and-evaluating",
    "title": "Chapter 13: Hyperparameter Tuning - Finding the Sweet Spot",
    "section": "Finalizing and Evaluating",
    "text": "Finalizing and Evaluating\nAfter tuning, finalize your workflow:\n\n# Select best overall parameters\nbest_params &lt;- select_best(fine_results, metric = \"rmse\")\n\n# Finalize workflow\nfinal_workflow &lt;- elastic_workflow %&gt;%\n  finalize_workflow(best_params)\n\nprint(final_workflow)\n\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: linear_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n2 Recipe Steps\n\n* step_dummy()\n* step_normalize()\n\n-- Model -----------------------------------------------------------------------\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 31.6227766016838\n  mixture = 1\n\nComputational engine: glmnet \n\n# Fit on all training data\nfinal_fit &lt;- final_workflow %&gt;%\n  fit(ames_train)\n\n# Evaluate on test set\ntest_pred &lt;- predict(final_fit, ames_test) %&gt;%\n  bind_cols(ames_test %&gt;% select(Sale_Price))\n\ntest_metrics &lt;- test_pred %&gt;%\n  metrics(Sale_Price, .pred)\n\nknitr::kable(test_metrics, digits = 4)\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n37674.7825\n\n\nrsq\nstandard\n0.7856\n\n\nmae\nstandard\n24116.8959\n\n\n\n\n# Visualize final model performance\nggplot(test_pred, aes(x = Sale_Price, y = .pred)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(\n    title = \"Final Model Performance\",\n    subtitle = paste(\"Test RMSE:\", round(test_metrics$.estimate[1], 2)),\n    x = \"Actual Sale Price\",\n    y = \"Predicted Sale Price\"\n  ) +\n  coord_equal()"
  },
  {
    "objectID": "13-hyperparameter-tuning.html#best-practices",
    "href": "13-hyperparameter-tuning.html#best-practices",
    "title": "Chapter 13: Hyperparameter Tuning - Finding the Sweet Spot",
    "section": "Best Practices",
    "text": "Best Practices\n\n1. Start Simple, Add Complexity\n\n# Start with few hyperparameters\nsimple_tuning &lt;- linear_reg(penalty = tune()) %&gt;%\n  set_engine(\"glmnet\")\n\n# Then add more if needed\ncomplex_tuning &lt;- linear_reg(\n  penalty = tune(),\n  mixture = tune()\n) %&gt;%\n  set_engine(\"glmnet\")\n\n\n\n2. Use Appropriate Search Strategies\n\ntuning_guide &lt;- tibble(\n  Scenario = c(\n    \"Few hyperparameters (≤3)\",\n    \"Many hyperparameters (&gt;5)\",\n    \"Expensive models\",\n    \"Quick exploration\",\n    \"Final optimization\",\n    \"Limited budget\"\n  ),\n  `Recommended Method` = c(\n    \"Grid search\",\n    \"Random search or Bayesian\",\n    \"Bayesian optimization\",\n    \"Random search\",\n    \"Bayesian or fine grid\",\n    \"Racing methods\"\n  ),\n  Reasoning = c(\n    \"Can afford exhaustive search\",\n    \"Grid search becomes prohibitive\",\n    \"Minimize number of evaluations\",\n    \"Good coverage quickly\",\n    \"Find true optimum\",\n    \"Eliminate bad candidates early\"\n  )\n)\n\nknitr::kable(tuning_guide)\n\n\n\n\n\n\n\n\n\nScenario\nRecommended Method\nReasoning\n\n\n\n\nFew hyperparameters (&lt;U+2264&gt;3)\nGrid search\nCan afford exhaustive search\n\n\nMany hyperparameters (&gt;5)\nRandom search or Bayesian\nGrid search becomes prohibitive\n\n\nExpensive models\nBayesian optimization\nMinimize number of evaluations\n\n\nQuick exploration\nRandom search\nGood coverage quickly\n\n\nFinal optimization\nBayesian or fine grid\nFind true optimum\n\n\nLimited budget\nRacing methods\nEliminate bad candidates early\n\n\n\n\n\n\n\n3. Monitor for Overfitting\n\n# Check for overfitting during tuning\ntuning_diagnostics &lt;- fine_results %&gt;%\n  collect_metrics(summarize = FALSE) %&gt;%\n  group_by(penalty, mixture) %&gt;%\n  summarise(\n    mean_rmse = mean(.estimate),\n    sd_rmse = sd(.estimate),\n    cv_variation = sd_rmse / mean_rmse,\n    .groups = \"drop\"\n  )\n\n# High CV variation might indicate overfitting\nggplot(tuning_diagnostics, aes(x = penalty, y = cv_variation, color = factor(mixture))) +\n  geom_point(size = 2) +\n  geom_line() +\n  scale_x_log10() +\n  labs(\n    title = \"Cross-Validation Stability\",\n    subtitle = \"High variation suggests potential overfitting\",\n    x = \"Penalty (log scale)\",\n    y = \"CV Coefficient of Variation\",\n    color = \"Mixture\"\n  )"
  },
  {
    "objectID": "13-hyperparameter-tuning.html#exercises",
    "href": "13-hyperparameter-tuning.html#exercises",
    "title": "Chapter 13: Hyperparameter Tuning - Finding the Sweet Spot",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Compare Search Strategies\nCompare different search strategies on the same problem:\n\n# Your solution\n# Define a tunable XGBoost model\nxgb_spec &lt;- boost_tree(\n  trees = 300,\n  tree_depth = tune(),\n  learn_rate = tune(),\n  min_n = tune()\n) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\nxgb_workflow &lt;- workflow() %&gt;%\n  add_recipe(simple_recipe) %&gt;%\n  add_model(xgb_spec)\n\n# Compare strategies (using smaller grids for speed)\n# 1. Regular grid\nregular_time &lt;- system.time({\n  regular_xgb &lt;- xgb_workflow %&gt;%\n    tune_grid(\n      resamples = ames_folds,\n      grid = grid_regular(\n        tree_depth(c(3, 10)),\n        learn_rate(c(-3, -1)),\n        min_n(c(5, 20)),\n        levels = c(3, 3, 3)  # 27 combinations\n      ),\n      metrics = yardstick::metric_set(yardstick::rmse)\n    )\n})\n\n# 2. Random search\nrandom_time &lt;- system.time({\n  random_xgb &lt;- xgb_workflow %&gt;%\n    tune_grid(\n      resamples = ames_folds,\n      grid = 15,  # 15 random combinations\n      metrics = yardstick::metric_set(yardstick::rmse)\n    )\n})\n\n# 3. Latin hypercube\nlhs_time &lt;- system.time({\n  lhs_xgb &lt;- xgb_workflow %&gt;%\n    tune_grid(\n      resamples = ames_folds,\n      grid = grid_latin_hypercube(\n        tree_depth(c(3, 10)),\n        learn_rate(c(-3, -1)),\n        min_n(c(5, 20)),\n        size = 15\n      ),\n      metrics = yardstick::metric_set(yardstick::rmse)\n    )\n})\n\n# Compare results\nstrategy_comparison &lt;- bind_rows(\n  show_best(regular_xgb, metric = \"rmse\", n = 1) %&gt;% \n    mutate(strategy = \"Regular Grid\", time = regular_time[3], points = 27),\n  show_best(random_xgb, metric = \"rmse\", n = 1) %&gt;% \n    mutate(strategy = \"Random\", time = random_time[3], points = 15),\n  show_best(lhs_xgb, metric = \"rmse\", n = 1) %&gt;% \n    mutate(strategy = \"Latin Hypercube\", time = lhs_time[3], points = 15)\n) %&gt;%\n  select(strategy, points, time, mean, tree_depth, learn_rate, min_n)\n\nknitr::kable(strategy_comparison, digits = 4)\n\n\n\n\n\n\n\n\n\n\n\n\n\nstrategy\npoints\ntime\nmean\ntree_depth\nlearn_rate\nmin_n\n\n\n\n\nRegular Grid\n27\n90.705\n33501.18\n3\n0.1000\n12\n\n\nRandom\n15\n63.571\n34230.75\n4\n0.2096\n12\n\n\nLatin Hypercube\n15\n57.519\n33799.37\n7\n0.0628\n12\n\n\n\n\n\n\n\nExercise 2: Implement Custom Tuning\nCreate a custom tuning strategy:\n\n# Your solution\n# Implement iterative refinement strategy\niterative_tuning &lt;- function(workflow, resamples, n_iterations = 3) {\n  results &lt;- list()\n  \n  # Start with coarse grid\n  current_range_tree &lt;- c(3, 15)\n  current_range_rate &lt;- c(-3, -0.5)\n  \n  for (i in 1:n_iterations) {\n    # Create grid for this iteration\n    current_grid &lt;- grid_regular(\n      tree_depth(current_range_tree),\n      learn_rate(current_range_rate, trans = log10_trans()),\n      levels = c(5, 5)\n    )\n    \n    # Tune\n    iter_results &lt;- workflow %&gt;%\n      tune_grid(\n        resamples = resamples,\n        grid = current_grid,\n        metrics = yardstick::metric_set(yardstick::rmse)\n      )\n    \n    results[[i]] &lt;- iter_results\n    \n    # Get best parameters\n    best &lt;- select_best(iter_results, metric = \"rmse\")\n    \n    # Refine ranges for next iteration (zoom in by 50%)\n    tree_width &lt;- diff(current_range_tree) * 0.25\n    rate_width &lt;- diff(current_range_rate) * 0.25\n    \n    current_range_tree &lt;- c(\n      max(3, best$tree_depth - tree_width),\n      min(15, best$tree_depth + tree_width)\n    )\n    \n    current_range_rate &lt;- c(\n      max(-3, log10(best$learn_rate) - rate_width),\n      min(-0.5, log10(best$learn_rate) + rate_width)\n    )\n    \n    cat(\"Iteration\", i, \"- Best RMSE:\", best$mean, \"\\n\")\n  }\n  \n  return(results)\n}\n\n# Apply custom strategy (simplified for demonstration)\nsimple_xgb_workflow &lt;- workflow() %&gt;%\n  add_recipe(recipe(Sale_Price ~ Gr_Liv_Area + Overall_Cond, data = ames_train)) %&gt;%\n  add_model(\n    boost_tree(trees = 100, tree_depth = tune(), learn_rate = tune()) %&gt;%\n      set_engine(\"xgboost\") %&gt;%\n      set_mode(\"regression\")\n  )\n\n# custom_results &lt;- iterative_tuning(simple_xgb_workflow, ames_folds, n_iterations = 2)\n\n\n\nExercise 3: Multi-Objective Optimization\nBalance multiple objectives:\n\n# Your solution\n# Tune for both performance and model complexity\ncomplexity_workflow &lt;- workflow() %&gt;%\n  add_recipe(simple_recipe) %&gt;%\n  add_model(\n    rand_forest(mtry = tune(), min_n = tune(), trees = tune()) %&gt;%\n      set_engine(\"ranger\") %&gt;%\n      set_mode(\"regression\")\n  )\n\n# Create grid\ncomplexity_grid &lt;- grid_latin_hypercube(\n  mtry(c(2, 10)),\n  min_n(c(5, 40)),\n  trees(c(100, 1000)),\n  size = 30\n)\n\n# Tune\ncomplexity_results &lt;- complexity_workflow %&gt;%\n  tune_grid(\n    resamples = ames_folds,\n    grid = complexity_grid,\n    metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq)\n  )\n\n# Add complexity metric (total trees × average tree size estimate)\ncomplexity_metrics &lt;- complexity_results %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  mutate(\n    complexity = trees * (1 / min_n) * mtry,  # Rough complexity measure\n    performance = -mean,  # Negative RMSE (higher is better)\n    # Pareto optimal if no other point has both better performance AND lower complexity\n    pareto = TRUE  # Simplified - would need proper calculation\n  )\n\n# Visualize trade-off\nggplot(complexity_metrics, aes(x = complexity, y = performance)) +\n  geom_point(aes(color = trees), size = 3) +\n  scale_color_viridis_c() +\n  labs(\n    title = \"Performance vs Complexity Trade-off\",\n    subtitle = \"Balancing model accuracy and computational cost\",\n    x = \"Model Complexity (arbitrary units)\",\n    y = \"Performance (-RMSE)\"\n  )\n\n\n\n\n\n\n\n# Select based on custom criterion\nbest_balanced &lt;- complexity_metrics %&gt;%\n  mutate(\n    score = performance - 0.0001 * complexity  # Custom weighting\n  ) %&gt;%\n  arrange(desc(score)) %&gt;%\n  slice(1)\n\ncat(\"Best balanced model:\\n\")\n\nBest balanced model:\n\ncat(\"Trees:\", best_balanced$trees, \"\\n\")\n\nTrees: 574 \n\ncat(\"mtry:\", best_balanced$mtry, \"\\n\")\n\nmtry: 10 \n\ncat(\"min_n:\", best_balanced$min_n, \"\\n\")\n\nmin_n: 15 \n\ncat(\"RMSE:\", -best_balanced$performance, \"\\n\")\n\nRMSE: 35013.85"
  },
  {
    "objectID": "13-hyperparameter-tuning.html#summary",
    "href": "13-hyperparameter-tuning.html#summary",
    "title": "Chapter 13: Hyperparameter Tuning - Finding the Sweet Spot",
    "section": "Summary",
    "text": "Summary\nIn this comprehensive chapter, you’ve mastered:\n✅ Hyperparameter fundamentals - Difference from parameters - Impact on model performance - Model-specific hyperparameters\n✅ Search strategies - Grid search for exhaustive exploration - Random search for efficiency - Latin hypercube for space-filling\n✅ Advanced methods - Bayesian optimization for intelligent search - Racing for early stopping - Simulated annealing for flexibility\n✅ Practical considerations - Nested resampling for unbiased evaluation - Parallel processing for speed - Multi-metric optimization\n✅ Best practices - Choosing appropriate strategies - Monitoring for overfitting - Iterative refinement\nKey takeaways: - No single best tuning method - choose based on context - Start coarse, refine gradually - Use parallel processing for speed - Always validate on held-out data - Consider computational budget - Balance exploration and exploitation"
  },
  {
    "objectID": "13-hyperparameter-tuning.html#whats-next",
    "href": "13-hyperparameter-tuning.html#whats-next",
    "title": "Chapter 13: Hyperparameter Tuning - Finding the Sweet Spot",
    "section": "What’s Next?",
    "text": "What’s Next?\nYou’ve completed Block 2 of the workshop, covering all the essential tidymodels concepts! Before diving into advanced machine learning topics, we recommend taking our Block 2 Assessment to test your understanding of the tidymodels framework and machine learning foundations.\nAfter mastering these concepts, continue to Chapter 14 to explore classification models in depth."
  },
  {
    "objectID": "13-hyperparameter-tuning.html#additional-resources",
    "href": "13-hyperparameter-tuning.html#additional-resources",
    "title": "Chapter 13: Hyperparameter Tuning - Finding the Sweet Spot",
    "section": "Additional Resources",
    "text": "Additional Resources\n\ntune Documentation\nfinetune Documentation\ndials Documentation\nHyperparameter Optimization Review\nPractical Bayesian Optimization"
  },
  {
    "objectID": "09-data-splitting.html",
    "href": "09-data-splitting.html",
    "title": "Chapter 9: Data Splitting and Resampling - The Foundation of Model Validation",
    "section": "",
    "text": "By the end of this chapter, you will master:\n\nThe critical importance of proper data splitting\nTrain/test/validation splits and when to use them\nStratified sampling for balanced splits\nCross-validation techniques and theory\nBootstrap methods for uncertainty estimation\nTime series splitting strategies\nNested resampling for unbiased evaluation\nBest practices and common pitfalls"
  },
  {
    "objectID": "09-data-splitting.html#learning-objectives",
    "href": "09-data-splitting.html#learning-objectives",
    "title": "Chapter 9: Data Splitting and Resampling - The Foundation of Model Validation",
    "section": "",
    "text": "By the end of this chapter, you will master:\n\nThe critical importance of proper data splitting\nTrain/test/validation splits and when to use them\nStratified sampling for balanced splits\nCross-validation techniques and theory\nBootstrap methods for uncertainty estimation\nTime series splitting strategies\nNested resampling for unbiased evaluation\nBest practices and common pitfalls"
  },
  {
    "objectID": "09-data-splitting.html#why-data-splitting-matters",
    "href": "09-data-splitting.html#why-data-splitting-matters",
    "title": "Chapter 9: Data Splitting and Resampling - The Foundation of Model Validation",
    "section": "Why Data Splitting Matters",
    "text": "Why Data Splitting Matters\nImagine you’re studying for an exam. If you only practice with questions you’ve already seen and memorized the answers to, you might think you understand the material perfectly. But when you face new questions on the actual exam, you realize you’ve only memorized specific answers rather than understanding the concepts. This is exactly what happens in machine learning when we don’t properly split our data.\n\nThe Fundamental Problem: Overfitting\nWhen we train a model on data and evaluate it on the same data, we get an overly optimistic estimate of performance. The model has essentially “memorized” the training data, including its noise and peculiarities.\n\nlibrary(tidymodels)\n\n-- Attaching packages -------------------------------------- tidymodels 1.4.1 --\n\n\nv broom        1.0.10     v recipes      1.3.1 \nv dials        1.4.2      v rsample      1.3.1 \nv dplyr        1.1.4      v tailor       0.1.0 \nv ggplot2      4.0.0      v tidyr        1.3.1 \nv infer        1.0.9      v tune         2.0.0 \nv modeldata    1.5.1      v workflows    1.3.0 \nv parsnip      1.3.3      v workflowsets 1.1.1 \nv purrr        1.1.0      v yardstick    1.3.2 \n\n\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx purrr::discard() masks scales::discard()\nx dplyr::filter()  masks stats::filter()\nx dplyr::lag()     masks stats::lag()\nx recipes::step()  masks stats::step()\n\nlibrary(tidyverse)\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv forcats   1.0.0     v stringr   1.5.2\nv lubridate 1.9.4     v tibble    3.3.0\nv readr     2.1.5     \n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx readr::col_factor() masks scales::col_factor()\nx purrr::discard()    masks scales::discard()\nx dplyr::filter()     masks stats::filter()\nx stringr::fixed()    masks recipes::fixed()\nx dplyr::lag()        masks stats::lag()\nx readr::spec()       masks yardstick::spec()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(modeldata)\nlibrary(vip)\n\n\nAdjuntando el paquete: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(patchwork)\n\n# Set theme and seed\ntheme_set(theme_minimal())\nset.seed(123)\n\n# Demonstrate overfitting with a simple example\nn &lt;- 100\nsimple_data &lt;- tibble(\n  x = seq(0, 10, length.out = n),\n  y_true = 2 * x + 5,  # True relationship\n  y = y_true + rnorm(n, sd = 3)  # Add noise\n)\n\n# Fit increasingly complex models\nmodels &lt;- list(\n  linear = lm(y ~ x, data = simple_data),\n  poly5 = lm(y ~ poly(x, 5), data = simple_data),\n  poly15 = lm(y ~ poly(x, 15), data = simple_data)\n)\n\n# Calculate training error (biased!)\ntraining_errors &lt;- map_dbl(models, ~ sqrt(mean(residuals(.)^2)))\n\n# Generate new test data\ntest_data &lt;- tibble(\n  x = seq(0, 10, length.out = 50),\n  y_true = 2 * x + 5,\n  y = y_true + rnorm(50, sd = 3)\n)\n\n# Calculate test error (unbiased)\ntest_errors &lt;- map_dbl(models, ~ {\n  predictions &lt;- predict(., newdata = test_data)\n  sqrt(mean((test_data$y - predictions)^2))\n})\n\n# Compare errors\nerror_comparison &lt;- tibble(\n  Model = c(\"Linear\", \"Polynomial (5)\", \"Polynomial (15)\"),\n  `Training RMSE` = training_errors,\n  `Test RMSE` = test_errors,\n  `Overfit Amount` = test_errors - training_errors\n)\n\nknitr::kable(error_comparison, digits = 2)\n\n\n\n\nModel\nTraining RMSE\nTest RMSE\nOverfit Amount\n\n\n\n\nLinear\n2.72\n3.12\n0.40\n\n\nPolynomial (5)\n2.68\n3.19\n0.51\n\n\nPolynomial (15)\n2.46\n3.22\n0.76\n\n\n\n\n# Visualize the fits\nplot_data &lt;- simple_data %&gt;%\n  mutate(\n    linear = predict(models$linear),\n    poly5 = predict(models$poly5),\n    poly15 = predict(models$poly15)\n  )\n\nggplot(plot_data, aes(x = x)) +\n  geom_point(aes(y = y), alpha = 0.5) +\n  geom_line(aes(y = y_true), color = \"black\", linewidth = 1, linetype = \"dashed\") +\n  geom_line(aes(y = linear, color = \"Linear\"), linewidth = 1) +\n  geom_line(aes(y = poly5, color = \"Poly(5)\"), linewidth = 1) +\n  geom_line(aes(y = poly15, color = \"Poly(15)\"), linewidth = 1) +\n  scale_color_manual(values = c(\"Linear\" = \"blue\", \"Poly(5)\" = \"green\", \"Poly(15)\" = \"red\")) +\n  labs(\n    title = \"Model Complexity and Overfitting\",\n    subtitle = \"Complex models fit training data better but may generalize worse\",\n    x = \"X\", y = \"Y\", color = \"Model\"\n  )\n\n\n\n\n\n\n\n\nNotice how the complex polynomial model (degree 15) has lower training error but higher test error - classic overfitting! This is why we need proper data splitting strategies."
  },
  {
    "objectID": "09-data-splitting.html#the-traintest-split",
    "href": "09-data-splitting.html#the-traintest-split",
    "title": "Chapter 9: Data Splitting and Resampling - The Foundation of Model Validation",
    "section": "The Train/Test Split",
    "text": "The Train/Test Split\nThe simplest approach is to split data into training and testing sets. The training set is used to fit the model, and the test set provides an unbiased estimate of performance on new data.\n\nBasic Splitting with rsample\nThe rsample package provides powerful tools for data splitting:\n\n# Load the Ames housing data\ndata(ames)\n\n# Basic 75/25 split\names_split &lt;- initial_split(ames, prop = 0.75)\n\n# Extract the datasets\names_train &lt;- training(ames_split)\names_test &lt;- testing(ames_split)\n\n# Check the sizes\nsplit_summary &lt;- tibble(\n  Dataset = c(\"Original\", \"Training\", \"Testing\"),\n  `Number of Rows` = c(nrow(ames), nrow(ames_train), nrow(ames_test)),\n  Percentage = c(100, \n                 nrow(ames_train) / nrow(ames) * 100,\n                 nrow(ames_test) / nrow(ames) * 100)\n)\n\nknitr::kable(split_summary, digits = 1)\n\n\n\n\nDataset\nNumber of Rows\nPercentage\n\n\n\n\nOriginal\n2930\n100\n\n\nTraining\n2197\n75\n\n\nTesting\n733\n25\n\n\n\n\n# The split object contains indices\names_split\n\n&lt;Training/Testing/Total&gt;\n&lt;2197/733/2930&gt;\n\n\nThe initial_split() function doesn’t just randomly split the data - it’s designed with several important features: - Reproducible with set.seed() - Maintains data structure - Can do stratified sampling - Preserves data types\n\n\nStratified Sampling\nWhen you have imbalanced classes or want to ensure representative splits, use stratified sampling:\n\n# Create a classification example with imbalanced classes\nset.seed(123)\nimbalanced_data &lt;- tibble(\n  feature1 = rnorm(1000),\n  feature2 = rnorm(1000),\n  class = factor(c(rep(\"Common\", 900), rep(\"Rare\", 100)))\n)\n\n# Regular split (might not preserve class balance)\nregular_split &lt;- initial_split(imbalanced_data, prop = 0.75)\nregular_train &lt;- training(regular_split)\n\n# Stratified split (preserves class balance)\nstratified_split &lt;- initial_split(imbalanced_data, prop = 0.75, strata = class)\nstratified_train &lt;- training(stratified_split)\n\n# Compare class distributions\ndistribution_comparison &lt;- bind_rows(\n  imbalanced_data %&gt;% count(class) %&gt;% mutate(Dataset = \"Original\"),\n  regular_train %&gt;% count(class) %&gt;% mutate(Dataset = \"Regular Split\"),\n  stratified_train %&gt;% count(class) %&gt;% mutate(Dataset = \"Stratified Split\")\n) %&gt;%\n  group_by(Dataset) %&gt;%\n  mutate(Percentage = n / sum(n) * 100)\n\nggplot(distribution_comparison, aes(x = Dataset, y = Percentage, fill = class)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Common\" = \"steelblue\", \"Rare\" = \"coral\")) +\n  labs(\n    title = \"Class Distribution: Regular vs Stratified Splitting\",\n    subtitle = \"Stratified sampling preserves class proportions\",\n    y = \"Percentage (%)\"\n  ) +\n  geom_hline(yintercept = c(10, 90), linetype = \"dashed\", alpha = 0.5)\n\n\n\n\n\n\n\n\nStratified sampling is crucial for: - Classification with imbalanced classes - Regression with skewed target distributions - Ensuring all groups are represented in both sets\n\n\nMulti-level Splits: Train/Validation/Test\nSometimes we need three sets: - Training: For model fitting - Validation: For hyperparameter tuning - Test: For final, unbiased evaluation\n\n# Create initial split\ninitial_split &lt;- initial_split(ames, prop = 0.8)\ntrain_val &lt;- training(initial_split)\ntest &lt;- testing(initial_split)\n\n# Split training into train/validation\ntrain_val_split &lt;- initial_split(train_val, prop = 0.75)\ntrain &lt;- training(train_val_split)\nvalidation &lt;- testing(train_val_split)\n\n# Summary of three-way split\nthree_way_summary &lt;- tibble(\n  Dataset = c(\"Training\", \"Validation\", \"Test\", \"Total\"),\n  Rows = c(nrow(train), nrow(validation), nrow(test), nrow(ames)),\n  `Percentage of Total` = c(\n    nrow(train) / nrow(ames) * 100,\n    nrow(validation) / nrow(ames) * 100,\n    nrow(test) / nrow(ames) * 100,\n    100\n  )\n)\n\nknitr::kable(three_way_summary, digits = 1)\n\n\n\n\nDataset\nRows\nPercentage of Total\n\n\n\n\nTraining\n1758\n60\n\n\nValidation\n586\n20\n\n\nTest\n586\n20\n\n\nTotal\n2930\n100\n\n\n\n\n# Alternative: Use initial_validation_split (tidymodels 1.1.0+)\n# This creates all three splits at once\n# val_split &lt;- initial_validation_split(ames, prop = c(0.6, 0.2))\n# This would give: 60% train, 20% validation, 20% test"
  },
  {
    "objectID": "09-data-splitting.html#cross-validation-making-the-most-of-your-data",
    "href": "09-data-splitting.html#cross-validation-making-the-most-of-your-data",
    "title": "Chapter 9: Data Splitting and Resampling - The Foundation of Model Validation",
    "section": "Cross-Validation: Making the Most of Your Data",
    "text": "Cross-Validation: Making the Most of Your Data\nWhile train/test splits are simple, they have limitations: - Only use part of the data for training - Results can vary based on the specific split - May not be reliable for small datasets\nCross-validation addresses these issues by using multiple splits.\n\nK-Fold Cross-Validation\nK-fold CV divides data into k equal parts (folds), trains on k-1 folds, and tests on the remaining fold. This process repeats k times.\n\n# Create 5-fold cross-validation\names_cv &lt;- vfold_cv(ames_train, v = 5)\names_cv\n\n#  5-fold cross-validation \n# A tibble: 5 x 2\n  splits             id   \n  &lt;list&gt;             &lt;chr&gt;\n1 &lt;split [1757/440]&gt; Fold1\n2 &lt;split [1757/440]&gt; Fold2\n3 &lt;split [1758/439]&gt; Fold3\n4 &lt;split [1758/439]&gt; Fold4\n5 &lt;split [1758/439]&gt; Fold5\n\n# Examine the structure\nfirst_fold &lt;- ames_cv$splits[[1]]\nanalysis(first_fold) %&gt;% nrow()  # Training data for this fold\n\n[1] 1757\n\nassessment(first_fold) %&gt;% nrow()  # Test data for this fold\n\n[1] 440\n\n# Visualize how data is split across folds\nfold_assignments &lt;- map_df(1:5, function(fold) {\n  assessment_rows &lt;- which(ames_cv$splits[[fold]]$in_id == 0)\n  ames_train %&gt;%\n    mutate(\n      row_id = row_number(),\n      fold = fold,\n      in_assessment = row_id %in% assessment_rows\n    )\n}) %&gt;%\n  filter(row_id &lt;= 100)  # Show first 100 rows for visualization\n\nggplot(fold_assignments, aes(x = row_id, y = factor(fold), fill = in_assessment)) +\n  geom_tile(height = 0.8) +\n  scale_fill_manual(values = c(\"FALSE\" = \"steelblue\", \"TRUE\" = \"coral\"),\n                    labels = c(\"Training\", \"Testing\")) +\n  labs(\n    title = \"5-Fold Cross-Validation Data Assignment\",\n    subtitle = \"Each row is used for testing exactly once\",\n    x = \"Observation ID\", y = \"Fold\", fill = \"Role\"\n  )\n\n\n\n\n\n\n\n\nThe mathematics of cross-validation: - Each observation is used for testing exactly once - Each observation is used for training k-1 times - We get k performance estimates - Final estimate is the average across folds - Standard error provides uncertainty estimate\n\n\nRepeated Cross-Validation\nFor more stable estimates, we can repeat the CV process with different random splits:\n\n# Repeated 5-fold CV (3 repeats = 15 total resamples)\names_repeated_cv &lt;- vfold_cv(ames_train, v = 5, repeats = 3)\names_repeated_cv\n\n#  5-fold cross-validation repeated 3 times \n# A tibble: 15 x 3\n   splits             id      id2  \n   &lt;list&gt;             &lt;chr&gt;   &lt;chr&gt;\n 1 &lt;split [1757/440]&gt; Repeat1 Fold1\n 2 &lt;split [1757/440]&gt; Repeat1 Fold2\n 3 &lt;split [1758/439]&gt; Repeat1 Fold3\n 4 &lt;split [1758/439]&gt; Repeat1 Fold4\n 5 &lt;split [1758/439]&gt; Repeat1 Fold5\n 6 &lt;split [1757/440]&gt; Repeat2 Fold1\n 7 &lt;split [1757/440]&gt; Repeat2 Fold2\n 8 &lt;split [1758/439]&gt; Repeat2 Fold3\n 9 &lt;split [1758/439]&gt; Repeat2 Fold4\n10 &lt;split [1758/439]&gt; Repeat2 Fold5\n11 &lt;split [1757/440]&gt; Repeat3 Fold1\n12 &lt;split [1757/440]&gt; Repeat3 Fold2\n13 &lt;split [1758/439]&gt; Repeat3 Fold3\n14 &lt;split [1758/439]&gt; Repeat3 Fold4\n15 &lt;split [1758/439]&gt; Repeat3 Fold5\n\n# Compare variability: single vs repeated CV\n# Fit a simple model to demonstrate\nsimple_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\nsimple_recipe &lt;- recipe(Sale_Price ~ Gr_Liv_Area + Overall_Cond, data = ames_train)\n\n# Single CV\nsingle_cv_results &lt;- workflow() %&gt;%\n  add_model(simple_model) %&gt;%\n  add_recipe(simple_recipe) %&gt;%\n  fit_resamples(vfold_cv(ames_train, v = 5)) %&gt;%\n  collect_metrics()\n\n# Repeated CV\nrepeated_cv_results &lt;- workflow() %&gt;%\n  add_model(simple_model) %&gt;%\n  add_recipe(simple_recipe) %&gt;%\n  fit_resamples(ames_repeated_cv) %&gt;%\n  collect_metrics()\n\ncomparison &lt;- bind_rows(\n  single_cv_results %&gt;% mutate(Method = \"Single 5-fold CV\"),\n  repeated_cv_results %&gt;% mutate(Method = \"Repeated 5-fold CV (3x)\")\n)\n\nggplot(comparison, aes(x = Method, y = mean, fill = Method)) +\n  geom_col() +\n  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), width = 0.2) +\n  facet_wrap(~.metric, scales = \"free_y\") +\n  labs(\n    title = \"Single vs Repeated Cross-Validation\",\n    subtitle = \"Repeated CV has smaller standard errors (more stable estimates)\",\n    y = \"Metric Value\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nLeave-One-Out Cross-Validation (LOOCV)\nLOOCV is a special case where k = n (number of observations):\n\n# Create a small dataset for LOOCV demonstration\nsmall_data &lt;- ames_train %&gt;% \n  slice_sample(n = 50)  # LOOCV is computationally expensive\n\n# Create LOOCV splits\nloocv_splits &lt;- loo_cv(small_data)\nloocv_splits\n\n# Leave-one-out cross-validation \n# A tibble: 50 x 2\n   splits         id        \n   &lt;list&gt;         &lt;chr&gt;     \n 1 &lt;split [49/1]&gt; Resample1 \n 2 &lt;split [49/1]&gt; Resample2 \n 3 &lt;split [49/1]&gt; Resample3 \n 4 &lt;split [49/1]&gt; Resample4 \n 5 &lt;split [49/1]&gt; Resample5 \n 6 &lt;split [49/1]&gt; Resample6 \n 7 &lt;split [49/1]&gt; Resample7 \n 8 &lt;split [49/1]&gt; Resample8 \n 9 &lt;split [49/1]&gt; Resample9 \n10 &lt;split [49/1]&gt; Resample10\n# i 40 more rows\n\n# Compare computational cost\ncv_comparison &lt;- tibble(\n  Method = c(\"5-fold CV\", \"10-fold CV\", \"LOOCV\"),\n  `Number of Models` = c(5, 10, nrow(small_data)),\n  `Training Set Size` = c(\n    floor(nrow(small_data) * 4/5),\n    floor(nrow(small_data) * 9/10),\n    nrow(small_data) - 1\n  ),\n  `Test Set Size` = c(\n    floor(nrow(small_data) / 5),\n    floor(nrow(small_data) / 10),\n    1\n  )\n)\n\nknitr::kable(cv_comparison)\n\n\n\n\nMethod\nNumber of Models\nTraining Set Size\nTest Set Size\n\n\n\n\n5-fold CV\n5\n40\n10\n\n\n10-fold CV\n10\n45\n5\n\n\nLOOCV\n50\n49\n1\n\n\n\n\n\nLOOCV characteristics: - Pros: Uses maximum data for training, deterministic (no randomness) - Cons: Computationally expensive, high variance, can overfit to the dataset"
  },
  {
    "objectID": "09-data-splitting.html#bootstrap-methods",
    "href": "09-data-splitting.html#bootstrap-methods",
    "title": "Chapter 9: Data Splitting and Resampling - The Foundation of Model Validation",
    "section": "Bootstrap Methods",
    "text": "Bootstrap Methods\nBootstrap resampling draws samples WITH replacement from the original data. This creates datasets of the same size but with some observations repeated and others omitted.\n\nUnderstanding Bootstrap\n\n# Create bootstrap samples\names_boot &lt;- bootstraps(ames_train, times = 25)\names_boot\n\n# Bootstrap sampling \n# A tibble: 25 x 2\n   splits             id         \n   &lt;list&gt;             &lt;chr&gt;      \n 1 &lt;split [2197/804]&gt; Bootstrap01\n 2 &lt;split [2197/819]&gt; Bootstrap02\n 3 &lt;split [2197/822]&gt; Bootstrap03\n 4 &lt;split [2197/798]&gt; Bootstrap04\n 5 &lt;split [2197/814]&gt; Bootstrap05\n 6 &lt;split [2197/812]&gt; Bootstrap06\n 7 &lt;split [2197/794]&gt; Bootstrap07\n 8 &lt;split [2197/825]&gt; Bootstrap08\n 9 &lt;split [2197/819]&gt; Bootstrap09\n10 &lt;split [2197/806]&gt; Bootstrap10\n# i 15 more rows\n\n# Examine one bootstrap sample\nfirst_boot &lt;- ames_boot$splits[[1]]\n\n# In bootstrap, some observations appear multiple times\nboot_sample &lt;- analysis(first_boot)\noriginal_ids &lt;- 1:nrow(ames_train)\nsampled_ids &lt;- as.integer(rownames(boot_sample))\n\n# Count frequency of observations\nid_frequency &lt;- table(sampled_ids)\nfreq_summary &lt;- tibble(\n  `Times Sampled` = 0:max(id_frequency),\n  Count = c(\n    sum(!(original_ids %in% sampled_ids)),  # Not sampled (0 times)\n    sapply(1:max(id_frequency), function(x) sum(id_frequency == x))\n  ),\n  Percentage = Count / nrow(ames_train) * 100\n)\n\nknitr::kable(freq_summary, digits = 1)\n\n\n\n\nTimes Sampled\nCount\nPercentage\n\n\n\n\n0\n0\n0\n\n\n1\n2197\n100\n\n\n\n\n# Visualize bootstrap sampling\nset.seed(456)\nsample_viz &lt;- tibble(\n  original_id = 1:20,\n  bootstrap_sample = sample(1:20, 20, replace = TRUE)\n) %&gt;%\n  count(bootstrap_sample) %&gt;%\n  complete(bootstrap_sample = 1:20, fill = list(n = 0))\n\nggplot(sample_viz, aes(x = bootstrap_sample, y = n)) +\n  geom_col(fill = \"steelblue\") +\n  geom_hline(yintercept = 1, linetype = \"dashed\", color = \"red\") +\n  scale_x_continuous(breaks = 1:20) +\n  labs(\n    title = \"Bootstrap Sampling Example (n=20)\",\n    subtitle = \"Some observations appear multiple times, others not at all\",\n    x = \"Original Observation ID\",\n    y = \"Times Selected\"\n  )\n\n\n\n\n\n\n\n\nThe mathematics of bootstrap: - Probability of being selected at least once: \\(1 - (1 - 1/n)^n \\approx 0.632\\) as \\(n \\to \\infty\\) - About 37% of observations are not selected (out-of-bag) - Provides estimates of sampling distribution - Useful for confidence intervals and bias estimation\n\n\nOut-of-Bag (OOB) Error Estimation\nBootstrap’s unique property is that ~37% of data is not sampled, providing a natural test set:\n\n# Calculate OOB predictions for each bootstrap sample\noob_analysis &lt;- map_df(1:25, function(i) {\n  boot_split &lt;- ames_boot$splits[[i]]\n  \n  # Observations in this bootstrap sample\n  in_bag &lt;- as.integer(rownames(analysis(boot_split)))\n  \n  # OOB observations\n  all_ids &lt;- 1:nrow(ames_train)\n  oob_ids &lt;- setdiff(all_ids, unique(in_bag))\n  \n  tibble(\n    bootstrap = i,\n    n_unique_in_bag = length(unique(in_bag)),\n    n_oob = length(oob_ids),\n    pct_oob = length(oob_ids) / length(all_ids) * 100\n  )\n})\n\n# Summary statistics\noob_summary &lt;- oob_analysis %&gt;%\n  summarise(\n    mean_pct_oob = mean(pct_oob),\n    sd_pct_oob = sd(pct_oob),\n    theoretical_oob = (1 - 1/exp(1)) * 100  # ~36.8%\n  )\n\nknitr::kable(oob_summary, digits = 1)\n\n\n\n\nmean_pct_oob\nsd_pct_oob\ntheoretical_oob\n\n\n\n\n0\n0\n63.2\n\n\n\n\n# Visualize OOB percentages\nggplot(oob_analysis, aes(x = pct_oob)) +\n  geom_histogram(bins = 15, fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(xintercept = oob_summary$theoretical_oob, \n             color = \"red\", linewidth = 1, linetype = \"dashed\") +\n  labs(\n    title = \"Out-of-Bag Percentage Across Bootstrap Samples\",\n    subtitle = \"Red line shows theoretical value (~36.8%)\",\n    x = \"OOB Percentage\",\n    y = \"Count\"\n  )"
  },
  {
    "objectID": "09-data-splitting.html#time-series-splitting",
    "href": "09-data-splitting.html#time-series-splitting",
    "title": "Chapter 9: Data Splitting and Resampling - The Foundation of Model Validation",
    "section": "Time Series Splitting",
    "text": "Time Series Splitting\nTime series data requires special handling because: - Observations are not independent - Future cannot be used to predict past - Temporal patterns must be preserved\n\nTime Series Cross-Validation\n\n# Create time series data\nset.seed(789)\nn_days &lt;- 365\nts_data &lt;- tibble(\n  date = seq(as.Date(\"2022-01-01\"), by = \"day\", length.out = n_days),\n  trend = seq(100, 200, length.out = n_days),\n  seasonal = 20 * sin(2 * pi * (1:n_days) / 7),  # Weekly pattern\n  noise = rnorm(n_days, sd = 10),\n  value = trend + seasonal + noise\n)\n\n# Time series split - expanding window\nts_initial &lt;- 180  # Initial training size\nts_assess &lt;- 30    # Assessment size\n\nts_splits &lt;- sliding_period(\n  ts_data,\n  date,\n  period = \"month\",\n  lookback = 5,  # Use 6 months of data\n  assess_stop = 1  # Assess on next month\n)\n\n# Visualize time series CV\nsplit_viz &lt;- map_df(1:min(5, length(ts_splits$splits)), function(i) {\n  split &lt;- ts_splits$splits[[i]]\n  train_data &lt;- analysis(split) %&gt;% mutate(role = \"Training\", split_id = i)\n  test_data &lt;- assessment(split) %&gt;% mutate(role = \"Testing\", split_id = i)\n  bind_rows(train_data, test_data)\n})\n\nggplot(split_viz, aes(x = date, y = split_id, color = role)) +\n  geom_point(size = 0.5) +\n  scale_color_manual(values = c(\"Training\" = \"steelblue\", \"Testing\" = \"coral\")) +\n  labs(\n    title = \"Time Series Cross-Validation\",\n    subtitle = \"Training window slides forward, always predicting future\",\n    x = \"Date\", y = \"Split\", color = \"Role\"\n  ) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nTime series splitting strategies: - Expanding window: Training set grows over time - Sliding window: Fixed-size training window - Skip periods: Gap between training and testing\n\n\nRolling Origin Evaluation\n\n# Rolling origin (expanding window)\nrolling_splits &lt;- rolling_origin(\n  ts_data,\n  initial = 180,    # Start with 180 days\n  assess = 30,      # Assess next 30 days\n  skip = 30,        # Skip 30 days between splits\n  cumulative = TRUE # Expanding window\n)\n\n# Calculate how training size grows\nrolling_summary &lt;- rolling_splits %&gt;%\n  mutate(\n    train_size = map_int(splits, ~ nrow(analysis(.))),\n    test_size = map_int(splits, ~ nrow(assessment(.)))\n  ) %&gt;%\n  select(id, train_size, test_size) %&gt;%\n  mutate(split = row_number())\n\nggplot(rolling_summary, aes(x = split)) +\n  geom_line(aes(y = train_size, color = \"Training\"), linewidth = 1) +\n  geom_line(aes(y = test_size, color = \"Testing\"), linewidth = 1) +\n  scale_color_manual(values = c(\"Training\" = \"steelblue\", \"Testing\" = \"coral\")) +\n  labs(\n    title = \"Rolling Origin: Expanding Window\",\n    subtitle = \"Training set grows while test set remains constant\",\n    x = \"Split Number\", y = \"Number of Observations\",\n    color = \"Dataset\"\n  )"
  },
  {
    "objectID": "09-data-splitting.html#nested-resampling",
    "href": "09-data-splitting.html#nested-resampling",
    "title": "Chapter 9: Data Splitting and Resampling - The Foundation of Model Validation",
    "section": "Nested Resampling",
    "text": "Nested Resampling\nWhen we need to tune hyperparameters AND get an unbiased performance estimate, we use nested resampling: - Outer loop: For performance estimation - Inner loop: For hyperparameter tuning\n\n# Create nested resampling\n# Outer: 5-fold CV for performance estimation\n# Inner: 5-fold CV for hyperparameter tuning\n\n# Outer resampling\nouter_cv &lt;- vfold_cv(ames_train, v = 5)\n\n# For each outer fold, create inner resampling\nnested_cv &lt;- map(outer_cv$splits, function(outer_split) {\n  # Training data for this outer fold\n  outer_train &lt;- analysis(outer_split)\n  \n  # Create inner CV on the outer training data\n  inner_cv &lt;- vfold_cv(outer_train, v = 5)\n  \n  list(\n    outer_split = outer_split,\n    inner_cv = inner_cv\n  )\n})\n\n# Visualize nested structure\nnested_viz &lt;- tibble(\n  `Outer Fold` = 1:5,\n  `Outer Training Size` = map_int(outer_cv$splits, ~ nrow(analysis(.))),\n  `Outer Test Size` = map_int(outer_cv$splits, ~ nrow(assessment(.))),\n  `Inner Folds` = 5,\n  `Total Models per Outer Fold` = 5,\n  `Total Models Overall` = 25\n)\n\nknitr::kable(nested_viz)\n\n\n\n\n\n\n\n\n\n\n\n\nOuter Fold\nOuter Training Size\nOuter Test Size\nInner Folds\nTotal Models per Outer Fold\nTotal Models Overall\n\n\n\n\n1\n1757\n440\n5\n5\n25\n\n\n2\n1757\n440\n5\n5\n25\n\n\n3\n1758\n439\n5\n5\n25\n\n\n4\n1758\n439\n5\n5\n25\n\n\n5\n1758\n439\n5\n5\n25\n\n\n\n\n# Conceptual diagram\ncat(\"\nNested Resampling Structure:\n============================\nFull Training Data\n  |\n  ├── Outer Fold 1\n  │   ├── Outer Training (80%)\n  │   │   ├── Inner Fold 1: Train (64%) + Val (16%)\n  │   │   ├── Inner Fold 2: Train (64%) + Val (16%)\n  │   │   ├── Inner Fold 3: Train (64%) + Val (16%)\n  │   │   ├── Inner Fold 4: Train (64%) + Val (16%)\n  │   │   └── Inner Fold 5: Train (64%) + Val (16%)\n  │   └── Outer Test (20%) - Never seen during tuning\n  │\n  ├── Outer Fold 2\n  │   └── [Same structure]\n  ...\n\")\n\n\nNested Resampling Structure:\n============================\nFull Training Data\n  |\n  &lt;U+251C&gt;&lt;U+2500&gt;&lt;U+2500&gt; Outer Fold 1\n  &lt;U+2502&gt;   &lt;U+251C&gt;&lt;U+2500&gt;&lt;U+2500&gt; Outer Training (80%)\n  &lt;U+2502&gt;   &lt;U+2502&gt;   &lt;U+251C&gt;&lt;U+2500&gt;&lt;U+2500&gt; Inner Fold 1: Train (64%) + Val (16%)\n  &lt;U+2502&gt;   &lt;U+2502&gt;   &lt;U+251C&gt;&lt;U+2500&gt;&lt;U+2500&gt; Inner Fold 2: Train (64%) + Val (16%)\n  &lt;U+2502&gt;   &lt;U+2502&gt;   &lt;U+251C&gt;&lt;U+2500&gt;&lt;U+2500&gt; Inner Fold 3: Train (64%) + Val (16%)\n  &lt;U+2502&gt;   &lt;U+2502&gt;   &lt;U+251C&gt;&lt;U+2500&gt;&lt;U+2500&gt; Inner Fold 4: Train (64%) + Val (16%)\n  &lt;U+2502&gt;   &lt;U+2502&gt;   &lt;U+2514&gt;&lt;U+2500&gt;&lt;U+2500&gt; Inner Fold 5: Train (64%) + Val (16%)\n  &lt;U+2502&gt;   &lt;U+2514&gt;&lt;U+2500&gt;&lt;U+2500&gt; Outer Test (20%) - Never seen during tuning\n  &lt;U+2502&gt;\n  &lt;U+251C&gt;&lt;U+2500&gt;&lt;U+2500&gt; Outer Fold 2\n  &lt;U+2502&gt;   &lt;U+2514&gt;&lt;U+2500&gt;&lt;U+2500&gt; [Same structure]\n  ...\n\n\nNested resampling is crucial for: - Unbiased performance estimation when tuning - Comparing different modeling strategies - Understanding generalization performance"
  },
  {
    "objectID": "09-data-splitting.html#validation-sets-in-practice",
    "href": "09-data-splitting.html#validation-sets-in-practice",
    "title": "Chapter 9: Data Splitting and Resampling - The Foundation of Model Validation",
    "section": "Validation Sets in Practice",
    "text": "Validation Sets in Practice\nSometimes we want a single validation set for quick iterations:\n\n# Create a validation set\nval_split &lt;- initial_validation_split(ames, prop = c(0.6, 0.2))\n\n# Extract all three sets\ntrain_set &lt;- training(val_split)\nval_set &lt;- validation(val_split)\ntest_set &lt;- testing(val_split)\n\n# Use validation set for model selection\nmodels_to_compare &lt;- list(\n  simple = recipe(Sale_Price ~ Gr_Liv_Area + Overall_Cond, data = train_set) %&gt;%\n    step_dummy(all_nominal_predictors()),\n  moderate = recipe(Sale_Price ~ Gr_Liv_Area + Overall_Cond + Year_Built + \n                   Neighborhood, data = train_set) %&gt;%\n    step_dummy(all_nominal_predictors()),\n  complex = recipe(Sale_Price ~ Gr_Liv_Area + Overall_Cond + Year_Built + \n                  Neighborhood + Total_Bsmt_SF + First_Flr_SF, data = train_set) %&gt;%\n    step_dummy(all_nominal_predictors())\n)\n\n# Fit models and evaluate on validation set\nlm_spec &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\nvalidation_results &lt;- map_df(names(models_to_compare), function(model_name) {\n  recipe &lt;- models_to_compare[[model_name]]\n  \n  wf &lt;- workflow() %&gt;%\n    add_recipe(recipe) %&gt;%\n    add_model(lm_spec)\n  \n  # Fit on training\n  fit &lt;- wf %&gt;% fit(train_set)\n  \n  # Predict on validation\n  val_pred &lt;- fit %&gt;%\n    predict(val_set) %&gt;%\n    bind_cols(val_set)\n  \n  # Calculate metrics\n  val_pred %&gt;%\n    metrics(truth = Sale_Price, estimate = .pred) %&gt;%\n    mutate(model = model_name)\n})\n\n# Select best model based on validation performance\nbest_model &lt;- validation_results %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  arrange(.estimate) %&gt;%\n  slice(1) %&gt;%\n  pull(model)\n\nprint(paste(\"Best model based on validation set:\", best_model))\n\n[1] \"Best model based on validation set: complex\"\n\n# Final evaluation on test set (only done once!)\nfinal_recipe &lt;- models_to_compare[[best_model]]\nfinal_fit &lt;- workflow() %&gt;%\n  add_recipe(final_recipe) %&gt;%\n  add_model(lm_spec) %&gt;%\n  fit(train_set)\n\ntest_pred &lt;- final_fit %&gt;%\n  predict(test_set) %&gt;%\n  bind_cols(test_set)\n\nfinal_performance &lt;- test_pred %&gt;%\n  metrics(truth = Sale_Price, estimate = .pred)\n\nknitr::kable(final_performance, digits = 3)\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n38837.693\n\n\nrsq\nstandard\n0.797\n\n\nmae\nstandard\n24125.346"
  },
  {
    "objectID": "09-data-splitting.html#choosing-the-right-resampling-strategy",
    "href": "09-data-splitting.html#choosing-the-right-resampling-strategy",
    "title": "Chapter 9: Data Splitting and Resampling - The Foundation of Model Validation",
    "section": "Choosing the Right Resampling Strategy",
    "text": "Choosing the Right Resampling Strategy\nDifferent strategies for different situations:\n\n# Decision guide\nstrategy_guide &lt;- tibble(\n  Scenario = c(\n    \"Large dataset (n &gt; 10,000)\",\n    \"Medium dataset (1,000 &lt; n &lt; 10,000)\",\n    \"Small dataset (n &lt; 1,000)\",\n    \"Imbalanced classes\",\n    \"Time series data\",\n    \"Quick prototyping\",\n    \"Final evaluation\",\n    \"Hyperparameter tuning\",\n    \"Model comparison\",\n    \"Uncertainty estimation\"\n  ),\n  `Recommended Strategy` = c(\n    \"Simple train/test split or validation set\",\n    \"5 or 10-fold CV\",\n    \"Repeated CV or LOOCV\",\n    \"Stratified CV\",\n    \"Time series CV or rolling origin\",\n    \"Validation set\",\n    \"Held-out test set (touched only once)\",\n    \"Nested CV\",\n    \"Repeated CV\",\n    \"Bootstrap\"\n  ),\n  Reasoning = c(\n    \"Sufficient data for reliable estimates\",\n    \"Balance between bias and variance\",\n    \"Maximize training data usage\",\n    \"Preserve class proportions\",\n    \"Respect temporal ordering\",\n    \"Fast iteration and feedback\",\n    \"Unbiased final assessment\",\n    \"Avoid overfitting to validation set\",\n    \"Stable comparison metrics\",\n    \"Confidence intervals and distributions\"\n  )\n)\n\nknitr::kable(strategy_guide)\n\n\n\n\n\n\n\n\n\nScenario\nRecommended Strategy\nReasoning\n\n\n\n\nLarge dataset (n &gt; 10,000)\nSimple train/test split or validation set\nSufficient data for reliable estimates\n\n\nMedium dataset (1,000 &lt; n &lt; 10,000)\n5 or 10-fold CV\nBalance between bias and variance\n\n\nSmall dataset (n &lt; 1,000)\nRepeated CV or LOOCV\nMaximize training data usage\n\n\nImbalanced classes\nStratified CV\nPreserve class proportions\n\n\nTime series data\nTime series CV or rolling origin\nRespect temporal ordering\n\n\nQuick prototyping\nValidation set\nFast iteration and feedback\n\n\nFinal evaluation\nHeld-out test set (touched only once)\nUnbiased final assessment\n\n\nHyperparameter tuning\nNested CV\nAvoid overfitting to validation set\n\n\nModel comparison\nRepeated CV\nStable comparison metrics\n\n\nUncertainty estimation\nBootstrap\nConfidence intervals and distributions"
  },
  {
    "objectID": "09-data-splitting.html#common-pitfalls-and-best-practices",
    "href": "09-data-splitting.html#common-pitfalls-and-best-practices",
    "title": "Chapter 9: Data Splitting and Resampling - The Foundation of Model Validation",
    "section": "Common Pitfalls and Best Practices",
    "text": "Common Pitfalls and Best Practices\n\nPitfall 1: Data Leakage\n\n# WRONG: Preprocessing before splitting\nwrong_data &lt;- ames %&gt;%\n  mutate(\n    # This uses information from ALL data including test!\n    Gr_Liv_Area_scaled = scale(Gr_Liv_Area)[,1],\n    Sale_Price_log = log(Sale_Price)\n  )\n\nwrong_split &lt;- initial_split(wrong_data)\nwrong_train &lt;- training(wrong_split)\nwrong_test &lt;- testing(wrong_split)\n\n# RIGHT: Preprocessing after splitting (using recipes)\nright_split &lt;- initial_split(ames)\nright_train &lt;- training(right_split)\nright_test &lt;- testing(right_split)\n\nright_recipe &lt;- recipe(Sale_Price ~ ., data = right_train) %&gt;%\n  step_log(Sale_Price) %&gt;%\n  step_normalize(all_numeric_predictors())\n\n# The recipe learns parameters only from training data\n\n\n\nPitfall 2: Multiple Testing on Test Set\n\n# WRONG: Using test set multiple times\n# This is pseudocode - don't actually do this!\n# for (model in models) {\n#   performance &lt;- evaluate(model, test_set)\n#   if (performance &lt; best_performance) {\n#     adjust_model(model)\n#     # Testing again - overfitting to test set!\n#   }\n# }\n\n# RIGHT: Use validation set or CV for model selection\n# Test set only for final evaluation\n\n\n\nPitfall 3: Improper Stratification\n\n# Create data with important subgroups\ngrouped_data &lt;- ames %&gt;%\n  mutate(\n    price_category = cut(Sale_Price, \n                        breaks = quantile(Sale_Price, c(0, 0.33, 0.67, 1)),\n                        labels = c(\"Low\", \"Medium\", \"High\"))\n  )\n\n# WRONG: Not stratifying on important variable\nwrong_split &lt;- initial_split(grouped_data)\nwrong_train &lt;- training(wrong_split)\n\n# RIGHT: Stratify to preserve distribution\nright_split &lt;- initial_split(grouped_data, strata = price_category)\nright_train &lt;- training(right_split)\n\n# Compare distributions\ncomparison &lt;- bind_rows(\n  grouped_data %&gt;% count(price_category) %&gt;% mutate(Set = \"Original\"),\n  wrong_train %&gt;% count(price_category) %&gt;% mutate(Set = \"No Stratification\"),\n  right_train %&gt;% count(price_category) %&gt;% mutate(Set = \"With Stratification\")\n) %&gt;%\n  group_by(Set) %&gt;%\n  mutate(Percentage = n / sum(n) * 100)\n\nggplot(comparison, aes(x = price_category, y = Percentage, fill = Set)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Effect of Stratification on Distribution\",\n    subtitle = \"Stratification preserves the original distribution\",\n    x = \"Price Category\", y = \"Percentage\"\n  )"
  },
  {
    "objectID": "09-data-splitting.html#exercises",
    "href": "09-data-splitting.html#exercises",
    "title": "Chapter 9: Data Splitting and Resampling - The Foundation of Model Validation",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Implement Custom Resampling\nCreate a custom resampling strategy for grouped data:\n\n# Your solution\n# Data with natural groups (e.g., different stores)\nstore_data &lt;- tibble(\n  store_id = rep(LETTERS[1:10], each = 100),\n  date = rep(seq(as.Date(\"2023-01-01\"), length.out = 100, by = \"day\"), 10),\n  sales = rnorm(1000, mean = rep(seq(100, 190, 10), each = 100), sd = 20)\n)\n\n# Custom group-based CV: leave-one-store-out\ngroup_splits &lt;- group_vfold_cv(store_data, group = store_id)\n\n# Examine splits\nsplit_summary &lt;- map_df(1:length(group_splits$splits), function(i) {\n  split &lt;- group_splits$splits[[i]]\n  train &lt;- analysis(split)\n  test &lt;- assessment(split)\n  \n  tibble(\n    fold = i,\n    train_stores = n_distinct(train$store_id),\n    test_stores = n_distinct(test$store_id),\n    train_rows = nrow(train),\n    test_rows = nrow(test)\n  )\n})\n\nknitr::kable(split_summary)\n\n\n\n\nfold\ntrain_stores\ntest_stores\ntrain_rows\ntest_rows\n\n\n\n\n1\n9\n1\n900\n100\n\n\n2\n9\n1\n900\n100\n\n\n3\n9\n1\n900\n100\n\n\n4\n9\n1\n900\n100\n\n\n5\n9\n1\n900\n100\n\n\n6\n9\n1\n900\n100\n\n\n7\n9\n1\n900\n100\n\n\n8\n9\n1\n900\n100\n\n\n9\n9\n1\n900\n100\n\n\n10\n9\n1\n900\n100\n\n\n\n\n\n\n\nExercise 2: Compare Resampling Strategies\nEvaluate different resampling methods on the same dataset:\n\n# Your solution\n# Use a subset of Ames data for speed\names_subset &lt;- ames %&gt;%\n  select(Sale_Price, Gr_Liv_Area, Overall_Cond, Year_Built, Lot_Area) %&gt;%\n  slice_sample(n = 500)\n\n# Define resampling strategies\nstrategies &lt;- list(\n  holdout = initial_split(ames_subset, prop = 0.75),\n  cv5 = vfold_cv(ames_subset, v = 5),\n  cv10 = vfold_cv(ames_subset, v = 10),\n  repeated_cv = vfold_cv(ames_subset, v = 5, repeats = 3),\n  bootstrap = bootstraps(ames_subset, times = 25)\n)\n\n# Simple model for comparison\nmodel_spec &lt;- linear_reg() %&gt;% set_engine(\"lm\")\nrecipe_spec &lt;- recipe(Sale_Price ~ ., data = ames_subset)\n\n# Evaluate each strategy (except holdout which needs different handling)\nresults &lt;- map_df(names(strategies)[-1], function(strategy_name) {\n  wf &lt;- workflow() %&gt;%\n    add_recipe(recipe_spec) %&gt;%\n    add_model(model_spec)\n  \n  fit_resamples(wf, strategies[[strategy_name]]) %&gt;%\n    collect_metrics() %&gt;%\n    mutate(strategy = strategy_name)\n})\n\n# Visualize results\nggplot(results, aes(x = strategy, y = mean, fill = strategy)) +\n  geom_col() +\n  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), width = 0.2) +\n  facet_wrap(~.metric, scales = \"free_y\") +\n  labs(\n    title = \"Comparison of Resampling Strategies\",\n    subtitle = \"Error bars show standard error\",\n    x = \"Strategy\", y = \"Metric Value\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nExercise 3: Time Series Validation\nImplement proper time series validation:\n\n# Your solution\n# Generate time series data with trend and seasonality\nset.seed(123)\nts_exercise &lt;- tibble(\n  date = seq(as.Date(\"2021-01-01\"), as.Date(\"2023-12-31\"), by = \"day\"),\n  day_of_week = lubridate::wday(date),\n  month = lubridate::month(date),\n  trend = seq(1000, 2000, length.out = length(date)),\n  seasonal = 100 * sin(2 * pi * as.numeric(date) / 365),\n  weekly = 50 * (day_of_week %in% c(1, 7)),  # Weekend effect\n  noise = rnorm(length(date), 0, 50),\n  sales = trend + seasonal + weekly + noise\n)\n\n# Create time-based splits\n# Initial training: 2 years, assess: 1 month, skip: 1 month\nts_splits &lt;- rolling_origin(\n  ts_exercise,\n  initial = 730,  # 2 years\n  assess = 30,    # 1 month\n  skip = 30,      # Skip 1 month\n  cumulative = FALSE  # Sliding window\n)\n\n# Evaluate a simple model\nts_recipe &lt;- recipe(sales ~ day_of_week + month + trend, data = ts_exercise)\nts_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\nts_workflow &lt;- workflow() %&gt;%\n  add_recipe(ts_recipe) %&gt;%\n  add_model(ts_model)\n\n# Fit and evaluate\nts_results &lt;- fit_resamples(\n  ts_workflow,\n  ts_splits,\n  metrics = yardstick::metric_set(yardstick::rmse, yardstick::mae, yardstick::rsq)\n)\n\n# Examine performance over time\nts_metrics &lt;- ts_results %&gt;%\n  collect_metrics() %&gt;%\n  mutate(\n    split_num = rep(1:(n()/3), 3)\n  )\n\nggplot(ts_metrics, aes(x = split_num, y = mean, color = .metric)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 2) +\n  facet_wrap(~.metric, scales = \"free_y\") +\n  labs(\n    title = \"Model Performance Over Time\",\n    subtitle = \"Time series cross-validation results\",\n    x = \"Split Number (Time →)\", y = \"Metric Value\"\n  ) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "09-data-splitting.html#summary",
    "href": "09-data-splitting.html#summary",
    "title": "Chapter 9: Data Splitting and Resampling - The Foundation of Model Validation",
    "section": "Summary",
    "text": "Summary\nIn this comprehensive chapter, you’ve mastered:\n✅ Fundamental concepts - Why proper data splitting is critical - The bias-variance tradeoff in evaluation - Training, validation, and test sets\n✅ Splitting strategies - Simple holdout splits - Stratified sampling for balance - Multi-level splits for complex workflows\n✅ Cross-validation techniques - K-fold and repeated CV - Leave-one-out CV - Time series CV\n✅ Advanced methods - Bootstrap resampling - Nested resampling for tuning - Group-based splitting\n✅ Best practices - Avoiding data leakage - Choosing appropriate strategies - Proper use of test sets\nKey takeaways: - Never evaluate on training data - Choose resampling based on your data and goals - Stratify when you have imbalanced data - Respect temporal ordering in time series - Use nested CV for unbiased tuning - Touch the test set only once!"
  },
  {
    "objectID": "09-data-splitting.html#whats-next",
    "href": "09-data-splitting.html#whats-next",
    "title": "Chapter 9: Data Splitting and Resampling - The Foundation of Model Validation",
    "section": "What’s Next?",
    "text": "What’s Next?\nIn Chapter 10, we’ll explore feature engineering with recipes, learning how to transform raw data into model-ready features."
  },
  {
    "objectID": "09-data-splitting.html#additional-resources",
    "href": "09-data-splitting.html#additional-resources",
    "title": "Chapter 9: Data Splitting and Resampling - The Foundation of Model Validation",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nrsample Documentation\nCross-Validation: The Right and Wrong Way\nNested Resampling Tutorial\nTime Series Cross-Validation\nBootstrap Methods and Their Application"
  },
  {
    "objectID": "quiz-block-2.html",
    "href": "quiz-block-2.html",
    "title": "Block 2 Assessment: Tidymodels and Machine Learning Foundations",
    "section": "",
    "text": "Progress\n    Score: 0/15\n    \n      \n    \n    \n      0 questions answered\n    \n  \n\n  \n    Block 2 Assessment: Tidymodels and Machine Learning Foundations\n  \n\n  \n    Instructions\n    This quiz covers the tidymodels concepts from Block 2 (Chapters 8-13). Click on your chosen answer for each question. You'll receive immediate feedback with explanations for incorrect options.\n  \n\n  \n    1. What is the main cause of overfitting in machine learning?\n    \n      The learning rate is too high\n      The dataset is too small\n      The model is too simple for the data\n      The model learns patterns specific to the training data, including noise\n    \n    \n  \n\n  \n    2. Which type of data splitting would you use for a highly imbalanced classification dataset?\n    \n      Random splitting\n      Stratified splitting\n      Time-based splitting\n      Bootstrap splitting\n    \n    \n  \n\n  \n    3. What is the main advantage of k-fold cross-validation over a single train-test split?\n    \n      It provides more reliable estimates of model performance\n      It requires less data\n      It's faster to compute\n      It prevents overfitting completely\n    \n    \n  \n\n  \n    4. In the tidymodels framework, what is the purpose of a 'recipe'?\n    \n      To evaluate model performance\n      To set hyperparameter values\n      To specify which model algorithm to use\n      To define data preprocessing and feature engineering steps\n    \n    \n  \n\n  \n    5. Which step would you use in a recipe to handle categorical variables with too many levels?\n    \n      step_normalize()\n      step_dummy()\n      step_other()\n      step_pca()\n    \n    \n  \n\n  \n    6. What is the main philosophy of the parsnip package?\n    \n      To provide a unified interface for different modeling engines\n      To handle missing values in datasets\n      To automatically select the best model\n      To create faster modeling algorithms\n    \n    \n  \n\n  \n    7. In parsnip, what does setting the 'mode' of a model specification do?\n    \n      It specifies the data format\n      It determines whether the model is for regression or classification\n      It sets the hyperparameter values\n      It selects the computational engine\n    \n    \n  \n\n  \n    8. What is the main benefit of using workflows in tidymodels?\n    \n      Workflows automatically tune hyperparameters\n      Workflows eliminate the need for cross-validation\n      Workflows make models run faster\n      Workflows bundle preprocessing and modeling to ensure consistency\n    \n    \n  \n\n  \n    9. Which yardstick metric is specifically designed for classification problems?\n    \n      rmse()\n      rsq()\n      roc_auc()\n      mae()\n    \n    \n  \n\n  \n    10. What is the difference between parameters and hyperparameters?\n    \n      Parameters are numeric, hyperparameters are categorical\n      Parameters are learned from data, hyperparameters are set before training\n      There is no difference, they are synonymous\n      Parameters are for classification, hyperparameters are for regression\n    \n    \n  \n\n  \n    11. What does the tune() function indicate in a model specification?\n    \n      The parameter should be manually specified by the user\n      The parameter should be removed from the model\n      The parameter should be optimized during tuning\n      The parameter should be fixed at its default value\n    \n    \n  \n\n  \n    12. Which tuning approach explores the hyperparameter space most systematically?\n    \n      Grid search\n      Random search\n      Bayesian optimization\n      Simulated annealing\n    \n    \n  \n\n  \n    13. What is the main purpose of the initial_split() function?\n    \n      To divide data into training and testing sets\n      To separate features from the target variable\n      To split categorical variables into dummy variables\n      To create cross-validation folds\n    \n    \n  \n\n  \n    14. In the bias-variance tradeoff, what typically happens when you increase model complexity?\n    \n      Bias increases and variance decreases\n      Bias decreases and variance increases\n      Both bias and variance decrease\n      Both bias and variance increase\n    \n    \n  \n\n  \n    15. What is the recommended way to select the final hyperparameters after tuning?\n    \n      Choose the values that perform best on cross-validation\n      Choose the values that minimize training error\n      Choose the values that are closest to the defaults\n      Choose the values randomly from the tested range\n    \n    \n  \n\n  \n    Quiz Complete!\n    \n    \n    \n    \n      Retake Quiz\n      \n        Continue to Chapter 14: Classification"
  },
  {
    "objectID": "quiz-block-1.html",
    "href": "quiz-block-1.html",
    "title": "Block 1 Assessment: R and Tidyverse Fundamentals",
    "section": "",
    "text": "Progress\n    Score: 0/15\n    \n      \n    \n    \n      0 questions answered\n    \n  \n\n  \n    Block 1 Assessment: R and Tidyverse Fundamentals\n  \n\n  \n    Instructions\n    This quiz covers the fundamental concepts from Block 1 (Chapters 1-7). Click on your chosen answer for each question. You'll receive immediate feedback with explanations for incorrect options.\n  \n\n  \n    1. What are the three fundamental rules of tidy data?\n    \n      Each variable forms a column, each observation forms a row, each cell contains a value\n      Data should be in wide format, column names should be descriptive, no missing values allowed\n      Use factors for categorical data, dates should be character strings, all numeric data should be integers\n      Sort by the first column, use consistent naming, remove duplicates\n    \n    \n  \n\n  \n    2. Which pipe operator is recommended for new code in R 4.1+?\n    \n      -&gt; (assignment pipe)\n      %&gt;% (magrittr pipe)\n      |&gt; (native R pipe)\n      %&lt;% (reverse pipe)\n    \n    \n  \n\n  \n    3. What does the dplyr::filter() function do?\n    \n      Selects specific columns from a dataset\n      Keeps rows that match specified conditions\n      Sorts the data in ascending order\n      Groups the data by categories\n    \n    \n  \n\n  \n    4. In ggplot2, what does the aes() function define?\n    \n      The statistical transformation to apply\n      The color theme of the plot\n      The size of the plot window\n      The aesthetic mappings between data variables and visual properties\n    \n    \n  \n\n  \n    5. What is the main difference between pivot_longer() and pivot_wider()?\n    \n      pivot_longer() converts wide data to long format, pivot_wider() does the opposite\n      pivot_longer() is for numeric data, pivot_wider() is for categorical data\n      pivot_longer() removes missing values, pivot_wider() keeps them\n      They are identical functions with different names\n    \n    \n  \n\n  \n    6. Which readr function would you use to import a CSV file with custom column types?\n    \n      import_csv() with type specifications\n      load_csv() with custom parameters\n      read_csv() with the col_types argument\n      read.csv() with stringsAsFactors = FALSE\n    \n    \n  \n\n  \n    7. What does the purrr::map() function return?\n    \n      Returns the same type as the input\n      Always returns a vector\n      Always returns a data frame\n      Always returns a list\n    \n    \n  \n\n  \n    8. In dplyr, what is the purpose of group_by()?\n    \n      To sort data by groups\n      To split data into groups for subsequent operations\n      To create new grouping variables\n      To filter out groups with few observations\n    \n    \n  \n\n  \n    9. Which stringr function would you use to detect if a pattern exists in strings?\n    \n      str_count()\n      str_detect()\n      str_locate()\n      str_extract()\n    \n    \n  \n\n  \n    10. What is the main advantage of using tibbles over data.frames?\n    \n      Tibbles provide better printing, preserve data types, and have stricter subset behavior\n      Tibbles automatically remove missing values\n      Tibbles can only contain numeric data\n      Tibbles are faster for large datasets\n    \n    \n  \n\n  \n    11. What type of join would you use to keep all rows from both tables, regardless of matches?\n    \n      inner_join()\n      left_join()\n      right_join()\n      full_join()\n    \n    \n  \n\n  \n    12. In lubridate, which function would you use to extract the year from a date?\n    \n      date_year()\n      extract_year()\n      year()\n      get_year()\n    \n    \n  \n\n  \n    13. What does the tidyr::separate() function do?\n    \n      Splits one column into multiple columns\n      Removes rows with missing values\n      Creates separate datasets from groups\n      Divides numeric values by a constant\n    \n    \n  \n\n  \n    14. Which ggplot2 function would you use to create subplots based on a categorical variable?\n    \n      split_plot()\n      group_plot()\n      facet_wrap() or facet_grid()\n      geom_subplot()\n    \n    \n  \n\n  \n    15. What is the main purpose of the purrr::safely() function?\n    \n      To make functions run faster\n      To handle errors gracefully and return results with error information\n      To validate input data types\n      To create backup copies of data\n    \n    \n  \n\n  \n    Quiz Complete!\n    \n    \n    \n    \n      Retake Quiz\n      \n        Continue to Chapter 8: Tidymodels"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tidyverse & Machine Learning Workshop",
    "section": "",
    "text": "This comprehensive workshop provides a complete journey through data science in R, from basic data manipulation with the tidyverse to advanced machine learning with tidymodels. Whether you’re new to R or looking to expand your skills, this course offers practical, hands-on learning with real-world datasets.\n\n\n\n\n\nMaster the core packages of the tidyverse ecosystem:\n\nData Import & Export: Read and write various file formats efficiently\nData Wrangling: Transform and manipulate data with dplyr\nData Tidying: Reshape and organize messy data with tidyr\nData Visualization: Create publication-ready plots with ggplot2\nFunctional Programming: Apply functions efficiently with purrr\nText & Date Processing: Handle strings and temporal data\n\n\n\n\nBuild a solid foundation in modern machine learning workflows:\n\nTidymodels Ecosystem: Understand the framework and its components\nData Preparation: Split data and create resampling schemes\nFeature Engineering: Preprocess data with recipes\nModel Building: Specify and train models with parsnip\nWorkflows: Create reproducible modeling pipelines\nModel Tuning: Optimize hyperparameters systematically\n\n\n\n\nApply your skills to real-world problems:\n\nClassification: Build and evaluate classification models\nRegression: Develop predictive regression models\nEnsemble Methods: Combine models for better performance\nUnsupervised Learning: Discover patterns with clustering and PCA\nModel Deployment: Put models into production\n\n\n\n\n\n\nBasic R knowledge is helpful but not required\nRStudio or another R IDE installed\nInternet connection for package installation\n\n\n\n\nTo get started with this workshop, you’ll need to install several R packages. Run the following code to install everything you need:\n\n# Core tidyverse packages\ninstall.packages(\"tidyverse\")\n\n# Tidymodels packages\ninstall.packages(\"tidymodels\")\n\n# Additional packages for specific chapters\ninstall.packages(c(\n  # Datasets\n  \"palmerpenguins\",   # Penguin dataset\n  \"gapminder\",        # Gapminder dataset\n  \"nycflights13\",     # Flights dataset\n  \"modeldata\",        # Additional datasets\n  \n  # Visualization and exploration\n  \"corrplot\",         # Correlation plots\n  \"viridis\",          # Color palettes\n  \"GGally\",           # Extended ggplot2\n  \"skimr\",            # Data summaries\n  \"plotly\",           # Interactive plots\n  \"patchwork\",        # Combine plots\n  \n  # Data manipulation\n  \"janitor\",          # Data cleaning\n  \"lubridate\",        # Date handling\n  \"stringr\",          # String manipulation\n  \"forcats\",          # Factor handling\n  \"readxl\",           # Read Excel files\n  \"haven\",            # Read SPSS/SAS/Stata\n  \"jsonlite\",         # JSON handling\n  \"xml2\",             # XML handling\n  \"httr\",             # HTTP requests\n  \"rvest\",            # Web scraping\n  \"DBI\",              # Database interface\n  \"RSQLite\",          # SQLite\n  \n  # Machine learning models\n  \"ranger\",           # Random forests\n  \"xgboost\",          # Gradient boosting\n  \"tidyclust\",        # Clustering for tidymodels\n  \"factoextra\",       # Visualization for clustering\n  \"glmnet\",           # Regularized regression\n  \"kknn\",             # K-nearest neighbors\n  \"kernlab\",          # SVM\n  \"discrim\",          # Discriminant analysis\n  \"baguette\",         # Bagging\n  \"earth\",            # MARS models\n  \"rules\",            # Rule-based models\n  \"naivebayes\",       # Naive Bayes\n  \"klaR\",             # Classification\n  \"mda\",              # Mixture discriminant\n  \"poissonreg\",       # Poisson regression\n  \"nnet\",             # Neural networks\n  \"lightgbm\",         # LightGBM\n  \"dbarts\",           # BART\n  \n  # Feature engineering\n  \"textrecipes\",      # Text preprocessing\n  \"themis\",           # Class imbalance\n  \"embed\",            # Embeddings\n  \"bestNormalize\",    # Normalization\n  \"tidytext\",         # Text mining\n  \"stopwords\",        # Stop words\n  \"moments\",          # Statistical moments\n  \n  # Performance and parallel processing\n  \"microbenchmark\",   # Performance benchmarking\n  \"furrr\",            # Parallel purrr functions\n  \"future\",           # Parallel processing backend\n  \n  # Model evaluation and deployment\n  \"vip\",              # Variable importance\n  \"broom\",            # Model tidying\n  \"yardstick\",        # Model metrics\n  \"probably\",         # Uncertainty estimation\n  \"stacks\",           # Model stacking\n  \"vetiver\",          # Model deployment\n  \"plumber\",          # API creation\n  \"pins\",             # Model versioning\n  \"butcher\",          # Model size reduction\n  \"bundle\",           # Model bundling\n  \n  # Advanced tidymodels\n  \"agua\",             # H2O integration\n  \"bonsai\",           # Flexible models\n  \"finetune\",         # Advanced tuning\n  \"sda\",              # Shrinkage discriminant\n  \"sparsediscrim\",    # Sparse discriminant\n  \"workflowsets\",     # Multiple workflows\n  \n  # Additional packages\n  \"shiny\",            # Web apps\n  \"shinydashboard\",   # Dashboard UI\n  \"DT\",               # Interactive tables\n  \"cluster\",          # Clustering algorithms\n  \"Rtsne\",            # t-SNE\n  \"dbscan\",           # DBSCAN clustering\n  \"isotree\",          # Isolation forest\n  \"arules\",           # Association rules\n  \"arulesViz\",        # Visualize rules\n  \"doParallel\",       # Parallel backend\n  \"dtplyr\",           # dplyr for data.table\n  \"jose\",             # JWT tokens\n  \"logger\",           # Logging\n  \"performance\",      # Model performance\n  \"ggfortify\",        # PCA/clustering plots\n  \"dendextend\",       # Dendrogram manipulation\n  \"scales\",           # Scale formatting\n  \"ggrepel\",          # Better text labels\n  \"hms\"               # Time-of-day values\n))\n\n\n\n\n\nSequential Learning: Chapters build upon each other, especially within each part\nHands-on Practice: Each chapter includes practical examples and exercises\nCode Along: All code is provided and tested - run it yourself!\nExercises: Complete the exercises to reinforce your learning\nProjects: Apply your skills to the capstone projects\n\n\n\n\nReady to begin? Start with Chapter 1: Introduction to the Tidyverse!\n\n\n\n\nTidyverse Documentation\nTidymodels Documentation\nR for Data Science Book\nTidy Modeling with R\n\n\n\n\nThis workshop was made by David Sarrat González and Juan R González from the Bioinformatic Research Group in Epidemiology at the Barcelona Institute for Global Health (ISGlobal) as a comprehensive resource for learning modern data science in R."
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Tidyverse & Machine Learning Workshop",
    "section": "",
    "text": "This comprehensive workshop provides a complete journey through data science in R, from basic data manipulation with the tidyverse to advanced machine learning with tidymodels. Whether you’re new to R or looking to expand your skills, this course offers practical, hands-on learning with real-world datasets."
  },
  {
    "objectID": "index.html#what-youll-learn",
    "href": "index.html#what-youll-learn",
    "title": "Tidyverse & Machine Learning Workshop",
    "section": "",
    "text": "Master the core packages of the tidyverse ecosystem:\n\nData Import & Export: Read and write various file formats efficiently\nData Wrangling: Transform and manipulate data with dplyr\nData Tidying: Reshape and organize messy data with tidyr\nData Visualization: Create publication-ready plots with ggplot2\nFunctional Programming: Apply functions efficiently with purrr\nText & Date Processing: Handle strings and temporal data\n\n\n\n\nBuild a solid foundation in modern machine learning workflows:\n\nTidymodels Ecosystem: Understand the framework and its components\nData Preparation: Split data and create resampling schemes\nFeature Engineering: Preprocess data with recipes\nModel Building: Specify and train models with parsnip\nWorkflows: Create reproducible modeling pipelines\nModel Tuning: Optimize hyperparameters systematically\n\n\n\n\nApply your skills to real-world problems:\n\nClassification: Build and evaluate classification models\nRegression: Develop predictive regression models\nEnsemble Methods: Combine models for better performance\nUnsupervised Learning: Discover patterns with clustering and PCA\nModel Deployment: Put models into production"
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Tidyverse & Machine Learning Workshop",
    "section": "",
    "text": "Basic R knowledge is helpful but not required\nRStudio or another R IDE installed\nInternet connection for package installation"
  },
  {
    "objectID": "index.html#required-packages",
    "href": "index.html#required-packages",
    "title": "Tidyverse & Machine Learning Workshop",
    "section": "",
    "text": "To get started with this workshop, you’ll need to install several R packages. Run the following code to install everything you need:\n\n# Core tidyverse packages\ninstall.packages(\"tidyverse\")\n\n# Tidymodels packages\ninstall.packages(\"tidymodels\")\n\n# Additional packages for specific chapters\ninstall.packages(c(\n  # Datasets\n  \"palmerpenguins\",   # Penguin dataset\n  \"gapminder\",        # Gapminder dataset\n  \"nycflights13\",     # Flights dataset\n  \"modeldata\",        # Additional datasets\n  \n  # Visualization and exploration\n  \"corrplot\",         # Correlation plots\n  \"viridis\",          # Color palettes\n  \"GGally\",           # Extended ggplot2\n  \"skimr\",            # Data summaries\n  \"plotly\",           # Interactive plots\n  \"patchwork\",        # Combine plots\n  \n  # Data manipulation\n  \"janitor\",          # Data cleaning\n  \"lubridate\",        # Date handling\n  \"stringr\",          # String manipulation\n  \"forcats\",          # Factor handling\n  \"readxl\",           # Read Excel files\n  \"haven\",            # Read SPSS/SAS/Stata\n  \"jsonlite\",         # JSON handling\n  \"xml2\",             # XML handling\n  \"httr\",             # HTTP requests\n  \"rvest\",            # Web scraping\n  \"DBI\",              # Database interface\n  \"RSQLite\",          # SQLite\n  \n  # Machine learning models\n  \"ranger\",           # Random forests\n  \"xgboost\",          # Gradient boosting\n  \"tidyclust\",        # Clustering for tidymodels\n  \"factoextra\",       # Visualization for clustering\n  \"glmnet\",           # Regularized regression\n  \"kknn\",             # K-nearest neighbors\n  \"kernlab\",          # SVM\n  \"discrim\",          # Discriminant analysis\n  \"baguette\",         # Bagging\n  \"earth\",            # MARS models\n  \"rules\",            # Rule-based models\n  \"naivebayes\",       # Naive Bayes\n  \"klaR\",             # Classification\n  \"mda\",              # Mixture discriminant\n  \"poissonreg\",       # Poisson regression\n  \"nnet\",             # Neural networks\n  \"lightgbm\",         # LightGBM\n  \"dbarts\",           # BART\n  \n  # Feature engineering\n  \"textrecipes\",      # Text preprocessing\n  \"themis\",           # Class imbalance\n  \"embed\",            # Embeddings\n  \"bestNormalize\",    # Normalization\n  \"tidytext\",         # Text mining\n  \"stopwords\",        # Stop words\n  \"moments\",          # Statistical moments\n  \n  # Performance and parallel processing\n  \"microbenchmark\",   # Performance benchmarking\n  \"furrr\",            # Parallel purrr functions\n  \"future\",           # Parallel processing backend\n  \n  # Model evaluation and deployment\n  \"vip\",              # Variable importance\n  \"broom\",            # Model tidying\n  \"yardstick\",        # Model metrics\n  \"probably\",         # Uncertainty estimation\n  \"stacks\",           # Model stacking\n  \"vetiver\",          # Model deployment\n  \"plumber\",          # API creation\n  \"pins\",             # Model versioning\n  \"butcher\",          # Model size reduction\n  \"bundle\",           # Model bundling\n  \n  # Advanced tidymodels\n  \"agua\",             # H2O integration\n  \"bonsai\",           # Flexible models\n  \"finetune\",         # Advanced tuning\n  \"sda\",              # Shrinkage discriminant\n  \"sparsediscrim\",    # Sparse discriminant\n  \"workflowsets\",     # Multiple workflows\n  \n  # Additional packages\n  \"shiny\",            # Web apps\n  \"shinydashboard\",   # Dashboard UI\n  \"DT\",               # Interactive tables\n  \"cluster\",          # Clustering algorithms\n  \"Rtsne\",            # t-SNE\n  \"dbscan\",           # DBSCAN clustering\n  \"isotree\",          # Isolation forest\n  \"arules\",           # Association rules\n  \"arulesViz\",        # Visualize rules\n  \"doParallel\",       # Parallel backend\n  \"dtplyr\",           # dplyr for data.table\n  \"jose\",             # JWT tokens\n  \"logger\",           # Logging\n  \"performance\",      # Model performance\n  \"ggfortify\",        # PCA/clustering plots\n  \"dendextend\",       # Dendrogram manipulation\n  \"scales\",           # Scale formatting\n  \"ggrepel\",          # Better text labels\n  \"hms\"               # Time-of-day values\n))"
  },
  {
    "objectID": "index.html#how-to-use-this-workshop",
    "href": "index.html#how-to-use-this-workshop",
    "title": "Tidyverse & Machine Learning Workshop",
    "section": "",
    "text": "Sequential Learning: Chapters build upon each other, especially within each part\nHands-on Practice: Each chapter includes practical examples and exercises\nCode Along: All code is provided and tested - run it yourself!\nExercises: Complete the exercises to reinforce your learning\nProjects: Apply your skills to the capstone projects"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Tidyverse & Machine Learning Workshop",
    "section": "",
    "text": "Ready to begin? Start with Chapter 1: Introduction to the Tidyverse!"
  },
  {
    "objectID": "index.html#support-and-resources",
    "href": "index.html#support-and-resources",
    "title": "Tidyverse & Machine Learning Workshop",
    "section": "",
    "text": "Tidyverse Documentation\nTidymodels Documentation\nR for Data Science Book\nTidy Modeling with R"
  },
  {
    "objectID": "index.html#about-this-workshop",
    "href": "index.html#about-this-workshop",
    "title": "Tidyverse & Machine Learning Workshop",
    "section": "",
    "text": "This workshop was made by David Sarrat González and Juan R González from the Bioinformatic Research Group in Epidemiology at the Barcelona Institute for Global Health (ISGlobal) as a comprehensive resource for learning modern data science in R."
  },
  {
    "objectID": "12-workflows-evaluation.html",
    "href": "12-workflows-evaluation.html",
    "title": "Chapter 12: Workflows and Model Evaluation - Building Reproducible ML Pipelines",
    "section": "",
    "text": "By the end of this chapter, you will master:\n\nThe workflow concept and its importance\nCombining preprocessing and modeling\nWorkflow sets for comparing multiple approaches\nComprehensive model evaluation with yardstick\nCustom metrics and metric sets\nVisualizing model performance\nWorkflow extraction and modification\nBest practices for reproducible ML pipelines"
  },
  {
    "objectID": "12-workflows-evaluation.html#learning-objectives",
    "href": "12-workflows-evaluation.html#learning-objectives",
    "title": "Chapter 12: Workflows and Model Evaluation - Building Reproducible ML Pipelines",
    "section": "",
    "text": "By the end of this chapter, you will master:\n\nThe workflow concept and its importance\nCombining preprocessing and modeling\nWorkflow sets for comparing multiple approaches\nComprehensive model evaluation with yardstick\nCustom metrics and metric sets\nVisualizing model performance\nWorkflow extraction and modification\nBest practices for reproducible ML pipelines"
  },
  {
    "objectID": "12-workflows-evaluation.html#why-workflows-matter",
    "href": "12-workflows-evaluation.html#why-workflows-matter",
    "title": "Chapter 12: Workflows and Model Evaluation - Building Reproducible ML Pipelines",
    "section": "Why Workflows Matter",
    "text": "Why Workflows Matter\nImagine you’re baking a complex cake. You wouldn’t just throw ingredients together randomly - you’d follow a recipe that specifies the exact order of operations: mix dry ingredients, cream butter and sugar, combine wet and dry, bake at specific temperature. Machine learning is similar: the order and combination of steps matters immensely.\nA workflow in tidymodels bundles together: 1. Preprocessing (recipes) 2. Model specification (parsnip) 3. Post-processing (if needed)\nThis bundling ensures: - Reproducibility: The exact same steps are applied to new data - Prevention of data leakage: Preprocessing parameters are learned only from training data - Simplicity: One object contains your entire modeling pipeline - Flexibility: Easy to swap components and compare approaches\n\nlibrary(tidymodels)\n\n-- Attaching packages -------------------------------------- tidymodels 1.4.1 --\n\n\nv broom        1.0.10     v recipes      1.3.1 \nv dials        1.4.2      v rsample      1.3.1 \nv dplyr        1.1.4      v tailor       0.1.0 \nv ggplot2      4.0.0      v tidyr        1.3.1 \nv infer        1.0.9      v tune         2.0.0 \nv modeldata    1.5.1      v workflows    1.3.0 \nv parsnip      1.3.3      v workflowsets 1.1.1 \nv purrr        1.1.0      v yardstick    1.3.2 \n\n\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx purrr::discard() masks scales::discard()\nx dplyr::filter()  masks stats::filter()\nx dplyr::lag()     masks stats::lag()\nx recipes::step()  masks stats::step()\n\nlibrary(tidyverse)\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv forcats   1.0.0     v stringr   1.5.2\nv lubridate 1.9.4     v tibble    3.3.0\nv readr     2.1.5     \n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx readr::col_factor() masks scales::col_factor()\nx purrr::discard()    masks scales::discard()\nx dplyr::filter()     masks stats::filter()\nx stringr::fixed()    masks recipes::fixed()\nx dplyr::lag()        masks stats::lag()\nx readr::spec()       masks yardstick::spec()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(modeldata)\nlibrary(vip)\n\n\nAdjuntando el paquete: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(patchwork)\nlibrary(workflowsets)\nlibrary(probably)\n\n\nAdjuntando el paquete: 'probably'\n\nThe following objects are masked from 'package:base':\n\n    as.factor, as.ordered\n\n# Set theme and seed\ntheme_set(theme_minimal())\nset.seed(123)\n\n# Load example data\ndata(ames)\names_split &lt;- initial_split(ames, prop = 0.75, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test &lt;- testing(ames_split)"
  },
  {
    "objectID": "12-workflows-evaluation.html#building-your-first-workflow",
    "href": "12-workflows-evaluation.html#building-your-first-workflow",
    "title": "Chapter 12: Workflows and Model Evaluation - Building Reproducible ML Pipelines",
    "section": "Building Your First Workflow",
    "text": "Building Your First Workflow\nLet’s start by understanding the problem with not using workflows:\n\n# The WRONG way - manual preprocessing\n# This approach is error-prone and can lead to data leakage\n\n# Manual preprocessing on training data\names_train_processed &lt;- ames_train %&gt;%\n  mutate(\n    # Log transform - uses training data statistics\n    Sale_Price_log = log(Sale_Price),\n    # Scaling - WRONG! Uses all training data including validation folds\n    Gr_Liv_Area_scaled = scale(Gr_Liv_Area)[,1]\n  )\n\n# Now we need to remember these transformations for test data\n# And apply them consistently... but what were the scaling parameters?\n\n# The RIGHT way - using workflows\n# Step 1: Create a recipe\names_recipe &lt;- recipe(Sale_Price ~ Gr_Liv_Area + Year_Built + Total_Bsmt_SF + \n                      Neighborhood, \n                      data = ames_train) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors())\n\n# Step 2: Create a model specification\nlm_spec &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\n\n# Step 3: Combine into a workflow\nlm_workflow &lt;- workflow() %&gt;%\n  add_recipe(ames_recipe) %&gt;%\n  add_model(lm_spec)\n\nprint(lm_workflow)\n\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: linear_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n2 Recipe Steps\n\n* step_normalize()\n* step_dummy()\n\n-- Model -----------------------------------------------------------------------\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nThe workflow object now contains everything needed to go from raw data to predictions. This is incredibly powerful for maintaining consistency across training, validation, and deployment."
  },
  {
    "objectID": "12-workflows-evaluation.html#workflow-components",
    "href": "12-workflows-evaluation.html#workflow-components",
    "title": "Chapter 12: Workflows and Model Evaluation - Building Reproducible ML Pipelines",
    "section": "Workflow Components",
    "text": "Workflow Components\n\nAdding and Modifying Components\nWorkflows are modular - you can add, remove, or update components:\n\n# Start with an empty workflow\nbase_workflow &lt;- workflow()\n\n# Add components step by step\nbase_workflow &lt;- base_workflow %&gt;%\n  add_recipe(ames_recipe)\n\nprint(\"After adding recipe:\")\n\n[1] \"After adding recipe:\"\n\nprint(base_workflow)\n\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: None\n\n-- Preprocessor ----------------------------------------------------------------\n2 Recipe Steps\n\n* step_normalize()\n* step_dummy()\n\nbase_workflow &lt;- base_workflow %&gt;%\n  add_model(lm_spec)\n\nprint(\"After adding model:\")\n\n[1] \"After adding model:\"\n\nprint(base_workflow)\n\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: linear_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n2 Recipe Steps\n\n* step_normalize()\n* step_dummy()\n\n-- Model -----------------------------------------------------------------------\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n# You can also update components\nupdated_workflow &lt;- lm_workflow %&gt;%\n  update_model(\n    linear_reg(penalty = 0.01) %&gt;%\n      set_engine(\"glmnet\")\n  )\n\nprint(\"After updating model:\")\n\n[1] \"After updating model:\"\n\nprint(updated_workflow)\n\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: linear_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n2 Recipe Steps\n\n* step_normalize()\n* step_dummy()\n\n-- Model -----------------------------------------------------------------------\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 0.01\n\nComputational engine: glmnet \n\n# Or remove components\nrecipe_only &lt;- lm_workflow %&gt;%\n  remove_model()\n\nprint(\"After removing model:\")\n\n[1] \"After removing model:\"\n\nprint(recipe_only)\n\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: None\n\n-- Preprocessor ----------------------------------------------------------------\n2 Recipe Steps\n\n* step_normalize()\n* step_dummy()\n\n\nThis modularity is essential for: - Experimentation: Quickly try different models with same preprocessing - Model comparison: Keep preprocessing constant while varying models - Debugging: Test components independently\n\n\nFormula vs Recipe Interface\nWorkflows support two interfaces for specifying predictors:\n\n# Method 1: Formula interface (simple, no preprocessing)\nformula_workflow &lt;- workflow() %&gt;%\n  add_formula(Sale_Price ~ Gr_Liv_Area + Overall_Cond) %&gt;%\n  add_model(lm_spec)\n\n# Method 2: Recipe interface (complex preprocessing)\nrecipe_workflow &lt;- workflow() %&gt;%\n  add_recipe(ames_recipe) %&gt;%\n  add_model(lm_spec)\n\n# Method 3: Variables interface (programmatic)\nvars_workflow &lt;- workflow() %&gt;%\n  add_variables(\n    outcomes = Sale_Price,\n    predictors = c(Gr_Liv_Area, Overall_Cond, Neighborhood)\n  ) %&gt;%\n  add_model(lm_spec)\n\n# Compare the approaches\nprint(\"Formula approach:\")\n\n[1] \"Formula approach:\"\n\nformula_workflow\n\n== Workflow ====================================================================\nPreprocessor: Formula\nModel: linear_reg()\n\n-- Preprocessor ----------------------------------------------------------------\nSale_Price ~ Gr_Liv_Area + Overall_Cond\n\n-- Model -----------------------------------------------------------------------\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nprint(\"\\nRecipe approach:\")\n\n[1] \"\\nRecipe approach:\"\n\nrecipe_workflow\n\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: linear_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n2 Recipe Steps\n\n* step_normalize()\n* step_dummy()\n\n-- Model -----------------------------------------------------------------------\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nprint(\"\\nVariables approach:\")\n\n[1] \"\\nVariables approach:\"\n\nvars_workflow\n\n== Workflow ====================================================================\nPreprocessor: Variables\nModel: linear_reg()\n\n-- Preprocessor ----------------------------------------------------------------\nOutcomes: Sale_Price\nPredictors: c(Gr_Liv_Area, Overall_Cond, Neighborhood)\n\n-- Model -----------------------------------------------------------------------\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nChoose based on your needs: - Formula: Quick prototyping, simple models - Recipe: Complex preprocessing, feature engineering - Variables: Programmatic variable selection"
  },
  {
    "objectID": "12-workflows-evaluation.html#fitting-and-predicting-with-workflows",
    "href": "12-workflows-evaluation.html#fitting-and-predicting-with-workflows",
    "title": "Chapter 12: Workflows and Model Evaluation - Building Reproducible ML Pipelines",
    "section": "Fitting and Predicting with Workflows",
    "text": "Fitting and Predicting with Workflows\nThe workflow handles all the complexity of applying transformations consistently:\n\n# Fit the workflow\nlm_fit &lt;- lm_workflow %&gt;%\n  fit(data = ames_train)\n\n# The fitted workflow contains:\n# 1. The prepared recipe (with learned parameters)\n# 2. The fitted model\nprint(lm_fit)\n\n== Workflow [trained] ==========================================================\nPreprocessor: Recipe\nModel: linear_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n2 Recipe Steps\n\n* step_normalize()\n* step_dummy()\n\n-- Model -----------------------------------------------------------------------\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n                                         (Intercept)  \n                                              164586  \n                                         Gr_Liv_Area  \n                                               33020  \n                                          Year_Built  \n                                               16040  \n                                       Total_Bsmt_SF  \n                                               15285  \n                          Neighborhood_College_Creek  \n                                               18650  \n                               Neighborhood_Old_Town  \n                                               -1959  \n                                Neighborhood_Edwards  \n                                              -10541  \n                               Neighborhood_Somerset  \n                                               37339  \n                     Neighborhood_Northridge_Heights  \n                                               89923  \n                                Neighborhood_Gilbert  \n                                                9375  \n                                 Neighborhood_Sawyer  \n                                                2027  \n                         Neighborhood_Northwest_Ames  \n                                                3646  \n                            Neighborhood_Sawyer_West  \n                                                4397  \n                               Neighborhood_Mitchell  \n                                                1085  \n                              Neighborhood_Brookside  \n                                                6919  \n                               Neighborhood_Crawford  \n                                               44017  \n                 Neighborhood_Iowa_DOT_and_Rail_Road  \n                                               -9375  \n                             Neighborhood_Timberland  \n                                               44374  \n                             Neighborhood_Northridge  \n                                               74323  \n                            Neighborhood_Stone_Brook  \n                                              101480  \nNeighborhood_South_and_West_of_Iowa_State_University  \n                                               -8260  \n                            Neighborhood_Clear_Creek  \n                                               15883  \n                         Neighborhood_Meadow_Village  \n\n...\nand 20 more lines.\n\n# Make predictions - automatically applies all preprocessing!\npredictions &lt;- lm_fit %&gt;%\n  predict(ames_test)\n\nhead(predictions)\n\n# A tibble: 6 x 1\n    .pred\n    &lt;dbl&gt;\n1 114373.\n2 155891.\n3 191539.\n4 190392.\n5 270562.\n6 206107.\n\n# Get multiple types of predictions\nall_predictions &lt;- bind_cols(\n  ames_test %&gt;% select(Sale_Price),\n  predict(lm_fit, ames_test),           # Point predictions\n  predict(lm_fit, ames_test, type = \"conf_int\")  # Confidence intervals for lm\n)\n\nhead(all_predictions)\n\n# A tibble: 6 x 4\n  Sale_Price   .pred .pred_lower .pred_upper\n       &lt;int&gt;   &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1     105000 114373.     110063.     118683.\n2     172000 155891.     151593.     160189.\n3     189900 191539.     184896.     198181.\n4     195500 190392.     183751.     197033.\n5     191500 270562.     258711.     282414.\n6     189000 206107.     199435.     212779.\n\n# The workflow ensures consistency\n# These transformations are automatically applied:\n# 1. Log transform of Sale_Price (inverse transformed for predictions)\n# 2. Normalization of numeric predictors\n# 3. Dummy encoding of Neighborhood"
  },
  {
    "objectID": "12-workflows-evaluation.html#workflow-sets-comparing-multiple-approaches",
    "href": "12-workflows-evaluation.html#workflow-sets-comparing-multiple-approaches",
    "title": "Chapter 12: Workflows and Model Evaluation - Building Reproducible ML Pipelines",
    "section": "Workflow Sets: Comparing Multiple Approaches",
    "text": "Workflow Sets: Comparing Multiple Approaches\nOne of the most powerful features is comparing multiple workflows systematically:\n\n# Create multiple preprocessing recipes\nrecipe_simple &lt;- recipe(Sale_Price ~ Gr_Liv_Area + Year_Built, \n                       data = ames_train)\n\nrecipe_normalized &lt;- recipe(Sale_Price ~ Gr_Liv_Area + Year_Built + Total_Bsmt_SF, \n                           data = ames_train) %&gt;%\n  step_normalize(all_numeric_predictors())\n\nrecipe_complex &lt;- recipe(Sale_Price ~ Gr_Liv_Area + Year_Built + \n                         Neighborhood + Total_Bsmt_SF + Garage_Cars + \n                         First_Flr_SF + Full_Bath, data = ames_train) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_other(all_nominal_predictors(), threshold = 0.05) %&gt;%\n  step_dummy(all_nominal_predictors())\n\n# Create multiple model specifications\nmodels &lt;- list(\n  lm = linear_reg() %&gt;% set_engine(\"lm\"),\n  ridge = linear_reg(penalty = 0.1, mixture = 0) %&gt;% set_engine(\"glmnet\"),\n  lasso = linear_reg(penalty = 0.1, mixture = 1) %&gt;% set_engine(\"glmnet\"),\n  tree = decision_tree(tree_depth = 10) %&gt;% \n    set_engine(\"rpart\") %&gt;% \n    set_mode(\"regression\")\n)\n\n# Create workflow set\nworkflow_set &lt;- workflow_set(\n  preproc = list(\n    simple = recipe_simple,\n    normalized = recipe_normalized\n  ),\n  models = models\n)\n\nprint(workflow_set)\n\n# A workflow set/tibble: 8 x 4\n  wflow_id         info             option    result    \n  &lt;chr&gt;            &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 simple_lm        &lt;tibble [1 x 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 simple_ridge     &lt;tibble [1 x 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 simple_lasso     &lt;tibble [1 x 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 simple_tree      &lt;tibble [1 x 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n5 normalized_lm    &lt;tibble [1 x 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n6 normalized_ridge &lt;tibble [1 x 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n7 normalized_lasso &lt;tibble [1 x 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n8 normalized_tree  &lt;tibble [1 x 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\n# This creates 8 workflows (2 recipes × 4 models)!\n\n\nEvaluating Workflow Sets\nNow we can evaluate all workflows systematically:\n\n# Create resamples for evaluation\names_folds &lt;- vfold_cv(ames_train, v = 5, strata = Sale_Price)\n\n# Evaluate workflows individually to avoid errors\n# We'll evaluate just a subset for demonstration\nsimple_lm &lt;- workflow() %&gt;%\n  add_recipe(recipe_simple) %&gt;%\n  add_model(linear_reg() %&gt;% set_engine(\"lm\"))\n\nsimple_lm_results &lt;- simple_lm %&gt;%\n  fit_resamples(\n    resamples = ames_folds,\n    metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq, yardstick::mae)\n  )\n\n# Show metrics\ncollect_metrics(simple_lm_results)\n\n# A tibble: 3 x 6\n  .metric .estimator      mean     n   std_err .config        \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;          \n1 mae     standard   31681.        5  727.     pre0_mod0_post0\n2 rmse    standard   46911.        5 2520.     pre0_mod0_post0\n3 rsq     standard       0.655     5    0.0192 pre0_mod0_post0\n\n# Visualize performance\nsimple_lm_results %&gt;%\n  collect_metrics() %&gt;%\n  ggplot(aes(x = .metric, y = mean)) +\n  geom_col(fill = \"steelblue\") +\n  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), width = 0.2) +\n  facet_wrap(~.metric, scales = \"free\") +\n  labs(title = \"Model Performance with Cross-Validation\",\n       subtitle = \"Simple linear model with 5-fold CV\")\n\n\n\n\n\n\n\n\nThis systematic comparison helps identify: - Which preprocessing steps add value - Which models work best with your data - Interaction effects between preprocessing and models"
  },
  {
    "objectID": "12-workflows-evaluation.html#model-evaluation-with-yardstick",
    "href": "12-workflows-evaluation.html#model-evaluation-with-yardstick",
    "title": "Chapter 12: Workflows and Model Evaluation - Building Reproducible ML Pipelines",
    "section": "Model Evaluation with yardstick",
    "text": "Model Evaluation with yardstick\nThe yardstick package provides comprehensive metrics for model evaluation:\n\nRegression Metrics\n\n# Fit our simple workflow\nbest_fit &lt;- simple_lm %&gt;%\n  fit(ames_train)\n\n# Get test predictions\ntest_predictions &lt;- best_fit %&gt;%\n  predict(ames_test) %&gt;%\n  bind_cols(ames_test %&gt;% select(Sale_Price))\n\n# Calculate multiple metrics\nregression_metrics &lt;- metric_set(\n  rmse,      # Root Mean Squared Error\n  mae,       # Mean Absolute Error\n  mape,      # Mean Absolute Percentage Error\n  rsq,       # R-squared\n  ccc        # Concordance Correlation Coefficient\n)\n\ntest_performance &lt;- test_predictions %&gt;%\n  regression_metrics(truth = Sale_Price, estimate = .pred)\n\nprint(test_performance)\n\n# A tibble: 5 x 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard   45366.   \n2 mae     standard   31162.   \n3 mape    standard      17.6  \n4 rsq     standard       0.690\n5 ccc     standard       0.804\n\n# Visualize predictions vs actual\nggplot(test_predictions, aes(x = Sale_Price, y = .pred)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(\n    title = \"Predicted vs Actual Sale Prices\",\n    subtitle = paste(\"Test RMSE:\", round(test_performance$.estimate[1], 2)),\n    x = \"Actual Sale Price\",\n    y = \"Predicted Sale Price\"\n  ) +\n  coord_equal()\n\n\n\n\n\n\n\n\n\n\nClassification Metrics\nLet’s also explore classification metrics:\n\n# Create a classification problem\names_class &lt;- ames_train %&gt;%\n  mutate(expensive = factor(if_else(Sale_Price &gt; median(Sale_Price), \n                                    \"yes\", \"no\"))) %&gt;%\n  select(-Sale_Price)\n\n# Simple classification workflow\nclass_recipe &lt;- recipe(expensive ~ Gr_Liv_Area + Overall_Cond + Year_Built, \n                      data = ames_class) %&gt;%\n  step_normalize(all_numeric_predictors())\n\nclass_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\")\n\nclass_workflow &lt;- workflow() %&gt;%\n  add_recipe(class_recipe) %&gt;%\n  add_model(class_spec)\n\n# Fit and predict\nclass_split &lt;- initial_split(ames_class, strata = expensive)\nclass_train &lt;- training(class_split)\nclass_test &lt;- testing(class_split)\n\nclass_fit &lt;- class_workflow %&gt;%\n  fit(class_train)\n\nclass_predictions &lt;- bind_cols(\n  class_test %&gt;% select(expensive),\n  predict(class_fit, class_test),\n  predict(class_fit, class_test, type = \"prob\")\n)\n\n# Classification metrics\nclass_metrics &lt;- metric_set(\n  accuracy,\n  precision,\n  recall,\n  f_meas,\n  roc_auc,\n  pr_auc\n)\n\nclass_performance &lt;- class_predictions %&gt;%\n  class_metrics(truth = expensive, estimate = .pred_class, .pred_yes)\n\nprint(class_performance)\n\n# A tibble: 6 x 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy  binary        0.885 \n2 precision binary        0.902 \n3 recall    binary        0.865 \n4 f_meas    binary        0.883 \n5 roc_auc   binary        0.0380\n6 pr_auc    binary        0.310 \n\n# Confusion matrix\nconf_matrix &lt;- class_predictions %&gt;%\n  conf_mat(truth = expensive, estimate = .pred_class)\n\nautoplot(conf_matrix, type = \"heatmap\") +\n  labs(title = \"Confusion Matrix\")\n\n\n\n\n\n\n\n# ROC curve\nroc_curve_data &lt;- class_predictions %&gt;%\n  roc_curve(truth = expensive, .pred_yes)\n\nautoplot(roc_curve_data) +\n  labs(title = \"ROC Curve\") +\n  annotate(\"text\", x = 0.5, y = 0.5, \n           label = paste(\"AUC:\", round(class_performance$.estimate[5], 3)))"
  },
  {
    "objectID": "12-workflows-evaluation.html#custom-metrics-and-evaluation",
    "href": "12-workflows-evaluation.html#custom-metrics-and-evaluation",
    "title": "Chapter 12: Workflows and Model Evaluation - Building Reproducible ML Pipelines",
    "section": "Custom Metrics and Evaluation",
    "text": "Custom Metrics and Evaluation\nSometimes you need custom metrics for your specific problem:\n\n# Create a custom metric - Mean Absolute Percentage Error with threshold\nmape_vec_threshold &lt;- function(truth, estimate, threshold = 0.2, na_rm = TRUE) {\n  errors &lt;- abs((truth - estimate) / truth)\n  errors[errors &gt; threshold] &lt;- threshold  # Cap errors at threshold\n  mean(errors, na.rm = na_rm)\n}\n\n# Use the custom metric directly\ncustom_mape &lt;- test_predictions %&gt;%\n  summarise(\n    custom_mape = mape_vec_threshold(Sale_Price, .pred),\n    regular_mape = mean(abs((Sale_Price - .pred) / Sale_Price))\n  )\n\nprint(custom_mape)\n\n# A tibble: 1 x 2\n  custom_mape regular_mape\n        &lt;dbl&gt;        &lt;dbl&gt;\n1       0.127        0.176\n\n# Create a business-specific metric\n# For example: penalize underestimation more than overestimation\nasymmetric_loss &lt;- function(truth, estimate, under_weight = 2) {\n  errors &lt;- truth - estimate\n  result &lt;- ifelse(errors &gt; 0, \n                   under_weight * errors^2,  # Underestimation penalty\n                   errors^2)                  # Overestimation penalty\n  sqrt(mean(result))\n}\n\n# Apply custom metric\ntest_predictions %&gt;%\n  mutate(\n    asymmetric_rmse = asymmetric_loss(Sale_Price, .pred)\n  ) %&gt;%\n  summarise(\n    regular_rmse = rmse_vec(Sale_Price, .pred),\n    asymmetric_rmse = mean(asymmetric_rmse)\n  )\n\n# A tibble: 1 x 2\n  regular_rmse asymmetric_rmse\n         &lt;dbl&gt;           &lt;dbl&gt;\n1       45366.          59141."
  },
  {
    "objectID": "12-workflows-evaluation.html#advanced-workflow-techniques",
    "href": "12-workflows-evaluation.html#advanced-workflow-techniques",
    "title": "Chapter 12: Workflows and Model Evaluation - Building Reproducible ML Pipelines",
    "section": "Advanced Workflow Techniques",
    "text": "Advanced Workflow Techniques\n\nExtracting and Modifying Fitted Workflows\n\n# Extract components from fitted workflow\nextracted_recipe &lt;- lm_fit %&gt;%\n  extract_recipe()\n\nextracted_model &lt;- lm_fit %&gt;%\n  extract_fit_parsnip()\n\n# Get preprocessing results\npreprocessed_data &lt;- lm_fit %&gt;%\n  extract_recipe() %&gt;%\n  bake(new_data = ames_test)\n\nhead(preprocessed_data)\n\n# A tibble: 6 x 32\n  Gr_Liv_Area Year_Built Total_Bsmt_SF Sale_Price Neighborhood_College_Creek\n        &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;      &lt;int&gt;                      &lt;dbl&gt;\n1      -1.18      -0.353        -0.364     105000                          0\n2      -0.337     -0.452         0.634     172000                          0\n3       0.247      0.836        -0.262     189900                          0\n4       0.198      0.869        -0.266     195500                          0\n5      -0.433      0.671         0.525     191500                          0\n6       0.588      0.902        -0.114     189000                          0\n# i 27 more variables: Neighborhood_Old_Town &lt;dbl&gt;, Neighborhood_Edwards &lt;dbl&gt;,\n#   Neighborhood_Somerset &lt;dbl&gt;, Neighborhood_Northridge_Heights &lt;dbl&gt;,\n#   Neighborhood_Gilbert &lt;dbl&gt;, Neighborhood_Sawyer &lt;dbl&gt;,\n#   Neighborhood_Northwest_Ames &lt;dbl&gt;, Neighborhood_Sawyer_West &lt;dbl&gt;,\n#   Neighborhood_Mitchell &lt;dbl&gt;, Neighborhood_Brookside &lt;dbl&gt;,\n#   Neighborhood_Crawford &lt;dbl&gt;, Neighborhood_Iowa_DOT_and_Rail_Road &lt;dbl&gt;,\n#   Neighborhood_Timberland &lt;dbl&gt;, Neighborhood_Northridge &lt;dbl&gt;, ...\n\n# Extract model coefficients\ncoefficients &lt;- lm_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  tidy()\n\nhead(coefficients)\n\n# A tibble: 6 x 5\n  term                       estimate std.error statistic   p.value\n  &lt;chr&gt;                         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)                 164586.     2214.    74.3   0        \n2 Gr_Liv_Area                  33020.     1003.    32.9   4.40e-193\n3 Year_Built                   16040.     1732.     9.26  4.64e- 20\n4 Total_Bsmt_SF                15285.     1029.    14.9   1.28e- 47\n5 Neighborhood_College_Creek   18650.     3982.     4.68  2.99e-  6\n6 Neighborhood_Old_Town        -1959.     4085.    -0.480 6.32e-  1\n\n# Variable importance (if applicable)\n# For models that support it\nrf_workflow &lt;- workflow() %&gt;%\n  add_recipe(recipe_simple) %&gt;%\n  add_model(rand_forest() %&gt;% set_engine(\"ranger\", importance = \"impurity\") %&gt;% set_mode(\"regression\"))\n\nrf_fit &lt;- rf_workflow %&gt;%\n  fit(ames_train)\n\nrf_importance &lt;- rf_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  vip()\n\nprint(rf_importance)\n\n\n\n\n\n\n\n\n\n\nWorkflow Finalization\nAfter tuning (covered in next chapter), you finalize workflows:\n\n# Example: Finalize a workflow with best parameters\n# This would typically come from tuning\nbest_params &lt;- tibble(\n  penalty = 0.01,\n  mixture = 0.5\n)\n\n# Create tunable workflow\ntunable_spec &lt;- linear_reg(\n  penalty = tune(),\n  mixture = tune()\n) %&gt;%\n  set_engine(\"glmnet\")\n\ntunable_workflow &lt;- workflow() %&gt;%\n  add_recipe(recipe_normalized) %&gt;%\n  add_model(tunable_spec)\n\n# Finalize with best parameters\nfinal_workflow &lt;- tunable_workflow %&gt;%\n  finalize_workflow(best_params)\n\nprint(final_workflow)\n\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: linear_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n1 Recipe Step\n\n* step_normalize()\n\n-- Model -----------------------------------------------------------------------\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 0.01\n  mixture = 0.5\n\nComputational engine: glmnet \n\n# Fit final model on all training data\nfinal_fit &lt;- final_workflow %&gt;%\n  fit(ames_train)\n\n# Last fit - train on all data, evaluate on test\nlast_fit_results &lt;- final_workflow %&gt;%\n  last_fit(split = ames_split, metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq))\n\n# Extract final metrics\nfinal_metrics &lt;- last_fit_results %&gt;%\n  collect_metrics()\n\nprint(final_metrics)\n\n# A tibble: 2 x 4\n  .metric .estimator .estimate .config        \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 rmse    standard   40233.    pre0_mod0_post0\n2 rsq     standard       0.760 pre0_mod0_post0\n\n# Extract final model\nfinal_model &lt;- last_fit_results %&gt;%\n  extract_workflow()"
  },
  {
    "objectID": "12-workflows-evaluation.html#evaluating-model-assumptions",
    "href": "12-workflows-evaluation.html#evaluating-model-assumptions",
    "title": "Chapter 12: Workflows and Model Evaluation - Building Reproducible ML Pipelines",
    "section": "Evaluating Model Assumptions",
    "text": "Evaluating Model Assumptions\nWorkflows make it easy to check model assumptions:\n\n# Residual analysis for regression\nresidual_analysis &lt;- test_predictions %&gt;%\n  mutate(\n    residual = Sale_Price - .pred,\n    std_residual = residual / sd(residual),\n    abs_residual = abs(residual)\n  )\n\n# Residual plots\np1 &lt;- ggplot(residual_analysis, aes(x = .pred, y = residual)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  labs(title = \"Residuals vs Fitted\", x = \"Fitted Values\", y = \"Residuals\")\n\np2 &lt;- ggplot(residual_analysis, aes(sample = std_residual)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  labs(title = \"Q-Q Plot\", x = \"Theoretical Quantiles\", y = \"Standardized Residuals\")\n\np3 &lt;- ggplot(residual_analysis, aes(x = .pred, y = sqrt(abs_residual))) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  labs(title = \"Scale-Location\", x = \"Fitted Values\", y = \"√|Residuals|\")\n\np4 &lt;- ggplot(residual_analysis, aes(x = residual)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", alpha = 0.7) +\n  geom_density(aes(y = after_stat(count)), color = \"red\", linewidth = 1) +\n  labs(title = \"Residual Distribution\", x = \"Residuals\", y = \"Count\")\n\n# Combine plots\n(p1 + p2) / (p3 + p4) +\n  plot_annotation(title = \"Regression Diagnostics\")"
  },
  {
    "objectID": "12-workflows-evaluation.html#probability-calibration",
    "href": "12-workflows-evaluation.html#probability-calibration",
    "title": "Chapter 12: Workflows and Model Evaluation - Building Reproducible ML Pipelines",
    "section": "Probability Calibration",
    "text": "Probability Calibration\nFor classification, we often need well-calibrated probabilities:\n\n# Calibration analysis\ncalibration_data &lt;- class_predictions %&gt;%\n  mutate(\n    prob_bin = cut(.pred_yes, breaks = seq(0, 1, 0.1), include.lowest = TRUE)\n  ) %&gt;%\n  group_by(prob_bin) %&gt;%\n  summarise(\n    mean_predicted = mean(.pred_yes),\n    fraction_positive = mean(expensive == \"yes\"),\n    n = n(),\n    .groups = \"drop\"\n  ) %&gt;%\n  filter(n &gt; 5)  # Remove bins with few observations\n\n# Calibration plot\nggplot(calibration_data, aes(x = mean_predicted, y = fraction_positive)) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_point(aes(size = n), color = \"darkblue\") +\n  geom_line(color = \"darkblue\") +\n  scale_size_continuous(range = c(2, 8)) +\n  labs(\n    title = \"Probability Calibration Plot\",\n    subtitle = \"Well-calibrated models follow the diagonal\",\n    x = \"Mean Predicted Probability\",\n    y = \"Observed Frequency\",\n    size = \"Count\"\n  ) +\n  coord_equal() +\n  xlim(0, 1) + ylim(0, 1)\n\n\n\n\n\n\n\n# Use probably package for calibration\nlibrary(probably)\n\n# Simple calibration visualization\n# We'll just show the calibration plot without the probably package functions\n# that are causing issues\n\n# Create a simple comparison\nggplot(calibration_data, aes(x = mean_predicted, y = fraction_positive)) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\", size = 1) +\n  geom_point(aes(size = n), color = \"darkblue\", alpha = 0.7) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"blue\") +\n  scale_size_continuous(range = c(2, 8)) +\n  labs(\n    title = \"Model Calibration Assessment\",\n    subtitle = \"Points should follow the red diagonal for perfect calibration\",\n    x = \"Mean Predicted Probability\",\n    y = \"Observed Frequency\",\n    size = \"Count\"\n  ) +\n  coord_equal() +\n  xlim(0, 1) + ylim(0, 1) +\n  theme_minimal()"
  },
  {
    "objectID": "12-workflows-evaluation.html#performance-visualization",
    "href": "12-workflows-evaluation.html#performance-visualization",
    "title": "Chapter 12: Workflows and Model Evaluation - Building Reproducible ML Pipelines",
    "section": "Performance Visualization",
    "text": "Performance Visualization\nCreate comprehensive performance visualizations:\n\n# For regression: actual vs predicted with confidence bands\nprediction_plot &lt;- test_predictions %&gt;%\n  ggplot(aes(x = Sale_Price, y = .pred)) +\n  geom_hex(bins = 30) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"blue\") +\n  scale_fill_viridis_c() +\n  labs(\n    title = \"Prediction Accuracy\",\n    subtitle = paste(\"R² =\", round(rsq_vec(test_predictions$Sale_Price, \n                                          test_predictions$.pred), 3)),\n    x = \"Actual Price\",\n    y = \"Predicted Price\"\n  ) +\n  coord_equal()\n\n# Error distribution\nerror_plot &lt;- test_predictions %&gt;%\n  mutate(error = .pred - Sale_Price,\n         pct_error = error / Sale_Price * 100) %&gt;%\n  ggplot(aes(x = pct_error)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Prediction Error Distribution\",\n    subtitle = \"Percentage error\",\n    x = \"Error (%)\",\n    y = \"Count\"\n  )\n\nprediction_plot + error_plot"
  },
  {
    "objectID": "12-workflows-evaluation.html#best-practices-for-workflows",
    "href": "12-workflows-evaluation.html#best-practices-for-workflows",
    "title": "Chapter 12: Workflows and Model Evaluation - Building Reproducible ML Pipelines",
    "section": "Best Practices for Workflows",
    "text": "Best Practices for Workflows\n\n1. Always Use Workflows for Production\n\n# Good practice: Complete workflow\nproduction_workflow &lt;- workflow() %&gt;%\n  add_recipe(\n    recipe(Sale_Price ~ ., data = ames_train) %&gt;%\n      step_impute_median(all_numeric_predictors()) %&gt;%\n      step_impute_mode(all_nominal_predictors()) %&gt;%\n      step_normalize(all_numeric_predictors()) %&gt;%\n      step_dummy(all_nominal_predictors())\n  ) %&gt;%\n  add_model(\n    linear_reg() %&gt;% set_engine(\"lm\")\n  )\n\n# This ensures all preprocessing is contained and reproducible\n\n\n\n2. Version Control Your Workflows\n\n# Save workflow for reproducibility\nsaveRDS(final_fit, \"models/final_workflow_v1.rds\")\n\n# Load and use later\n# loaded_workflow &lt;- readRDS(\"models/final_workflow_v1.rds\")\n# new_predictions &lt;- predict(loaded_workflow, new_data)\n\n\n\n3. Document Your Choices\n\n# Create workflow with documentation\ndocumented_workflow &lt;- workflow() %&gt;%\n  add_recipe(\n    recipe(Sale_Price ~ ., data = ames_train) %&gt;%\n      # Handle missing values before other steps\n      step_impute_median(all_numeric_predictors()) %&gt;%\n      # Normalize for model stability\n      step_normalize(all_numeric_predictors()) %&gt;%\n      # Create dummies for linear model\n      step_dummy(all_nominal_predictors())\n  ) %&gt;%\n  add_model(\n    # Ridge regression to handle multicollinearity\n    linear_reg(penalty = 0.01, mixture = 0) %&gt;%\n      set_engine(\"glmnet\")\n  )"
  },
  {
    "objectID": "12-workflows-evaluation.html#exercises",
    "href": "12-workflows-evaluation.html#exercises",
    "title": "Chapter 12: Workflows and Model Evaluation - Building Reproducible ML Pipelines",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Build a Complete Evaluation Pipeline\nCreate a workflow and comprehensive evaluation:\n\n# Your solution\n# Create a complete evaluation pipeline\neval_recipe &lt;- recipe(Sale_Price ~ Gr_Liv_Area + Year_Built + Overall_Cond + \n                      Neighborhood + Total_Bsmt_SF, data = ames_train) %&gt;%\n  step_log(Sale_Price, skip = TRUE) %&gt;%  # Skip for prediction\n  step_impute_median(all_numeric_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors())\n\neval_spec &lt;- linear_reg(penalty = 0.01, mixture = 0.5) %&gt;%\n  set_engine(\"glmnet\")\n\neval_workflow &lt;- workflow() %&gt;%\n  add_recipe(eval_recipe) %&gt;%\n  add_model(eval_spec)\n\n# Fit with cross-validation\neval_folds &lt;- vfold_cv(ames_train, v = 10, strata = Sale_Price)\n\neval_results &lt;- eval_workflow %&gt;%\n  fit_resamples(\n    resamples = eval_folds,\n    metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq, yardstick::mae, yardstick::mape),\n    control = control_resamples(save_pred = TRUE)\n  )\n\n# Summarize performance\ncollect_metrics(eval_results)\n\n# A tibble: 4 x 6\n  .metric .estimator       mean     n      std_err .config        \n  &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt; &lt;int&gt;        &lt;dbl&gt; &lt;chr&gt;          \n1 mae     standard   180453.       10  696.        pre0_mod0_post0\n2 mape    standard      100.0      10    0.0000425 pre0_mod0_post0\n3 rmse    standard   197162.       10 1400.        pre0_mod0_post0\n4 rsq     standard        0.764    10    0.0157    pre0_mod0_post0\n\n# Get predictions for visualization\neval_predictions &lt;- collect_predictions(eval_results)\n\n# Visualize CV performance\nggplot(eval_predictions, aes(x = Sale_Price, y = .pred)) +\n  geom_point(alpha = 0.1) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\") +\n  facet_wrap(~id, ncol = 5) +\n  labs(title = \"Predictions Across CV Folds\")\n\n\n\n\n\n\n\n\n\n\nExercise 2: Compare Preprocessing Strategies\nEvaluate different preprocessing approaches:\n\n# Your solution\n# Define different preprocessing strategies\npreproc_minimal &lt;- recipe(Sale_Price ~ Gr_Liv_Area + Year_Built, \n                         data = ames_train)\n\npreproc_standard &lt;- recipe(Sale_Price ~ Gr_Liv_Area + Year_Built + Total_Bsmt_SF, \n                          data = ames_train) %&gt;%\n  step_normalize(all_numeric_predictors())\n\npreproc_complex &lt;- recipe(Sale_Price ~ Gr_Liv_Area + Year_Built + \n                         Neighborhood + Total_Bsmt_SF + Garage_Cars, data = ames_train) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_interact(terms = ~ Gr_Liv_Area:Year_Built)\n\n# Create workflows\nstrategies &lt;- list(\n  minimal = preproc_minimal,\n  standard = preproc_standard,\n  complex = preproc_complex\n)\n\n# Same model for all\nmodel_spec &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\n# Create and evaluate workflows\nstrategy_results &lt;- map_df(names(strategies), function(strategy_name) {\n  wf &lt;- workflow() %&gt;%\n    add_recipe(strategies[[strategy_name]]) %&gt;%\n    add_model(model_spec)\n  \n  # Fit and evaluate\n  fit_resamples(\n    wf,\n    resamples = vfold_cv(ames_train, v = 5),\n    metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq)\n  ) %&gt;%\n    collect_metrics() %&gt;%\n    mutate(strategy = strategy_name)\n})\n\n# Compare strategies\nggplot(strategy_results, aes(x = strategy, y = mean, fill = strategy)) +\n  geom_col() +\n  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), width = 0.2) +\n  facet_wrap(~.metric, scales = \"free_y\") +\n  labs(title = \"Preprocessing Strategy Comparison\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nExercise 3: Custom Metrics for Business Goals\nCreate business-specific metrics:\n\n# Your solution\n# Business scenario: Real estate company\n# - Overestimating is bad (disappointed customers)\n# - Underestimating by &lt;5% is acceptable\n# - Underestimating by &gt;5% is very bad (lost opportunity)\n\nbusiness_metric &lt;- function(truth, estimate) {\n  pct_error &lt;- (estimate - truth) / truth * 100\n  \n  penalties &lt;- case_when(\n    pct_error &gt; 0 ~ abs(pct_error) * 2,        # Overestimate penalty\n    pct_error &gt; -5 ~ abs(pct_error) * 0.5,     # Small underestimate\n    TRUE ~ abs(pct_error) * 3                  # Large underestimate penalty\n  )\n  \n  mean(penalties)\n}\n\n# Apply to test predictions\ntest_predictions %&gt;%\n  mutate(\n    business_score = business_metric(Sale_Price, .pred),\n    standard_mape = mape_vec(Sale_Price, .pred)\n  ) %&gt;%\n  summarise(\n    mean_business_score = mean(business_score),\n    mean_standard_mape = mean(standard_mape)\n  )\n\n# A tibble: 1 x 2\n  mean_business_score mean_standard_mape\n                &lt;dbl&gt;              &lt;dbl&gt;\n1                42.4               17.6\n\n# Visualize business metric\ntest_predictions %&gt;%\n  mutate(\n    pct_error = (.pred - Sale_Price) / Sale_Price * 100,\n    error_category = case_when(\n      pct_error &gt; 0 ~ \"Overestimate\",\n      pct_error &gt; -5 ~ \"Small Underestimate\",\n      TRUE ~ \"Large Underestimate\"\n    )\n  ) %&gt;%\n  ggplot(aes(x = pct_error, fill = error_category)) +\n  geom_histogram(bins = 30, alpha = 0.7) +\n  scale_fill_manual(values = c(\n    \"Overestimate\" = \"red\",\n    \"Small Underestimate\" = \"yellow\",\n    \"Large Underestimate\" = \"darkred\"\n  )) +\n  labs(\n    title = \"Business Impact of Prediction Errors\",\n    x = \"Percentage Error\",\n    y = \"Count\",\n    fill = \"Error Category\"\n  )"
  },
  {
    "objectID": "12-workflows-evaluation.html#summary",
    "href": "12-workflows-evaluation.html#summary",
    "title": "Chapter 12: Workflows and Model Evaluation - Building Reproducible ML Pipelines",
    "section": "Summary",
    "text": "Summary\nIn this comprehensive chapter, you’ve mastered:\n✅ Workflow fundamentals - Combining preprocessing and models - Preventing data leakage - Ensuring reproducibility\n✅ Workflow components - Adding and updating recipes/models - Formula vs recipe interfaces - Modular design\n✅ Workflow sets - Comparing multiple approaches - Systematic evaluation - Ranking and selection\n✅ Model evaluation - Comprehensive metrics with yardstick - Custom metrics for business needs - Visualization techniques\n✅ Advanced techniques - Extracting workflow components - Probability calibration - Model diagnostics\nKey takeaways: - Always use workflows for production models - Workflows prevent data leakage automatically - Workflow sets enable systematic comparison - Custom metrics align models with business goals - Proper evaluation is crucial for model trust"
  },
  {
    "objectID": "12-workflows-evaluation.html#whats-next",
    "href": "12-workflows-evaluation.html#whats-next",
    "title": "Chapter 12: Workflows and Model Evaluation - Building Reproducible ML Pipelines",
    "section": "What’s Next?",
    "text": "What’s Next?\nIn Chapter 13, we’ll explore hyperparameter tuning to optimize model performance."
  },
  {
    "objectID": "12-workflows-evaluation.html#additional-resources",
    "href": "12-workflows-evaluation.html#additional-resources",
    "title": "Chapter 12: Workflows and Model Evaluation - Building Reproducible ML Pipelines",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nworkflows Documentation\nworkflowsets Documentation\nyardstick Documentation\nTidy Modeling with R - Workflows Chapter"
  },
  {
    "objectID": "07-strings-dates.html",
    "href": "07-strings-dates.html",
    "title": "Chapter 7: Working with Strings and Dates - Text and Time in the Tidyverse",
    "section": "",
    "text": "By the end of this chapter, you will master:\n\nString manipulation with stringr\nPattern matching with regular expressions\nText cleaning and extraction\nDate and time handling with lubridate\nTime zone management\nDate arithmetic and intervals\nCombining text and temporal data in analyses\nReal-world applications and best practices"
  },
  {
    "objectID": "07-strings-dates.html#learning-objectives",
    "href": "07-strings-dates.html#learning-objectives",
    "title": "Chapter 7: Working with Strings and Dates - Text and Time in the Tidyverse",
    "section": "",
    "text": "By the end of this chapter, you will master:\n\nString manipulation with stringr\nPattern matching with regular expressions\nText cleaning and extraction\nDate and time handling with lubridate\nTime zone management\nDate arithmetic and intervals\nCombining text and temporal data in analyses\nReal-world applications and best practices"
  },
  {
    "objectID": "07-strings-dates.html#why-strings-and-dates-matter",
    "href": "07-strings-dates.html#why-strings-and-dates-matter",
    "title": "Chapter 7: Working with Strings and Dates - Text and Time in the Tidyverse",
    "section": "Why Strings and Dates Matter",
    "text": "Why Strings and Dates Matter\nIn real-world data analysis, you’ll rarely work with perfectly clean numeric data. Most datasets contain: - Text data: Names, addresses, descriptions, categories, IDs - Temporal data: Timestamps, dates, durations, time zones\nThese data types are notoriously tricky to work with because: - Text can be messy, inconsistent, and encoded differently - Dates come in countless formats - Time zones add complexity - Both require special handling for analysis\nThe tidyverse provides powerful, consistent tools to tackle these challenges. Let’s master them!"
  },
  {
    "objectID": "07-strings-dates.html#setup",
    "href": "07-strings-dates.html#setup",
    "title": "Chapter 7: Working with Strings and Dates - Text and Time in the Tidyverse",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.4     v readr     2.1.5\nv forcats   1.0.0     v stringr   1.5.2\nv ggplot2   4.0.0     v tibble    3.3.0\nv lubridate 1.9.4     v tidyr     1.3.1\nv purrr     1.1.0     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)  # For dates and times\nlibrary(stringr)    # For string manipulation (loaded with tidyverse)\nlibrary(hms)        # For time-of-day values\n\n\nAdjuntando el paquete: 'hms'\n\nThe following object is masked from 'package:lubridate':\n\n    hms\n\n# Set seed for reproducibility\nset.seed(123)\n\n# We'll create various example datasets throughout\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "07-strings-dates.html#part-1-string-manipulation-with-stringr",
    "href": "07-strings-dates.html#part-1-string-manipulation-with-stringr",
    "title": "Chapter 7: Working with Strings and Dates - Text and Time in the Tidyverse",
    "section": "Part 1: String Manipulation with stringr",
    "text": "Part 1: String Manipulation with stringr\n\nUnderstanding Strings in R\nStrings (text data) are fundamental to data analysis, but they’re often the messiest part of our data. The stringr package provides a consistent, intuitive interface for string manipulation.\nAll stringr functions: - Start with str_ for easy autocomplete - Take the string as the first argument (pipe-friendly) - Use consistent naming conventions - Handle NA values gracefully\nLet’s start with basic operations:\n\n# Create example strings\nmessy_names &lt;- c(\n  \"  John Smith  \",\n  \"mary jones\",\n  \"ROBERT BROWN\",\n  \"Sarah O'Connor\",\n  \"José García\",\n  NA,\n  \"anne-marie wilson\"\n)\n\n# Basic string information\nstr_length(messy_names)  # Length of each string\n\n[1] 14 10 12 14 25 NA 17\n\nstr_count(messy_names, pattern = \" \")  # Count spaces\n\n[1]  5  1  1  1  1 NA  1\n\n# The power of vectorization - operations work on entire vectors\ndata.frame(\n  original = messy_names,\n  length = str_length(messy_names),\n  n_spaces = str_count(messy_names, \" \"),\n  n_vowels = str_count(messy_names, \"[aeiouAEIOU]\")\n)\n\n                   original length n_spaces n_vowels\n1              John Smith       14        5        2\n2                mary jones     10        1        3\n3              ROBERT BROWN     12        1        3\n4            Sarah O'Connor     14        1        5\n5 Jos&lt;U+00E9&gt; Garc&lt;U+00ED&gt;a     25        1        7\n6                      &lt;NA&gt;     NA       NA       NA\n7         anne-marie wilson     17        1        7\n\n\nNotice how stringr handles NA values gracefully - operations on NA return NA rather than erroring out. This is crucial for real-world data processing.\n\n\nString Transformation\nLet’s clean up our messy names systematically:\n\n# Step-by-step cleaning\ncleaned_names &lt;- messy_names %&gt;%\n  str_trim() %&gt;%                    # Remove leading/trailing whitespace\n  str_squish() %&gt;%                  # Remove extra internal whitespace\n  str_to_title() %&gt;%                # Proper case (Title Case)\n  str_replace_all(\"'\", \"'\")         # Standardize apostrophes\n\n# Compare before and after\ntibble(\n  original = messy_names,\n  cleaned = cleaned_names\n) %&gt;%\n  print(n = 10)\n\n# A tibble: 7 x 2\n  original                    cleaned                  \n  &lt;chr&gt;                       &lt;chr&gt;                    \n1 \"  John Smith  \"            John Smith               \n2 \"mary jones\"                Mary Jones               \n3 \"ROBERT BROWN\"              Robert Brown             \n4 \"Sarah O'Connor\"            Sarah O'connor           \n5 \"Jos&lt;U+00E9&gt; Garc&lt;U+00ED&gt;a\" Jos&lt;U+00e9&gt; Garc&lt;U+00ed&gt;A\n6  &lt;NA&gt;                       &lt;NA&gt;                     \n7 \"anne-marie wilson\"         Anne-Marie Wilson        \n\n# Other case transformations\ncase_examples &lt;- \"The Quick BROWN Fox\"\ntibble(\n  original = case_examples,\n  lower = str_to_lower(case_examples),\n  upper = str_to_upper(case_examples),\n  title = str_to_title(case_examples),\n  sentence = str_to_sentence(case_examples)\n)\n\n# A tibble: 1 x 5\n  original            lower               upper               title     sentence\n  &lt;chr&gt;               &lt;chr&gt;               &lt;chr&gt;               &lt;chr&gt;     &lt;chr&gt;   \n1 The Quick BROWN Fox the quick brown fox THE QUICK BROWN FOX The Quic~ The qui~\n\n\nEach transformation serves a specific purpose: - str_trim(): Removes accidental spaces from data entry - str_squish(): Fixes multiple spaces between words - str_to_title(): Standardizes capitalization for names - str_replace_all(): Fixes encoding issues with special characters\n\n\nPattern Matching and Regular Expressions\nRegular expressions (regex) are powerful pattern-matching tools. While they look cryptic at first, they’re invaluable for text processing.\n\n# Sample text data\nemails &lt;- c(\n  \"john.doe@company.com\",\n  \"alice@university.edu\",\n  \"bob.smith@email.co.uk\",\n  \"invalid-email\",\n  \"another@domain.org\",\n  \"not an email at all\",\n  NA\n)\n\n# Detect patterns\nhas_at &lt;- str_detect(emails, \"@\")  # Simple pattern\nhas_dot_com &lt;- str_detect(emails, \"\\\\.com$\")  # .com at end ($ = end of string)\n\n# Extract patterns\ndomains &lt;- str_extract(emails, \"@[^.]+\\\\.[a-z]+\")  # Extract domain\nusernames &lt;- str_extract(emails, \"^[^@]+\")  # Everything before @\n\n# Create a summary\nemail_analysis &lt;- tibble(\n  email = emails,\n  is_valid = str_detect(emails, \"^[^@]+@[^@]+\\\\.[^@]+$\"),\n  username = usernames,\n  domain = str_remove(domains, \"@\"),\n  tld = str_extract(emails, \"\\\\.[a-z]+$\")\n)\n\nprint(email_analysis)\n\n# A tibble: 7 x 5\n  email                 is_valid username            domain         tld  \n  &lt;chr&gt;                 &lt;lgl&gt;    &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;\n1 john.doe@company.com  TRUE     john.doe            company.com    .com \n2 alice@university.edu  TRUE     alice               university.edu .edu \n3 bob.smith@email.co.uk TRUE     bob.smith           email.co       .uk  \n4 invalid-email         FALSE    invalid-email       &lt;NA&gt;           &lt;NA&gt; \n5 another@domain.org    TRUE     another             domain.org     .org \n6 not an email at all   FALSE    not an email at all &lt;NA&gt;           &lt;NA&gt; \n7 &lt;NA&gt;                  NA       &lt;NA&gt;                &lt;NA&gt;           &lt;NA&gt; \n\n\nLet’s break down these regex patterns: - @ - Literal @ symbol - \\\\. - Literal period (. alone means “any character”) - ^ - Start of string - $ - End of string - [^@]+ - One or more characters that are NOT @ - [a-z]+ - One or more lowercase letters\n\n\nAdvanced Pattern Matching\nLet’s work with more complex patterns:\n\n# Phone numbers in various formats\nphone_numbers &lt;- c(\n  \"(555) 123-4567\",\n  \"555-123-4567\",\n  \"5551234567\",\n  \"555.123.4567\",\n  \"+1 555 123 4567\",\n  \"Call me at 555-1234\",\n  \"invalid\",\n  NA\n)\n\n# Extract digits only\ndigits_only &lt;- str_remove_all(phone_numbers, \"[^0-9]\")\nprint(digits_only)\n\n[1] \"5551234567\"  \"5551234567\"  \"5551234567\"  \"5551234567\"  \"15551234567\"\n[6] \"5551234\"     \"\"            NA           \n\n# Standardize format\nstandardized &lt;- phone_numbers %&gt;%\n  str_remove_all(\"[^0-9]\") %&gt;%  # Remove non-digits\n  str_replace(\"^1\", \"\") %&gt;%      # Remove leading 1\n  str_replace(\"(\\\\d{3})(\\\\d{3})(\\\\d{4})\", \"(\\\\1) \\\\2-\\\\3\")  # Format\n\ntibble(\n  original = phone_numbers,\n  standardized = standardized\n)\n\n# A tibble: 8 x 2\n  original            standardized    \n  &lt;chr&gt;               &lt;chr&gt;           \n1 (555) 123-4567      \"(555) 123-4567\"\n2 555-123-4567        \"(555) 123-4567\"\n3 5551234567          \"(555) 123-4567\"\n4 555.123.4567        \"(555) 123-4567\"\n5 +1 555 123 4567     \"(555) 123-4567\"\n6 Call me at 555-1234 \"5551234\"       \n7 invalid             \"\"              \n8 &lt;NA&gt;                 &lt;NA&gt;           \n\n# Extract structured information\naddresses &lt;- c(\n  \"123 Main St, New York, NY 10001\",\n  \"456 Oak Avenue, Los Angeles, CA 90028\",\n  \"789 Elm Rd, Chicago, IL 60601\",\n  \"321 Pine Street, Houston, TX 77002\"\n)\n\n# Extract components using regex groups\naddress_parts &lt;- str_match(addresses, \n  \"(\\\\d+) ([^,]+), ([^,]+), ([A-Z]{2}) (\\\\d{5})\")\n\ncolnames(address_parts) &lt;- c(\"full\", \"number\", \"street\", \"city\", \"state\", \"zip\")\nas_tibble(address_parts) %&gt;%\n  select(-full)  # Remove the full match column\n\n# A tibble: 4 x 5\n  number street      city        state zip  \n  &lt;chr&gt;  &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;\n1 123    Main St     New York    NY    10001\n2 456    Oak Avenue  Los Angeles CA    90028\n3 789    Elm Rd      Chicago     IL    60601\n4 321    Pine Street Houston     TX    77002\n\n\nThe power of regex groups (parentheses) is that they let us extract multiple pieces of information in one operation. Each group becomes a separate column in the output.\n\n\nString Splitting and Combining\nReal data often requires splitting combined fields or joining separate ones:\n\n# Full names that need splitting\nfull_names &lt;- c(\n  \"Smith, John\",\n  \"Jones, Mary Ann\",\n  \"Brown Jr., Robert\",\n  \"O'Connor, Sarah\",\n  \"García-López, José María\"\n)\n\n# Split on comma and clean up\nname_parts &lt;- str_split_fixed(full_names, \", \", n = 2)\ncolnames(name_parts) &lt;- c(\"last_name\", \"first_name\")\n\nname_df &lt;- as_tibble(name_parts) %&gt;%\n  mutate(\n    # Clean up any extra spaces\n    first_name = str_trim(first_name),\n    last_name = str_trim(last_name),\n    # Create display name\n    display_name = str_c(first_name, \" \", last_name),\n    # Create email-friendly version\n    email_name = str_c(\n      str_to_lower(str_extract(first_name, \"\\\\w+\")),  # First word only\n      \".\",\n      str_to_lower(str_replace_all(last_name, \"[^a-zA-Z]\", \"\")),  # Letters only\n      \"@company.com\"\n    )\n  )\n\nprint(name_df)\n\n# A tibble: 5 x 4\n  last_name                  first_name               display_name    email_name\n  &lt;chr&gt;                      &lt;chr&gt;                    &lt;chr&gt;           &lt;chr&gt;     \n1 Smith                      John                     John Smith      john.smit~\n2 Jones                      Mary Ann                 Mary Ann Jones  mary.jone~\n3 Brown Jr.                  Robert                   Robert Brown J~ robert.br~\n4 O'Connor                   Sarah                    Sarah O'Connor  sarah.oco~\n5 Garc&lt;U+00ED&gt;a-L&lt;U+00F3&gt;pez Jos&lt;U+00E9&gt; Mar&lt;U+00ED&gt;a Jos&lt;U+00E9&gt; Ma~ jos.garcu~\n\n# Combining multiple fields\nproduct_data &lt;- tibble(\n  brand = c(\"Apple\", \"Samsung\", \"Google\"),\n  model = c(\"iPhone\", \"Galaxy\", \"Pixel\"),\n  version = c(\"14 Pro\", \"S23\", \"7a\"),\n  year = c(2022, 2023, 2023)\n)\n\nproduct_data %&gt;%\n  mutate(\n    # Various ways to combine\n    full_name = str_c(brand, model, version, sep = \" \"),\n    sku = str_c(\n      str_to_upper(str_sub(brand, 1, 3)),  # First 3 letters of brand\n      str_extract(version, \"\\\\d+\"),         # Version number\n      year,\n      sep = \"-\"\n    ),\n    marketing_name = str_glue(\"{brand} {model} {version} ({year})\")\n  )\n\n# A tibble: 3 x 7\n  brand   model  version  year full_name           sku         marketing_name   \n  &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;       &lt;glue&gt;           \n1 Apple   iPhone 14 Pro   2022 Apple iPhone 14 Pro APP-14-2022 Apple iPhone 14 ~\n2 Samsung Galaxy S23      2023 Samsung Galaxy S23  SAM-23-2023 Samsung Galaxy S~\n3 Google  Pixel  7a       2023 Google Pixel 7a     GOO-7-2023  Google Pixel 7a ~\n\n\nNotice the different joining functions: - str_c(): Basic concatenation with separator - str_glue(): Template-based joining (like Python f-strings) - paste() and paste0(): Base R alternatives (less consistent)"
  },
  {
    "objectID": "07-strings-dates.html#part-2-date-and-time-with-lubridate",
    "href": "07-strings-dates.html#part-2-date-and-time-with-lubridate",
    "title": "Chapter 7: Working with Strings and Dates - Text and Time in the Tidyverse",
    "section": "Part 2: Date and Time with lubridate",
    "text": "Part 2: Date and Time with lubridate\n\nUnderstanding Temporal Data\nDates and times are deceptively complex: - Different formats (MM/DD/YYYY vs DD/MM/YYYY) - Time zones - Daylight saving time - Leap years and leap seconds - Different calendar systems\nLubridate makes working with dates intuitive and reliable.\n\n# Today's date and current time\ntoday()\n\n[1] \"2025-10-01\"\n\nnow()\n\n[1] \"2025-10-01 12:48:28 CEST\"\n\n# Parse dates from strings - lubridate is smart!\ndate_strings &lt;- c(\n  \"2023-01-15\",\n  \"01/15/2023\",\n  \"15-Jan-2023\",\n  \"January 15, 2023\",\n  \"20230115\"\n)\n\n# Different parsing functions for different formats\nparsed_dates &lt;- tibble(\n  original = date_strings,\n  ymd = ymd(\"2023-01-15\"),\n  mdy = mdy(\"01/15/2023\"),\n  dmy = dmy(\"15-01-2023\"),\n  smart_parse = parse_date_time(date_strings, \n    orders = c(\"ymd\", \"mdy\", \"dmy\", \"Bdy\", \"ymd\"))\n)\n\nprint(parsed_dates)\n\n# A tibble: 5 x 5\n  original         ymd        mdy        dmy        smart_parse        \n  &lt;chr&gt;            &lt;date&gt;     &lt;date&gt;     &lt;date&gt;     &lt;dttm&gt;             \n1 2023-01-15       2023-01-15 2023-01-15 2023-01-15 2023-01-15 00:00:00\n2 01/15/2023       2023-01-15 2023-01-15 2023-01-15 2023-01-15 00:00:00\n3 15-Jan-2023      2023-01-15 2023-01-15 2023-01-15 2023-01-15 00:00:00\n4 January 15, 2023 2023-01-15 2023-01-15 2023-01-15 2023-01-15 00:00:00\n5 20230115         2023-01-15 2023-01-15 2023-01-15 2023-01-15 00:00:00\n\n\nThe genius of lubridate is the intuitive function names: - ymd(): Year-Month-Day - mdy(): Month-Day-Year - dmy(): Day-Month-Year - ymd_hms(): With time included\n\n\nDate Components and Arithmetic\nOnce we have dates, we can extract components and do arithmetic:\n\n# Create a date range\ndates &lt;- seq(ymd(\"2023-01-01\"), ymd(\"2023-12-31\"), by = \"month\")\n\ndate_analysis &lt;- tibble(\n  date = dates,\n  year = year(date),\n  month = month(date),\n  month_name = month(date, label = TRUE, abbr = FALSE),\n  day = day(date),\n  weekday = wday(date, label = TRUE, abbr = FALSE),\n  quarter = quarter(date),\n  week_of_year = week(date),\n  day_of_year = yday(date),\n  is_weekend = wday(date) %in% c(1, 7),  # Sunday = 1, Saturday = 7\n  days_in_month = days_in_month(date)\n)\n\nprint(date_analysis, n = 12)\n\n# A tibble: 12 x 11\n   date        year month month_name   day weekday   quarter week_of_year\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;      &lt;int&gt; &lt;ord&gt;       &lt;int&gt;        &lt;dbl&gt;\n 1 2023-01-01  2023     1 January        1 Sunday          1            1\n 2 2023-02-01  2023     2 February       1 Wednesday       1            5\n 3 2023-03-01  2023     3 March          1 Wednesday       1            9\n 4 2023-04-01  2023     4 April          1 Saturday        2           13\n 5 2023-05-01  2023     5 May            1 Monday          2           18\n 6 2023-06-01  2023     6 June           1 Thursday        2           22\n 7 2023-07-01  2023     7 July           1 Saturday        3           26\n 8 2023-08-01  2023     8 August         1 Tuesday         3           31\n 9 2023-09-01  2023     9 September      1 Friday          3           35\n10 2023-10-01  2023    10 October        1 Sunday          4           40\n11 2023-11-01  2023    11 November       1 Wednesday       4           44\n12 2023-12-01  2023    12 December       1 Friday          4           48\n# i 3 more variables: day_of_year &lt;dbl&gt;, is_weekend &lt;lgl&gt;, days_in_month &lt;int&gt;\n\n# Date arithmetic is intuitive\nreference_date &lt;- ymd(\"2023-06-15\")\n\ntibble(\n  description = c(\n    \"Original date\",\n    \"Plus 30 days\",\n    \"Plus 2 months\",\n    \"Plus 1 year\",\n    \"Next Monday\",\n    \"End of month\",\n    \"Beginning of year\"\n  ),\n  date = c(\n    reference_date,\n    reference_date + days(30),\n    reference_date + months(2),\n    reference_date + years(1),\n    ceiling_date(reference_date, \"week\", week_start = 1),\n    ceiling_date(reference_date, \"month\") - days(1),\n    floor_date(reference_date, \"year\")\n  )\n)\n\n# A tibble: 7 x 2\n  description       date      \n  &lt;chr&gt;             &lt;date&gt;    \n1 Original date     2023-06-15\n2 Plus 30 days      2023-07-15\n3 Plus 2 months     2023-08-15\n4 Plus 1 year       2024-06-15\n5 Next Monday       2023-06-19\n6 End of month      2023-06-30\n7 Beginning of year 2023-01-01\n\n\nKey insights about date arithmetic: - Adding days is straightforward - Adding months/years handles edge cases (e.g., Jan 31 + 1 month = Feb 28/29) - floor_date() and ceiling_date() are perfect for grouping\n\n\nWorking with Time\nTime adds another layer of complexity:\n\n# Different time representations\ntime_examples &lt;- tibble(\n  datetime_string = c(\n    \"2023-06-15 14:30:00\",\n    \"2023-06-15 2:30:00 PM\",\n    \"15/06/2023 14:30\",\n    \"June 15, 2023 2:30 PM\"\n  )\n)\n\n# Parse with different formats\nparsed_times &lt;- time_examples %&gt;%\n  mutate(\n    parsed = parse_date_time(datetime_string, \n      orders = c(\"ymd HMS\", \"ymd IMS p\", \"dmy HM\", \"Bdy IMS p\")),\n    date_only = as_date(parsed),\n    time_only = format(parsed, \"%H:%M:%S\"),\n    hour = hour(parsed),\n    minute = minute(parsed),\n    am_pm = if_else(hour(parsed) &lt; 12, \"AM\", \"PM\")\n  )\n\nprint(parsed_times)\n\n# A tibble: 4 x 7\n  datetime_string    parsed              date_only  time_only  hour minute am_pm\n  &lt;chr&gt;              &lt;dttm&gt;              &lt;date&gt;     &lt;chr&gt;     &lt;int&gt;  &lt;int&gt; &lt;chr&gt;\n1 2023-06-15 14:30:~ 2023-06-15 14:30:00 2023-06-15 14:30:00     14     30 PM   \n2 2023-06-15 2:30:0~ 2023-06-15 14:30:00 2023-06-15 14:30:00     14     30 PM   \n3 15/06/2023 14:30   2015-06-20 23:14:30 2015-06-20 23:14:30     23     14 PM   \n4 June 15, 2023 2:3~ NA                  NA         &lt;NA&gt;         NA     NA &lt;NA&gt; \n\n# Time differences and durations\nstart_time &lt;- ymd_hms(\"2023-06-15 09:00:00\")\nend_time &lt;- ymd_hms(\"2023-06-15 17:30:00\")\n\n# Different ways to express time differences\ntime_diff &lt;- end_time - start_time\nprint(time_diff)  # In seconds by default\n\nTime difference of 8.5 hours\n\n# Convert to different units\ntibble(\n  unit = c(\"seconds\", \"minutes\", \"hours\", \"days\"),\n  value = c(\n    as.numeric(time_diff, units = \"secs\"),\n    as.numeric(time_diff, units = \"mins\"),\n    as.numeric(time_diff, units = \"hours\"),\n    as.numeric(time_diff, units = \"days\")\n  )\n)\n\n# A tibble: 4 x 2\n  unit        value\n  &lt;chr&gt;       &lt;dbl&gt;\n1 seconds 30600    \n2 minutes   510    \n3 hours       8.5  \n4 days        0.354\n\n\n\n\nTime Zones - The Hidden Complexity\nTime zones are often the source of subtle bugs in data analysis:\n\n# Working with time zones\nutc_time &lt;- ymd_hms(\"2023-06-15 12:00:00\", tz = \"UTC\")\n\n# Convert to different time zones\ntime_zones &lt;- tibble(\n  timezone = c(\"UTC\", \"America/New_York\", \"Europe/London\", \n               \"Asia/Tokyo\", \"Australia/Sydney\"),\n  local_time = c(\n    utc_time,\n    with_tz(utc_time, \"America/New_York\"),\n    with_tz(utc_time, \"Europe/London\"),\n    with_tz(utc_time, \"Asia/Tokyo\"),\n    with_tz(utc_time, \"Australia/Sydney\")\n  ),\n  offset_hours = c(0, -4, 1, 9, 10)  # Offset from UTC (varies with DST!)\n)\n\nprint(time_zones)\n\n# A tibble: 5 x 3\n  timezone         local_time          offset_hours\n  &lt;chr&gt;            &lt;dttm&gt;                     &lt;dbl&gt;\n1 UTC              2023-06-15 12:00:00            0\n2 America/New_York 2023-06-15 12:00:00           -4\n3 Europe/London    2023-06-15 12:00:00            1\n4 Asia/Tokyo       2023-06-15 12:00:00            9\n5 Australia/Sydney 2023-06-15 12:00:00           10\n\n# Force_tz vs with_tz\nambiguous_time &lt;- ymd_hms(\"2023-06-15 12:00:00\")  # No timezone specified\n\ntibble(\n  description = c(\n    \"Original (no tz)\",\n    \"with_tz - converts time\",\n    \"force_tz - keeps time, changes zone\"\n  ),\n  result = c(\n    as.character(ambiguous_time),\n    as.character(with_tz(ambiguous_time, \"America/New_York\")),\n    as.character(force_tz(ambiguous_time, \"America/New_York\"))\n  )\n)\n\n# A tibble: 3 x 2\n  description                         result             \n  &lt;chr&gt;                               &lt;chr&gt;              \n1 Original (no tz)                    2023-06-15 12:00:00\n2 with_tz - converts time             2023-06-15 08:00:00\n3 force_tz - keeps time, changes zone 2023-06-15 12:00:00\n\n\nThe difference between with_tz() and force_tz(): - with_tz(): Changes the display timezone (same moment in time) - force_tz(): Changes the timezone interpretation (different moment)\n\n\nIntervals, Durations, and Periods\nLubridate distinguishes between three concepts:\n\n# Intervals: specific start and end times\ninterval_1 &lt;- interval(ymd(\"2023-01-01\"), ymd(\"2023-12-31\"))\ninterval_2 &lt;- interval(ymd(\"2023-06-01\"), ymd(\"2023-08-31\"))\n\n# Check overlap\nint_overlaps(interval_1, interval_2)\n\n[1] TRUE\n\n# Durations: exact time spans (in seconds)\nduration_1 &lt;- ddays(7) + dhours(3) + dminutes(30)\nprint(duration_1)\n\n[1] \"617400s (~1.02 weeks)\"\n\n# Periods: human-friendly time spans\nperiod_1 &lt;- days(7) + hours(3) + minutes(30)\nprint(period_1)\n\n[1] \"7d 3H 30M 0S\"\n\n# The difference matters!\n# Consider daylight saving time\ndst_start &lt;- ymd_hms(\"2023-03-11 00:00:00\", tz = \"America/New_York\")\n\ntibble(\n  description = c(\n    \"Original time\",\n    \"Plus duration (24 hours)\",\n    \"Plus period (1 day)\"\n  ),\n  result = c(\n    dst_start,\n    dst_start + ddays(1),  # Exactly 24 hours later\n    dst_start + days(1)    # Next calendar day at same time\n  )\n)\n\n# A tibble: 3 x 2\n  description              result             \n  &lt;chr&gt;                    &lt;dttm&gt;             \n1 Original time            2023-03-11 00:00:00\n2 Plus duration (24 hours) 2023-03-12 00:00:00\n3 Plus period (1 day)      2023-03-12 00:00:00\n\n\nUse: - Intervals: When you need specific start/end times - Durations: For exact time calculations - Periods: For human-friendly calculations"
  },
  {
    "objectID": "07-strings-dates.html#combining-strings-and-dates-in-real-analysis",
    "href": "07-strings-dates.html#combining-strings-and-dates-in-real-analysis",
    "title": "Chapter 7: Working with Strings and Dates - Text and Time in the Tidyverse",
    "section": "Combining Strings and Dates in Real Analysis",
    "text": "Combining Strings and Dates in Real Analysis\nLet’s work with a realistic example combining both:\n\n# Create a realistic dataset - customer support tickets\nset.seed(123)\nn_tickets &lt;- 500\n\nsupport_tickets &lt;- tibble(\n  ticket_id = sprintf(\"TICK-%06d\", 1:n_tickets),\n  created_at = ymd_hms(\"2023-01-01 00:00:00\") + \n    seconds(runif(n_tickets, 0, 365*24*60*60)),\n  customer_email = str_c(\n    sample(c(\"john\", \"mary\", \"bob\", \"alice\", \"charlie\"), n_tickets, replace = TRUE),\n    sample(1:100, n_tickets, replace = TRUE),\n    \"@\",\n    sample(c(\"gmail.com\", \"yahoo.com\", \"company.com\"), n_tickets, replace = TRUE)\n  ),\n  subject = sample(c(\n    \"Login issue - can't access account\",\n    \"Payment failed - ERROR 402\",\n    \"Question about pricing\",\n    \"Feature request: dark mode\",\n    \"Bug report - app crashes on startup\",\n    \"Refund request #12345\"\n  ), n_tickets, replace = TRUE),\n  priority = sample(c(\"Low\", \"Medium\", \"High\", \"Critical\"), n_tickets, \n                   replace = TRUE, prob = c(0.4, 0.3, 0.2, 0.1)),\n  resolved_at = created_at + hours(round(rexp(n_tickets, rate = 1/24)))  # Exponential resolution time\n) %&gt;%\n  mutate(\n    resolved_at = if_else(runif(n_tickets) &lt; 0.1, NA_POSIXct_, resolved_at)  # 10% unresolved\n  )\n\n# Analyze the tickets\nticket_analysis &lt;- support_tickets %&gt;%\n  mutate(\n    # Extract information from strings\n    issue_type = case_when(\n      str_detect(subject, \"Login|access|password\") ~ \"Authentication\",\n      str_detect(subject, \"Payment|refund|billing\") ~ \"Billing\",\n      str_detect(subject, \"Bug|crash|error\") ~ \"Bug\",\n      str_detect(subject, \"Feature|request\") ~ \"Feature Request\",\n      TRUE ~ \"Other\"\n    ),\n    has_error_code = str_detect(subject, \"ERROR \\\\d+\"),\n    error_code = str_extract(subject, \"ERROR \\\\d+\"),\n    \n    # Extract from email\n    email_domain = str_extract(customer_email, \"@(.+)$\") %&gt;% str_remove(\"@\"),\n    is_corporate = email_domain == \"company.com\",\n    \n    # Date/time analysis\n    created_date = as_date(created_at),\n    created_hour = hour(created_at),\n    created_weekday = wday(created_at, label = TRUE),\n    created_month = month(created_at, label = TRUE),\n    is_business_hours = between(created_hour, 9, 17),\n    is_weekend = wday(created_at) %in% c(1, 7),\n    \n    # Resolution time\n    resolution_time = as.numeric(resolved_at - created_at, units = \"hours\"),\n    is_resolved = !is.na(resolved_at),\n    resolution_category = case_when(\n      is.na(resolution_time) ~ \"Unresolved\",\n      resolution_time &lt; 1 ~ \"&lt; 1 hour\",\n      resolution_time &lt; 4 ~ \"1-4 hours\",\n      resolution_time &lt; 24 ~ \"4-24 hours\",\n      resolution_time &lt; 72 ~ \"1-3 days\",\n      TRUE ~ \"&gt; 3 days\"\n    )\n  )\n\n# Summary statistics\nsummary_stats &lt;- ticket_analysis %&gt;%\n  group_by(issue_type, priority) %&gt;%\n  summarise(\n    count = n(),\n    resolved_pct = mean(is_resolved) * 100,\n    avg_resolution_hours = mean(resolution_time, na.rm = TRUE),\n    median_resolution_hours = median(resolution_time, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(issue_type, priority)\n\nprint(summary_stats, n = 20)\n\n# A tibble: 20 x 6\n   issue_type      priority count resolved_pct avg_resolution_hours\n   &lt;chr&gt;           &lt;chr&gt;    &lt;int&gt;        &lt;dbl&gt;                &lt;dbl&gt;\n 1 Authentication  Critical    12         91.7                 20.8\n 2 Authentication  High        11         90.9                 20.3\n 3 Authentication  Low         28         78.6                 23.9\n 4 Authentication  Medium      29         93.1                 20.4\n 5 Billing         Critical     4         75                   25.7\n 6 Billing         High        18         83.3                 11.6\n 7 Billing         Low         29         96.6                 30.7\n 8 Billing         Medium      22         86.4                 29.2\n 9 Bug             Critical    10        100                   32  \n10 Bug             High        18         83.3                 25.6\n11 Bug             Low         45         97.8                 39.7\n12 Bug             Medium      25         88                   18.6\n13 Feature Request Critical    23        100                   17.7\n14 Feature Request High        27         92.6                 16.6\n15 Feature Request Low         61         88.5                 29.9\n16 Feature Request Medium      50         84                   21.5\n17 Other           Critical    11         72.7                 20.6\n18 Other           High        14         92.9                 26.8\n19 Other           Low         31         96.8                 23.2\n20 Other           Medium      32         84.4                 19.4\n# i 1 more variable: median_resolution_hours &lt;dbl&gt;\n\n\nThis analysis demonstrates: - Extracting categories from free text - Finding patterns in strings - Working with timestamps - Calculating time differences - Combining string and date operations\n\nVisualization of String and Date Patterns\nLet’s visualize our findings:\n\n# Ticket volume over time\np1 &lt;- ticket_analysis %&gt;%\n  count(created_date) %&gt;%\n  ggplot(aes(x = created_date, y = n)) +\n  geom_line(color = \"steelblue\", linewidth = 1) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"red\", linewidth = 0.8) +\n  labs(\n    title = \"Daily Ticket Volume\",\n    subtitle = \"With smoothed trend line\",\n    x = \"Date\",\n    y = \"Number of Tickets\"\n  )\n\n# Hour of day pattern\np2 &lt;- ticket_analysis %&gt;%\n  count(created_hour, is_weekend) %&gt;%\n  ggplot(aes(x = created_hour, y = n, fill = is_weekend)) +\n  geom_col(position = \"dodge\", alpha = 0.8) +\n  scale_fill_manual(values = c(\"FALSE\" = \"steelblue\", \"TRUE\" = \"coral\")) +\n  labs(\n    title = \"Tickets by Hour of Day\",\n    subtitle = \"Weekday vs Weekend patterns\",\n    x = \"Hour of Day\",\n    y = \"Number of Tickets\",\n    fill = \"Weekend\"\n  ) +\n  scale_x_continuous(breaks = seq(0, 23, 3))\n\n# Issue types by priority\np3 &lt;- ticket_analysis %&gt;%\n  count(issue_type, priority) %&gt;%\n  mutate(priority = factor(priority, \n    levels = c(\"Low\", \"Medium\", \"High\", \"Critical\"))) %&gt;%\n  ggplot(aes(x = issue_type, y = n, fill = priority)) +\n  geom_col(position = \"fill\") +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"Issue Types by Priority\",\n    subtitle = \"Proportion of priority levels\",\n    x = \"Issue Type\",\n    y = \"Proportion\",\n    fill = \"Priority\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Resolution time distribution\np4 &lt;- ticket_analysis %&gt;%\n  filter(is_resolved) %&gt;%\n  ggplot(aes(x = resolution_time, fill = priority)) +\n  geom_histogram(bins = 30, alpha = 0.7) +\n  scale_x_log10() +\n  scale_fill_viridis_d() +\n  facet_wrap(~priority) +\n  labs(\n    title = \"Resolution Time Distribution\",\n    subtitle = \"Log scale, by priority\",\n    x = \"Resolution Time (hours, log scale)\",\n    y = \"Count\"\n  ) +\n  theme(legend.position = \"none\")\n\n# Combine plots\nlibrary(patchwork)\n(p1 + p2) / (p3 + p4) +\n  plot_annotation(\n    title = \"Support Ticket Analysis\",\n    subtitle = \"Combining string extraction and temporal patterns\"\n  )"
  },
  {
    "objectID": "07-strings-dates.html#advanced-string-patterns",
    "href": "07-strings-dates.html#advanced-string-patterns",
    "title": "Chapter 7: Working with Strings and Dates - Text and Time in the Tidyverse",
    "section": "Advanced String Patterns",
    "text": "Advanced String Patterns\n\nWorking with Structured Text\nMany datasets contain semi-structured text that needs parsing:\n\n# Log file entries\nlog_entries &lt;- c(\n  \"[2023-06-15 10:30:45] INFO: User john_doe logged in from 192.168.1.100\",\n  \"[2023-06-15 10:31:02] ERROR: Database connection failed - timeout after 30s\",\n  \"[2023-06-15 10:31:15] WARNING: High memory usage detected (85%)\",\n  \"[2023-06-15 10:32:00] INFO: Backup completed successfully\",\n  \"[2023-06-15 10:33:21] ERROR: File not found: /data/config.json\"\n)\n\n# Parse log entries with regex\nparsed_logs &lt;- tibble(log = log_entries) %&gt;%\n  mutate(\n    # Extract timestamp\n    timestamp = str_extract(log, \"\\\\[([^\\\\]]+)\\\\]\") %&gt;% \n      str_remove_all(\"\\\\[|\\\\]\") %&gt;%\n      ymd_hms(),\n    \n    # Extract log level\n    level = str_extract(log, \"INFO|ERROR|WARNING|DEBUG\"),\n    \n    # Extract message\n    message = str_extract(log, \"(?&lt;=: ).*$\"),  # Everything after \": \"\n    \n    # Extract specific patterns\n    username = str_extract(message, \"User (\\\\w+)\") %&gt;% str_remove(\"User \"),\n    ip_address = str_extract(message, \"\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+\"),\n    file_path = str_extract(message, \"/[\\\\w/\\\\.]+\"),\n    duration = str_extract(message, \"\\\\d+s\") %&gt;% str_remove(\"s\") %&gt;% as.numeric()\n  )\n\nparsed_logs %&gt;%\n  select(-log) %&gt;%  # Remove original for display\n  print()\n\n# A tibble: 5 x 7\n  timestamp           level   message     username ip_address file_path duration\n  &lt;dttm&gt;              &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt;\n1 2023-06-15 10:30:45 INFO    User john_~ john_doe 192.168.1~ &lt;NA&gt;            NA\n2 2023-06-15 10:31:02 ERROR   Database c~ &lt;NA&gt;     &lt;NA&gt;       &lt;NA&gt;            30\n3 2023-06-15 10:31:15 WARNING High memor~ &lt;NA&gt;     &lt;NA&gt;       &lt;NA&gt;            NA\n4 2023-06-15 10:32:00 INFO    Backup com~ &lt;NA&gt;     &lt;NA&gt;       &lt;NA&gt;            NA\n5 2023-06-15 10:33:21 ERROR   File not f~ &lt;NA&gt;     &lt;NA&gt;       /data/co~       NA\n\n\n\n\nText Normalization and Cleaning\nReal text data needs extensive cleaning:\n\n# Messy product descriptions\nmessy_products &lt;- c(\n  \"Apple iPhone 14 Pro Max - 256GB - Space Black!!!\",\n  \"samsung galaxy s23 ultra 512gb phantom black\",\n  \"Google Pixel 7 Pro (128 GB) -- Obsidian\",\n  \"OnePlus 11 5G | 16GB RAM | 256GB Storage\",\n  \"  Xiaomi 13 Pro    256GB/12GB    Ceramic Black  \"\n)\n\n# Comprehensive cleaning pipeline\nclean_products &lt;- tibble(original = messy_products) %&gt;%\n  mutate(\n    # Step 1: Basic cleaning\n    cleaned = str_trim(original) %&gt;%\n      str_squish() %&gt;%\n      str_remove_all(\"!!!|\\\\||--\") %&gt;%\n      str_replace_all(\"\\\\s+\", \" \"),\n    \n    # Step 2: Extract components\n    brand = str_extract(cleaned, \"^\\\\w+\") %&gt;% str_to_title(),\n    \n    storage = str_extract(cleaned, \"\\\\d+\\\\s*GB\") %&gt;%\n      str_remove_all(\"\\\\s\") %&gt;%\n      str_to_upper(),\n    \n    ram = str_extract(cleaned, \"\\\\d+GB\\\\s*RAM|RAM\\\\s*\\\\d+GB\") %&gt;%\n      str_extract(\"\\\\d+\") %&gt;%\n      paste0(\"GB\"),\n    \n    color = str_extract(cleaned, \n      \"Space Black|Phantom Black|Obsidian|Ceramic Black\") %&gt;%\n      str_to_title(),\n    \n    # Step 3: Standardized format\n    standardized = str_glue(\"{brand} - {storage} Storage - {color}\")\n  )\n\nclean_products %&gt;%\n  select(original, standardized)\n\n# A tibble: 5 x 2\n  original                                           standardized               \n  &lt;chr&gt;                                              &lt;glue&gt;                     \n1 \"Apple iPhone 14 Pro Max - 256GB - Space Black!!!\" Apple - 256GB Storage - Sp~\n2 \"samsung galaxy s23 ultra 512gb phantom black\"     Samsung - NA Storage - NA  \n3 \"Google Pixel 7 Pro (128 GB) -- Obsidian\"          Google - 128GB Storage - O~\n4 \"OnePlus 11 5G | 16GB RAM | 256GB Storage\"         Oneplus - 16GB Storage - NA\n5 \"  Xiaomi 13 Pro    256GB/12GB    Ceramic Black  \" Xiaomi - 256GB Storage - C~"
  },
  {
    "objectID": "07-strings-dates.html#working-with-date-ranges-and-business-logic",
    "href": "07-strings-dates.html#working-with-date-ranges-and-business-logic",
    "title": "Chapter 7: Working with Strings and Dates - Text and Time in the Tidyverse",
    "section": "Working with Date Ranges and Business Logic",
    "text": "Working with Date Ranges and Business Logic\nReal-world applications often involve complex date logic:\n\n# Business calendar functions\nis_business_day &lt;- function(date) {\n  !wday(date) %in% c(1, 7)  # Not weekend\n}\n\nnext_business_day &lt;- function(date) {\n  next_day &lt;- date + days(1)\n  while (!is_business_day(next_day)) {\n    next_day &lt;- next_day + days(1)\n  }\n  return(next_day)\n}\n\nadd_business_days &lt;- function(date, n) {\n  result &lt;- date\n  for (i in 1:n) {\n    result &lt;- next_business_day(result)\n  }\n  return(result)\n}\n\n# Example: Project timeline\nproject_tasks &lt;- tibble(\n  task = c(\"Planning\", \"Development\", \"Testing\", \"Deployment\", \"Review\"),\n  duration_days = c(5, 15, 10, 2, 3),\n  start_date = ymd(\"2023-07-03\")  # Starting on a Monday\n)\n\n# Calculate end dates considering only business days\nproject_timeline &lt;- project_tasks %&gt;%\n  mutate(\n    # Simple calendar days\n    end_date_calendar = start_date + days(duration_days - 1),\n    \n    # Business days only\n    end_date_business = map2(start_date, duration_days, \n                            ~add_business_days(.x, .y - 1)),\n    end_date_business = as_date(unlist(end_date_business)),\n    \n    # Actual duration including weekends\n    actual_calendar_days = as.numeric(end_date_business - start_date + 1),\n    \n    # Next task start\n    next_start = lead(end_date_business) %&gt;% \n      map_if(~!is.na(.), ~next_business_day(.)) %&gt;%\n      unlist() %&gt;%\n      as_date()\n  )\n\nproject_timeline %&gt;%\n  select(task, duration_days, start_date, end_date_business, actual_calendar_days)\n\n# A tibble: 5 x 5\n  task        duration_days start_date end_date_business actual_calendar_days\n  &lt;chr&gt;               &lt;dbl&gt; &lt;date&gt;     &lt;date&gt;                           &lt;dbl&gt;\n1 Planning                5 2023-07-03 2023-07-07                           5\n2 Development            15 2023-07-03 2023-07-21                          19\n3 Testing                10 2023-07-03 2023-07-14                          12\n4 Deployment              2 2023-07-03 2023-07-04                           2\n5 Review                  3 2023-07-03 2023-07-05                           3"
  },
  {
    "objectID": "07-strings-dates.html#exercises",
    "href": "07-strings-dates.html#exercises",
    "title": "Chapter 7: Working with Strings and Dates - Text and Time in the Tidyverse",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Email Parsing and Validation\nParse and validate a set of email addresses:\n\n# Your solution\nemail_data &lt;- c(\n  \"john.doe@company.com\",\n  \"alice_smith@university.edu\",\n  \"bob@sub.domain.co.uk\",\n  \"invalid.email\",\n  \"no-at-sign.com\",\n  \"@missing-user.com\",\n  \"user@\",\n  \"first.last+tag@gmail.com\"\n)\n\nemail_validation &lt;- tibble(email = email_data) %&gt;%\n  mutate(\n    # Check basic structure\n    has_at = str_detect(email, \"@\"),\n    has_dot_after_at = str_detect(email, \"@.*\\\\.\"),\n    \n    # Extract parts\n    username = str_extract(email, \"^[^@]+\"),\n    domain = str_extract(email, \"@(.+)$\") %&gt;% str_remove(\"@\"),\n    tld = str_extract(email, \"\\\\.[a-z]+$\"),\n    \n    # Validate\n    valid_username = str_detect(username, \"^[a-zA-Z0-9._+-]+$\"),\n    valid_domain = str_detect(domain, \"^[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\"),\n    \n    # Overall validity\n    is_valid = has_at & has_dot_after_at & \n               !is.na(username) & !is.na(domain) &\n               valid_username & valid_domain\n  )\n\nemail_validation %&gt;%\n  select(email, username, domain, is_valid)\n\n# A tibble: 8 x 4\n  email                      username       domain           is_valid\n  &lt;chr&gt;                      &lt;chr&gt;          &lt;chr&gt;            &lt;lgl&gt;   \n1 john.doe@company.com       john.doe       company.com      TRUE    \n2 alice_smith@university.edu alice_smith    university.edu   TRUE    \n3 bob@sub.domain.co.uk       bob            sub.domain.co.uk TRUE    \n4 invalid.email              invalid.email  &lt;NA&gt;             FALSE   \n5 no-at-sign.com             no-at-sign.com &lt;NA&gt;             FALSE   \n6 @missing-user.com          &lt;NA&gt;           missing-user.com FALSE   \n7 user@                      user           &lt;NA&gt;             FALSE   \n8 first.last+tag@gmail.com   first.last+tag gmail.com        TRUE    \n\n\n\n\nExercise 2: Date Range Calculations\nCalculate various date ranges and intervals:\n\n# Your solution\n# Calculate age, next birthday, days until birthday\npeople &lt;- tibble(\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"Diana\"),\n  birth_date = ymd(c(\"1990-03-15\", \"1985-07-22\", \"1995-12-01\", \"2000-01-30\"))\n)\n\ntoday_date &lt;- today()\n\npeople_ages &lt;- people %&gt;%\n  mutate(\n    # Calculate age\n    age_years = interval(birth_date, today_date) / years(1),\n    age_exact = floor(age_years),\n    \n    # Next birthday\n    birthday_this_year = `year&lt;-`(birth_date, year(today_date)),\n    next_birthday = if_else(\n      birthday_this_year &lt; today_date,\n      `year&lt;-`(birth_date, year(today_date) + 1),\n      birthday_this_year\n    ),\n    \n    # Days until birthday\n    days_to_birthday = as.numeric(next_birthday - today_date),\n    \n    # Day of week for next birthday\n    birthday_weekday = wday(next_birthday, label = TRUE, abbr = FALSE)\n  )\n\npeople_ages %&gt;%\n  select(name, age_exact, next_birthday, days_to_birthday, birthday_weekday)\n\n# A tibble: 4 x 5\n  name    age_exact next_birthday days_to_birthday birthday_weekday\n  &lt;chr&gt;       &lt;dbl&gt; &lt;date&gt;                   &lt;dbl&gt; &lt;ord&gt;           \n1 Alice          35 2026-03-15                 165 Sunday          \n2 Bob            40 2026-07-22                 294 Wednesday       \n3 Charlie        29 2025-12-01                  61 Monday          \n4 Diana          25 2026-01-30                 121 Friday          \n\n\n\n\nExercise 3: Log File Analysis\nAnalyze server log patterns:\n\n# Your solution\n# Generate sample log data\nset.seed(456)\nn_logs &lt;- 1000\n\nlog_data &lt;- tibble(\n  timestamp = ymd_hms(\"2023-06-01 00:00:00\") + seconds(sort(runif(n_logs, 0, 30*24*60*60))),\n  level = sample(c(\"INFO\", \"WARNING\", \"ERROR\", \"DEBUG\"), n_logs, \n                replace = TRUE, prob = c(0.5, 0.3, 0.15, 0.05)),\n  service = sample(c(\"auth\", \"api\", \"database\", \"cache\"), n_logs, replace = TRUE),\n  message = sample(c(\n    \"Request processed successfully\",\n    \"Connection timeout after 30s\",\n    \"Invalid authentication token\",\n    \"Cache miss for key: user_123\",\n    \"Database query took 1250ms\",\n    \"Rate limit exceeded for IP 192.168.1.1\"\n  ), n_logs, replace = TRUE)\n) %&gt;%\n  mutate(\n    log_entry = str_glue(\"[{timestamp}] {level} [{service}]: {message}\")\n  )\n\n# Analyze the logs\nlog_analysis &lt;- log_data %&gt;%\n  mutate(\n    date = as_date(timestamp),\n    hour = hour(timestamp),\n    \n    # Extract metrics from messages\n    has_timeout = str_detect(message, \"timeout\"),\n    timeout_duration = str_extract(message, \"\\\\d+s\") %&gt;% \n      str_remove(\"s\") %&gt;% as.numeric(),\n    \n    has_ip = str_detect(message, \"\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+\"),\n    ip_address = str_extract(message, \"\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+\"),\n    \n    query_time = str_extract(message, \"\\\\d+ms\") %&gt;% \n      str_remove(\"ms\") %&gt;% as.numeric(),\n    \n    # Categorize issues\n    issue_type = case_when(\n      str_detect(message, \"timeout|slow|took \\\\d+ms\") ~ \"Performance\",\n      str_detect(message, \"Invalid|error|failed\") ~ \"Error\",\n      str_detect(message, \"exceeded|limit\") ~ \"Rate Limiting\",\n      TRUE ~ \"Normal\"\n    )\n  )\n\n# Summary by service and level\nlog_summary &lt;- log_analysis %&gt;%\n  group_by(service, level) %&gt;%\n  summarise(\n    count = n(),\n    pct_errors = mean(issue_type == \"Error\") * 100,\n    avg_query_time = mean(query_time, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(service, level)\n\nprint(log_summary)\n\n# A tibble: 16 x 5\n   service  level   count pct_errors avg_query_time\n   &lt;chr&gt;    &lt;chr&gt;   &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt;\n 1 api      DEBUG       8      25               NaN\n 2 api      ERROR      48      25              1250\n 3 api      INFO      122      16.4            1250\n 4 api      WARNING    80      15              1250\n 5 auth     DEBUG      11      18.2            1250\n 6 auth     ERROR      34      14.7            1250\n 7 auth     INFO      137      13.9            1250\n 8 auth     WARNING    65      16.9            1250\n 9 cache    DEBUG      13       0              1250\n10 cache    ERROR      33      27.3            1250\n11 cache    INFO      116      20.7            1250\n12 cache    WARNING    81      18.5            1250\n13 database DEBUG      15       6.67           1250\n14 database ERROR      42       9.52           1250\n15 database INFO      127      15.0            1250\n16 database WARNING    68      23.5            1250\n\n# Time pattern analysis\nhourly_pattern &lt;- log_analysis %&gt;%\n  group_by(hour, level) %&gt;%\n  summarise(count = n(), .groups = \"drop\") %&gt;%\n  ggplot(aes(x = hour, y = count, color = level)) +\n  geom_line(linewidth = 1) +\n  scale_color_manual(values = c(\n    \"INFO\" = \"green\",\n    \"WARNING\" = \"orange\", \n    \"ERROR\" = \"red\",\n    \"DEBUG\" = \"blue\"\n  )) +\n  labs(\n    title = \"Log Patterns by Hour\",\n    x = \"Hour of Day\",\n    y = \"Log Count\"\n  )\n\nprint(hourly_pattern)\n\n\n\n\n\n\n\n\n\n\nExercise 4: Text Mining Product Reviews\nAnalyze product review text and dates:\n\n# Your solution\n# Generate sample review data\nset.seed(789)\nn_reviews &lt;- 200\n\nreviews &lt;- tibble(\n  review_id = 1:n_reviews,\n  review_date = ymd(\"2023-01-01\") + days(sample(0:180, n_reviews, replace = TRUE)),\n  rating = sample(1:5, n_reviews, replace = TRUE, prob = c(0.05, 0.1, 0.2, 0.35, 0.3)),\n  review_text = sample(c(\n    \"Great product! Highly recommend. Fast shipping too.\",\n    \"Terrible quality. Broke after one day. Very disappointed.\",\n    \"Good value for money. Works as expected.\",\n    \"Amazing! Best purchase ever! 10/10 would buy again!\",\n    \"Not bad, but not great either. Average product.\",\n    \"Excellent customer service. Product is okay.\",\n    \"Waste of money. Do not buy!\",\n    \"Pretty good, some minor issues but overall satisfied.\",\n    \"Perfect! Exactly what I needed. Five stars!\",\n    \"Meh. Expected better for the price.\"\n  ), n_reviews, replace = TRUE)\n)\n\n# Analyze reviews\nreview_analysis &lt;- reviews %&gt;%\n  mutate(\n    # Text length and complexity\n    text_length = str_length(review_text),\n    word_count = str_count(review_text, \"\\\\w+\"),\n    sentence_count = str_count(review_text, \"[.!?]\"),\n    avg_word_length = text_length / word_count,\n    \n    # Sentiment indicators\n    has_positive = str_detect(review_text, \n      \"(?i)great|excellent|amazing|perfect|love|best\"),\n    has_negative = str_detect(review_text, \n      \"(?i)terrible|bad|worst|hate|disappoint|waste\"),\n    \n    exclamation_count = str_count(review_text, \"!\"),\n    \n    # Time-based features\n    review_month = month(review_date, label = TRUE),\n    review_weekday = wday(review_date, label = TRUE),\n    days_since_launch = as.numeric(review_date - min(review_date)),\n    \n    # Categorize sentiment\n    sentiment = case_when(\n      has_positive & !has_negative ~ \"Positive\",\n      has_negative & !has_positive ~ \"Negative\",\n      has_positive & has_negative ~ \"Mixed\",\n      TRUE ~ \"Neutral\"\n    )\n  )\n\n# Correlation between text features and rating\nfeature_correlation &lt;- review_analysis %&gt;%\n  summarise(\n    cor_length_rating = cor(text_length, rating),\n    cor_exclamation_rating = cor(exclamation_count, rating),\n    cor_positive_rating = cor(as.numeric(has_positive), rating),\n    cor_negative_rating = cor(as.numeric(has_negative), rating)\n  )\n\nprint(\"Feature correlations with rating:\")\n\n[1] \"Feature correlations with rating:\"\n\nprint(feature_correlation)\n\n# A tibble: 1 x 4\n  cor_length_rating cor_exclamation_rating cor_positive_rating\n              &lt;dbl&gt;                  &lt;dbl&gt;               &lt;dbl&gt;\n1           -0.0412                -0.0605             -0.0880\n# i 1 more variable: cor_negative_rating &lt;dbl&gt;\n\n# Sentiment vs actual rating\nsentiment_accuracy &lt;- review_analysis %&gt;%\n  group_by(sentiment) %&gt;%\n  summarise(\n    avg_rating = mean(rating),\n    count = n(),\n    .groups = \"drop\"\n  )\n\nprint(\"Sentiment analysis accuracy:\")\n\n[1] \"Sentiment analysis accuracy:\"\n\nprint(sentiment_accuracy)\n\n# A tibble: 4 x 3\n  sentiment avg_rating count\n  &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt;\n1 Mixed           3.64    22\n2 Negative        3.47    32\n3 Neutral         3.77    64\n4 Positive        3.41    82"
  },
  {
    "objectID": "07-strings-dates.html#summary",
    "href": "07-strings-dates.html#summary",
    "title": "Chapter 7: Working with Strings and Dates - Text and Time in the Tidyverse",
    "section": "Summary",
    "text": "Summary\nIn this comprehensive chapter, you’ve mastered:\n✅ String manipulation with stringr - Pattern matching and regular expressions - Text cleaning and standardization - String splitting and combining - Advanced text extraction\n✅ Date and time handling with lubridate - Parsing various date formats - Date arithmetic and components - Time zones and daylight saving - Intervals, durations, and periods\n✅ Combined analysis - Real-world text and temporal data - Log file parsing - Business date calculations - Pattern extraction and validation\nKey takeaways: - Always validate and clean text data before analysis - Be explicit about time zones to avoid bugs - Regular expressions are powerful but need practice - Combine string and date operations for rich insights - Document your cleaning and parsing decisions"
  },
  {
    "objectID": "07-strings-dates.html#whats-next",
    "href": "07-strings-dates.html#whats-next",
    "title": "Chapter 7: Working with Strings and Dates - Text and Time in the Tidyverse",
    "section": "What’s Next?",
    "text": "What’s Next?\nCongratulations! You’ve completed all the fundamental tidyverse concepts from Block 1. Before moving to machine learning, we recommend taking our Block 1 Assessment to test your understanding of the core R and tidyverse concepts you’ve learned.\nOnce you’ve mastered these fundamentals, continue to Chapter 8 where we transition to machine learning with tidymodels, building on your data manipulation skills."
  },
  {
    "objectID": "07-strings-dates.html#additional-resources",
    "href": "07-strings-dates.html#additional-resources",
    "title": "Chapter 7: Working with Strings and Dates - Text and Time in the Tidyverse",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nstringr Documentation\nlubridate Documentation\nRegular Expression Testing\nR for Data Science - Strings\nR for Data Science - Dates and Times\nTime Zone Database"
  },
  {
    "objectID": "06-functional-programming.html",
    "href": "06-functional-programming.html",
    "title": "Chapter 6: Functional Programming with purrr - Iteration and Mapping Made Elegant",
    "section": "",
    "text": "By the end of this chapter, you will master:\n\nThe philosophy of functional programming in R\nThe map family of functions\nWorking with lists and nested data\nSafe function execution and error handling\nParallel iteration with map2 and pmap\nFunctional programming patterns\nIntegration with tidyverse workflows\nPerformance optimization techniques"
  },
  {
    "objectID": "06-functional-programming.html#learning-objectives",
    "href": "06-functional-programming.html#learning-objectives",
    "title": "Chapter 6: Functional Programming with purrr - Iteration and Mapping Made Elegant",
    "section": "",
    "text": "By the end of this chapter, you will master:\n\nThe philosophy of functional programming in R\nThe map family of functions\nWorking with lists and nested data\nSafe function execution and error handling\nParallel iteration with map2 and pmap\nFunctional programming patterns\nIntegration with tidyverse workflows\nPerformance optimization techniques"
  },
  {
    "objectID": "06-functional-programming.html#why-functional-programming",
    "href": "06-functional-programming.html#why-functional-programming",
    "title": "Chapter 6: Functional Programming with purrr - Iteration and Mapping Made Elegant",
    "section": "Why Functional Programming?",
    "text": "Why Functional Programming?\nFunctional programming (FP) is a programming paradigm that treats computation as the evaluation of mathematical functions. In data science, FP principles help us write: - Cleaner code: Less repetition, more abstraction - Safer code: Fewer side effects, predictable behavior - More maintainable code: Modular, testable functions - Scalable code: Easy to parallelize and optimize\nThe purrr package brings functional programming to the tidyverse, replacing loops with elegant, expressive functions.\n\nThe Problem with Loops\nLet’s start by understanding why we need functional programming:\n\nlibrary(tidyverse)\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.4     v readr     2.1.5\nv forcats   1.0.0     v stringr   1.5.2\nv ggplot2   4.0.0     v tibble    3.3.0\nv lubridate 1.9.4     v tidyr     1.3.1\nv purrr     1.1.0     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(purrr)\nlibrary(broom)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Traditional approach with loops\ndata_list &lt;- list(\n  group_a = rnorm(100, mean = 10, sd = 2),\n  group_b = rnorm(100, mean = 15, sd = 3),\n  group_c = rnorm(100, mean = 12, sd = 2.5)\n)\n\n# Calculate mean with a for loop (the old way)\nmeans_loop &lt;- numeric(length(data_list))\nnames(means_loop) &lt;- names(data_list)\n\nfor (i in seq_along(data_list)) {\n  means_loop[i] &lt;- mean(data_list[[i]])\n}\n\nprint(\"Means calculated with loop:\")\n\n[1] \"Means calculated with loop:\"\n\nprint(means_loop)\n\n group_a  group_b  group_c \n10.18081 14.67736 12.30116 \n\n# The functional approach with map\nmeans_map &lt;- map_dbl(data_list, mean)\n\nprint(\"Means calculated with map:\")\n\n[1] \"Means calculated with map:\"\n\nprint(means_map)\n\n group_a  group_b  group_c \n10.18081 14.67736 12.30116 \n\n# Are they the same?\nidentical(means_loop, means_map)\n\n[1] TRUE\n\n\nWhile both approaches give the same result, the functional approach is: - More concise: One line instead of four - More readable: Intent is clear - Less error-prone: No index management - Vectorized: Can be easily parallelized"
  },
  {
    "objectID": "06-functional-programming.html#the-map-family",
    "href": "06-functional-programming.html#the-map-family",
    "title": "Chapter 6: Functional Programming with purrr - Iteration and Mapping Made Elegant",
    "section": "The map() Family",
    "text": "The map() Family\nThe map() functions are the workhorses of purrr. They apply a function to each element of a list or vector.\n\nBasic map() Function\nThe basic map() function always returns a list:\n\n# Apply a function to each element\nnumbers &lt;- list(\n  small = 1:5,\n  medium = 10:15,\n  large = 100:105\n)\n\n# Calculate summary statistics for each group\nsummaries &lt;- map(numbers, summary)\nprint(summaries)\n\n$small\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      1       2       3       3       4       5 \n\n$medium\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10.00   11.25   12.50   12.50   13.75   15.00 \n\n$large\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  100.0   101.2   102.5   102.5   103.8   105.0 \n\n# The beauty of map: it preserves names and structure\nstr(summaries)\n\nList of 3\n $ small : 'summaryDefault' Named num [1:6] 1 2 3 3 4 5\n  ..- attr(*, \"names\")= chr [1:6] \"Min.\" \"1st Qu.\" \"Median\" \"Mean\" ...\n $ medium: 'summaryDefault' Named num [1:6] 10 11.2 12.5 12.5 13.8 ...\n  ..- attr(*, \"names\")= chr [1:6] \"Min.\" \"1st Qu.\" \"Median\" \"Mean\" ...\n $ large : 'summaryDefault' Named num [1:6] 100 101 102 102 104 ...\n  ..- attr(*, \"names\")= chr [1:6] \"Min.\" \"1st Qu.\" \"Median\" \"Mean\" ...\n\n\n\n\nType-Specific map Variants\nOften we want a specific type of output. Purrr provides typed variants:\n\n# map_dbl returns a numeric vector\nmeans &lt;- map_dbl(numbers, mean)\nprint(means)\n\n small medium  large \n   3.0   12.5  102.5 \n\n# map_chr returns a character vector\ndescriptions &lt;- map_chr(numbers, ~ paste(\"Range:\", min(.), \"-\", max(.)))\nprint(descriptions)\n\n             small             medium              large \n    \"Range: 1 - 5\"   \"Range: 10 - 15\" \"Range: 100 - 105\" \n\n# map_lgl returns a logical vector\nhas_even_length &lt;- map_lgl(numbers, ~ length(.) %% 2 == 0)\nprint(has_even_length)\n\n small medium  large \n FALSE   TRUE   TRUE \n\n# map_int returns an integer vector\nlengths &lt;- map_int(numbers, length)\nprint(lengths)\n\n small medium  large \n     5      6      6 \n\n# Demonstrate type safety\n# This would error: map_dbl(numbers, class)\n# Because class returns a character, not a double\n\nThe typed variants are important because they: - Ensure type consistency - Catch errors early - Make code more predictable - Enable further vectorized operations\n\n\nAnonymous Functions and Shortcuts\nPurrr provides multiple ways to specify functions, making code more concise:\n\n# Sample data\nvalues &lt;- list(\n  a = 1:10,\n  b = 11:20,\n  c = 21:30\n)\n\n# Method 1: Named function\nresult1 &lt;- map_dbl(values, mean)\n\n# Method 2: Anonymous function (traditional)\nresult2 &lt;- map_dbl(values, function(x) mean(x))\n\n# Method 3: Formula notation (purrr style)\nresult3 &lt;- map_dbl(values, ~ mean(.))\n\n# Method 4: Even more concise for simple extractions\n# Extract the 3rd element from each vector\nthird_elements &lt;- map_dbl(values, 3)  # Same as ~ .[[3]]\n\nprint(\"All methods produce the same result:\")\n\n[1] \"All methods produce the same result:\"\n\nprint(all(result1 == result2, result2 == result3))\n\n[1] TRUE\n\nprint(\"Third elements:\")\n\n[1] \"Third elements:\"\n\nprint(third_elements)\n\n a  b  c \n 3 13 23 \n\n# More complex anonymous functions\ncustom_summary &lt;- map(values, ~ {\n  tibble(\n    min = min(.),\n    median = median(.),\n    max = max(.),\n    range = max(.) - min(.),\n    cv = sd(.) / mean(.)  # Coefficient of variation\n  )\n})\n\n# Combine into a single data frame\nbind_rows(custom_summary, .id = \"group\")\n\n# A tibble: 3 x 6\n  group   min median   max range    cv\n  &lt;chr&gt; &lt;int&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1 a         1    5.5    10     9 0.550\n2 b        11   15.5    20     9 0.195\n3 c        21   25.5    30     9 0.119\n\n\nThe ~ notation is particularly elegant: - ~ starts an anonymous function - . represents the current element - Can include complex expressions"
  },
  {
    "objectID": "06-functional-programming.html#working-with-data-frames",
    "href": "06-functional-programming.html#working-with-data-frames",
    "title": "Chapter 6: Functional Programming with purrr - Iteration and Mapping Made Elegant",
    "section": "Working with Data Frames",
    "text": "Working with Data Frames\nOne of purrr’s strengths is working with data frames and nested data:\n\n# Create a nested data frame\nnested_data &lt;- tibble(\n  group = c(\"A\", \"B\", \"C\"),\n  n = c(50, 75, 100)\n) %&gt;%\n  mutate(\n    # Generate data for each group\n    data = map2(n, group, ~ {\n      tibble(\n        value = rnorm(.x, mean = match(.y, LETTERS) * 10, sd = 2),\n        category = sample(c(\"Type1\", \"Type2\"), .x, replace = TRUE)\n      )\n    })\n  )\n\nprint(\"Nested data structure:\")\n\n[1] \"Nested data structure:\"\n\nprint(nested_data)\n\n# A tibble: 3 x 3\n  group     n data              \n  &lt;chr&gt; &lt;dbl&gt; &lt;list&gt;            \n1 A        50 &lt;tibble [50 x 2]&gt; \n2 B        75 &lt;tibble [75 x 2]&gt; \n3 C       100 &lt;tibble [100 x 2]&gt;\n\n# Work with nested data\nanalysis_results &lt;- nested_data %&gt;%\n  mutate(\n    # Calculate statistics for each nested data frame\n    n_obs = map_int(data, nrow),\n    mean_value = map_dbl(data, ~ mean(.$value)),\n    sd_value = map_dbl(data, ~ sd(.$value)),\n    \n    # Fit models to each group\n    model = map(data, ~ lm(value ~ category, data = .)),\n    \n    # Extract model information\n    model_summary = map(model, broom::glance),\n    coefficients = map(model, broom::tidy)\n  )\n\n# View the results\nanalysis_results %&gt;%\n  select(group, n_obs, mean_value, sd_value)\n\n# A tibble: 3 x 4\n  group n_obs mean_value sd_value\n  &lt;chr&gt; &lt;int&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1 A        50       10.0     2.06\n2 B        75       19.7     1.98\n3 C       100       29.9     1.78\n\n# Unnest model summaries\nanalysis_results %&gt;%\n  select(group, model_summary) %&gt;%\n  unnest(model_summary) %&gt;%\n  select(group, r.squared, p.value, AIC, BIC)\n\n# A tibble: 3 x 5\n  group r.squared p.value   AIC   BIC\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 A       0.00109   0.820  219.  225.\n2 B       0.0249    0.176  318.  325.\n3 C       0.00282   0.600  404.  412.\n\n\nThis pattern of nest-map-unnest is incredibly powerful for: - Group-wise analysis - Multiple model fitting - Simulation studies - Bootstrap procedures"
  },
  {
    "objectID": "06-functional-programming.html#map2-and-pmap-for-multiple-inputs",
    "href": "06-functional-programming.html#map2-and-pmap-for-multiple-inputs",
    "title": "Chapter 6: Functional Programming with purrr - Iteration and Mapping Made Elegant",
    "section": "map2() and pmap() for Multiple Inputs",
    "text": "map2() and pmap() for Multiple Inputs\nSometimes we need to iterate over multiple inputs simultaneously:\n\nmap2() for Two Inputs\n\n# Two vectors to iterate over\nmeans &lt;- c(10, 20, 30)\nsds &lt;- c(1, 2, 3)\nsample_sizes &lt;- c(100, 200, 300)\n\n# Generate samples with different parameters\nsamples &lt;- map2(means, sds, ~ rnorm(100, mean = .x, sd = .y))\n\n# Visualize\nsamples_df &lt;- tibble(\n  group = rep(c(\"A\", \"B\", \"C\"), each = 100),\n  value = unlist(samples)\n)\n\nggplot(samples_df, aes(x = value, fill = group)) +\n  geom_density(alpha = 0.5) +\n  labs(\n    title = \"Distributions Generated with map2()\",\n    subtitle = \"Different means and standard deviations\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n# More complex map2 example\nx_values &lt;- list(1:5, 6:10, 11:15)\ny_values &lt;- list(2:6, 7:11, 12:16)\n\n# Calculate correlations between paired lists\ncorrelations &lt;- map2_dbl(x_values, y_values, cor)\nprint(paste(\"Correlations:\", paste(correlations, collapse = \", \")))\n\n[1] \"Correlations: 1, 1, 1\"\n\n\n\n\npmap() for Multiple Inputs\nWhen you have more than two inputs, use pmap():\n\n# Multiple parameters for simulation\nparams &lt;- tibble(\n  n = c(100, 200, 150),\n  mean = c(10, 15, 12),\n  sd = c(2, 3, 2.5),\n  distribution = c(\"normal\", \"uniform\", \"exponential\")\n)\n\n# Generate samples based on parameters\nsimulated_data &lt;- pmap(params, function(n, mean, sd, distribution) {\n  switch(distribution,\n    normal = rnorm(n, mean, sd),\n    uniform = runif(n, mean - sd, mean + sd),\n    exponential = rexp(n, rate = 1/mean)\n  )\n})\n\n# Add to our data frame - simulated_data is already a list\nparams_with_data &lt;- bind_cols(\n  params,\n  tibble(\n    data = simulated_data,\n    actual_mean = map_dbl(simulated_data, mean),\n    actual_sd = map_dbl(simulated_data, sd)\n  )\n)\n\nparams_with_data %&gt;%\n  select(-data)\n\n# A tibble: 3 x 6\n      n  mean    sd distribution actual_mean actual_sd\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;\n1   100    10   2   normal              9.79      1.86\n2   200    15   3   uniform            15.0       1.82\n3   150    12   2.5 exponential        11.8      11.9 \n\n# Visualize all distributions\nparams_with_data %&gt;%\n  mutate(plot_data = map2(data, distribution, ~ {\n    tibble(value = .x, dist_type = .y)\n  })) %&gt;%\n  select(distribution, plot_data) %&gt;%\n  unnest(plot_data) %&gt;%\n  ggplot(aes(x = value, fill = distribution)) +\n  geom_histogram(bins = 30, alpha = 0.7) +\n  facet_wrap(~distribution, scales = \"free\") +\n  theme_minimal() +\n  labs(title = \"Different Distributions Generated with pmap()\")\n\n\n\n\n\n\n\n\nThe power of pmap(): - Handles any number of inputs - Works with data frames naturally - Maintains alignment of inputs - Enables complex parameterized operations"
  },
  {
    "objectID": "06-functional-programming.html#error-handling-with-safely-and-possibly",
    "href": "06-functional-programming.html#error-handling-with-safely-and-possibly",
    "title": "Chapter 6: Functional Programming with purrr - Iteration and Mapping Made Elegant",
    "section": "Error Handling with safely() and possibly()",
    "text": "Error Handling with safely() and possibly()\nReal-world data is messy. Functions fail. Purrr helps us handle errors gracefully:\n\nsafely() - Capture Errors\n\n# Create data with potential problems\nmessy_list &lt;- list(\n  good = 1:10,\n  bad = c(1, 2, \"three\", 4),  # Contains a string\n  empty = numeric(0),\n  null = NULL,\n  also_good = 11:20\n)\n\n# This would fail:\n# map_dbl(messy_list, mean)\n\n# Use safely to capture errors\nsafe_mean &lt;- safely(mean)\nresults &lt;- map(messy_list, safe_mean)\n\n# Examine structure\nstr(results[[1]])  # Good result\n\nList of 2\n $ result: num 5.5\n $ error : NULL\n\nstr(results[[2]])  # Error result\n\nList of 2\n $ result: num NA\n $ error : NULL\n\n# Extract results and errors\nextracted_results &lt;- tibble(\n  name = names(messy_list),\n  result = map(results, \"result\"),\n  error = map(results, \"error\")\n) %&gt;%\n  mutate(\n    has_error = map_lgl(error, ~ !is.null(.)),\n    error_message = map_chr(error, ~ ifelse(is.null(.), \"No error\", as.character(.)))\n  )\n\nextracted_results %&gt;%\n  select(name, has_error, error_message)\n\n# A tibble: 5 x 3\n  name      has_error error_message\n  &lt;chr&gt;     &lt;lgl&gt;     &lt;chr&gt;        \n1 good      FALSE     No error     \n2 bad       FALSE     No error     \n3 empty     FALSE     No error     \n4 null      FALSE     No error     \n5 also_good FALSE     No error     \n\n\n\n\npossibly() - Provide Default Values\n\n# possibly() is simpler when you just want a default\npossible_mean &lt;- possibly(mean, otherwise = NA_real_)\n\n# Apply to messy list\nmeans_with_default &lt;- map_dbl(messy_list, possible_mean)\nprint(means_with_default)\n\n     good       bad     empty      null also_good \n      5.5        NA       NaN        NA      15.5 \n\n# More complex example with custom function\ncalculate_cv &lt;- function(x) {\n  if (length(x) &lt; 2) stop(\"Need at least 2 values\")\n  sd(x) / mean(x)\n}\n\nsafe_cv &lt;- possibly(calculate_cv, otherwise = NA_real_)\n\n# Test data with edge cases\ntest_data &lt;- list(\n  normal = rnorm(100, 10, 2),\n  single = 5,\n  empty = numeric(0),\n  zeros = rep(0, 10),\n  negative = c(-1, -2, -3)\n)\n\ncv_results &lt;- map_dbl(test_data, safe_cv)\nprint(cv_results)\n\n    normal     single      empty      zeros   negative \n 0.2023601         NA         NA        NaN -0.5000000 \n\n\n\n\nquietly() - Capture Warnings and Messages\n\n# Function that produces warnings\nnoisy_function &lt;- function(x) {\n  if (any(x &lt; 0)) warning(\"Negative values detected\")\n  if (length(x) &gt; 100) message(\"Large dataset\")\n  mean(x)\n}\n\n# Capture all output\nquiet_function &lt;- quietly(noisy_function)\n\n# Test with various inputs\ntest_inputs &lt;- list(\n  clean = 1:10,\n  negative = c(-5, 0, 5),\n  large = 1:150\n)\n\nquiet_results &lt;- map(test_inputs, quiet_function)\n\n# Extract components\noutput_summary &lt;- tibble(\n  name = names(test_inputs),\n  result = map_dbl(quiet_results, \"result\"),\n  warnings = map(quiet_results, \"warnings\"),\n  messages = map(quiet_results, \"messages\")\n) %&gt;%\n  mutate(\n    has_warnings = map_lgl(warnings, ~ length(.) &gt; 0),\n    has_messages = map_lgl(messages, ~ length(.) &gt; 0)\n  )\n\noutput_summary %&gt;%\n  select(name, result, has_warnings, has_messages)\n\n# A tibble: 3 x 4\n  name     result has_warnings has_messages\n  &lt;chr&gt;     &lt;dbl&gt; &lt;lgl&gt;        &lt;lgl&gt;       \n1 clean       5.5 FALSE        FALSE       \n2 negative    0   TRUE         FALSE       \n3 large      75.5 FALSE        TRUE"
  },
  {
    "objectID": "06-functional-programming.html#advanced-functional-patterns",
    "href": "06-functional-programming.html#advanced-functional-patterns",
    "title": "Chapter 6: Functional Programming with purrr - Iteration and Mapping Made Elegant",
    "section": "Advanced Functional Patterns",
    "text": "Advanced Functional Patterns\n\nFunction Factories\nCreate functions that return other functions:\n\n# Create a function factory for power functions\nmake_power &lt;- function(n) {\n  function(x) x^n\n}\n\n# Create specific power functions\nsquare &lt;- make_power(2)\ncube &lt;- make_power(3)\n\n# Use them\nprint(square(5))\n\n[1] 25\n\nprint(cube(3))\n\n[1] 27\n\n# Apply to data\nvalues &lt;- 1:5\npowers &lt;- 2:4\n\n# Create multiple power functions\npower_functions &lt;- map(powers, make_power)\n\n# Apply each to our values\npower_results &lt;- map(power_functions, ~ .(values))\nnames(power_results) &lt;- paste0(\"power_\", powers)\n\n# Convert to data frame\nas_tibble(power_results) %&gt;%\n  mutate(original = values)\n\n# A tibble: 5 x 4\n  power_2 power_3 power_4 original\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;int&gt;\n1       1       1       1        1\n2       4       8      16        2\n3       9      27      81        3\n4      16      64     256        4\n5      25     125     625        5\n\n\n\n\nFunction Composition\nCombine multiple functions into pipelines:\n\n# Create a data processing pipeline\nprocess_data &lt;- compose(\n  ~ mutate(., z_score = (value - mean(value)) / sd(value)),\n  ~ filter(., !is.na(value)),\n  ~ select(., -unwanted_column),\n  .dir = \"forward\"  # Apply left to right\n)\n\n# Test data\ntest_df &lt;- tibble(\n  value = c(1, 2, NA, 4, 5, 100),\n  unwanted_column = \"remove me\"\n)\n\n# This won't work directly with compose, so let's do it manually\npipeline &lt;- . %&gt;%\n  select(-unwanted_column) %&gt;%\n  filter(!is.na(value)) %&gt;%\n  mutate(z_score = (value - mean(value)) / sd(value))\n\nresult &lt;- test_df %&gt;% pipeline\nprint(result)\n\n# A tibble: 5 x 2\n  value z_score\n  &lt;dbl&gt;   &lt;dbl&gt;\n1     1  -0.493\n2     2  -0.470\n3     4  -0.424\n4     5  -0.401\n5   100   1.79 \n\n# More practical: Create reusable statistical transformations\nstandardize &lt;- function(x) (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)\nwinsorize &lt;- function(x, probs = c(0.05, 0.95)) {\n  limits &lt;- quantile(x, probs, na.rm = TRUE)\n  x[x &lt; limits[1]] &lt;- limits[1]\n  x[x &gt; limits[2]] &lt;- limits[2]\n  x\n}\n\n# Apply transformations to multiple columns\ndata_to_transform &lt;- tibble(\n  a = rnorm(100, 10, 5),\n  b = rexp(100, 0.1),\n  c = runif(100, 0, 100)\n)\n\ntransformed_data &lt;- data_to_transform %&gt;%\n  mutate(across(everything(), list(\n    standardized = standardize,\n    winsorized = winsorize\n  )))\n\nglimpse(transformed_data)\n\nRows: 100\nColumns: 9\n$ a              &lt;dbl&gt; 16.5263077, 14.3804805, 12.3189807, 12.3855712, 7.54297~\n$ b              &lt;dbl&gt; 7.581326695, 11.584588028, 1.362727634, 12.252324372, 2~\n$ c              &lt;dbl&gt; 81.968328, 24.600342, 9.701797, 16.370145, 66.435520, 6~\n$ a_standardized &lt;dbl&gt; 1.30721992, 0.85014788, 0.41103798, 0.42522210, -0.6062~\n$ a_winsorized   &lt;dbl&gt; 16.526308, 14.380481, 12.318981, 12.385571, 7.542973, 3~\n$ b_standardized &lt;dbl&gt; -0.2882992, 0.1875600, -1.0274909, 0.2669324, 1.2023889~\n$ b_winsorized   &lt;dbl&gt; 7.5813267, 11.5845880, 1.3627276, 12.2523244, 20.122039~\n$ c_standardized &lt;dbl&gt; 1.1951049, -0.8248040, -1.3493771, -1.1145866, 0.648199~\n$ c_winsorized   &lt;dbl&gt; 81.968328, 24.600342, 9.701797, 16.370145, 66.435520, 6~\n\n\n\n\nReduce and Accumulate\nCombine elements of a list iteratively:\n\n# reduce() combines all elements into one\nnumbers_list &lt;- list(\n  c(1, 2, 3),\n  c(4, 5, 6),\n  c(7, 8, 9)\n)\n\n# Sum all vectors element-wise\ntotal &lt;- reduce(numbers_list, `+`)\nprint(total)\n\n[1] 12 15 18\n\n# Find intersection of multiple sets\nsets &lt;- list(\n  set1 = c(\"a\", \"b\", \"c\", \"d\"),\n  set2 = c(\"b\", \"c\", \"d\", \"e\"),\n  set3 = c(\"c\", \"d\", \"e\", \"f\")\n)\n\ncommon_elements &lt;- reduce(sets, intersect)\nprint(paste(\"Common elements:\", paste(common_elements, collapse = \", \")))\n\n[1] \"Common elements: c, d\"\n\n# accumulate() keeps intermediate results\ncumulative_sums &lt;- accumulate(1:5, `+`)\nprint(cumulative_sums)\n\n[1]  1  3  6 10 15\n\n# More complex: Merge multiple data frames\ndf_list &lt;- list(\n  tibble(id = 1:3, a = letters[1:3]),\n  tibble(id = 2:4, b = letters[2:4]),\n  tibble(id = 3:5, c = letters[3:5])\n)\n\nmerged_df &lt;- reduce(df_list, full_join, by = \"id\")\nprint(merged_df)\n\n# A tibble: 5 x 4\n     id a     b     c    \n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1     1 a     &lt;NA&gt;  &lt;NA&gt; \n2     2 b     b     &lt;NA&gt; \n3     3 c     c     c    \n4     4 &lt;NA&gt;  d     d    \n5     5 &lt;NA&gt;  &lt;NA&gt;  e"
  },
  {
    "objectID": "06-functional-programming.html#real-world-applications",
    "href": "06-functional-programming.html#real-world-applications",
    "title": "Chapter 6: Functional Programming with purrr - Iteration and Mapping Made Elegant",
    "section": "Real-World Applications",
    "text": "Real-World Applications\n\nBootstrap Analysis\nUse purrr for bootstrap confidence intervals:\n\n# Original data\noriginal_data &lt;- rnorm(100, mean = 50, sd = 10)\n\n# Bootstrap function\nbootstrap_mean &lt;- function(data, n_boot = 1000) {\n  # Generate bootstrap samples\n  boot_samples &lt;- map_dbl(1:n_boot, ~ {\n    sample(data, replace = TRUE) %&gt;% mean()\n  })\n  \n  # Calculate confidence interval\n  ci &lt;- quantile(boot_samples, c(0.025, 0.975))\n  \n  tibble(\n    mean = mean(data),\n    boot_mean = mean(boot_samples),\n    boot_sd = sd(boot_samples),\n    ci_lower = ci[1],\n    ci_upper = ci[2]\n  )\n}\n\n# Apply bootstrap\nboot_result &lt;- bootstrap_mean(original_data)\nprint(boot_result)\n\n# A tibble: 1 x 5\n   mean boot_mean boot_sd ci_lower ci_upper\n  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1  50.8      50.9    1.13     48.7     53.1\n\n# Visualize bootstrap distribution\nboot_samples &lt;- map_dbl(1:1000, ~ mean(sample(original_data, replace = TRUE)))\n\nggplot(tibble(x = boot_samples), aes(x = x)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(xintercept = mean(original_data), color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = boot_result$ci_lower, color = \"red\", linetype = \"dashed\") +\n  geom_vline(xintercept = boot_result$ci_upper, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Bootstrap Distribution of Sample Mean\",\n    subtitle = \"Red lines show original mean and 95% CI\",\n    x = \"Bootstrap Mean\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nMultiple Model Fitting\nFit and compare multiple models efficiently:\n\n# Generate sample data\nset.seed(123)\nmodel_data &lt;- tibble(\n  x1 = rnorm(100),\n  x2 = rnorm(100),\n  x3 = rnorm(100),\n  y = 2 * x1 + 3 * x2 - x3 + rnorm(100, sd = 0.5)\n)\n\n# Define model formulas\nformulas &lt;- list(\n  simple = y ~ x1,\n  additive = y ~ x1 + x2,\n  full = y ~ x1 + x2 + x3,\n  interaction = y ~ x1 * x2 + x3,\n  quadratic = y ~ poly(x1, 2) + x2 + x3\n)\n\n# Fit all models\nmodels &lt;- map(formulas, ~ lm(., data = model_data))\n\n# Extract model metrics\nmodel_comparison &lt;- tibble(\n  model_name = names(formulas),\n  formula = map_chr(formulas, deparse),\n  model = models\n) %&gt;%\n  mutate(\n    glance = map(model, broom::glance),\n    tidy = map(model, broom::tidy),\n    augment = map(model, broom::augment)\n  ) %&gt;%\n  unnest(glance) %&gt;%\n  select(model_name, formula, r.squared, adj.r.squared, AIC, BIC) %&gt;%\n  arrange(desc(adj.r.squared))\n\nprint(model_comparison)\n\n# A tibble: 5 x 6\n  model_name  formula                   r.squared adj.r.squared   AIC   BIC\n  &lt;chr&gt;       &lt;chr&gt;                         &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 full        y ~ x1 + x2 + x3              0.979         0.978  161.  174.\n2 interaction y ~ x1 * x2 + x3              0.979         0.978  163.  178.\n3 quadratic   y ~ poly(x1, 2) + x2 + x3     0.979         0.978  163.  179.\n4 additive    y ~ x1 + x2                   0.906         0.904  310.  320.\n5 simple      y ~ x1                        0.249         0.241  515.  523.\n\n# Visualize model performance\nmodel_comparison %&gt;%\n  pivot_longer(cols = c(r.squared, adj.r.squared, AIC, BIC),\n               names_to = \"metric\", values_to = \"value\") %&gt;%\n  ggplot(aes(x = model_name, y = value, fill = model_name)) +\n  geom_col() +\n  facet_wrap(~metric, scales = \"free_y\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = \"Model Comparison Metrics\") +\n  guides(fill = \"none\")\n\n\n\n\n\n\n\n\n\n\nSimulation Study\nUse purrr for Monte Carlo simulations:\n\n# Simulation parameters\nn_simulations &lt;- 1000\nsample_sizes &lt;- c(10, 30, 100, 300)\n\n# Function to simulate one t-test\nsimulate_t_test &lt;- function(n, true_diff = 0.5) {\n  group1 &lt;- rnorm(n, mean = 0, sd = 1)\n  group2 &lt;- rnorm(n, mean = true_diff, sd = 1)\n  \n  test_result &lt;- t.test(group1, group2)\n  \n  tibble(\n    p_value = test_result$p.value,\n    significant = test_result$p.value &lt; 0.05,\n    estimate = test_result$estimate[1] - test_result$estimate[2],\n    ci_lower = test_result$conf.int[1],\n    ci_upper = test_result$conf.int[2]\n  )\n}\n\n# Run simulation for different sample sizes\nsimulation_results &lt;- map_df(sample_sizes, function(n) {\n  map_df(1:n_simulations, ~ simulate_t_test(n), .id = \"sim\") %&gt;%\n    mutate(sample_size = n)\n})\n\n# Calculate power for each sample size\npower_analysis &lt;- simulation_results %&gt;%\n  group_by(sample_size) %&gt;%\n  summarise(\n    power = mean(significant),\n    mean_estimate = mean(estimate),\n    sd_estimate = sd(estimate),\n    coverage = mean(ci_lower &lt; -0.5 & ci_upper &gt; -0.5),\n    .groups = \"drop\"\n  )\n\nprint(power_analysis)\n\n# A tibble: 4 x 5\n  sample_size power mean_estimate sd_estimate coverage\n        &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1          10 0.18         -0.504      0.451     0.95 \n2          30 0.481        -0.500      0.256     0.949\n3         100 0.942        -0.505      0.140     0.953\n4         300 1            -0.501      0.0840    0.94 \n\n# Visualize power curve\nggplot(power_analysis, aes(x = sample_size, y = power)) +\n  geom_line(linewidth = 1.5, color = \"darkblue\") +\n  geom_point(size = 4, color = \"darkblue\") +\n  geom_hline(yintercept = 0.8, linetype = \"dashed\", color = \"red\") +\n  scale_x_log10() +\n  scale_y_continuous(limits = c(0, 1)) +\n  labs(\n    title = \"Statistical Power vs Sample Size\",\n    subtitle = \"True effect size = 0.5, α = 0.05\",\n    x = \"Sample Size (log scale)\",\n    y = \"Statistical Power\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "06-functional-programming.html#performance-considerations",
    "href": "06-functional-programming.html#performance-considerations",
    "title": "Chapter 6: Functional Programming with purrr - Iteration and Mapping Made Elegant",
    "section": "Performance Considerations",
    "text": "Performance Considerations\n\nBenchmarking map vs loops\n\n# Create test data\ntest_list &lt;- map(1:1000, ~ rnorm(100))\n\n# Benchmark different approaches\nlibrary(microbenchmark)\n\nbenchmark_results &lt;- microbenchmark(\n  for_loop = {\n    results &lt;- numeric(length(test_list))\n    for (i in seq_along(test_list)) {\n      results[i] &lt;- mean(test_list[[i]])\n    }\n  },\n  \n  map_dbl = map_dbl(test_list, mean),\n  \n  vapply = vapply(test_list, mean, numeric(1)),\n  \n  lapply = unlist(lapply(test_list, mean)),\n  \n  times = 100\n)\n\nprint(benchmark_results)\n\nUnit: milliseconds\n     expr      min       lq     mean   median       uq      max neval\n for_loop 2.324003 2.410226 2.528149 2.488659 2.572894 4.431649   100\n  map_dbl 1.680877 1.717264 1.811503 1.765931 1.818288 3.754165   100\n   vapply 1.499247 1.546192 1.739641 1.579484 1.652833 4.058016   100\n   lapply 1.527701 1.563433 1.775901 1.596663 1.669766 7.912303   100\n\n# Visualize results\nautoplot(benchmark_results) +\n  labs(title = \"Performance Comparison: Iteration Methods\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nParallel Processing with furrr\nFor large-scale operations, use parallel processing:\n\n# Note: furrr requires the future package\nlibrary(furrr)\nlibrary(future)\n\n# Set up parallel processing\nplan(multisession, workers = 2)  # Use 2 cores\n\n# Large simulation\nlarge_simulation &lt;- function(n = 10000) {\n  # Simulate expensive computation\n  data &lt;- rnorm(n)\n  tibble(\n    mean = mean(data),\n    sd = sd(data),\n    median = median(data),\n    mad = mad(data)\n  )\n}\n\n# Sequential processing\nsystem.time({\n  sequential_results &lt;- map_df(1:100, ~ large_simulation())\n})\n\n   user  system elapsed \n  0.113   0.006   0.119 \n\n# Parallel processing\nsystem.time({\n  parallel_results &lt;- furrr::future_map_dfr(1:100, ~ large_simulation())\n})\n\n   user  system elapsed \n  0.030   0.001   0.328 \n\n# Reset to sequential processing\nplan(sequential)\n\nprint(\"Results are identical:\")\n\n[1] \"Results are identical:\"\n\nprint(identical(sequential_results, parallel_results))\n\n[1] FALSE"
  },
  {
    "objectID": "06-functional-programming.html#exercises",
    "href": "06-functional-programming.html#exercises",
    "title": "Chapter 6: Functional Programming with purrr - Iteration and Mapping Made Elegant",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Nested Data Analysis\nWork with nested data frames to perform group-wise analysis:\n\n# Your solution\n# Create nested data\nsales_data &lt;- tibble(\n  region = rep(c(\"North\", \"South\", \"East\", \"West\"), each = 50),\n  month = rep(1:12, length.out = 200),\n  sales = c(\n    rnorm(50, 1000, 200),  # North\n    rnorm(50, 1200, 250),  # South\n    rnorm(50, 900, 150),   # East\n    rnorm(50, 1100, 300)   # West\n  )\n)\n\n# Nest and analyze\nsales_analysis &lt;- sales_data %&gt;%\n  nest(data = c(month, sales)) %&gt;%\n  mutate(\n    # Calculate statistics\n    total_sales = map_dbl(data, ~ sum(.$sales)),\n    avg_sales = map_dbl(data, ~ mean(.$sales)),\n    cv = map_dbl(data, ~ sd(.$sales) / mean(.$sales)),\n    \n    # Fit trend model\n    model = map(data, ~ lm(sales ~ month, data = .)),\n    \n    # Extract slope (trend)\n    trend = map_dbl(model, ~ coef(.)[2]),\n    \n    # Model quality\n    r_squared = map_dbl(model, ~ summary(.)$r.squared)\n  )\n\nsales_analysis %&gt;%\n  select(-data, -model) %&gt;%\n  arrange(desc(total_sales))\n\n# A tibble: 4 x 6\n  region total_sales avg_sales    cv trend r_squared\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 South       60306.     1206. 0.202 -8.47   0.0145 \n2 West        54712.     1094. 0.258 22.5    0.0748 \n3 North       51065.     1021. 0.208 -3.71   0.00386\n4 East        44373.      887. 0.162  2.31   0.00302\n\n\n\n\nExercise 2: Safe Data Cleaning\nCreate a robust data cleaning pipeline that handles errors:\n\n# Your solution\n# Messy data with various problems\nmessy_data &lt;- list(\n  clean = tibble(x = 1:5, y = 2:6),\n  missing_col = tibble(x = 1:5),\n  wrong_type = tibble(x = letters[1:5], y = 2:6),\n  empty = tibble(),\n  null = NULL\n)\n\n# Create safe cleaning function\nsafe_clean &lt;- possibly(function(df) {\n  df %&gt;%\n    mutate(\n      z = x + y,\n      category = if_else(z &gt; 5, \"high\", \"low\")\n    )\n}, otherwise = tibble(error = \"Processing failed\"))\n\n# Apply to all data\ncleaned_results &lt;- map(messy_data, safe_clean)\n\n# Check which ones succeeded\nsuccess_status &lt;- map_lgl(cleaned_results, ~ !(\"error\" %in% names(.)))\nnames(success_status) &lt;- names(messy_data)\nprint(success_status)\n\n      clean missing_col  wrong_type       empty        null \n       TRUE       FALSE       FALSE       FALSE       FALSE \n\n\n\n\nExercise 3: Bootstrap Confidence Intervals\nImplement bootstrap for multiple statistics:\n\n# Your solution\n# Sample data\nsample_data &lt;- rgamma(100, shape = 2, rate = 0.5)\n\n# Bootstrap function for multiple statistics\nbootstrap_stats &lt;- function(data, n_boot = 1000, conf_level = 0.95) {\n  alpha &lt;- 1 - conf_level\n  \n  # Generate bootstrap samples\n  boot_results &lt;- map_df(1:n_boot, function(i) {\n    boot_sample &lt;- sample(data, replace = TRUE)\n    tibble(\n      mean = mean(boot_sample),\n      median = median(boot_sample),\n      sd = sd(boot_sample),\n      iqr = IQR(boot_sample)\n    )\n  })\n  \n  # Calculate confidence intervals\n  stats_summary &lt;- boot_results %&gt;%\n    pivot_longer(everything(), names_to = \"statistic\", values_to = \"value\") %&gt;%\n    group_by(statistic) %&gt;%\n    summarise(\n      estimate = mean(value),\n      se = sd(value),\n      ci_lower = quantile(value, alpha/2),\n      ci_upper = quantile(value, 1 - alpha/2),\n      .groups = \"drop\"\n    )\n  \n  return(stats_summary)\n}\n\n# Apply bootstrap\nboot_ci &lt;- bootstrap_stats(sample_data)\nprint(boot_ci)\n\n# A tibble: 4 x 5\n  statistic estimate    se ci_lower ci_upper\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 iqr           3.25 0.414     2.58     4.19\n2 mean          3.84 0.300     3.28     4.41\n3 median        2.93 0.218     2.56     3.44\n4 sd            2.91 0.321     2.29     3.56\n\n# Visualize bootstrap distributions\nboot_samples &lt;- map_df(1:1000, function(i) {\n  boot_sample &lt;- sample(sample_data, replace = TRUE)\n  tibble(\n    iteration = i,\n    mean = mean(boot_sample),\n    median = median(boot_sample)\n  )\n})\n\nboot_samples %&gt;%\n  pivot_longer(c(mean, median), names_to = \"statistic\", values_to = \"value\") %&gt;%\n  ggplot(aes(x = value, fill = statistic)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(~statistic, scales = \"free\") +\n  theme_minimal() +\n  labs(title = \"Bootstrap Distributions\")\n\n\n\n\n\n\n\n\n\n\nExercise 4: Simulation Power Analysis\nSimulate power for different effect sizes:\n\n# Your solution\n# Simulation parameters\neffect_sizes &lt;- seq(0, 1, by = 0.1)\nn_sims &lt;- 500\nsample_size &lt;- 50\n\n# Power simulation function\ncalculate_power &lt;- function(effect_size, n = sample_size, n_sim = n_sims) {\n  p_values &lt;- map_dbl(1:n_sim, function(i) {\n    control &lt;- rnorm(n, mean = 0, sd = 1)\n    treatment &lt;- rnorm(n, mean = effect_size, sd = 1)\n    t.test(treatment, control)$p.value\n  })\n  \n  mean(p_values &lt; 0.05)\n}\n\n# Calculate power for each effect size\npower_results &lt;- tibble(\n  effect_size = effect_sizes,\n  power = map_dbl(effect_sizes, calculate_power)\n)\n\n# Visualize power curve\nggplot(power_results, aes(x = effect_size, y = power)) +\n  geom_line(linewidth = 1.5) +\n  geom_point(size = 3) +\n  geom_hline(yintercept = 0.8, linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = 0.5, linetype = \"dashed\", color = \"blue\") +\n  scale_y_continuous(limits = c(0, 1)) +\n  labs(\n    title = \"Power Analysis: Effect Size vs Statistical Power\",\n    subtitle = paste(\"n =\", sample_size, \"per group, α = 0.05\"),\n    x = \"Effect Size (Cohen's d)\",\n    y = \"Statistical Power\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "06-functional-programming.html#summary",
    "href": "06-functional-programming.html#summary",
    "title": "Chapter 6: Functional Programming with purrr - Iteration and Mapping Made Elegant",
    "section": "Summary",
    "text": "Summary\nIn this comprehensive chapter, you’ve mastered:\n✅ Functional programming concepts - Replacing loops with map functions - Type-safe iterations - Anonymous functions and shortcuts\n✅ Advanced purrr techniques - map2() and pmap() for multiple inputs - Nested data manipulation - Error handling with safely() and possibly()\n✅ Functional patterns - Function factories and composition - Reduce and accumulate operations - Parallel processing with furrr\n✅ Real-world applications - Bootstrap analysis - Multiple model fitting - Simulation studies - Performance optimization\nKey takeaways: - Functional programming makes code more readable and maintainable - purrr integrates seamlessly with tidyverse workflows - Error handling is crucial for robust data pipelines - Vectorization and parallelization improve performance - Think in terms of functions, not loops"
  },
  {
    "objectID": "06-functional-programming.html#whats-next",
    "href": "06-functional-programming.html#whats-next",
    "title": "Chapter 6: Functional Programming with purrr - Iteration and Mapping Made Elegant",
    "section": "What’s Next?",
    "text": "What’s Next?\nIn Chapter 7, we’ll explore string manipulation and date/time handling with stringr and lubridate."
  },
  {
    "objectID": "06-functional-programming.html#additional-resources",
    "href": "06-functional-programming.html#additional-resources",
    "title": "Chapter 6: Functional Programming with purrr - Iteration and Mapping Made Elegant",
    "section": "Additional Resources",
    "text": "Additional Resources\n\npurrr Documentation\nAdvanced R - Functionals\nR for Data Science - Iteration\nJenny Bryan’s purrr Tutorial\nfurrr Documentation"
  },
  {
    "objectID": "05-visualization.html",
    "href": "05-visualization.html",
    "title": "Chapter 5: Data Visualization with ggplot2",
    "section": "",
    "text": "By the end of this chapter, you will master:\n\nThe grammar of graphics philosophy\nCreating basic plots: scatter, line, bar, histogram\nCustomizing aesthetics, scales, and themes\nCreating complex multi-panel visualizations\nStatistical layers and transformations\nInteractive visualizations\nPublication-ready graphics"
  },
  {
    "objectID": "05-visualization.html#learning-objectives",
    "href": "05-visualization.html#learning-objectives",
    "title": "Chapter 5: Data Visualization with ggplot2",
    "section": "",
    "text": "By the end of this chapter, you will master:\n\nThe grammar of graphics philosophy\nCreating basic plots: scatter, line, bar, histogram\nCustomizing aesthetics, scales, and themes\nCreating complex multi-panel visualizations\nStatistical layers and transformations\nInteractive visualizations\nPublication-ready graphics"
  },
  {
    "objectID": "05-visualization.html#setup",
    "href": "05-visualization.html#setup",
    "title": "Chapter 5: Data Visualization with ggplot2",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.4     v readr     2.1.5\nv forcats   1.0.0     v stringr   1.5.2\nv ggplot2   4.0.0     v tibble    3.3.0\nv lubridate 1.9.4     v tidyr     1.3.1\nv purrr     1.1.0     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(palmerpenguins)\nlibrary(gapminder)\nlibrary(patchwork)  # For combining plots\nlibrary(scales)     # For scale formatting\n\n\nAdjuntando el paquete: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nlibrary(viridis)   # For color palettes\n\nCargando paquete requerido: viridisLite\n\nAdjuntando el paquete: 'viridis'\n\nThe following object is masked from 'package:scales':\n\n    viridis_pal\n\nlibrary(ggrepel)   # For better text labels\nlibrary(GGally)    # For pair plots\n\n# Set a default theme\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "05-visualization.html#the-grammar-of-graphics",
    "href": "05-visualization.html#the-grammar-of-graphics",
    "title": "Chapter 5: Data Visualization with ggplot2",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nggplot2 builds plots layer by layer:\n\nData: The dataset\nAesthetics (aes): How variables map to visual properties\nGeometries (geom): The type of plot\nFacets: Subplots\nStatistics: Statistical transformations\nCoordinates: Coordinate systems\nThemes: Overall visual appearance"
  },
  {
    "objectID": "05-visualization.html#basic-plots",
    "href": "05-visualization.html#basic-plots",
    "title": "Chapter 5: Data Visualization with ggplot2",
    "section": "Basic Plots",
    "text": "Basic Plots\n\nScatter Plots\n\n# Basic scatter plot\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()\n\n\n\n\n\n\n\n# Enhanced scatter plot\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species, shape = species), size = 3, alpha = 0.7) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(\n    title = \"Penguin Body Mass vs Flipper Length\",\n    subtitle = \"Data from Palmer Station, Antarctica\",\n    x = \"Flipper Length (mm)\",\n    y = \"Body Mass (g)\",\n    color = \"Species\",\n    shape = \"Species\",\n    caption = \"Source: palmerpenguins package\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n# Adding trend lines\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point(size = 2, alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = TRUE) +  # Linear regression with confidence interval\n  scale_color_viridis_d() +\n  facet_wrap(~island) +  # Separate panels by island\n  labs(\n    title = \"Penguin Morphology by Island\",\n    x = \"Flipper Length (mm)\",\n    y = \"Body Mass (g)\"\n  )\n\n\n\n\n\n\n\n\n\n\nBar Plots\n\n# Count bar plot\nggplot(penguins, aes(x = species)) +\n  geom_bar(fill = \"steelblue\") +\n  labs(title = \"Penguin Count by Species\")\n\n\n\n\n\n\n\n# Grouped bar plot\npenguins %&gt;%\n  count(species, island) %&gt;%\n  ggplot(aes(x = species, y = n, fill = island)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_brewer(palette = \"Set3\") +\n  labs(\n    title = \"Penguin Distribution Across Islands\",\n    x = \"Species\",\n    y = \"Count\",\n    fill = \"Island\"\n  ) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n# Stacked percentage bar plot\npenguins %&gt;%\n  count(species, sex) %&gt;%\n  drop_na() %&gt;%\n  group_by(species) %&gt;%\n  mutate(percentage = n / sum(n) * 100) %&gt;%\n  ggplot(aes(x = species, y = percentage, fill = sex)) +\n  geom_col() +\n  scale_fill_manual(values = c(\"female\" = \"#FF6B9D\", \"male\" = \"#4ECDC4\")) +\n  labs(\n    title = \"Sex Distribution by Species\",\n    x = \"Species\",\n    y = \"Percentage\",\n    fill = \"Sex\"\n  ) +\n  geom_text(aes(label = round(percentage, 1)), \n            position = position_stack(vjust = 0.5),\n            color = \"white\", fontface = \"bold\")\n\n\n\n\n\n\n\n\n\n\nHistograms and Density Plots\n\n# Basic histogram\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(bins = 30, fill = \"darkblue\", alpha = 0.7) +\n  labs(title = \"Distribution of Penguin Body Mass\")\n\n\n\n\n\n\n\n# Density plot with fill\nggplot(penguins, aes(x = body_mass_g, fill = species)) +\n  geom_density(alpha = 0.5) +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"Body Mass Distribution by Species\",\n    x = \"Body Mass (g)\",\n    y = \"Density\"\n  )\n\n\n\n\n\n\n\n# Histogram with density overlay\nggplot(penguins, aes(x = flipper_length_mm)) +\n  geom_histogram(aes(y = after_stat(density)), \n                 bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  geom_density(color = \"red\", linewidth = 1.2) +\n  facet_wrap(~species, scales = \"free_y\") +\n  labs(\n    title = \"Flipper Length Distribution by Species\",\n    x = \"Flipper Length (mm)\",\n    y = \"Density\"\n  )\n\n\n\n\n\n\n\n# Ridge plot (requires ggridges)\nif (require(ggridges, quietly = TRUE)) {\n  ggplot(penguins, aes(x = body_mass_g, y = species, fill = species)) +\n    geom_density_ridges(alpha = 0.7, scale = 1.5) +\n    scale_fill_viridis_d() +\n    labs(\n      title = \"Body Mass Distribution Ridge Plot\",\n      x = \"Body Mass (g)\",\n      y = \"Species\"\n    ) +\n    theme_minimal() +\n    theme(legend.position = \"none\")\n}\n\n\n\nBox Plots and Violin Plots\n\n# Basic box plot\nggplot(penguins, aes(x = species, y = bill_length_mm)) +\n  geom_boxplot(fill = \"lightgreen\", alpha = 0.7) +\n  labs(title = \"Bill Length by Species\")\n\n\n\n\n\n\n\n# Box plot with points\nggplot(penguins, aes(x = species, y = bill_length_mm, fill = species)) +\n  geom_boxplot(alpha = 0.5, outlier.shape = NA) +  # Hide outliers\n  geom_jitter(width = 0.2, alpha = 0.3) +  # Add jittered points\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(\n    title = \"Bill Length Distribution with Individual Points\",\n    x = \"Species\",\n    y = \"Bill Length (mm)\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n# Violin plot with box plot overlay\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_violin(alpha = 0.5) +\n  geom_boxplot(width = 0.2, alpha = 0.7) +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"Body Mass Distribution: Violin + Box Plot\",\n    x = \"Species\",\n    y = \"Body Mass (g)\"\n  ) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "05-visualization.html#line-plots-and-time-series",
    "href": "05-visualization.html#line-plots-and-time-series",
    "title": "Chapter 5: Data Visualization with ggplot2",
    "section": "Line Plots and Time Series",
    "text": "Line Plots and Time Series\n\n# Time series with gapminder\ngapminder %&gt;%\n  filter(country %in% c(\"United States\", \"China\", \"India\", \"Brazil\", \"Germany\")) %&gt;%\n  ggplot(aes(x = year, y = gdpPercap, color = country)) +\n  geom_line(linewidth = 1.2) +\n  geom_point(size = 2) +\n  scale_y_log10(labels = scales::dollar) +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(\n    title = \"GDP per Capita Over Time\",\n    subtitle = \"Selected Countries (1952-2007)\",\n    x = \"Year\",\n    y = \"GDP per Capita (log scale)\",\n    color = \"Country\"\n  ) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n# Area plot\ngapminder %&gt;%\n  filter(continent == \"Americas\") %&gt;%\n  group_by(year) %&gt;%\n  summarise(total_pop = sum(pop) / 1e9) %&gt;%\n  ggplot(aes(x = year, y = total_pop)) +\n  geom_area(fill = \"steelblue\", alpha = 0.7) +\n  geom_line(color = \"darkblue\", linewidth = 1) +\n  labs(\n    title = \"Total Population of the Americas\",\n    x = \"Year\",\n    y = \"Population (billions)\"\n  )\n\n\n\n\n\n\n\n# Multiple lines with confidence intervals\npenguins %&gt;%\n  drop_na() %&gt;%\n  group_by(species, year) %&gt;%\n  summarise(\n    mean_mass = mean(body_mass_g),\n    se_mass = sd(body_mass_g) / sqrt(n()),\n    .groups = \"drop\"\n  ) %&gt;%\n  ggplot(aes(x = year, y = mean_mass, color = species)) +\n  geom_ribbon(aes(ymin = mean_mass - se_mass, \n                  ymax = mean_mass + se_mass, \n                  fill = species), alpha = 0.2) +\n  geom_line(linewidth = 1.2) +\n  geom_point(size = 3) +\n  scale_color_viridis_d() +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"Average Penguin Body Mass by Year\",\n    subtitle = \"With standard error bands\",\n    x = \"Year\",\n    y = \"Mean Body Mass (g)\"\n  )"
  },
  {
    "objectID": "05-visualization.html#advanced-aesthetics-and-scales",
    "href": "05-visualization.html#advanced-aesthetics-and-scales",
    "title": "Chapter 5: Data Visualization with ggplot2",
    "section": "Advanced Aesthetics and Scales",
    "text": "Advanced Aesthetics and Scales\n\n# Custom color scales\np1 &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = body_mass_g)) +\n  geom_point(size = 3) +\n  scale_color_gradient(low = \"blue\", high = \"red\") +\n  labs(title = \"Gradient Color Scale\")\n\np2 &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = body_mass_g)) +\n  geom_point(size = 3) +\n  scale_color_viridis_c(option = \"plasma\") +\n  labs(title = \"Viridis Color Scale\")\n\np3 &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point(size = 3) +\n  scale_color_manual(values = c(\"#E69F00\", \"#56B4E9\", \"#009E73\")) +\n  labs(title = \"Manual Color Scale\")\n\np4 &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm, size = body_mass_g)) +\n  geom_point(alpha = 0.5, color = \"darkblue\") +\n  scale_size_continuous(range = c(1, 10)) +\n  labs(title = \"Size Scale\")\n\n# Combine plots with patchwork\n(p1 + p2) / (p3 + p4) +\n  plot_annotation(title = \"Different Scale Examples\")"
  },
  {
    "objectID": "05-visualization.html#statistical-layers",
    "href": "05-visualization.html#statistical-layers",
    "title": "Chapter 5: Data Visualization with ggplot2",
    "section": "Statistical Layers",
    "text": "Statistical Layers\n\n# Smoothing methods\np1 &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(title = \"Linear Regression\")\n\np2 &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"loess\", color = \"blue\") +\n  labs(title = \"LOESS Smoothing\")\n\n# 2D density\np3 &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_density_2d_filled() +\n  geom_point(alpha = 0.3) +\n  labs(title = \"2D Density Contours\")\n\n# Hexbin plot\np4 &lt;- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_hex(bins = 20) +\n  scale_fill_viridis_c() +\n  labs(title = \"Hexagonal Binning\")\n\n(p1 + p2) / (p3 + p4)"
  },
  {
    "objectID": "05-visualization.html#faceting",
    "href": "05-visualization.html#faceting",
    "title": "Chapter 5: Data Visualization with ggplot2",
    "section": "Faceting",
    "text": "Faceting\n\n# Facet wrap\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point() +\n  facet_wrap(~year, ncol = 2) +\n  scale_color_viridis_d() +\n  labs(title = \"Penguin Bills by Year\")\n\n\n\n\n\n\n\n# Facet grid\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(color = sex)) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_grid(species ~ island) +\n  labs(title = \"Penguin Bills: Species by Island Grid\") +\n  theme(strip.text = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n# Free scales\npenguins %&gt;%\n  pivot_longer(cols = contains(\"mm\"), names_to = \"measurement\", values_to = \"value\") %&gt;%\n  ggplot(aes(x = value, fill = species)) +\n  geom_histogram(bins = 30, alpha = 0.7) +\n  facet_wrap(~measurement, scales = \"free\", ncol = 2) +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"Distribution of Penguin Measurements\",\n    x = \"Value (mm)\",\n    y = \"Count\"\n  )"
  },
  {
    "objectID": "05-visualization.html#annotations-and-labels",
    "href": "05-visualization.html#annotations-and-labels",
    "title": "Chapter 5: Data Visualization with ggplot2",
    "section": "Annotations and Labels",
    "text": "Annotations and Labels\n\n# Text annotations\navg_by_species &lt;- penguins %&gt;%\n  group_by(species) %&gt;%\n  summarise(\n    avg_bill_length = mean(bill_length_mm, na.rm = TRUE),\n    avg_bill_depth = mean(bill_depth_mm, na.rm = TRUE)\n  )\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point(alpha = 0.5) +\n  geom_point(data = avg_by_species, \n             aes(x = avg_bill_length, y = avg_bill_depth),\n             size = 5, shape = 18) +\n  geom_text(data = avg_by_species,\n            aes(x = avg_bill_length, y = avg_bill_depth, label = species),\n            vjust = -1.5, fontface = \"bold\") +\n  annotate(\"text\", x = 55, y = 21, \n           label = \"Diamonds show\\nspecies averages\", \n           size = 4, fontface = \"italic\") +\n  annotate(\"segment\", x = 53, y = 20.5, xend = 51, yend = 19,\n           arrow = arrow(length = unit(0.3, \"cm\"))) +\n  scale_color_viridis_d() +\n  labs(\n    title = \"Penguin Bill Dimensions with Species Averages\",\n    x = \"Bill Length (mm)\",\n    y = \"Bill Depth (mm)\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n# Using ggrepel for better label placement\nif (require(ggrepel, quietly = TRUE)) {\n  top_countries &lt;- gapminder %&gt;%\n    filter(year == 2007) %&gt;%\n    arrange(desc(gdpPercap)) %&gt;%\n    head(10)\n  \n  ggplot(top_countries, aes(x = gdpPercap, y = lifeExp)) +\n    geom_point(aes(size = pop), alpha = 0.7, color = \"steelblue\") +\n    geom_text_repel(aes(label = country), size = 3) +\n    scale_x_log10(labels = scales::dollar) +\n    scale_size_continuous(range = c(3, 15), labels = scales::comma) +\n    labs(\n      title = \"Top 10 Countries by GDP per Capita (2007)\",\n      x = \"GDP per Capita (log scale)\",\n      y = \"Life Expectancy\",\n      size = \"Population\"\n    )\n}"
  },
  {
    "objectID": "05-visualization.html#custom-themes",
    "href": "05-visualization.html#custom-themes",
    "title": "Chapter 5: Data Visualization with ggplot2",
    "section": "Custom Themes",
    "text": "Custom Themes\n\n# Create a custom theme\ntheme_workshop &lt;- function() {\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 12, hjust = 0.5, face = \"italic\"),\n    axis.title = element_text(size = 12, face = \"bold\"),\n    axis.text = element_text(size = 10),\n    legend.title = element_text(size = 11, face = \"bold\"),\n    legend.position = \"bottom\",\n    panel.grid.minor = element_blank(),\n    panel.border = element_rect(color = \"gray80\", fill = NA),\n    plot.background = element_rect(fill = \"gray98\"),\n    strip.background = element_rect(fill = \"gray90\"),\n    strip.text = element_text(face = \"bold\")\n  )\n}\n\n# Apply custom theme\np1 &lt;- ggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(title = \"theme_minimal()\") +\n  theme_minimal()\n\np2 &lt;- ggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(title = \"theme_classic()\") +\n  theme_classic()\n\np3 &lt;- ggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(title = \"theme_dark()\") +\n  theme_dark()\n\np4 &lt;- ggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(title = \"Custom Theme\") +\n  theme_workshop()\n\n(p1 + p2) / (p3 + p4)"
  },
  {
    "objectID": "05-visualization.html#complex-visualizations",
    "href": "05-visualization.html#complex-visualizations",
    "title": "Chapter 5: Data Visualization with ggplot2",
    "section": "Complex Visualizations",
    "text": "Complex Visualizations\n\n# Correlation heatmap\nif (require(corrplot, quietly = TRUE)) {\n  penguins %&gt;%\n    select(where(is.numeric)) %&gt;%\n    drop_na() %&gt;%\n    cor() %&gt;%\n    corrplot(method = \"color\", type = \"upper\", \n             order = \"hclust\", tl.cex = 0.8)\n}\n\n\n\n\n\n\n\n# Pair plot with GGally\nif (require(GGally, quietly = TRUE)) {\n  penguins %&gt;%\n    select(species, bill_length_mm, bill_depth_mm, \n           flipper_length_mm, body_mass_g) %&gt;%\n    drop_na() %&gt;%\n    ggpairs(\n      aes(color = species),\n      upper = list(continuous = \"density\", combo = \"box_no_facet\"),\n      lower = list(continuous = \"points\", combo = \"dot_no_facet\"),\n      diag = list(continuous = \"densityDiag\"),\n      title = \"Penguin Measurements Pair Plot\"\n    ) +\n    scale_color_viridis_d() +\n    scale_fill_viridis_d()\n}\n\n\n\n\n\n\n\n# Complex composite visualization\nplot_data &lt;- gapminder %&gt;%\n  filter(year == 2007) %&gt;%\n  mutate(pop_millions = pop / 1e6)\n\nmain_plot &lt;- ggplot(plot_data, aes(x = gdpPercap, y = lifeExp)) +\n  geom_point(aes(size = pop_millions, color = continent), alpha = 0.6) +\n  scale_x_log10(labels = scales::dollar) +\n  scale_size_continuous(range = c(1, 15), \n                       labels = scales::comma,\n                       name = \"Population (millions)\") +\n  scale_color_brewer(palette = \"Set2\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"gray30\", linetype = \"dashed\") +\n  labs(\n    title = \"Global Development in 2007\",\n    subtitle = \"Relationship between wealth, health, and population\",\n    x = \"GDP per Capita (log scale)\",\n    y = \"Life Expectancy (years)\",\n    caption = \"Source: Gapminder dataset\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n# Marginal distributions\nx_density &lt;- ggplot(plot_data, aes(x = gdpPercap, fill = continent)) +\n  geom_density(alpha = 0.5) +\n  scale_x_log10() +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme_void() +\n  theme(legend.position = \"none\")\n\ny_density &lt;- ggplot(plot_data, aes(y = lifeExp, fill = continent)) +\n  geom_density(alpha = 0.5) +\n  scale_fill_brewer(palette = \"Set2\") +\n  coord_flip() +\n  theme_void() +\n  theme(legend.position = \"none\")\n\n# Combine with patchwork\nx_density / main_plot / plot_spacer() + \n  plot_layout(heights = c(1, 4, 0.1))"
  },
  {
    "objectID": "05-visualization.html#interactive-visualizations",
    "href": "05-visualization.html#interactive-visualizations",
    "title": "Chapter 5: Data Visualization with ggplot2",
    "section": "Interactive Visualizations",
    "text": "Interactive Visualizations\n\n# Using plotly for interactivity\nif (require(plotly, quietly = TRUE)) {\n  # Create a simple interactive plot\n  # Note: plotly works better with simpler ggplot objects\n  penguins_clean &lt;- penguins %&gt;% \n    drop_na(bill_length_mm, bill_depth_mm, species)\n  \n  p &lt;- ggplot(penguins_clean, aes(x = bill_length_mm, \n                            y = bill_depth_mm, \n                            color = species)) +\n    geom_point(size = 2) +\n    theme_minimal() +\n    labs(\n      title = \"Interactive Penguin Plot\",\n      x = \"Bill Length (mm)\",\n      y = \"Bill Depth (mm)\"\n    )\n  \n  # Try to create plotly plot, fallback to static if error\n  tryCatch({\n    ggplotly(p)\n  }, error = function(e) {\n    print(\"Note: Interactive plot failed, showing static version\")\n    print(p)\n  })\n}\n\n[1] \"Note: Interactive plot failed, showing static version\""
  },
  {
    "objectID": "05-visualization.html#saving-plots",
    "href": "05-visualization.html#saving-plots",
    "title": "Chapter 5: Data Visualization with ggplot2",
    "section": "Saving Plots",
    "text": "Saving Plots\n\n# Create a publication-ready plot\nfinal_plot &lt;- penguins %&gt;%\n  drop_na() %&gt;%\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species, shape = island), size = 2.5, alpha = 0.7) +\n  geom_smooth(aes(color = species), method = \"lm\", se = FALSE) +\n  scale_color_brewer(palette = \"Set1\") +\n  scale_shape_manual(values = c(16, 17, 15)) +\n  labs(\n    title = \"Palmer Penguins Morphology\",\n    subtitle = \"Relationship between flipper length and body mass\",\n    x = \"Flipper Length (mm)\",\n    y = \"Body Mass (g)\",\n    color = \"Species\",\n    shape = \"Island\",\n    caption = \"Data: Palmer Station, Antarctica\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"bottom\",\n    legend.box = \"horizontal\"\n  )\n\n# Save in different formats\nggsave(\"penguin_plot.png\", final_plot, width = 10, height = 8, dpi = 300)\nggsave(\"penguin_plot.pdf\", final_plot, width = 10, height = 8)\nggsave(\"penguin_plot.svg\", final_plot, width = 10, height = 8)\n\nprint(final_plot)"
  },
  {
    "objectID": "05-visualization.html#exercises",
    "href": "05-visualization.html#exercises",
    "title": "Chapter 5: Data Visualization with ggplot2",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Create a Complex Bar Chart\nCreate a bar chart showing average life expectancy by continent over time using gapminder data:\n\n# Your solution\ngapminder %&gt;%\n  group_by(continent, year) %&gt;%\n  summarise(avg_lifeExp = weighted.mean(lifeExp, pop), .groups = \"drop\") %&gt;%\n  ggplot(aes(x = year, y = avg_lifeExp, fill = continent)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(\n    title = \"Average Life Expectancy by Continent Over Time\",\n    subtitle = \"Population-weighted averages\",\n    x = \"Year\",\n    y = \"Life Expectancy (years)\",\n    fill = \"Continent\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\nExercise 2: Multi-panel Visualization\nCreate a visualization showing the relationship between GDP and life expectancy for each continent, with trend lines:\n\n# Your solution\ngapminder %&gt;%\n  filter(year == 2007) %&gt;%\n  ggplot(aes(x = gdpPercap, y = lifeExp)) +\n  geom_point(aes(size = pop, color = continent), alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\", linetype = \"dashed\") +\n  scale_x_log10(labels = scales::dollar) +\n  scale_size_continuous(range = c(1, 10), guide = \"none\") +\n  facet_wrap(~continent, scales = \"free\") +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(\n    title = \"Wealth vs Health by Continent (2007)\",\n    x = \"GDP per Capita (log scale)\",\n    y = \"Life Expectancy\",\n    color = \"Continent\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nExercise 3: Custom Theme\nCreate your own custom theme and apply it to a visualization:\n\n# Your solution\ntheme_exercise &lt;- function() {\n  theme_gray() +\n  theme(\n    plot.title = element_text(size = 18, face = \"bold\", color = \"navy\", hjust = 0.5),\n    plot.subtitle = element_text(size = 14, face = \"italic\", hjust = 0.5),\n    axis.title = element_text(size = 12, face = \"bold\", color = \"darkblue\"),\n    axis.text = element_text(size = 10),\n    panel.background = element_rect(fill = \"white\"),\n    panel.grid.major = element_line(color = \"gray90\"),\n    panel.grid.minor = element_blank(),\n    legend.background = element_rect(fill = \"gray95\"),\n    legend.key = element_rect(fill = \"white\"),\n    strip.background = element_rect(fill = \"navy\"),\n    strip.text = element_text(color = \"white\", face = \"bold\")\n  )\n}\n\npenguins %&gt;%\n  drop_na() %&gt;%\n  ggplot(aes(x = species, y = body_mass_g, fill = sex)) +\n  geom_violin(alpha = 0.7) +\n  geom_boxplot(width = 0.2, position = position_dodge(0.9)) +\n  scale_fill_manual(values = c(\"female\" = \"pink\", \"male\" = \"lightblue\")) +\n  labs(\n    title = \"Penguin Body Mass Distribution\",\n    subtitle = \"By species and sex\",\n    x = \"Species\",\n    y = \"Body Mass (g)\",\n    fill = \"Sex\"\n  ) +\n  theme_exercise()\n\n\n\n\n\n\n\n\n\n\nExercise 4: Advanced Composite Plot\nCreate a dashboard-style visualization with multiple related plots:\n\n# Your solution\n# Main scatter plot\np_main &lt;- penguins %&gt;%\n  drop_na() %&gt;%\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point(size = 2, alpha = 0.6) +\n  scale_color_viridis_d() +\n  labs(\n    title = \"Penguin Morphology Dashboard\",\n    x = \"Flipper Length (mm)\",\n    y = \"Body Mass (g)\"\n  ) +\n  theme_minimal()\n\n# Species distribution\np_bar &lt;- penguins %&gt;%\n  count(species) %&gt;%\n  ggplot(aes(x = species, y = n, fill = species)) +\n  geom_col() +\n  scale_fill_viridis_d() +\n  labs(title = \"Species Count\", x = \"\", y = \"Count\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Bill measurements\np_bill &lt;- penguins %&gt;%\n  drop_na() %&gt;%\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point(alpha = 0.6) +\n  scale_color_viridis_d() +\n  labs(\n    title = \"Bill Measurements\",\n    x = \"Length (mm)\",\n    y = \"Depth (mm)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Island distribution\np_island &lt;- penguins %&gt;%\n  count(island, species) %&gt;%\n  ggplot(aes(x = island, y = n, fill = species)) +\n  geom_col(position = \"fill\") +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"Species by Island\",\n    x = \"Island\",\n    y = \"Proportion\"\n  ) +\n  theme_minimal()\n\n# Combine all plots\n(p_main | (p_bar / p_bill)) / p_island +\n  plot_layout(heights = c(2, 1))"
  },
  {
    "objectID": "05-visualization.html#summary",
    "href": "05-visualization.html#summary",
    "title": "Chapter 5: Data Visualization with ggplot2",
    "section": "Summary",
    "text": "Summary\nYou’ve mastered ggplot2 essentials:\n✅ Grammar of graphics concepts\n✅ Creating various plot types\n✅ Customizing aesthetics and scales\n✅ Using statistical layers\n✅ Faceting for multi-panel plots\n✅ Creating custom themes\n✅ Building complex composite visualizations"
  },
  {
    "objectID": "05-visualization.html#whats-next",
    "href": "05-visualization.html#whats-next",
    "title": "Chapter 5: Data Visualization with ggplot2",
    "section": "What’s Next?",
    "text": "What’s Next?\nIn Chapter 6, we’ll explore functional programming with purrr, learning to apply functions efficiently across data structures."
  },
  {
    "objectID": "05-visualization.html#additional-resources",
    "href": "05-visualization.html#additional-resources",
    "title": "Chapter 5: Data Visualization with ggplot2",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nggplot2 Documentation\nR Graphics Cookbook\nggplot2 Cheat Sheet\nData Visualization: A Practical Introduction"
  },
  {
    "objectID": "02-data-import.html",
    "href": "02-data-import.html",
    "title": "Chapter 2: Data Import and Export with readr and Beyond",
    "section": "",
    "text": "By the end of this chapter, you will:\n\nImport CSV, TSV, and delimited files with readr\nHandle Excel files with readxl\nWork with JSON and XML data\nConnect to databases\nExport data in various formats\nHandle common import problems (encoding, data types, missing values)\nWork with large files efficiently"
  },
  {
    "objectID": "02-data-import.html#learning-objectives",
    "href": "02-data-import.html#learning-objectives",
    "title": "Chapter 2: Data Import and Export with readr and Beyond",
    "section": "",
    "text": "By the end of this chapter, you will:\n\nImport CSV, TSV, and delimited files with readr\nHandle Excel files with readxl\nWork with JSON and XML data\nConnect to databases\nExport data in various formats\nHandle common import problems (encoding, data types, missing values)\nWork with large files efficiently"
  },
  {
    "objectID": "02-data-import.html#setup",
    "href": "02-data-import.html#setup",
    "title": "Chapter 2: Data Import and Export with readr and Beyond",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)  # Includes readr\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.4     v readr     2.1.5\nv forcats   1.0.0     v stringr   1.5.2\nv ggplot2   4.0.0     v tibble    3.3.0\nv lubridate 1.9.4     v tidyr     1.3.1\nv purrr     1.1.0     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(readxl)     # For Excel files\nlibrary(jsonlite)   # For JSON files\n\n\nAdjuntando el paquete: 'jsonlite'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(DBI)        # For databases\nlibrary(RSQLite)    # For SQLite\nlibrary(haven)      # For SPSS, Stata, SAS files\nlibrary(palmerpenguins)  # For penguins dataset\n\n# Create a temporary directory for our examples\ntemp_dir &lt;- tempdir()\n\n# Load the penguins dataset\ndata(penguins)"
  },
  {
    "objectID": "02-data-import.html#reading-csv-files-with-readr",
    "href": "02-data-import.html#reading-csv-files-with-readr",
    "title": "Chapter 2: Data Import and Export with readr and Beyond",
    "section": "Reading CSV Files with readr",
    "text": "Reading CSV Files with readr\n\nBasic CSV Import\n\n# Create a sample CSV file\nsample_data &lt;- tibble(\n  id = 1:5,\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\"),\n  age = c(25, 30, 35, 28, 33),\n  salary = c(50000, 65000, 80000, 55000, 70000),\n  hire_date = c(\"2020-01-15\", \"2019-03-22\", \"2018-07-01\", \"2021-02-10\", \"2019-11-30\")\n)\n\n# Write to CSV\ncsv_file &lt;- file.path(temp_dir, \"employees.csv\")\nwrite_csv(sample_data, csv_file)\n\n# Read it back\nemployees &lt;- read_csv(csv_file)\nemployees\n\n# A tibble: 5 x 5\n     id name      age salary hire_date \n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;date&gt;    \n1     1 Alice      25  50000 2020-01-15\n2     2 Bob        30  65000 2019-03-22\n3     3 Charlie    35  80000 2018-07-01\n4     4 Diana      28  55000 2021-02-10\n5     5 Eve        33  70000 2019-11-30\n\n# Compare with base R read.csv\nemployees_base &lt;- read.csv(csv_file)\nstr(employees)      # readr version - note the data types\n\nspc_tbl_ [5 x 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ id       : num [1:5] 1 2 3 4 5\n $ name     : chr [1:5] \"Alice\" \"Bob\" \"Charlie\" \"Diana\" ...\n $ age      : num [1:5] 25 30 35 28 33\n $ salary   : num [1:5] 50000 65000 80000 55000 70000\n $ hire_date: Date[1:5], format: \"2020-01-15\" \"2019-03-22\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   id = col_double(),\n  ..   name = col_character(),\n  ..   age = col_double(),\n  ..   salary = col_double(),\n  ..   hire_date = col_date(format = \"\")\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nstr(employees_base) # base R version - different types!\n\n'data.frame':   5 obs. of  5 variables:\n $ id       : int  1 2 3 4 5\n $ name     : chr  \"Alice\" \"Bob\" \"Charlie\" \"Diana\" ...\n $ age      : int  25 30 35 28 33\n $ salary   : int  50000 65000 80000 55000 70000\n $ hire_date: chr  \"2020-01-15\" \"2019-03-22\" \"2018-07-01\" \"2021-02-10\" ...\n\n\n\n\nSpecifying Column Types\n\n# Explicit column types\nemployees_typed &lt;- read_csv(\n  csv_file,\n  col_types = cols(\n    id = col_integer(),\n    name = col_character(),\n    age = col_double(),\n    salary = col_number(),\n    hire_date = col_date(format = \"%Y-%m-%d\")\n  )\n)\n\nemployees_typed\n\n# A tibble: 5 x 5\n     id name      age salary hire_date \n  &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;date&gt;    \n1     1 Alice      25  50000 2020-01-15\n2     2 Bob        30  65000 2019-03-22\n3     3 Charlie    35  80000 2018-07-01\n4     4 Diana      28  55000 2021-02-10\n5     5 Eve        33  70000 2019-11-30\n\n\n\n\nHandling Messy Data\n\n# Create a messy CSV\nmessy_csv &lt;- \"\nName,Age,Income,Start Date\nAlice,25,$50,000,15/01/2020\nBob,30,$65,000,22/03/2019\nCharlie,35,$80,000,01/07/2018\n#Diana,28,$55,000,10/02/2021\nEve,NA,$70,000,30/11/2019\n\"\n\n# Write to file\nmessy_file &lt;- file.path(temp_dir, \"messy_data.csv\")\nwriteLines(messy_csv, messy_file)\n\n# Read with various options\nclean_data &lt;- read_csv(\n  messy_file,\n  skip = 1,  # Skip the first line\n  comment = \"#\",  # Treat lines starting with # as comments\n  na = c(\"\", \"NA\", \"N/A\", \"missing\"),  # Define NA values\n  col_names = c(\"name\", \"age\", \"income\", \"start_date\"),\n  col_types = cols(\n    name = col_character(),\n    age = col_double(),\n    income = col_character(),  # Keep as character for now\n    start_date = col_character()\n  )\n)\n\n# Clean the income column\nclean_data &lt;- clean_data %&gt;%\n  mutate(\n    income = parse_number(income),  # Remove $ and , from numbers\n    start_date = parse_date(start_date, format = \"%d/%m/%Y\")\n  )\n\nclean_data\n\n# A tibble: 5 x 4\n  name      age income start_date\n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;date&gt;    \n1 Name       NA     NA NA        \n2 Alice      25     50 NA        \n3 Bob        30     65 NA        \n4 Charlie    35     80 NA        \n5 Eve        NA     70 NA"
  },
  {
    "objectID": "02-data-import.html#reading-other-delimited-files",
    "href": "02-data-import.html#reading-other-delimited-files",
    "title": "Chapter 2: Data Import and Export with readr and Beyond",
    "section": "Reading Other Delimited Files",
    "text": "Reading Other Delimited Files\n\nTSV and Custom Delimiters\n\n# Tab-separated values\ntsv_data &lt;- \"name\\tage\\tcity\nAlice\\t25\\tNew York\nBob\\t30\\tLos Angeles\nCharlie\\t35\\tChicago\"\n\ntsv_file &lt;- file.path(temp_dir, \"data.tsv\")\nwriteLines(tsv_data, tsv_file)\n\n# Read TSV\nread_tsv(tsv_file)\n\n# A tibble: 3 x 3\n  name      age city       \n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;      \n1 Alice      25 New York   \n2 Bob        30 Los Angeles\n3 Charlie    35 Chicago    \n\n# Custom delimiter (pipe-separated)\npipe_data &lt;- \"name|age|city\nAlice|25|New York\nBob|30|Los Angeles\"\n\npipe_file &lt;- file.path(temp_dir, \"data.txt\")\nwriteLines(pipe_data, pipe_file)\n\nread_delim(pipe_file, delim = \"|\")\n\n# A tibble: 2 x 3\n  name    age city       \n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;      \n1 Alice    25 New York   \n2 Bob      30 Los Angeles\n\n\n\n\nFixed Width Files\n\n# Fixed width format\nfwf_data &lt;- \"Alice    25   50000\nBob      30   65000\nCharlie  35   80000\"\n\nfwf_file &lt;- file.path(temp_dir, \"fixed_width.txt\")\nwriteLines(fwf_data, fwf_file)\n\n# Read with column positions\nread_fwf(\n  fwf_file,\n  col_positions = fwf_widths(\n    widths = c(9, 5, 7),\n    col_names = c(\"name\", \"age\", \"salary\")\n  )\n)\n\n# A tibble: 3 x 3\n  name      age salary\n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 Alice      25  50000\n2 Bob        30  65000\n3 Charlie    35  80000"
  },
  {
    "objectID": "02-data-import.html#excel-files-with-readxl",
    "href": "02-data-import.html#excel-files-with-readxl",
    "title": "Chapter 2: Data Import and Export with readr and Beyond",
    "section": "Excel Files with readxl",
    "text": "Excel Files with readxl\n\n# Create an Excel file with multiple sheets\nexcel_file &lt;- file.path(temp_dir, \"company_data.xlsx\")\n\n# Create sample data\nsales_data &lt;- tibble(\n  month = month.name[1:6],\n  revenue = c(100000, 120000, 115000, 130000, 125000, 140000),\n  costs = c(80000, 85000, 82000, 90000, 88000, 95000)\n)\n\nemployee_data &lt;- tibble(\n  department = c(\"Sales\", \"Marketing\", \"IT\", \"HR\"),\n  headcount = c(25, 15, 20, 10),\n  avg_salary = c(60000, 55000, 75000, 50000)\n)\n\n# Write to Excel (requires writexl package)\nif (require(writexl, quietly = TRUE)) {\n  write_xlsx(\n    list(\n      Sales = sales_data,\n      Employees = employee_data\n    ),\n    excel_file\n  )\n  \n  # Read from Excel\n  # List all sheets\n  excel_sheets(excel_file)\n  \n  # Read specific sheet\n  sales &lt;- read_excel(excel_file, sheet = \"Sales\")\n  sales\n  \n  # Read with specific range\n  partial_data &lt;- read_excel(\n    excel_file, \n    sheet = \"Sales\",\n    range = \"A1:B4\"  # Read only first 3 rows and 2 columns\n  )\n  partial_data\n  \n  # Read all sheets at once\n  all_sheets &lt;- excel_sheets(excel_file) %&gt;%\n    set_names() %&gt;%\n    map(~ read_excel(excel_file, sheet = .))\n  \n  all_sheets\n}\n\n$Sales\n# A tibble: 6 x 3\n  month    revenue costs\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 January   100000 80000\n2 February  120000 85000\n3 March     115000 82000\n4 April     130000 90000\n5 May       125000 88000\n6 June      140000 95000\n\n$Employees\n# A tibble: 4 x 3\n  department headcount avg_salary\n  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n1 Sales             25      60000\n2 Marketing         15      55000\n3 IT                20      75000\n4 HR                10      50000"
  },
  {
    "objectID": "02-data-import.html#json-data",
    "href": "02-data-import.html#json-data",
    "title": "Chapter 2: Data Import and Export with readr and Beyond",
    "section": "JSON Data",
    "text": "JSON Data\n\n# Create JSON data\njson_data &lt;- list(\n  employees = list(\n    list(\n      id = 1,\n      name = \"Alice\",\n      skills = c(\"R\", \"Python\", \"SQL\"),\n      projects = list(\n        list(name = \"Project A\", status = \"completed\"),\n        list(name = \"Project B\", status = \"in progress\")\n      )\n    ),\n    list(\n      id = 2,\n      name = \"Bob\",\n      skills = c(\"JavaScript\", \"HTML\", \"CSS\"),\n      projects = list(\n        list(name = \"Project C\", status = \"completed\")\n      )\n    )\n  )\n)\n\n# Convert to JSON string\njson_string &lt;- toJSON(json_data, pretty = TRUE, auto_unbox = TRUE)\ncat(json_string)\n\n{\n  \"employees\": [\n    {\n      \"id\": 1,\n      \"name\": \"Alice\",\n      \"skills\": [\"R\", \"Python\", \"SQL\"],\n      \"projects\": [\n        {\n          \"name\": \"Project A\",\n          \"status\": \"completed\"\n        },\n        {\n          \"name\": \"Project B\",\n          \"status\": \"in progress\"\n        }\n      ]\n    },\n    {\n      \"id\": 2,\n      \"name\": \"Bob\",\n      \"skills\": [\"JavaScript\", \"HTML\", \"CSS\"],\n      \"projects\": [\n        {\n          \"name\": \"Project C\",\n          \"status\": \"completed\"\n        }\n      ]\n    }\n  ]\n}\n\n# Write to file\njson_file &lt;- file.path(temp_dir, \"employees.json\")\nwrite(json_string, json_file)\n\n# Read JSON\njson_imported &lt;- fromJSON(json_file)\nstr(json_imported)\n\nList of 1\n $ employees:'data.frame':  2 obs. of  4 variables:\n  ..$ id      : int [1:2] 1 2\n  ..$ name    : chr [1:2] \"Alice\" \"Bob\"\n  ..$ skills  :List of 2\n  .. ..$ : chr [1:3] \"R\" \"Python\" \"SQL\"\n  .. ..$ : chr [1:3] \"JavaScript\" \"HTML\" \"CSS\"\n  ..$ projects:List of 2\n  .. ..$ :'data.frame': 2 obs. of  2 variables:\n  .. .. ..$ name  : chr [1:2] \"Project A\" \"Project B\"\n  .. .. ..$ status: chr [1:2] \"completed\" \"in progress\"\n  .. ..$ :'data.frame': 1 obs. of  2 variables:\n  .. .. ..$ name  : chr \"Project C\"\n  .. .. ..$ status: chr \"completed\"\n\n# Flatten nested structure\nemployees_flat &lt;- json_imported$employees %&gt;%\n  as_tibble() %&gt;%\n  mutate(\n    skills = map_chr(skills, ~ paste(., collapse = \", \")),\n    num_projects = map_int(projects, length)\n  ) %&gt;%\n  select(-projects)  # Remove nested column for simplicity\n\nemployees_flat\n\n# A tibble: 2 x 4\n     id name  skills                num_projects\n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;                        &lt;int&gt;\n1     1 Alice R, Python, SQL                   2\n2     2 Bob   JavaScript, HTML, CSS            2"
  },
  {
    "objectID": "02-data-import.html#database-connections",
    "href": "02-data-import.html#database-connections",
    "title": "Chapter 2: Data Import and Export with readr and Beyond",
    "section": "Database Connections",
    "text": "Database Connections\n\n# Create a SQLite database\ndb_file &lt;- file.path(temp_dir, \"company.db\")\ncon &lt;- dbConnect(RSQLite::SQLite(), db_file)\n\n# Write data to database\ndbWriteTable(con, \"employees\", sample_data, overwrite = TRUE)\ndbWriteTable(con, \"sales\", sales_data, overwrite = TRUE)\n\n# List tables\ndbListTables(con)\n\n[1] \"employees\" \"sales\"    \n\n# Query data using SQL\nresult &lt;- dbGetQuery(\n  con,\n  \"SELECT * FROM employees WHERE salary &gt; 60000\"\n)\nresult\n\n  id    name age salary  hire_date\n1  2     Bob  30  65000 2019-03-22\n2  3 Charlie  35  80000 2018-07-01\n3  5     Eve  33  70000 2019-11-30\n\n# Using dplyr with databases\nemployees_db &lt;- tbl(con, \"employees\")\n\n# dplyr operations are translated to SQL\nemployees_db %&gt;%\n  filter(salary &gt; 60000) %&gt;%\n  select(name, salary) %&gt;%\n  arrange(desc(salary)) %&gt;%\n  show_query()  # Show the SQL query\n\n&lt;SQL&gt;\nSELECT `name`, `salary`\nFROM `employees`\nWHERE (`salary` &gt; 60000.0)\nORDER BY `salary` DESC\n\n# Collect results\nhigh_earners &lt;- employees_db %&gt;%\n  filter(salary &gt; 60000) %&gt;%\n  collect()  # Bring data into R\n\nhigh_earners\n\n# A tibble: 3 x 5\n     id name      age salary hire_date \n  &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n1     2 Bob        30  65000 2019-03-22\n2     3 Charlie    35  80000 2018-07-01\n3     5 Eve        33  70000 2019-11-30\n\n# Close connection\ndbDisconnect(con)"
  },
  {
    "objectID": "02-data-import.html#working-with-apis-and-web-data",
    "href": "02-data-import.html#working-with-apis-and-web-data",
    "title": "Chapter 2: Data Import and Export with readr and Beyond",
    "section": "Working with APIs and Web Data",
    "text": "Working with APIs and Web Data\n\n# Example: Reading data from a URL\n# Using a public dataset\nurl &lt;- \"https://raw.githubusercontent.com/tidyverse/ggplot2/main/data-raw/diamonds.csv\"\n\n# Direct read from URL\ndiamonds_web &lt;- read_csv(url, n_max = 1000)  # Read first 1000 rows\nglimpse(diamonds_web)\n\nRows: 1,000\nColumns: 10\n$ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.~\n$ cut     &lt;chr&gt; \"Ideal\", \"Premium\", \"Good\", \"Premium\", \"Good\", \"Very Good\", \"V~\n$ color   &lt;chr&gt; \"E\", \"E\", \"E\", \"I\", \"J\", \"J\", \"I\", \"H\", \"E\", \"H\", \"J\", \"J\", \"F~\n$ clarity &lt;chr&gt; \"SI2\", \"SI1\", \"VS1\", \"VS2\", \"SI2\", \"VVS2\", \"VVS1\", \"SI1\", \"VS2~\n$ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64~\n$ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58~\n$ price   &lt;dbl&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34~\n$ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.~\n$ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.~\n$ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.~\n\n# Download first, then read (useful for large files)\nlocal_file &lt;- file.path(temp_dir, \"diamonds.csv\")\ndownload.file(url, local_file, quiet = TRUE)\ndiamonds_local &lt;- read_csv(local_file, n_max = 1000)"
  },
  {
    "objectID": "02-data-import.html#statistical-software-files-spss-stata-sas",
    "href": "02-data-import.html#statistical-software-files-spss-stata-sas",
    "title": "Chapter 2: Data Import and Export with readr and Beyond",
    "section": "Statistical Software Files (SPSS, Stata, SAS)",
    "text": "Statistical Software Files (SPSS, Stata, SAS)\n\n# Create example data\nsurvey_data &lt;- tibble(\n  respondent_id = 1:10,\n  age_group = factor(c(1, 2, 1, 3, 2, 2, 1, 3, 2, 1),\n                     labels = c(\"18-30\", \"31-50\", \"51+\")),\n  satisfaction = c(4, 5, 3, 4, 5, 2, 3, 4, 5, 4),\n  comments = c(rep(\"Good service\", 5), rep(\"Needs improvement\", 5))\n)\n\n# Save as different formats\nsav_file &lt;- file.path(temp_dir, \"survey.sav\")\ndta_file &lt;- file.path(temp_dir, \"survey.dta\")\n\nwrite_sav(survey_data, sav_file)\nwrite_dta(survey_data, dta_file)\n\n# Read them back\nspss_data &lt;- read_sav(sav_file)\nstata_data &lt;- read_dta(dta_file)\n\nglimpse(spss_data)\n\nRows: 10\nColumns: 4\n$ respondent_id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n$ age_group     &lt;dbl+lbl&gt; 1, 2, 1, 3, 2, 2, 1, 3, 2, 1\n$ satisfaction  &lt;dbl&gt; 4, 5, 3, 4, 5, 2, 3, 4, 5, 4\n$ comments      &lt;chr&gt; \"Good service\", \"Good service\", \"Good service\", \"Good se~"
  },
  {
    "objectID": "02-data-import.html#handling-large-files",
    "href": "02-data-import.html#handling-large-files",
    "title": "Chapter 2: Data Import and Export with readr and Beyond",
    "section": "Handling Large Files",
    "text": "Handling Large Files\n\n# Create a large CSV file\nlarge_data &lt;- tibble(\n  id = 1:100000,\n  value1 = rnorm(100000),\n  value2 = runif(100000),\n  category = sample(LETTERS[1:5], 100000, replace = TRUE)\n)\n\nlarge_file &lt;- file.path(temp_dir, \"large_data.csv\")\nwrite_csv(large_data, large_file)\n\n# Read only first few rows to check structure\npreview &lt;- read_csv(large_file, n_max = 5)\npreview\n\n# A tibble: 5 x 4\n     id  value1 value2 category\n  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1     1 -0.843   0.544 A       \n2     2 -0.679   0.345 E       \n3     3  0.505   0.466 C       \n4     4  0.706   0.841 C       \n5     5  0.0569  0.546 A       \n\n# Read specific columns\nsubset_data &lt;- read_csv(\n  large_file,\n  col_select = c(id, category, value1)\n)\nglimpse(subset_data)\n\nRows: 100,000\nColumns: 3\n$ id       &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18~\n$ category &lt;chr&gt; \"A\", \"E\", \"C\", \"C\", \"A\", \"A\", \"E\", \"A\", \"D\", \"B\", \"C\", \"A\", \"~\n$ value1   &lt;dbl&gt; -0.84294707, -0.67948015, 0.50494394, 0.70600843, 0.05687878,~\n\n# Read in chunks\nprocess_chunk &lt;- function(data, pos) {\n  # Process each chunk\n  summarise(data,\n    chunk = pos,\n    mean_value1 = mean(value1),\n    n = n()\n  )\n}\n\n# Read and process in chunks\nchunked_results &lt;- read_csv_chunked(\n  large_file,\n  callback = DataFrameCallback$new(process_chunk),\n  chunk_size = 10000\n)\n\nchunked_results\n\n# A tibble: 10 x 3\n   chunk mean_value1     n\n   &lt;int&gt;       &lt;dbl&gt; &lt;int&gt;\n 1     1     0.00300 10000\n 2 10001     0.0153  10000\n 3 20001    -0.0135  10000\n 4 30001    -0.00530 10000\n 5 40001    -0.0180  10000\n 6 50001    -0.00333 10000\n 7 60001    -0.00729 10000\n 8 70001    -0.00633 10000\n 9 80001    -0.00363 10000\n10 90001     0.00922 10000"
  },
  {
    "objectID": "02-data-import.html#data-export",
    "href": "02-data-import.html#data-export",
    "title": "Chapter 2: Data Import and Export with readr and Beyond",
    "section": "Data Export",
    "text": "Data Export\n\nWriting CSV and TSV\n\n# Prepare data for export\nexport_data &lt;- penguins %&gt;%\n  drop_na() %&gt;%\n  group_by(species, island) %&gt;%\n  summarise(\n    count = n(),\n    avg_mass = mean(body_mass_g),\n    .groups = \"drop\"\n  )\n\n# Write CSV\nwrite_csv(export_data, file.path(temp_dir, \"penguin_summary.csv\"))\n\n# Write TSV\nwrite_tsv(export_data, file.path(temp_dir, \"penguin_summary.tsv\"))\n\n# Write with custom settings\nwrite_csv2(export_data, file.path(temp_dir, \"penguin_summary_eu.csv\"))  # European format (semicolon separator)\n\n# Write Excel\nif (require(writexl, quietly = TRUE)) {\n  write_xlsx(export_data, file.path(temp_dir, \"penguin_summary.xlsx\"))\n}\n\n\n\nAdvanced Export Options\n\n# Create a complex dataset\ncomplex_data &lt;- tibble(\n  id = 1:5,\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\"),\n  scores = list(\n    c(85, 90, 88),\n    c(92, 88, 95),\n    c(78, 85, 82),\n    c(90, 92, 94),\n    c(88, 86, 90)\n  ),\n  metadata = c(\n    '{\"department\": \"Sales\", \"level\": 2}',\n    '{\"department\": \"IT\", \"level\": 3}',\n    '{\"department\": \"Marketing\", \"level\": 1}',\n    '{\"department\": \"Sales\", \"level\": 3}',\n    '{\"department\": \"IT\", \"level\": 2}'\n  )\n)\n\n# Export to JSON\njson_export &lt;- toJSON(complex_data, pretty = TRUE)\nwrite(json_export, file.path(temp_dir, \"complex_data.json\"))\n\n# Export to RDS (R's native format - preserves all data types)\nsaveRDS(complex_data, file.path(temp_dir, \"complex_data.rds\"))\n\n# Read it back\nrestored_data &lt;- readRDS(file.path(temp_dir, \"complex_data.rds\"))\nidentical(complex_data, restored_data)  # TRUE - perfect preservation\n\n[1] TRUE"
  },
  {
    "objectID": "02-data-import.html#common-import-problems-and-solutions",
    "href": "02-data-import.html#common-import-problems-and-solutions",
    "title": "Chapter 2: Data Import and Export with readr and Beyond",
    "section": "Common Import Problems and Solutions",
    "text": "Common Import Problems and Solutions\n\nProblem 1: Encoding Issues\n\n# Create file with special characters\nspecial_text &lt;- \"Café,Zürich,São Paulo,北京\\n25,30,35,40\"\nspecial_file &lt;- file.path(temp_dir, \"special_chars.csv\")\n\n# Write with UTF-8 encoding\nwriteLines(special_text, special_file, useBytes = TRUE)\n\n# Read with encoding specification\nread_csv(special_file, locale = locale(encoding = \"UTF-8\"))\n\n# A tibble: 1 x 4\n  `Caf&lt;U+00E9&gt;` `Z&lt;U+00FC&gt;rich` `S&lt;U+00E3&gt;o Paulo` `&lt;U+5317&gt;&lt;U+4EAC&gt;`\n          &lt;dbl&gt;           &lt;dbl&gt;              &lt;dbl&gt;              &lt;dbl&gt;\n1            25              30                 35                 40\n\n\n\n\nProblem 2: Inconsistent Data Types\n\n# Mixed types in a column\nmixed_csv &lt;- \"id,value\n1,100\n2,200\n3,NA\n4,missing\n5,300\"\n\nmixed_file &lt;- file.path(temp_dir, \"mixed.csv\")\nwriteLines(mixed_csv, mixed_file)\n\n# Read with proper NA handling\nread_csv(\n  mixed_file,\n  na = c(\"NA\", \"missing\", \"\"),\n  col_types = cols(\n    id = col_integer(),\n    value = col_double()\n  )\n)\n\n# A tibble: 5 x 2\n     id value\n  &lt;int&gt; &lt;dbl&gt;\n1     1   100\n2     2   200\n3     3    NA\n4     4    NA\n5     5   300\n\n\n\n\nProblem 3: Date/Time Formats\n\n# Various date formats\ndates_csv &lt;- \"event,date\nMeeting,2023-01-15\nConference,15/01/2023\nWorkshop,Jan 15 2023\nSeminar,15-Jan-23\"\n\ndates_file &lt;- file.path(temp_dir, \"dates.csv\")\nwriteLines(dates_csv, dates_file)\n\n# Read and parse dates\ndates_data &lt;- read_csv(dates_file, col_types = cols(date = col_character()))\n\n# Parse different formats\ndates_parsed &lt;- dates_data %&gt;%\n  mutate(\n    parsed_date = case_when(\n      str_detect(date, \"^\\\\d{4}-\\\\d{2}-\\\\d{2}$\") ~ parse_date(date, \"%Y-%m-%d\"),\n      str_detect(date, \"^\\\\d{2}/\\\\d{2}/\\\\d{4}$\") ~ parse_date(date, \"%d/%m/%Y\"),\n      str_detect(date, \"^[A-Za-z]+ \\\\d+ \\\\d{4}$\") ~ parse_date(date, \"%b %d %Y\"),\n      str_detect(date, \"^\\\\d{2}-[A-Za-z]+-\\\\d{2}$\") ~ parse_date(date, \"%d-%b-%y\"),\n      TRUE ~ NA_Date_\n    )\n  )\n\ndates_parsed\n\n# A tibble: 4 x 3\n  event      date        parsed_date\n  &lt;chr&gt;      &lt;chr&gt;       &lt;date&gt;     \n1 Meeting    2023-01-15  2023-01-15 \n2 Conference 15/01/2023  2023-01-15 \n3 Workshop   Jan 15 2023 2023-01-15 \n4 Seminar    15-Jan-23   2023-01-15"
  },
  {
    "objectID": "02-data-import.html#exercises",
    "href": "02-data-import.html#exercises",
    "title": "Chapter 2: Data Import and Export with readr and Beyond",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Multi-format Import\nCreate a function that can read data from CSV, Excel, or JSON based on file extension:\n\nread_any &lt;- function(file_path) {\n  ext &lt;- tools::file_ext(file_path)\n  \n  data &lt;- switch(\n    tolower(ext),\n    \"csv\" = read_csv(file_path, show_col_types = FALSE),\n    \"tsv\" = read_tsv(file_path, show_col_types = FALSE),\n    \"xlsx\" = read_excel(file_path),\n    \"xls\" = read_excel(file_path),\n    \"json\" = fromJSON(file_path) %&gt;% as_tibble(),\n    stop(paste(\"Unsupported file type:\", ext))\n  )\n  \n  return(data)\n}\n\n# Test the function\ntest_files &lt;- c(\n  file.path(temp_dir, \"employees.csv\"),\n  file.path(temp_dir, \"data.tsv\")\n)\n\nfor (file in test_files) {\n  if (file.exists(file)) {\n    cat(\"Reading:\", basename(file), \"\\n\")\n    print(read_any(file))\n    cat(\"\\n\")\n  }\n}\n\nReading: employees.csv \n# A tibble: 5 x 5\n     id name      age salary hire_date \n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;date&gt;    \n1     1 Alice      25  50000 2020-01-15\n2     2 Bob        30  65000 2019-03-22\n3     3 Charlie    35  80000 2018-07-01\n4     4 Diana      28  55000 2021-02-10\n5     5 Eve        33  70000 2019-11-30\n\nReading: data.tsv \n# A tibble: 3 x 3\n  name      age city       \n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;      \n1 Alice      25 New York   \n2 Bob        30 Los Angeles\n3 Charlie    35 Chicago    \n\n\n\n\nExercise 2: Data Cleaning Pipeline\nClean and import this messy dataset:\n\nmessy_sales &lt;- \"\nDate,Product,,,Price,Quantity\n2023-01-15,Widget A,,,29.99,10\n2023/01/16,Widget B,,,39.99,missing\n17-01-2023,Widget C,,,49.99,5\n2023-01-18,Widget A,,,29.99,\n,Widget B,,,39.99,8\n\"\n\n# Write to file\nsales_file &lt;- file.path(temp_dir, \"messy_sales.csv\")\nwriteLines(messy_sales, sales_file)\n\n# Your cleaning pipeline here\ncleaned_sales &lt;- read_csv(\n  sales_file,\n  skip_empty_rows = TRUE,\n  na = c(\"\", \"missing\", \"NA\")\n) %&gt;%\n  select(Date, Product, Price, Quantity) %&gt;%\n  filter(!is.na(Date) & !is.na(Product)) %&gt;%\n  mutate(\n    Date = case_when(\n      str_detect(Date, \"^\\\\d{4}-\\\\d{2}-\\\\d{2}$\") ~ parse_date(Date, \"%Y-%m-%d\"),\n      str_detect(Date, \"^\\\\d{4}/\\\\d{2}/\\\\d{2}$\") ~ parse_date(Date, \"%Y/%m/%d\"),\n      str_detect(Date, \"^\\\\d{2}-\\\\d{2}-\\\\d{4}$\") ~ parse_date(Date, \"%d-%m-%Y\"),\n      TRUE ~ NA_Date_\n    ),\n    Quantity = replace_na(Quantity, 0),\n    Total = Price * Quantity\n  )\n\ncleaned_sales\n\n# A tibble: 4 x 5\n  Date       Product  Price Quantity Total\n  &lt;date&gt;     &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 2023-01-15 Widget A  30.0       10  300.\n2 2023-01-16 Widget B  40.0        0    0 \n3 2023-01-17 Widget C  50.0        5  250.\n4 2023-01-18 Widget A  30.0        0    0 \n\n\n\n\nExercise 3: Database Operations\nCreate a mini database with related tables and perform joins:\n\n# Create database\ndb &lt;- dbConnect(RSQLite::SQLite(), \":memory:\")\n\n# Create tables\ncustomers &lt;- tibble(\n  customer_id = 1:5,\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\"),\n  city = c(\"NYC\", \"LA\", \"Chicago\", \"Houston\", \"Phoenix\")\n)\n\norders &lt;- tibble(\n  order_id = 1:8,\n  customer_id = c(1, 2, 1, 3, 4, 2, 5, 1),\n  amount = c(100, 200, 150, 300, 250, 175, 400, 225),\n  order_date = seq(as.Date(\"2023-01-01\"), by = \"week\", length.out = 8)\n)\n\n# Write to database\ndbWriteTable(db, \"customers\", customers)\ndbWriteTable(db, \"orders\", orders)\n\n# Query with join\nresult &lt;- dbGetQuery(db, \"\n  SELECT c.name, c.city, COUNT(o.order_id) as num_orders, SUM(o.amount) as total_spent\n  FROM customers c\n  LEFT JOIN orders o ON c.customer_id = o.customer_id\n  GROUP BY c.customer_id, c.name, c.city\n  ORDER BY total_spent DESC\n\")\n\nresult\n\n     name    city num_orders total_spent\n1   Alice     NYC          3         475\n2     Eve Phoenix          1         400\n3     Bob      LA          2         375\n4 Charlie Chicago          1         300\n5   Diana Houston          1         250\n\n# Using dplyr\ncustomers_tbl &lt;- tbl(db, \"customers\")\norders_tbl &lt;- tbl(db, \"orders\")\n\nsummary_tbl &lt;- customers_tbl %&gt;%\n  left_join(orders_tbl, by = \"customer_id\") %&gt;%\n  group_by(customer_id, name, city) %&gt;%\n  summarise(\n    num_orders = n(),\n    total_spent = sum(amount, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(total_spent)) %&gt;%\n  collect()\n\nsummary_tbl\n\n# A tibble: 5 x 5\n  customer_id name    city    num_orders total_spent\n        &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;        &lt;int&gt;       &lt;dbl&gt;\n1           1 Alice   NYC              3         475\n2           5 Eve     Phoenix          1         400\n3           2 Bob     LA               2         375\n4           3 Charlie Chicago          1         300\n5           4 Diana   Houston          1         250\n\ndbDisconnect(db)\n\n\n\nExercise 4: Batch Processing\nProcess multiple files in a directory:\n\n# Create multiple CSV files\nfor (i in 1:3) {\n  data &lt;- tibble(\n    file_id = i,\n    values = rnorm(100, mean = i * 10, sd = 2),\n    category = sample(c(\"A\", \"B\", \"C\"), 100, replace = TRUE)\n  )\n  write_csv(data, file.path(temp_dir, paste0(\"batch_\", i, \".csv\")))\n}\n\n# Read and combine all files\nbatch_files &lt;- list.files(temp_dir, pattern = \"^batch_.*\\\\.csv$\", full.names = TRUE)\n\ncombined_data &lt;- batch_files %&gt;%\n  map_dfr(~ read_csv(., show_col_types = FALSE)) %&gt;%\n  group_by(file_id, category) %&gt;%\n  summarise(\n    mean_value = mean(values),\n    sd_value = sd(values),\n    n = n(),\n    .groups = \"drop\"\n  )\n\ncombined_data\n\n# A tibble: 9 x 5\n  file_id category mean_value sd_value     n\n    &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n1       1 A              9.80     1.63    29\n2       1 B             10.5      1.80    37\n3       1 C              9.89     2.06    34\n4       2 A             19.6      2.02    35\n5       2 B             20.5      2.26    36\n6       2 C             19.4      2.75    29\n7       3 A             30.3      2.19    35\n8       3 B             29.8      1.83    38\n9       3 C             30.1      2.15    27"
  },
  {
    "objectID": "02-data-import.html#summary",
    "href": "02-data-import.html#summary",
    "title": "Chapter 2: Data Import and Export with readr and Beyond",
    "section": "Summary",
    "text": "Summary\nIn this chapter, you learned:\n✅ Reading various file formats (CSV, Excel, JSON, databases)\n✅ Handling messy and inconsistent data\n✅ Working with large files efficiently\n✅ Exporting data in multiple formats\n✅ Solving common import problems\n✅ Connecting to and querying databases"
  },
  {
    "objectID": "02-data-import.html#whats-next",
    "href": "02-data-import.html#whats-next",
    "title": "Chapter 2: Data Import and Export with readr and Beyond",
    "section": "What’s Next?",
    "text": "What’s Next?\nIn Chapter 3, we’ll dive deep into data manipulation with dplyr, learning to filter, select, mutate, and summarize data efficiently."
  },
  {
    "objectID": "02-data-import.html#additional-resources",
    "href": "02-data-import.html#additional-resources",
    "title": "Chapter 2: Data Import and Export with readr and Beyond",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nreadr Documentation\nreadxl Documentation\nDBI Documentation\nData Import Cheat Sheet"
  },
  {
    "objectID": "15-regression.html",
    "href": "15-regression.html",
    "title": "Chapter 15: Regression Models - Theory, Implementation, and Best Practices",
    "section": "",
    "text": "By the end of this chapter, you will understand:\n\nLinear regression theory and assumptions\nPolynomial and non-linear regression\nRegularization techniques (Ridge, Lasso, Elastic Net)\nRegression trees and ensemble methods\nModel diagnostics and validation\nAdvanced regression techniques\nPractical implementation with tidymodels"
  },
  {
    "objectID": "15-regression.html#learning-objectives",
    "href": "15-regression.html#learning-objectives",
    "title": "Chapter 15: Regression Models - Theory, Implementation, and Best Practices",
    "section": "",
    "text": "By the end of this chapter, you will understand:\n\nLinear regression theory and assumptions\nPolynomial and non-linear regression\nRegularization techniques (Ridge, Lasso, Elastic Net)\nRegression trees and ensemble methods\nModel diagnostics and validation\nAdvanced regression techniques\nPractical implementation with tidymodels"
  },
  {
    "objectID": "15-regression.html#what-is-regression",
    "href": "15-regression.html#what-is-regression",
    "title": "Chapter 15: Regression Models - Theory, Implementation, and Best Practices",
    "section": "What is Regression?",
    "text": "What is Regression?\nRegression analysis is a fundamental statistical technique for modeling the relationship between a dependent variable (target) and one or more independent variables (features). Unlike classification, which predicts discrete categories, regression predicts continuous numerical values.\n\nThe Fundamental Question\nIn regression, we’re essentially asking: “Given these input features, what numerical value should we predict?” This could be: - Predicting house prices based on size, location, and amenities - Estimating customer lifetime value from behavior patterns - Forecasting sales based on historical data and market conditions - Determining optimal drug dosage based on patient characteristics"
  },
  {
    "objectID": "15-regression.html#setup-and-data-preparation",
    "href": "15-regression.html#setup-and-data-preparation",
    "title": "Chapter 15: Regression Models - Theory, Implementation, and Best Practices",
    "section": "Setup and Data Preparation",
    "text": "Setup and Data Preparation\nLet’s begin by loading the necessary libraries and preparing our datasets. We’ll use multiple datasets throughout this chapter to illustrate different concepts and techniques.\n\nlibrary(tidymodels)\n\n-- Attaching packages -------------------------------------- tidymodels 1.4.1 --\n\n\nv broom        1.0.10     v recipes      1.3.1 \nv dials        1.4.2      v rsample      1.3.1 \nv dplyr        1.1.4      v tailor       0.1.0 \nv ggplot2      4.0.0      v tidyr        1.3.1 \nv infer        1.0.9      v tune         2.0.0 \nv modeldata    1.5.1      v workflows    1.3.0 \nv parsnip      1.3.3      v workflowsets 1.1.1 \nv purrr        1.1.0      v yardstick    1.3.2 \n\n\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx purrr::discard() masks scales::discard()\nx dplyr::filter()  masks stats::filter()\nx dplyr::lag()     masks stats::lag()\nx recipes::step()  masks stats::step()\n\nlibrary(tidyverse)\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv forcats   1.0.0     v stringr   1.5.2\nv lubridate 1.9.4     v tibble    3.3.0\nv readr     2.1.5     \n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx readr::col_factor() masks scales::col_factor()\nx purrr::discard()    masks scales::discard()\nx dplyr::filter()     masks stats::filter()\nx stringr::fixed()    masks recipes::fixed()\nx dplyr::lag()        masks stats::lag()\nx readr::spec()       masks yardstick::spec()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(modeldata)\nlibrary(vip)\n\n\nAdjuntando el paquete: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(glmnet)\n\nCargando paquete requerido: Matrix\n\nAdjuntando el paquete: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoaded glmnet 4.1-10\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\nlibrary(patchwork)\nlibrary(broom)\nlibrary(performance)\n\n\nAdjuntando el paquete: 'performance'\n\nThe following objects are masked from 'package:yardstick':\n\n    mae, rmse\n\n# Set theme and seed for reproducibility\ntheme_set(theme_minimal())\nset.seed(123)\n\n# Load datasets\ndata(ames)        # House prices dataset\ndata(concrete)    # Concrete strength dataset\ndata(Chicago)     # Chicago ridership data\n\n# Quick overview of our main dataset\nglimpse(ames)\n\nRows: 2,930\nColumns: 74\n$ MS_SubClass        &lt;fct&gt; One_Story_1946_and_Newer_All_Styles, One_Story_1946~\n$ MS_Zoning          &lt;fct&gt; Residential_Low_Density, Residential_High_Density, ~\n$ Lot_Frontage       &lt;dbl&gt; 141, 80, 81, 93, 74, 78, 41, 43, 39, 60, 75, 0, 63,~\n$ Lot_Area           &lt;int&gt; 31770, 11622, 14267, 11160, 13830, 9978, 4920, 5005~\n$ Street             &lt;fct&gt; Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pav~\n$ Alley              &lt;fct&gt; No_Alley_Access, No_Alley_Access, No_Alley_Access, ~\n$ Lot_Shape          &lt;fct&gt; Slightly_Irregular, Regular, Slightly_Irregular, Re~\n$ Land_Contour       &lt;fct&gt; Lvl, Lvl, Lvl, Lvl, Lvl, Lvl, Lvl, HLS, Lvl, Lvl, L~\n$ Utilities          &lt;fct&gt; AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, All~\n$ Lot_Config         &lt;fct&gt; Corner, Inside, Corner, Corner, Inside, Inside, Ins~\n$ Land_Slope         &lt;fct&gt; Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, G~\n$ Neighborhood       &lt;fct&gt; North_Ames, North_Ames, North_Ames, North_Ames, Gil~\n$ Condition_1        &lt;fct&gt; Norm, Feedr, Norm, Norm, Norm, Norm, Norm, Norm, No~\n$ Condition_2        &lt;fct&gt; Norm, Norm, Norm, Norm, Norm, Norm, Norm, Norm, Nor~\n$ Bldg_Type          &lt;fct&gt; OneFam, OneFam, OneFam, OneFam, OneFam, OneFam, Twn~\n$ House_Style        &lt;fct&gt; One_Story, One_Story, One_Story, One_Story, Two_Sto~\n$ Overall_Cond       &lt;fct&gt; Average, Above_Average, Above_Average, Average, Ave~\n$ Year_Built         &lt;int&gt; 1960, 1961, 1958, 1968, 1997, 1998, 2001, 1992, 199~\n$ Year_Remod_Add     &lt;int&gt; 1960, 1961, 1958, 1968, 1998, 1998, 2001, 1992, 199~\n$ Roof_Style         &lt;fct&gt; Hip, Gable, Hip, Hip, Gable, Gable, Gable, Gable, G~\n$ Roof_Matl          &lt;fct&gt; CompShg, CompShg, CompShg, CompShg, CompShg, CompSh~\n$ Exterior_1st       &lt;fct&gt; BrkFace, VinylSd, Wd Sdng, BrkFace, VinylSd, VinylS~\n$ Exterior_2nd       &lt;fct&gt; Plywood, VinylSd, Wd Sdng, BrkFace, VinylSd, VinylS~\n$ Mas_Vnr_Type       &lt;fct&gt; Stone, None, BrkFace, None, None, BrkFace, None, No~\n$ Mas_Vnr_Area       &lt;dbl&gt; 112, 0, 108, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6~\n$ Exter_Cond         &lt;fct&gt; Typical, Typical, Typical, Typical, Typical, Typica~\n$ Foundation         &lt;fct&gt; CBlock, CBlock, CBlock, CBlock, PConc, PConc, PConc~\n$ Bsmt_Cond          &lt;fct&gt; Good, Typical, Typical, Typical, Typical, Typical, ~\n$ Bsmt_Exposure      &lt;fct&gt; Gd, No, No, No, No, No, Mn, No, No, No, No, No, No,~\n$ BsmtFin_Type_1     &lt;fct&gt; BLQ, Rec, ALQ, ALQ, GLQ, GLQ, GLQ, ALQ, GLQ, Unf, U~\n$ BsmtFin_SF_1       &lt;dbl&gt; 2, 6, 1, 1, 3, 3, 3, 1, 3, 7, 7, 1, 7, 3, 3, 1, 3, ~\n$ BsmtFin_Type_2     &lt;fct&gt; Unf, LwQ, Unf, Unf, Unf, Unf, Unf, Unf, Unf, Unf, U~\n$ BsmtFin_SF_2       &lt;dbl&gt; 0, 144, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1120, 0~\n$ Bsmt_Unf_SF        &lt;dbl&gt; 441, 270, 406, 1045, 137, 324, 722, 1017, 415, 994,~\n$ Total_Bsmt_SF      &lt;dbl&gt; 1080, 882, 1329, 2110, 928, 926, 1338, 1280, 1595, ~\n$ Heating            &lt;fct&gt; GasA, GasA, GasA, GasA, GasA, GasA, GasA, GasA, Gas~\n$ Heating_QC         &lt;fct&gt; Fair, Typical, Typical, Excellent, Good, Excellent,~\n$ Central_Air        &lt;fct&gt; Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, ~\n$ Electrical         &lt;fct&gt; SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SB~\n$ First_Flr_SF       &lt;int&gt; 1656, 896, 1329, 2110, 928, 926, 1338, 1280, 1616, ~\n$ Second_Flr_SF      &lt;int&gt; 0, 0, 0, 0, 701, 678, 0, 0, 0, 776, 892, 0, 676, 0,~\n$ Gr_Liv_Area        &lt;int&gt; 1656, 896, 1329, 2110, 1629, 1604, 1338, 1280, 1616~\n$ Bsmt_Full_Bath     &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, ~\n$ Bsmt_Half_Bath     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\n$ Full_Bath          &lt;int&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 3, 2, ~\n$ Half_Bath          &lt;int&gt; 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, ~\n$ Bedroom_AbvGr      &lt;int&gt; 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 1, 4, 4, ~\n$ Kitchen_AbvGr      &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~\n$ TotRms_AbvGrd      &lt;int&gt; 7, 5, 6, 8, 6, 7, 6, 5, 5, 7, 7, 6, 7, 5, 4, 12, 8,~\n$ Functional         &lt;fct&gt; Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, T~\n$ Fireplaces         &lt;int&gt; 2, 0, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, ~\n$ Garage_Type        &lt;fct&gt; Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, Att~\n$ Garage_Finish      &lt;fct&gt; Fin, Unf, Unf, Fin, Fin, Fin, Fin, RFn, RFn, Fin, F~\n$ Garage_Cars        &lt;dbl&gt; 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, ~\n$ Garage_Area        &lt;dbl&gt; 528, 730, 312, 522, 482, 470, 582, 506, 608, 442, 4~\n$ Garage_Cond        &lt;fct&gt; Typical, Typical, Typical, Typical, Typical, Typica~\n$ Paved_Drive        &lt;fct&gt; Partial_Pavement, Paved, Paved, Paved, Paved, Paved~\n$ Wood_Deck_SF       &lt;int&gt; 210, 140, 393, 0, 212, 360, 0, 0, 237, 140, 157, 48~\n$ Open_Porch_SF      &lt;int&gt; 62, 0, 36, 0, 34, 36, 0, 82, 152, 60, 84, 21, 75, 0~\n$ Enclosed_Porch     &lt;int&gt; 0, 0, 0, 0, 0, 0, 170, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~\n$ Three_season_porch &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\n$ Screen_Porch       &lt;int&gt; 0, 120, 0, 0, 0, 0, 0, 144, 0, 0, 0, 0, 0, 0, 140, ~\n$ Pool_Area          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\n$ Pool_QC            &lt;fct&gt; No_Pool, No_Pool, No_Pool, No_Pool, No_Pool, No_Poo~\n$ Fence              &lt;fct&gt; No_Fence, Minimum_Privacy, No_Fence, No_Fence, Mini~\n$ Misc_Feature       &lt;fct&gt; None, None, Gar2, None, None, None, None, None, Non~\n$ Misc_Val           &lt;int&gt; 0, 0, 12500, 0, 0, 0, 0, 0, 0, 0, 0, 500, 0, 0, 0, ~\n$ Mo_Sold            &lt;int&gt; 5, 6, 6, 4, 3, 6, 4, 1, 3, 6, 4, 3, 5, 2, 6, 6, 6, ~\n$ Year_Sold          &lt;int&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 201~\n$ Sale_Type          &lt;fct&gt; WD , WD , WD , WD , WD , WD , WD , WD , WD , WD , W~\n$ Sale_Condition     &lt;fct&gt; Normal, Normal, Normal, Normal, Normal, Normal, Nor~\n$ Sale_Price         &lt;int&gt; 215000, 105000, 172000, 244000, 189900, 195500, 213~\n$ Longitude          &lt;dbl&gt; -93.61975, -93.61976, -93.61939, -93.61732, -93.638~\n$ Latitude           &lt;dbl&gt; 42.05403, 42.05301, 42.05266, 42.05125, 42.06090, 4~\n\n\nThe Ames housing dataset contains information about residential homes sold in Ames, Iowa. It’s perfect for demonstrating regression techniques because: - It has a clear continuous target (Sale_Price) - It contains both numeric and categorical predictors - It has enough complexity to showcase advanced techniques - It’s real-world data with typical challenges (missing values, outliers, etc.)"
  },
  {
    "objectID": "15-regression.html#linear-regression-the-foundation",
    "href": "15-regression.html#linear-regression-the-foundation",
    "title": "Chapter 15: Regression Models - Theory, Implementation, and Best Practices",
    "section": "Linear Regression: The Foundation",
    "text": "Linear Regression: The Foundation\n\nMathematical Framework\nLinear regression assumes a linear relationship between predictors and the response. The model can be expressed as:\n\\[Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p + \\epsilon\\]\nWhere: - \\(Y\\) is the response variable - \\(X_1, ..., X_p\\) are the predictor variables - \\(\\beta_0\\) is the intercept (the expected value of Y when all X = 0) - \\(\\beta_1, ..., \\beta_p\\) are the coefficients (slopes) - \\(\\epsilon\\) is the error term (assumed to be normally distributed with mean 0)\n\n\nThe Ordinary Least Squares (OLS) Method\nThe most common method for estimating the coefficients is Ordinary Least Squares, which minimizes the sum of squared residuals:\n\\[\\text{RSS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij})^2\\]\nLet’s visualize this concept:\n\n# Create simple example data\nset.seed(123)\nsimple_data &lt;- tibble(\n  x = seq(1, 10, length.out = 50),\n  y_true = 2 + 3 * x,  # True relationship\n  y = y_true + rnorm(50, sd = 3)  # Add noise\n)\n\n# Fit simple linear regression\nsimple_lm &lt;- lm(y ~ x, data = simple_data)\n\n# Add predictions and residuals\nsimple_data &lt;- simple_data %&gt;%\n  mutate(\n    y_pred = predict(simple_lm),\n    residual = y - y_pred\n  )\n\n# Visualize OLS concept\np1 &lt;- ggplot(simple_data, aes(x = x)) +\n  geom_segment(aes(xend = x, y = y, yend = y_pred), \n               color = \"red\", alpha = 0.5) +\n  geom_point(aes(y = y), size = 2) +\n  geom_line(aes(y = y_pred), color = \"blue\", linewidth = 1.2) +\n  geom_line(aes(y = y_true), color = \"green\", linetype = \"dashed\") +\n  labs(\n    title = \"Ordinary Least Squares Visualization\",\n    subtitle = \"Red lines show residuals (errors) being minimized\",\n    x = \"Predictor (X)\",\n    y = \"Response (Y)\"\n  ) +\n  annotate(\"text\", x = 8, y = 15, label = \"True relationship\", \n           color = \"green\", size = 4) +\n  annotate(\"text\", x = 8, y = 25, label = \"Fitted line\", \n           color = \"blue\", size = 4)\n\n# Residual distribution\np2 &lt;- ggplot(simple_data, aes(x = residual)) +\n  geom_histogram(bins = 20, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Distribution of Residuals\",\n    subtitle = \"Should be centered at zero and approximately normal\",\n    x = \"Residual\",\n    y = \"Count\"\n  )\n\np1 + p2\n\n\n\n\n\n\n\n\n\n\nKey Assumptions of Linear Regression\nUnderstanding the assumptions is crucial for proper model interpretation and validation:\n\nLinearity: The relationship between X and Y is linear\nIndependence: Observations are independent of each other\nHomoscedasticity: Constant variance of residuals\nNormality: Residuals are normally distributed\nNo multicollinearity: Predictors are not highly correlated\n\nLet’s check these assumptions with real data:\n\n# Prepare Ames data for simple example\names_simple &lt;- ames %&gt;%\n  select(Sale_Price, Gr_Liv_Area, Year_Built, Overall_Cond) %&gt;%\n  drop_na()\n\n# Fit model\names_lm &lt;- lm(Sale_Price ~ Gr_Liv_Area + Year_Built + Overall_Cond, \n              data = ames_simple)\n\n# Create diagnostic plots\npar(mfrow = c(2, 2))\nplot(ames_lm)\n\n\n\n\n\n\n\n\nEach diagnostic plot tells us something important: - Residuals vs Fitted: Checks linearity and homoscedasticity. We want no clear pattern. - Q-Q Plot: Checks normality of residuals. Points should follow the diagonal line. - Scale-Location: Another check for homoscedasticity. We want a horizontal line. - Residuals vs Leverage: Identifies influential points that might unduly affect the model."
  },
  {
    "objectID": "15-regression.html#implementing-linear-regression-with-tidymodels",
    "href": "15-regression.html#implementing-linear-regression-with-tidymodels",
    "title": "Chapter 15: Regression Models - Theory, Implementation, and Best Practices",
    "section": "Implementing Linear Regression with Tidymodels",
    "text": "Implementing Linear Regression with Tidymodels\nNow let’s implement a proper regression workflow using tidymodels. We’ll predict house prices using multiple features.\n\nData Splitting and Preprocessing\nFirst, we need to properly split our data and create a preprocessing recipe. This is crucial to avoid data leakage and ensure fair model evaluation.\n\n# Clean and prepare the Ames data\names_clean &lt;- ames %&gt;%\n  mutate(Sale_Price = log10(Sale_Price)) %&gt;%  # Log transform for better distribution\n  select(-contains(\"_Condition\"))  # Remove some problematic variables\n\n# Split the data\names_split &lt;- initial_split(ames_clean, prop = 0.75, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test &lt;- testing(ames_split)\n\n# Create preprocessing recipe\names_recipe &lt;- recipe(Sale_Price ~ ., data = ames_train) %&gt;%\n  # Remove variables with near-zero variance\n  step_nzv(all_predictors()) %&gt;%\n  # Impute missing values\n  step_impute_median(all_numeric_predictors()) %&gt;%\n  step_impute_mode(all_nominal_predictors()) %&gt;%\n  # Create dummy variables for categorical predictors\n  step_dummy(all_nominal_predictors()) %&gt;%\n  # Normalize numeric predictors\n  step_normalize(all_numeric_predictors())\n\n# Check the recipe\nprep(ames_recipe) %&gt;%\n  bake(new_data = NULL) %&gt;%\n  glimpse()\n\nRows: 2,197\nColumns: 210\n$ Lot_Frontage                                          &lt;dbl&gt; 0.38099161, -1.0~\n$ Lot_Area                                              &lt;dbl&gt; -0.23232512, -1.~\n$ Year_Built                                            &lt;dbl&gt; -0.036951183, -0~\n$ Year_Remod_Add                                        &lt;dbl&gt; -0.671110551, -0~\n$ Mas_Vnr_Area                                          &lt;dbl&gt; -0.56439715, 2.2~\n$ BsmtFin_SF_1                                          &lt;dbl&gt; -1.4325147, 0.80~\n$ BsmtFin_SF_2                                          &lt;dbl&gt; 0.1623553, -0.29~\n$ Bsmt_Unf_SF                                           &lt;dbl&gt; -1.26807359, -0.~\n$ Total_Bsmt_SF                                         &lt;dbl&gt; -0.369147073, -1~\n$ First_Flr_SF                                          &lt;dbl&gt; -0.705259283, -1~\n$ Second_Flr_SF                                         &lt;dbl&gt; -0.78932267, 0.3~\n$ Gr_Liv_Area                                           &lt;dbl&gt; -1.2416271, -1.0~\n$ Bsmt_Full_Bath                                        &lt;dbl&gt; 1.0937422, -0.81~\n$ Bsmt_Half_Bath                                        &lt;dbl&gt; -0.2447953, -0.2~\n$ Full_Bath                                             &lt;dbl&gt; -1.0378355, -1.0~\n$ Half_Bath                                             &lt;dbl&gt; -0.7549524, 1.22~\n$ Bedroom_AbvGr                                         &lt;dbl&gt; -1.0622200, -1.0~\n$ TotRms_AbvGrd                                         &lt;dbl&gt; -1.5633313, -0.9~\n$ Fireplaces                                            &lt;dbl&gt; -0.9316181, -0.9~\n$ Garage_Cars                                           &lt;dbl&gt; 0.3179223, -1.00~\n$ Garage_Area                                           &lt;dbl&gt; 0.2574653, -0.96~\n$ Wood_Deck_SF                                          &lt;dbl&gt; 1.22164555, 1.50~\n$ Open_Porch_SF                                         &lt;dbl&gt; -0.69696285, -0.~\n$ Mo_Sold                                               &lt;dbl&gt; -0.81175416, -1.~\n$ Year_Sold                                             &lt;dbl&gt; 1.67607, 1.67607~\n$ Longitude                                             &lt;dbl&gt; 0.6146441, 0.608~\n$ Latitude                                              &lt;dbl&gt; 1.03189989, 0.94~\n$ Sale_Price                                            &lt;dbl&gt; 5.100371, 4.9822~\n$ MS_SubClass_One_Story_1945_and_Older                  &lt;dbl&gt; -0.2194655, -0.2~\n$ MS_SubClass_One_Story_with_Finished_Attic_All_Ages    &lt;dbl&gt; -0.04269842, -0.~\n$ MS_SubClass_One_and_Half_Story_Unfinished_All_Ages    &lt;dbl&gt; -0.08006419, -0.~\n$ MS_SubClass_One_and_Half_Story_Finished_All_Ages      &lt;dbl&gt; -0.3368694, -0.3~\n$ MS_SubClass_Two_Story_1946_and_Newer                  &lt;dbl&gt; -0.4924801, -0.4~\n$ MS_SubClass_Two_Story_1945_and_Older                  &lt;dbl&gt; -0.2148708, -0.2~\n$ MS_SubClass_Two_and_Half_Story_All_Ages               &lt;dbl&gt; -0.09086754, -0.~\n$ MS_SubClass_Split_or_Multilevel                       &lt;dbl&gt; -0.2066285, -0.2~\n$ MS_SubClass_Split_Foyer                               &lt;dbl&gt; -0.1290402, -0.1~\n$ MS_SubClass_Duplex_All_Styles_and_Ages                &lt;dbl&gt; -0.1968579, -0.1~\n$ MS_SubClass_One_Story_PUD_1946_and_Newer              &lt;dbl&gt; -0.2628085, -0.2~\n$ MS_SubClass_One_and_Half_Story_PUD_All_Ages           &lt;dbl&gt; -0.02133462, -0.~\n$ MS_SubClass_Two_Story_PUD_1946_and_Newer              &lt;dbl&gt; -0.2171781, 4.60~\n$ MS_SubClass_PUD_Multilevel_Split_Level_Foyer          &lt;dbl&gt; -0.08289329, -0.~\n$ MS_SubClass_Two_Family_conversion_All_Styles_and_Ages &lt;dbl&gt; -0.1412578, -0.1~\n$ MS_Zoning_Residential_High_Density                    &lt;dbl&gt; -0.08828712, -0.~\n$ MS_Zoning_Residential_Low_Density                     &lt;dbl&gt; 0.5433843, -1.83~\n$ MS_Zoning_Residential_Medium_Density                  &lt;dbl&gt; -0.4352129, 2.29~\n$ MS_Zoning_A_agr                                       &lt;dbl&gt; -0.03017859, -0.~\n$ MS_Zoning_C_all                                       &lt;dbl&gt; -0.1005502, -0.1~\n$ MS_Zoning_I_all                                       &lt;dbl&gt; -0.02133462, -0.~\n$ Lot_Shape_Slightly_Irregular                          &lt;dbl&gt; -0.7154097, -0.7~\n$ Lot_Shape_Moderately_Irregular                        &lt;dbl&gt; -0.1660823, -0.1~\n$ Lot_Shape_Irregular                                   &lt;dbl&gt; -0.06760468, -0.~\n$ Lot_Config_CulDSac                                    &lt;dbl&gt; -0.2548026, -0.2~\n$ Lot_Config_FR2                                        &lt;dbl&gt; -0.1786766, -0.1~\n$ Lot_Config_FR3                                        &lt;dbl&gt; -0.07713411, -0.~\n$ Lot_Config_Inside                                     &lt;dbl&gt; -1.649997, 0.605~\n$ Neighborhood_College_Creek                            &lt;dbl&gt; -0.318997, -0.31~\n$ Neighborhood_Old_Town                                 &lt;dbl&gt; -0.2950355, -0.2~\n$ Neighborhood_Edwards                                  &lt;dbl&gt; -0.266744, -0.26~\n$ Neighborhood_Somerset                                 &lt;dbl&gt; -0.2618178, -0.2~\n$ Neighborhood_Northridge_Heights                       &lt;dbl&gt; -0.2392499, -0.2~\n$ Neighborhood_Gilbert                                  &lt;dbl&gt; -0.2434722, -0.2~\n$ Neighborhood_Sawyer                                   &lt;dbl&gt; -0.2217336, -0.2~\n$ Neighborhood_Northwest_Ames                           &lt;dbl&gt; -0.2183242, -0.2~\n$ Neighborhood_Sawyer_West                              &lt;dbl&gt; -0.2171781, -0.2~\n$ Neighborhood_Mitchell                                 &lt;dbl&gt; -0.1981015, -0.1~\n$ Neighborhood_Brookside                                &lt;dbl&gt; -0.1981015, -0.1~\n$ Neighborhood_Crawford                                 &lt;dbl&gt; -0.1968579, -0.1~\n$ Neighborhood_Iowa_DOT_and_Rail_Road                   &lt;dbl&gt; -0.1918152, -0.1~\n$ Neighborhood_Timberland                               &lt;dbl&gt; -0.1587037, -0.1~\n$ Neighborhood_Northridge                               &lt;dbl&gt; -0.1571906, -0.1~\n$ Neighborhood_Stone_Brook                              &lt;dbl&gt; -0.1378697, -0.1~\n$ Neighborhood_South_and_West_of_Iowa_State_University  &lt;dbl&gt; -0.1196059, -0.1~\n$ Neighborhood_Clear_Creek                              &lt;dbl&gt; -0.1215478, -0.1~\n$ Neighborhood_Meadow_Village                           &lt;dbl&gt; -0.1135927, -0.1~\n$ Neighborhood_Briardale                                &lt;dbl&gt; -0.09582679, 10.~\n$ Neighborhood_Bloomington_Heights                      &lt;dbl&gt; -0.09582679, -0.~\n$ Neighborhood_Veenker                                  &lt;dbl&gt; -0.09582679, -0.~\n$ Neighborhood_Northpark_Villa                          &lt;dbl&gt; -0.09086754, -0.~\n$ Neighborhood_Blueste                                  &lt;dbl&gt; -0.06043983, -0.~\n$ Neighborhood_Greens                                   &lt;dbl&gt; -0.04269842, -0.~\n$ Neighborhood_Green_Hills                              &lt;dbl&gt; 0, 0, 0, 0, 0, 0~\n$ Neighborhood_Landmark                                 &lt;dbl&gt; -0.02133462, -0.~\n$ Neighborhood_Hayden_Lake                              &lt;dbl&gt; 0, 0, 0, 0, 0, 0~\n$ Condition_1_Feedr                                     &lt;dbl&gt; -0.2497013, -0.2~\n$ Condition_1_Norm                                      &lt;dbl&gt; 0.4059855, 0.405~\n$ Condition_1_PosA                                      &lt;dbl&gt; -0.08828712, -0.~\n$ Condition_1_PosN                                      &lt;dbl&gt; -0.1094102, -0.1~\n$ Condition_1_RRAe                                      &lt;dbl&gt; -0.09582679, -0.~\n$ Condition_1_RRAn                                      &lt;dbl&gt; -0.1308504, -0.1~\n$ Condition_1_RRNe                                      &lt;dbl&gt; -0.04269842, -0.~\n$ Condition_1_RRNn                                      &lt;dbl&gt; -0.05231854, -0.~\n$ Bldg_Type_TwoFmCon                                    &lt;dbl&gt; -0.1429241, -0.1~\n$ Bldg_Type_Duplex                                      &lt;dbl&gt; -0.1968579, -0.1~\n$ Bldg_Type_Twnhs                                       &lt;dbl&gt; -0.1892507, 5.28~\n$ Bldg_Type_TwnhsE                                      &lt;dbl&gt; -0.2950355, -0.2~\n$ House_Style_One_and_Half_Unf                          &lt;dbl&gt; -0.08289329, -0.~\n$ House_Style_One_Story                                 &lt;dbl&gt; 0.996592, -1.002~\n$ House_Style_SFoyer                                    &lt;dbl&gt; -0.1631668, -0.1~\n$ House_Style_SLvl                                      &lt;dbl&gt; -0.2171781, -0.2~\n$ House_Style_Two_and_Half_Fin                          &lt;dbl&gt; -0.05231854, -0.~\n$ House_Style_Two_and_Half_Unf                          &lt;dbl&gt; -0.0982158, -0.0~\n$ House_Style_Two_Story                                 &lt;dbl&gt; -0.6508892, 1.53~\n$ Overall_Cond_Poor                                     &lt;dbl&gt; -0.05652338, -0.~\n$ Overall_Cond_Fair                                     &lt;dbl&gt; -0.1326376, -0.1~\n$ Overall_Cond_Below_Average                            &lt;dbl&gt; -0.1813704, -0.1~\n$ Overall_Cond_Average                                  &lt;dbl&gt; 0.8734386, 0.873~\n$ Overall_Cond_Above_Average                            &lt;dbl&gt; -0.4688046, -0.4~\n$ Overall_Cond_Good                                     &lt;dbl&gt; -0.3859932, -0.3~\n$ Overall_Cond_Very_Good                                &lt;dbl&gt; -0.2317166, -0.2~\n$ Overall_Cond_Excellent                                &lt;dbl&gt; -0.1308504, -0.1~\n$ Overall_Cond_Very_Excellent                           &lt;dbl&gt; 0, 0, 0, 0, 0, 0~\n$ Roof_Style_Gable                                      &lt;dbl&gt; 0.5109551, 0.510~\n$ Roof_Style_Gambrel                                    &lt;dbl&gt; -0.09337895, -0.~\n$ Roof_Style_Hip                                        &lt;dbl&gt; -0.4788845, -0.4~\n$ Roof_Style_Mansard                                    &lt;dbl&gt; -0.06043983, -0.~\n$ Roof_Style_Shed                                       &lt;dbl&gt; -0.03696949, -0.~\n$ Exterior_1st_AsphShn                                  &lt;dbl&gt; -0.03017859, -0.~\n$ Exterior_1st_BrkComm                                  &lt;dbl&gt; -0.03696949, -0.~\n$ Exterior_1st_BrkFace                                  &lt;dbl&gt; -0.1800279, -0.1~\n$ Exterior_1st_CBlock                                   &lt;dbl&gt; -0.02133462, -0.~\n$ Exterior_1st_CemntBd                                  &lt;dbl&gt; -0.2171781, -0.2~\n$ Exterior_1st_HdBoard                                  &lt;dbl&gt; -0.4165718, 2.39~\n$ Exterior_1st_ImStucc                                  &lt;dbl&gt; -0.02133462, -0.~\n$ Exterior_1st_MetalSd                                  &lt;dbl&gt; -0.4337328, -0.4~\n$ Exterior_1st_Plywood                                  &lt;dbl&gt; 3.497052, -0.285~\n$ Exterior_1st_PreCast                                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0~\n$ Exterior_1st_Stone                                    &lt;dbl&gt; -0.03017859, -0.~\n$ Exterior_1st_Stucco                                   &lt;dbl&gt; -0.1272059, -0.1~\n$ Exterior_1st_VinylSd                                  &lt;dbl&gt; -0.7307357, -0.7~\n$ Exterior_1st_Wd.Sdng                                  &lt;dbl&gt; -0.4014121, -0.4~\n$ Exterior_1st_WdShing                                  &lt;dbl&gt; -0.1429241, -0.1~\n$ Exterior_2nd_AsphShn                                  &lt;dbl&gt; -0.04269842, -0.~\n$ Exterior_2nd_Brk.Cmn                                  &lt;dbl&gt; -0.08563145, -0.~\n$ Exterior_2nd_BrkFace                                  &lt;dbl&gt; -0.1290402, -0.1~\n$ Exterior_2nd_CBlock                                   &lt;dbl&gt; -0.02133462, -0.~\n$ Exterior_2nd_CmentBd                                  &lt;dbl&gt; -0.2171781, -0.2~\n$ Exterior_2nd_HdBoard                                  &lt;dbl&gt; -0.3937376, 2.53~\n$ Exterior_2nd_ImStucc                                  &lt;dbl&gt; -0.07713411, -0.~\n$ Exterior_2nd_MetalSd                                  &lt;dbl&gt; -0.4300246, -0.4~\n$ Exterior_2nd_Other                                    &lt;dbl&gt; 0, 0, 0, 0, 0, 0~\n$ Exterior_2nd_Plywood                                  &lt;dbl&gt; 3.1249258, -0.31~\n$ Exterior_2nd_PreCast                                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0~\n$ Exterior_2nd_Stone                                    &lt;dbl&gt; -0.04774917, -0.~\n$ Exterior_2nd_Stucco                                   &lt;dbl&gt; -0.1308504, -0.1~\n$ Exterior_2nd_VinylSd                                  &lt;dbl&gt; -0.7263444, -0.7~\n$ Exterior_2nd_Wd.Sdng                                  &lt;dbl&gt; -0.3898745, -0.3~\n$ Exterior_2nd_Wd.Shng                                  &lt;dbl&gt; -0.1786766, -0.1~\n$ Mas_Vnr_Type_BrkFace                                  &lt;dbl&gt; -0.6558526, 1.52~\n$ Mas_Vnr_Type_CBlock                                   &lt;dbl&gt; -0.02133462, -0.~\n$ Mas_Vnr_Type_None                                     &lt;dbl&gt; 0.8095158, -1.23~\n$ Mas_Vnr_Type_Stone                                    &lt;dbl&gt; -0.3049467, -0.3~\n$ Exter_Cond_Fair                                       &lt;dbl&gt; -0.1541244, -0.1~\n$ Exter_Cond_Good                                       &lt;dbl&gt; -0.3460075, -0.3~\n$ Exter_Cond_Poor                                       &lt;dbl&gt; -0.03017859, -0.~\n$ Exter_Cond_Typical                                    &lt;dbl&gt; 0.3975833, 0.397~\n$ Foundation_CBlock                                     &lt;dbl&gt; 1.1615155, 1.161~\n$ Foundation_PConc                                      &lt;dbl&gt; -0.8872845, -0.8~\n$ Foundation_Slab                                       &lt;dbl&gt; -0.1395732, -0.1~\n$ Foundation_Stone                                      &lt;dbl&gt; -0.06043983, -0.~\n$ Foundation_Wood                                       &lt;dbl&gt; -0.04774917, -0.~\n$ Bsmt_Exposure_Gd                                      &lt;dbl&gt; -0.3137757, -0.3~\n$ Bsmt_Exposure_Mn                                      &lt;dbl&gt; -0.3013679, -0.3~\n$ Bsmt_Exposure_No                                      &lt;dbl&gt; 0.7358721, 0.735~\n$ Bsmt_Exposure_No_Basement                             &lt;dbl&gt; -0.1773163, -0.1~\n$ BsmtFin_Type_1_BLQ                                    &lt;dbl&gt; -0.312900, -0.31~\n$ BsmtFin_Type_1_GLQ                                    &lt;dbl&gt; -0.6423969, -0.6~\n$ BsmtFin_Type_1_LwQ                                    &lt;dbl&gt; -0.2317166, -0.2~\n$ BsmtFin_Type_1_No_Basement                            &lt;dbl&gt; -0.1745678, -0.1~\n$ BsmtFin_Type_1_Rec                                    &lt;dbl&gt; -0.3351923, 2.98~\n$ BsmtFin_Type_1_Unf                                    &lt;dbl&gt; -0.6416901, -0.6~\n$ Heating_QC_Fair                                       &lt;dbl&gt; -0.1918152, -0.1~\n$ Heating_QC_Good                                       &lt;dbl&gt; -0.4374295, -0.4~\n$ Heating_QC_Poor                                       &lt;dbl&gt; -0.03696949, -0.~\n$ Heating_QC_Typical                                    &lt;dbl&gt; 1.5645654, 1.564~\n$ Central_Air_Y                                         &lt;dbl&gt; 0.2725689, 0.272~\n$ Electrical_FuseF                                      &lt;dbl&gt; -0.1412578, -0.1~\n$ Electrical_FuseP                                      &lt;dbl&gt; -0.05652338, -0.~\n$ Electrical_Mix                                        &lt;dbl&gt; 0, 0, 0, 0, 0, 0~\n$ Electrical_SBrkr                                      &lt;dbl&gt; 0.312900, 0.3129~\n$ Electrical_Unknown                                    &lt;dbl&gt; -0.02133462, -0.~\n$ Garage_Type_Basment                                   &lt;dbl&gt; -0.1196059, -0.1~\n$ Garage_Type_BuiltIn                                   &lt;dbl&gt; -0.2548026, -0.2~\n$ Garage_Type_CarPort                                   &lt;dbl&gt; -0.0709206, -0.0~\n$ Garage_Type_Detchd                                    &lt;dbl&gt; -0.6085939, 1.64~\n$ Garage_Type_More_Than_Two_Types                       &lt;dbl&gt; -0.0709206, -0.0~\n$ Garage_Type_No_Garage                                 &lt;dbl&gt; -0.2434722, -0.2~\n$ Garage_Finish_No_Garage                               &lt;dbl&gt; -0.2445188, -0.2~\n$ Garage_Finish_RFn                                     &lt;dbl&gt; -0.623355, -0.62~\n$ Garage_Finish_Unf                                     &lt;dbl&gt; -0.8478001, 1.17~\n$ Garage_Cond_Fair                                      &lt;dbl&gt; -0.1602038, -0.1~\n$ Garage_Cond_Good                                      &lt;dbl&gt; -0.06760468, -0.~\n$ Garage_Cond_No_Garage                                 &lt;dbl&gt; -0.2445188, -0.2~\n$ Garage_Cond_Poor                                      &lt;dbl&gt; -0.0740911, -0.0~\n$ Garage_Cond_Typical                                   &lt;dbl&gt; 0.3198619, 0.319~\n$ Paved_Drive_Partial_Pavement                          &lt;dbl&gt; -0.1541244, -0.1~\n$ Paved_Drive_Paved                                     &lt;dbl&gt; 0.3275821, 0.327~\n$ Fence_Good_Wood                                       &lt;dbl&gt; -0.2042228, -0.2~\n$ Fence_Minimum_Privacy                                 &lt;dbl&gt; 2.7713349, -0.36~\n$ Fence_Minimum_Wood_Wire                               &lt;dbl&gt; -0.06412077, -0.~\n$ Fence_No_Fence                                        &lt;dbl&gt; -1.9865551, 0.50~\n$ Sale_Type_Con                                         &lt;dbl&gt; -0.04269842, -0.~\n$ Sale_Type_ConLD                                       &lt;dbl&gt; -0.08289329, -0.~\n$ Sale_Type_ConLI                                       &lt;dbl&gt; -0.06043983, -0.~\n$ Sale_Type_ConLw                                       &lt;dbl&gt; -0.05231854, -0.~\n$ Sale_Type_CWD                                         &lt;dbl&gt; -0.0709206, -0.0~\n$ Sale_Type_New                                         &lt;dbl&gt; -0.2886094, -0.2~\n$ Sale_Type_Oth                                         &lt;dbl&gt; -0.04269842, -0.~\n$ Sale_Type_VWD                                         &lt;dbl&gt; -0.02133462, -0.~\n$ Sale_Type_WD.                                         &lt;dbl&gt; 0.3906485, -2.55~\n\n\nNotice how we’re taking several important steps: 1. Log transformation: Sale prices often have a right-skewed distribution. Log transformation makes the distribution more normal. 2. Stratified splitting: Ensures both training and test sets have similar distributions of the target variable. 3. Systematic preprocessing: Handles missing values, creates dummy variables, and normalizes features in a reproducible way.\n\n\nModel Specification and Training\n\n# Specify linear regression model\nlm_spec &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\n# Create workflow\nlm_workflow &lt;- workflow() %&gt;%\n  add_recipe(ames_recipe) %&gt;%\n  add_model(lm_spec)\n\n# Fit the model\nlm_fit &lt;- lm_workflow %&gt;%\n  fit(ames_train)\n\n# Extract and examine coefficients\nlm_coefs &lt;- lm_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  tidy() %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  arrange(desc(abs(estimate))) %&gt;%\n  head(20)\n\n# Visualize top coefficients\nggplot(lm_coefs, aes(x = reorder(term, estimate), y = estimate)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Top 20 Most Influential Features in Linear Regression\",\n    subtitle = \"Coefficients represent change in log10(Sale_Price) per unit change in feature\",\n    x = \"Feature\",\n    y = \"Coefficient\"\n  )\n\n\n\n\n\n\n\n\nThe coefficients tell us the relationship between each feature and the sale price. Since we normalized the predictors, we can compare coefficient magnitudes directly. Positive coefficients increase price, negative ones decrease it."
  },
  {
    "objectID": "15-regression.html#polynomial-and-non-linear-regression",
    "href": "15-regression.html#polynomial-and-non-linear-regression",
    "title": "Chapter 15: Regression Models - Theory, Implementation, and Best Practices",
    "section": "Polynomial and Non-linear Regression",
    "text": "Polynomial and Non-linear Regression\nSometimes relationships aren’t linear. Polynomial regression can capture curved relationships by including polynomial terms.\n\nUnderstanding Polynomial Regression\nPolynomial regression extends linear regression by adding polynomial terms:\n\\[Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + ... + \\beta_dX^d + \\epsilon\\]\nLet’s visualize different polynomial degrees:\n\n# Generate non-linear data\nset.seed(123)\nnonlinear_data &lt;- tibble(\n  x = seq(0, 10, length.out = 100),\n  y_true = 10 + 5*x - 0.5*x^2 + 0.02*x^3,\n  y = y_true + rnorm(100, sd = 5)\n)\n\n# Fit models with different polynomial degrees\ndegrees &lt;- c(1, 2, 3, 5, 10)\npoly_models &lt;- map(degrees, function(d) {\n  lm(y ~ poly(x, degree = d, raw = TRUE), data = nonlinear_data)\n})\n\n# Generate predictions\npoly_predictions &lt;- map2_df(poly_models, degrees, function(model, d) {\n  nonlinear_data %&gt;%\n    mutate(\n      y_pred = predict(model),\n      degree = paste(\"Degree\", d)\n    )\n})\n\n# Visualize\nggplot(poly_predictions, aes(x = x)) +\n  geom_point(aes(y = y), alpha = 0.3, data = nonlinear_data) +\n  geom_line(aes(y = y_pred, color = degree), linewidth = 1.2) +\n  geom_line(aes(y = y_true), color = \"black\", linetype = \"dashed\", \n            data = nonlinear_data, linewidth = 1) +\n  facet_wrap(~degree, ncol = 3) +\n  scale_color_viridis_d() +\n  labs(\n    title = \"Polynomial Regression with Different Degrees\",\n    subtitle = \"Black dashed line shows true relationship. Higher degrees can lead to overfitting.\",\n    x = \"X\",\n    y = \"Y\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nNotice how: - Degree 1 (linear) underfits - it can’t capture the curve - Degree 2-3 capture the main pattern well - Degree 10 overfits - it follows noise in the data\n\n\nImplementing Polynomial Features\n\n# Create recipe with polynomial features\npoly_recipe &lt;- recipe(Sale_Price ~ Gr_Liv_Area + Lot_Area, \n                      data = ames_train) %&gt;%\n  step_poly(Gr_Liv_Area, degree = 2) %&gt;%\n  step_poly(Lot_Area, degree = 2) %&gt;%\n  step_interact(terms = ~ Gr_Liv_Area_poly_1:Lot_Area_poly_1)\n\n# Fit model with polynomial features\npoly_workflow &lt;- workflow() %&gt;%\n  add_recipe(poly_recipe) %&gt;%\n  add_model(lm_spec)\n\npoly_fit &lt;- poly_workflow %&gt;%\n  fit(ames_train)\n\n# Compare with simple linear model\nsimple_recipe &lt;- recipe(Sale_Price ~ Gr_Liv_Area + Lot_Area, \n                       data = ames_train)\n\nsimple_workflow &lt;- workflow() %&gt;%\n  add_recipe(simple_recipe) %&gt;%\n  add_model(lm_spec)\n\nsimple_fit &lt;- simple_workflow %&gt;%\n  fit(ames_train)\n\n# Evaluate both models\nmodels_comparison &lt;- tibble(\n  model = c(\"Linear\", \"Polynomial\"),\n  workflow = list(simple_workflow, poly_workflow)\n) %&gt;%\n  mutate(\n    fit = map(workflow, ~ fit(., ames_train)),\n    train_pred = map(fit, ~ predict(., ames_train)),\n    test_pred = map(fit, ~ predict(., ames_test))\n  )\n\n# Calculate metrics\ncomparison_metrics &lt;- models_comparison %&gt;%\n  mutate(\n    train_metrics = map2(train_pred, list(ames_train), ~ {\n      bind_cols(.x, .y) %&gt;%\n        metrics(truth = Sale_Price, estimate = .pred)\n    }),\n    test_metrics = map2(test_pred, list(ames_test), ~ {\n      bind_cols(.x, .y) %&gt;%\n        metrics(truth = Sale_Price, estimate = .pred)\n    })\n  ) %&gt;%\n  select(model, train_metrics, test_metrics) %&gt;%\n  pivot_longer(c(train_metrics, test_metrics), \n               names_to = \"dataset\", values_to = \"metrics\") %&gt;%\n  unnest(metrics)\n\n# Visualize comparison\nggplot(comparison_metrics, \n       aes(x = model, y = .estimate, fill = dataset)) +\n  geom_col(position = \"dodge\") +\n  facet_wrap(~.metric, scales = \"free_y\") +\n  scale_fill_manual(values = c(\"train_metrics\" = \"lightblue\", \n                               \"test_metrics\" = \"darkblue\")) +\n  labs(\n    title = \"Linear vs Polynomial Model Comparison\",\n    subtitle = \"Watch for overfitting: train performance much better than test\",\n    y = \"Metric Value\"\n  )"
  },
  {
    "objectID": "15-regression.html#regularization-ridge-lasso-and-elastic-net",
    "href": "15-regression.html#regularization-ridge-lasso-and-elastic-net",
    "title": "Chapter 15: Regression Models - Theory, Implementation, and Best Practices",
    "section": "Regularization: Ridge, Lasso, and Elastic Net",
    "text": "Regularization: Ridge, Lasso, and Elastic Net\nRegularization adds a penalty term to the loss function to prevent overfitting and handle multicollinearity.\n\nUnderstanding Regularization Mathematics\nThe three main types of regularization are:\nRidge Regression (L2 penalty): \\[\\text{Loss} = \\text{RSS} + \\lambda \\sum_{j=1}^{p} \\beta_j^2\\]\nRidge regression shrinks coefficients toward zero but never exactly to zero. It’s good when you have many predictors with small effects.\nLasso Regression (L1 penalty): \\[\\text{Loss} = \\text{RSS} + \\lambda \\sum_{j=1}^{p} |\\beta_j|\\]\nLasso can shrink coefficients exactly to zero, performing automatic feature selection. It’s good when you believe only a subset of predictors are important.\nElastic Net (Combination): \\[\\text{Loss} = \\text{RSS} + \\lambda \\left[ \\alpha \\sum_{j=1}^{p} |\\beta_j| + (1-\\alpha) \\sum_{j=1}^{p} \\beta_j^2 \\right]\\]\nElastic Net combines Ridge and Lasso, controlled by the mixing parameter α.\nLet’s visualize how regularization affects coefficients:\n\n# Prepare data for regularization\names_reg &lt;- ames_train %&gt;%\n  select(Sale_Price, Gr_Liv_Area, Garage_Area, Year_Built, \n         Lot_Area, Total_Bsmt_SF, First_Flr_SF) %&gt;%\n  drop_na()\n\n# Create model matrix\nX &lt;- model.matrix(Sale_Price ~ . - 1, data = ames_reg)\ny &lt;- ames_reg$Sale_Price\n\n# Fit Ridge regression\nridge_fit &lt;- glmnet(X, y, alpha = 0)  # alpha = 0 for Ridge\n\n# Fit Lasso regression\nlasso_fit &lt;- glmnet(X, y, alpha = 1)  # alpha = 1 for Lasso\n\n# Fit Elastic Net\nelastic_fit &lt;- glmnet(X, y, alpha = 0.5)  # alpha = 0.5 for 50/50 mix\n\n# Create coefficient path plots\npar(mfrow = c(1, 3))\nplot(ridge_fit, xvar = \"lambda\", main = \"Ridge Regression Path\")\nplot(lasso_fit, xvar = \"lambda\", main = \"Lasso Regression Path\")\nplot(elastic_fit, xvar = \"lambda\", main = \"Elastic Net Path\")\n\n\n\n\n\n\n\n\nThese plots show how coefficients change as the penalty parameter λ increases: - Ridge: All coefficients shrink smoothly toward zero but never reach it - Lasso: Coefficients can hit zero abruptly (feature selection) - Elastic Net: Combination of both behaviors\n\n\nImplementing Regularized Regression\nNow let’s implement these methods properly with tidymodels:\n\n# Create a more complete recipe\nreg_recipe &lt;- recipe(Sale_Price ~ ., data = ames_train) %&gt;%\n  step_nzv(all_predictors()) %&gt;%\n  step_impute_median(all_numeric_predictors()) %&gt;%\n  step_impute_mode(all_nominal_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\n# Ridge regression\nridge_spec &lt;- linear_reg(penalty = 0.01, mixture = 0) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"regression\")\n\n# Lasso regression\nlasso_spec &lt;- linear_reg(penalty = 0.01, mixture = 1) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"regression\")\n\n# Elastic Net\nelastic_spec &lt;- linear_reg(penalty = 0.01, mixture = 0.5) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"regression\")\n\n# Create workflows\nridge_wf &lt;- workflow() %&gt;%\n  add_recipe(reg_recipe) %&gt;%\n  add_model(ridge_spec)\n\nlasso_wf &lt;- workflow() %&gt;%\n  add_recipe(reg_recipe) %&gt;%\n  add_model(lasso_spec)\n\nelastic_wf &lt;- workflow() %&gt;%\n  add_recipe(reg_recipe) %&gt;%\n  add_model(elastic_spec)\n\n# Fit all models\nridge_fit &lt;- ridge_wf %&gt;% fit(ames_train)\nlasso_fit &lt;- lasso_wf %&gt;% fit(ames_train)\nelastic_fit &lt;- elastic_wf %&gt;% fit(ames_train)\n\n# Compare number of non-zero coefficients\nn_features &lt;- tibble(\n  Model = c(\"Ridge\", \"Lasso\", \"Elastic Net\"),\n  `Non-zero Coefficients` = c(\n    sum(tidy(ridge_fit)$estimate != 0),\n    sum(tidy(lasso_fit)$estimate != 0),\n    sum(tidy(elastic_fit)$estimate != 0)\n  ),\n  `Total Features` = c(\n    nrow(tidy(ridge_fit)),\n    nrow(tidy(lasso_fit)),\n    nrow(tidy(elastic_fit))\n  )\n)\n\nknitr::kable(n_features)\n\n\n\n\nModel\nNon-zero Coefficients\nTotal Features\n\n\n\n\nRidge\n203\n210\n\n\nLasso\n34\n210\n\n\nElastic Net\n52\n210\n\n\n\n\n\nNotice how Lasso has fewer non-zero coefficients - it’s performing feature selection automatically!"
  },
  {
    "objectID": "15-regression.html#regression-trees-and-random-forests",
    "href": "15-regression.html#regression-trees-and-random-forests",
    "title": "Chapter 15: Regression Models - Theory, Implementation, and Best Practices",
    "section": "Regression Trees and Random Forests",
    "text": "Regression Trees and Random Forests\nTree-based methods can capture non-linear relationships and interactions without explicitly specifying them.\n\nHow Regression Trees Work\nRegression trees recursively partition the feature space into rectangles, predicting the mean value in each region. The splitting criterion for regression is typically the reduction in RSS:\n\\[\\text{RSS} = \\sum_{i \\in R_1} (y_i - \\hat{y}_{R_1})^2 + \\sum_{i \\in R_2} (y_i - \\hat{y}_{R_2})^2\\]\nWhere \\(R_1\\) and \\(R_2\\) are the two regions created by the split.\n\n# Fit a simple regression tree\ntree_spec &lt;- decision_tree(\n  cost_complexity = 0.01,\n  tree_depth = 4,\n  min_n = 20\n) %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"regression\")\n\ntree_fit &lt;- tree_spec %&gt;%\n  fit(Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames_train)\n\n# Visualize the tree\nif (require(rpart.plot, quietly = TRUE)) {\n  rpart.plot(tree_fit$fit, type = 4, extra = 1, roundint = FALSE)\n}\n\n# Create prediction surface\ngrid &lt;- expand_grid(\n  Gr_Liv_Area = seq(min(ames_train$Gr_Liv_Area, na.rm = TRUE),\n                     max(ames_train$Gr_Liv_Area, na.rm = TRUE),\n                     length.out = 100),\n  Year_Built = seq(min(ames_train$Year_Built, na.rm = TRUE),\n                   max(ames_train$Year_Built, na.rm = TRUE),\n                   length.out = 100)\n)\n\ngrid_pred &lt;- tree_fit %&gt;%\n  predict(grid) %&gt;%\n  bind_cols(grid)\n\n# Visualize prediction surface\nggplot(grid_pred, aes(x = Gr_Liv_Area, y = Year_Built, fill = .pred)) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  geom_point(data = sample_n(ames_train, 200), \n             aes(fill = NULL, color = Sale_Price), \n             size = 1, alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(\n    title = \"Regression Tree Prediction Surface\",\n    subtitle = \"Notice the rectangular regions - characteristic of tree-based methods\",\n    x = \"Living Area (sq ft)\",\n    y = \"Year Built\",\n    fill = \"Predicted\\nlog(Price)\",\n    color = \"Actual\\nlog(Price)\"\n  )\n\n\n\n\n\n\n\n\nThe rectangular regions are a key characteristic of tree-based methods. Each rectangle represents a leaf node in the tree, with all observations in that region receiving the same prediction.\n\n\nRandom Forests for Regression\nRandom Forests improve upon single trees by: 1. Building many trees on bootstrap samples (bagging) 2. Randomly selecting features at each split 3. Averaging predictions across all trees\nThis reduces overfitting and improves generalization:\n\n# Random Forest specification\nrf_spec &lt;- rand_forest(\n  trees = 500,\n  mtry = 10,\n  min_n = 5\n) %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\n# Create workflow\nrf_workflow &lt;- workflow() %&gt;%\n  add_recipe(reg_recipe) %&gt;%\n  add_model(rf_spec)\n\n# Fit with cross-validation for better evaluation\nset.seed(123)\names_folds &lt;- vfold_cv(ames_train, v = 5)\n\nrf_cv &lt;- rf_workflow %&gt;%\n  fit_resamples(\n    resamples = ames_folds,\n    metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq, yardstick::mae),\n    control = control_resamples(save_pred = TRUE)\n  )\n\n# Show performance\ncollect_metrics(rf_cv) %&gt;%\n  knitr::kable(digits = 4)\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nmae\nstandard\n0.0433\n5\n0.0011\npre0_mod0_post0\n\n\nrmse\nstandard\n0.0686\n5\n0.0032\npre0_mod0_post0\n\n\nrsq\nstandard\n0.8656\n5\n0.0074\npre0_mod0_post0\n\n\n\n\n# Fit final model for feature importance\nrf_final &lt;- rf_workflow %&gt;%\n  fit(ames_train)\n\n# Extract and plot feature importance\nrf_importance &lt;- rf_final %&gt;%\n  extract_fit_parsnip() %&gt;%\n  vip(num_features = 20)\n\nrf_importance +\n  labs(title = \"Top 20 Most Important Features in Random Forest\",\n       subtitle = \"Based on impurity reduction\")\n\n\n\n\n\n\n\n\nRandom Forests often provide excellent predictive performance with minimal tuning, making them a go-to method for many regression problems."
  },
  {
    "objectID": "15-regression.html#model-diagnostics-and-validation",
    "href": "15-regression.html#model-diagnostics-and-validation",
    "title": "Chapter 15: Regression Models - Theory, Implementation, and Best Practices",
    "section": "Model Diagnostics and Validation",
    "text": "Model Diagnostics and Validation\nProper model validation is crucial for ensuring your model will generalize well to new data.\n\nResidual Analysis\nResidual analysis helps identify problems with model assumptions:\n\n# Get predictions from our linear model\nlm_pred &lt;- lm_fit %&gt;%\n  predict(ames_test) %&gt;%\n  bind_cols(ames_test) %&gt;%\n  mutate(\n    residual = Sale_Price - .pred,\n    std_residual = residual / sd(residual)\n  )\n\n# Create diagnostic plots\np1 &lt;- ggplot(lm_pred, aes(x = .pred, y = residual)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"blue\") +\n  labs(\n    title = \"Residuals vs Fitted Values\",\n    subtitle = \"Look for patterns - should be random scatter around zero\",\n    x = \"Fitted Values\",\n    y = \"Residuals\"\n  )\n\np2 &lt;- ggplot(lm_pred, aes(sample = std_residual)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  labs(\n    title = \"Q-Q Plot\",\n    subtitle = \"Check normality - points should follow the line\",\n    x = \"Theoretical Quantiles\",\n    y = \"Standardized Residuals\"\n  )\n\np3 &lt;- ggplot(lm_pred, aes(x = .pred, y = sqrt(abs(std_residual)))) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"blue\") +\n  labs(\n    title = \"Scale-Location Plot\",\n    subtitle = \"Check homoscedasticity - spread should be constant\",\n    x = \"Fitted Values\",\n    y = \"√|Standardized Residuals|\"\n  )\n\np4 &lt;- ggplot(lm_pred, aes(x = residual)) +\n  geom_histogram(bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  geom_density(aes(y = after_stat(count)), color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Distribution of Residuals\",\n    subtitle = \"Should be approximately normal\",\n    x = \"Residual\",\n    y = \"Count\"\n  )\n\n(p1 + p2) / (p3 + p4)\n\n\n\n\n\n\n\n\nEach plot reveals different aspects: - Pattern in residuals vs fitted: Indicates non-linearity - Deviation from Q-Q line: Indicates non-normality - Funnel shape in scale-location: Indicates heteroscedasticity - Skewed residual distribution: May need transformation\n\n\nCross-Validation for Model Selection\nCross-validation provides unbiased estimates of model performance:\n\n# Compare multiple models using cross-validation\nmodels_list &lt;- list(\n  \"Linear\" = lm_spec,\n  \"Ridge\" = ridge_spec,\n  \"Lasso\" = lasso_spec,\n  \"Random Forest\" = rf_spec,\n  \"Decision Tree\" = tree_spec\n)\n\n# Evaluate all models\ncv_results &lt;- map_df(names(models_list), function(model_name) {\n  wf &lt;- workflow() %&gt;%\n    add_recipe(reg_recipe) %&gt;%\n    add_model(models_list[[model_name]])\n  \n  fit_resamples(\n    wf,\n    resamples = ames_folds,\n    metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq, yardstick::mae)\n  ) %&gt;%\n    collect_metrics() %&gt;%\n    mutate(model = model_name)\n})\n\n# Visualize comparison\nggplot(cv_results, aes(x = model, y = mean, fill = model)) +\n  geom_col() +\n  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), \n                width = 0.2) +\n  facet_wrap(~.metric, scales = \"free_y\") +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"Model Comparison via 5-Fold Cross-Validation\",\n    subtitle = \"Error bars show standard error across folds\",\n    y = \"Metric Value\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")"
  },
  {
    "objectID": "15-regression.html#advanced-topics",
    "href": "15-regression.html#advanced-topics",
    "title": "Chapter 15: Regression Models - Theory, Implementation, and Best Practices",
    "section": "Advanced Topics",
    "text": "Advanced Topics\n\nDealing with Multicollinearity\nMulticollinearity occurs when predictors are highly correlated, making coefficient interpretation difficult:\n\n# Check correlation among numeric predictors\nnumeric_features &lt;- ames_train %&gt;%\n  select(where(is.numeric)) %&gt;%\n  select(-Sale_Price) %&gt;%\n  drop_na()\n\ncor_matrix &lt;- cor(numeric_features)\n\n# Find highly correlated pairs\nhigh_cor &lt;- which(abs(cor_matrix) &gt; 0.8 & cor_matrix != 1, arr.ind = TRUE)\nhigh_cor_pairs &lt;- tibble(\n  var1 = rownames(cor_matrix)[high_cor[,1]],\n  var2 = colnames(cor_matrix)[high_cor[,2]],\n  correlation = cor_matrix[high_cor]\n) %&gt;%\n  filter(var1 &lt; var2) %&gt;%  # Remove duplicates\n  arrange(desc(abs(correlation)))\n\nprint(\"Highly correlated variable pairs:\")\n\n[1] \"Highly correlated variable pairs:\"\n\nhead(high_cor_pairs, 10) %&gt;% knitr::kable(digits = 3)\n\n\n\n\nvar1\nvar2\ncorrelation\n\n\n\n\nGarage_Area\nGarage_Cars\n0.892\n\n\nGr_Liv_Area\nTotRms_AbvGrd\n0.809\n\n\n\n\n# Visualize correlation matrix\ncorrplot(cor_matrix, method = \"color\", type = \"upper\", \n         order = \"hclust\", tl.cex = 0.6, tl.col = \"black\",\n         diag = FALSE, addCoef.col = \"black\", number.cex = 0.4)\n\n\n\n\n\n\n\n\nHigh correlations between predictors can cause: - Unstable coefficient estimates - Large standard errors - Difficulty interpreting individual effects\nSolutions include: - Remove one of the correlated variables - Use PCA to create uncorrelated components - Apply regularization (Ridge regression handles multicollinearity well)\n\n\nHandling Non-constant Variance (Heteroscedasticity)\nWhen variance changes with the fitted values, we can apply transformations:\n\n# Example with original scale prices (not log-transformed)\names_original &lt;- ames %&gt;%\n  select(Sale_Price, Gr_Liv_Area, Year_Built, Overall_Cond) %&gt;%\n  drop_na()\n\n# Fit model on original scale\noriginal_fit &lt;- lm(Sale_Price ~ ., data = ames_original)\n\n# Fit model on log scale\nlog_fit &lt;- lm(log(Sale_Price) ~ ., data = ames_original)\n\n# Compare residual plots\norig_pred &lt;- tibble(\n  fitted = fitted(original_fit),\n  residual = residuals(original_fit),\n  model = \"Original Scale\"\n)\n\nlog_pred &lt;- tibble(\n  fitted = fitted(log_fit),\n  residual = residuals(log_fit),\n  model = \"Log Scale\"\n)\n\ncombined_pred &lt;- bind_rows(orig_pred, log_pred)\n\nggplot(combined_pred, aes(x = fitted, y = residual)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"blue\") +\n  facet_wrap(~model, scales = \"free\") +\n  labs(\n    title = \"Effect of Log Transformation on Heteroscedasticity\",\n    subtitle = \"Log transformation often stabilizes variance\",\n    x = \"Fitted Values\",\n    y = \"Residuals\"\n  )\n\n\n\n\n\n\n\n\nThe log transformation has stabilized the variance - notice how the spread of residuals is more constant in the log scale model."
  },
  {
    "objectID": "15-regression.html#practical-example-complete-regression-pipeline",
    "href": "15-regression.html#practical-example-complete-regression-pipeline",
    "title": "Chapter 15: Regression Models - Theory, Implementation, and Best Practices",
    "section": "Practical Example: Complete Regression Pipeline",
    "text": "Practical Example: Complete Regression Pipeline\nLet’s put everything together in a comprehensive example:\n\n# Create a complete modeling pipeline\n# 1. Data preparation\nconcrete_clean &lt;- concrete %&gt;%\n  drop_na()\n\nconcrete_split &lt;- initial_split(concrete_clean, prop = 0.8)\nconcrete_train &lt;- training(concrete_split)\nconcrete_test &lt;- testing(concrete_split)\n\n# 2. Exploratory data analysis\nconcrete_summary &lt;- concrete_train %&gt;%\n  pivot_longer(everything(), names_to = \"variable\", values_to = \"value\") %&gt;%\n  group_by(variable) %&gt;%\n  summarise(\n    mean = mean(value),\n    sd = sd(value),\n    min = min(value),\n    max = max(value)\n  )\n\nprint(\"Dataset summary:\")\n\n[1] \"Dataset summary:\"\n\nconcrete_summary %&gt;% knitr::kable(digits = 2)\n\n\n\n\nvariable\nmean\nsd\nmin\nmax\n\n\n\n\nage\n45.85\n64.08\n1.00\n365.00\n\n\nblast_furnace_slag\n71.36\n84.29\n0.00\n359.40\n\n\ncement\n283.40\n104.81\n102.00\n540.00\n\n\ncoarse_aggregate\n970.40\n76.68\n801.00\n1134.30\n\n\ncompressive_strength\n35.56\n16.71\n2.33\n81.75\n\n\nfine_aggregate\n773.44\n81.49\n594.00\n992.60\n\n\nfly_ash\n54.42\n63.98\n0.00\n200.10\n\n\nsuperplasticizer\n6.13\n5.86\n0.00\n32.20\n\n\nwater\n182.12\n21.78\n121.80\n247.00\n\n\n\n\n# 3. Create recipe with feature engineering\nconcrete_recipe &lt;- recipe(compressive_strength ~ ., data = concrete_train) %&gt;%\n  step_normalize(all_predictors()) %&gt;%\n  step_poly(age, degree = 2) %&gt;%\n  step_interact(terms = ~ cement:water)\n\n# 4. Specify multiple models\nmodels &lt;- list(\n  linear = linear_reg() %&gt;% set_engine(\"lm\"),\n  ridge = linear_reg(penalty = tune(), mixture = 0) %&gt;% set_engine(\"glmnet\"),\n  rf = rand_forest(mtry = tune(), trees = 500, min_n = tune()) %&gt;% \n    set_engine(\"ranger\") %&gt;% set_mode(\"regression\")\n)\n\n# 5. Create workflows\nworkflows &lt;- map(models, function(model) {\n  workflow() %&gt;%\n    add_recipe(concrete_recipe) %&gt;%\n    add_model(model)\n})\n\n# 6. Tune hyperparameters for models that need it\nconcrete_folds &lt;- vfold_cv(concrete_train, v = 5)\n\n# Tune Ridge\nridge_grid &lt;- grid_regular(penalty(), levels = 10)\nridge_tune &lt;- workflows$ridge %&gt;%\n  tune_grid(\n    resamples = concrete_folds,\n    grid = ridge_grid,\n    metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq)\n  )\n\n# Tune Random Forest\nrf_grid &lt;- grid_regular(\n  mtry(range = c(2, 7)),\n  min_n(),\n  levels = 5\n)\nrf_tune &lt;- workflows$rf %&gt;%\n  tune_grid(\n    resamples = concrete_folds,\n    grid = rf_grid,\n    metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq)\n  )\n\n# 7. Select best models\nbest_ridge &lt;- select_best(ridge_tune, metric = \"rmse\")\nbest_rf &lt;- select_best(rf_tune, metric = \"rmse\")\n\n# 8. Finalize workflows\nfinal_linear &lt;- workflows$linear\nfinal_ridge &lt;- workflows$ridge %&gt;% finalize_workflow(best_ridge)\nfinal_rf &lt;- workflows$rf %&gt;% finalize_workflow(best_rf)\n\n# 9. Fit final models and evaluate\nfinal_fits &lt;- list(\n  linear = final_linear %&gt;% fit(concrete_train),\n  ridge = final_ridge %&gt;% fit(concrete_train),\n  rf = final_rf %&gt;% fit(concrete_train)\n)\n\n# 10. Make predictions and evaluate\ntest_results &lt;- map_df(names(final_fits), function(model_name) {\n  final_fits[[model_name]] %&gt;%\n    predict(concrete_test) %&gt;%\n    bind_cols(concrete_test) %&gt;%\n    metrics(truth = compressive_strength, estimate = .pred) %&gt;%\n    mutate(model = model_name)\n})\n\n# Visualize final comparison\nggplot(test_results, aes(x = model, y = .estimate, fill = model)) +\n  geom_col() +\n  facet_wrap(~.metric, scales = \"free_y\") +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"Final Model Comparison on Test Set\",\n    subtitle = \"Concrete compressive strength prediction\",\n    y = \"Metric Value\"\n  ) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "15-regression.html#exercises",
    "href": "15-regression.html#exercises",
    "title": "Chapter 15: Regression Models - Theory, Implementation, and Best Practices",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Implement Regularization with Tuning\nBuild a regularized regression model with proper hyperparameter tuning:\n\n# Your solution\n# Use the Ames dataset to predict Sale_Price\n# Create an elastic net model with tuned penalty and mixture\n\n# Simpler recipe for exercise\nelastic_recipe &lt;- recipe(Sale_Price ~ Gr_Liv_Area + Year_Built + Overall_Cond + \n                        Neighborhood + Total_Bsmt_SF, data = ames_train) %&gt;%\n  step_impute_median(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\n# Elastic net specification with tuning\nelastic_tune_spec &lt;- linear_reg(\n  penalty = tune(),\n  mixture = tune()\n) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"regression\")\n\n# Workflow\nelastic_tune_wf &lt;- workflow() %&gt;%\n  add_recipe(elastic_recipe) %&gt;%\n  add_model(elastic_tune_spec)\n\n# Tuning grid\nelastic_grid &lt;- grid_regular(\n  penalty(range = c(-3, 0)),\n  mixture(range = c(0, 1)),\n  levels = c(10, 5)\n)\n\n# Create resamples\names_folds &lt;- vfold_cv(ames_train, v = 5)\n\n# Tune the model\nelastic_tuned &lt;- elastic_tune_wf %&gt;%\n  tune_grid(\n    resamples = ames_folds,\n    grid = elastic_grid,\n    metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq, yardstick::mae)\n  )\n\n# Visualize tuning results\nautoplot(elastic_tuned) +\n  labs(title = \"Elastic Net Tuning Results\",\n       subtitle = \"Performance across different penalty and mixture values\")\n\n\n\n\n\n\n\n# Select and fit best model\nbest_elastic &lt;- select_best(elastic_tuned, metric = \"rmse\")\nfinal_elastic &lt;- elastic_tune_wf %&gt;%\n  finalize_workflow(best_elastic) %&gt;%\n  fit(ames_train)\n\n# Evaluate on test set\nelastic_test_pred &lt;- final_elastic %&gt;%\n  predict(ames_test) %&gt;%\n  bind_cols(ames_test)\n\nelastic_metrics &lt;- elastic_test_pred %&gt;%\n  metrics(truth = Sale_Price, estimate = .pred)\n\nprint(\"Best hyperparameters:\")\n\n[1] \"Best hyperparameters:\"\n\nprint(best_elastic)\n\n# A tibble: 1 x 3\n  penalty mixture .config         \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;           \n1   0.001    0.25 pre0_mod02_post0\n\nprint(\"Test set performance:\")\n\n[1] \"Test set performance:\"\n\nprint(elastic_metrics)\n\n# A tibble: 3 x 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard      0.0752\n2 rsq     standard      0.816 \n3 mae     standard      0.0479\n\n\n\n\nExercise 2: Diagnose and Fix Model Problems\nIdentify and address issues in a regression model:\n\n# Your solution\n# Create a problematic model and fix it\n\n# Intentionally problematic model (using highly correlated predictors)\nproblem_data &lt;- ames_train %&gt;%\n  select(Sale_Price, Gr_Liv_Area, Total_Bsmt_SF, First_Flr_SF, \n         Second_Flr_SF, Garage_Area, Garage_Cars) %&gt;%\n  drop_na()\n\n# Check correlations\ncor(problem_data[,-1]) %&gt;%\n  corrplot(method = \"number\", type = \"upper\")\n\n\n\n\n\n\n\n# Fit problematic model\nproblem_fit &lt;- lm(Sale_Price ~ ., data = problem_data)\nsummary(problem_fit)\n\n\nCall:\nlm(formula = Sale_Price ~ ., data = problem_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.05511 -0.03523  0.01104  0.05335  0.31904 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    4.740e+00  7.247e-03 654.021  &lt; 2e-16 ***\nGr_Liv_Area   -5.079e-05  4.297e-05  -1.182   0.2373    \nTotal_Bsmt_SF  1.368e-04  7.725e-06  17.707  &lt; 2e-16 ***\nFirst_Flr_SF   1.723e-04  4.409e-05   3.908 9.60e-05 ***\nSecond_Flr_SF  1.969e-04  4.341e-05   4.537 6.02e-06 ***\nGarage_Area    3.874e-05  2.156e-05   1.797   0.0725 .  \nGarage_Cars    7.311e-02  6.039e-03  12.106  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09484 on 2190 degrees of freedom\nMultiple R-squared:  0.7168,    Adjusted R-squared:  0.716 \nF-statistic: 923.8 on 6 and 2190 DF,  p-value: &lt; 2.2e-16\n\n# Note the high VIF (Variance Inflation Factor)\nif (require(car, quietly = TRUE)) {\n  vif_values &lt;- car::vif(problem_fit)\n  print(\"Variance Inflation Factors:\")\n  print(vif_values)\n}\n\n[1] \"Variance Inflation Factors:\"\n  Gr_Liv_Area Total_Bsmt_SF  First_Flr_SF Second_Flr_SF   Garage_Area \n   112.172194      2.772377     73.222425     84.197910      5.207468 \n  Garage_Cars \n     5.107801 \n\n# Fix 1: Remove highly correlated variables\nfixed_data1 &lt;- problem_data %&gt;%\n  select(-Garage_Cars, -Second_Flr_SF)  # Remove redundant variables\n\nfixed_fit1 &lt;- lm(Sale_Price ~ ., data = fixed_data1)\n\n# Fix 2: Use PCA\npca_recipe &lt;- recipe(Sale_Price ~ ., data = problem_data) %&gt;%\n  step_normalize(all_predictors()) %&gt;%\n  step_pca(all_predictors(), num_comp = 4)\n\npca_fit &lt;- workflow() %&gt;%\n  add_recipe(pca_recipe) %&gt;%\n  add_model(lm_spec) %&gt;%\n  fit(problem_data)\n\n# Fix 3: Use Ridge regression\nridge_fix &lt;- linear_reg(penalty = 0.1, mixture = 0) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  fit(Sale_Price ~ ., data = problem_data)\n\n# Compare solutions\ncat(\"Original model R-squared:\", summary(problem_fit)$r.squared, \"\\n\")\n\nOriginal model R-squared: 0.7167839 \n\ncat(\"Fixed model R-squared:\", summary(fixed_fit1)$r.squared, \"\\n\")\n\nFixed model R-squared: 0.6943782 \n\n\n\n\nExercise 3: Non-linear Relationships\nExplore and model non-linear relationships:\n\n# Your solution\n# Create synthetic data with non-linear relationship\nset.seed(456)\nnonlinear_ex &lt;- tibble(\n  x = seq(0, 10, length.out = 200),\n  y = 5 + 3*sin(x) + 0.5*x^2 - 0.05*x^3 + rnorm(200, sd = 1)\n)\n\n# Visualize the relationship\nggplot(nonlinear_ex, aes(x = x, y = y)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"blue\") +\n  labs(title = \"Non-linear Relationship to Model\")\n\n\n\n\n\n\n\n# Compare different approaches\n# 1. Linear model (will underfit)\nlinear_ex &lt;- lm(y ~ x, data = nonlinear_ex)\n\n# 2. Polynomial regression\npoly_ex &lt;- lm(y ~ poly(x, 5), data = nonlinear_ex)\n\n# 3. Spline regression\nspline_ex &lt;- lm(y ~ splines::ns(x, df = 5), data = nonlinear_ex)\n\n# 4. Random Forest\nrf_ex &lt;- rand_forest(trees = 100) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\") %&gt;%\n  fit(y ~ x, data = nonlinear_ex)\n\n# Generate predictions\npredictions_ex &lt;- nonlinear_ex %&gt;%\n  mutate(\n    linear = predict(linear_ex, nonlinear_ex),\n    polynomial = predict(poly_ex, nonlinear_ex),\n    spline = predict(spline_ex, nonlinear_ex),\n    rf = predict(rf_ex, nonlinear_ex)$.pred\n  ) %&gt;%\n  pivot_longer(c(linear, polynomial, spline, rf), \n               names_to = \"model\", values_to = \"prediction\")\n\n# Visualize all models\nggplot(predictions_ex, aes(x = x)) +\n  geom_point(aes(y = y), alpha = 0.2) +\n  geom_line(aes(y = prediction, color = model), linewidth = 1.2) +\n  facet_wrap(~model) +\n  scale_color_viridis_d() +\n  labs(\n    title = \"Comparison of Methods for Non-linear Relationships\",\n    subtitle = \"Different approaches to capturing non-linearity\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n# Calculate performance\nperformance_ex &lt;- predictions_ex %&gt;%\n  group_by(model) %&gt;%\n  summarise(\n    rmse = sqrt(mean((y - prediction)^2)),\n    mae = mean(abs(y - prediction)),\n    r_squared = cor(y, prediction)^2\n  )\n\nperformance_ex %&gt;% knitr::kable(digits = 3)\n\n\n\n\nmodel\nrmse\nmae\nr_squared\n\n\n\n\nlinear\n2.597\n2.011\n0.173\n\n\npolynomial\n1.074\n0.865\n0.858\n\n\nrf\n0.582\n0.472\n0.959\n\n\nspline\n0.980\n0.808\n0.882"
  },
  {
    "objectID": "15-regression.html#summary",
    "href": "15-regression.html#summary",
    "title": "Chapter 15: Regression Models - Theory, Implementation, and Best Practices",
    "section": "Summary",
    "text": "Summary\nIn this comprehensive chapter, you’ve learned:\n✅ Linear regression theory: OLS, assumptions, diagnostics\n✅ Polynomial and non-linear regression: Capturing curved relationships\n✅ Regularization techniques: Ridge, Lasso, and Elastic Net\n✅ Tree-based methods: Decision trees and Random Forests\n✅ Model validation: Cross-validation, residual analysis\n✅ Advanced topics: Multicollinearity, heteroscedasticity\n✅ Complete workflows: From data preparation to final evaluation\nKey takeaways: - Always check model assumptions before interpreting results - Use regularization when you have many predictors or multicollinearity - Tree-based methods are powerful for capturing non-linear patterns - Cross-validation is essential for honest model evaluation - Different problems require different approaches - no single best method"
  },
  {
    "objectID": "15-regression.html#whats-next",
    "href": "15-regression.html#whats-next",
    "title": "Chapter 15: Regression Models - Theory, Implementation, and Best Practices",
    "section": "What’s Next?",
    "text": "What’s Next?\nIn Chapter 16, we’ll explore ensemble methods in depth, learning how to combine multiple models for superior performance."
  },
  {
    "objectID": "15-regression.html#additional-resources",
    "href": "15-regression.html#additional-resources",
    "title": "Chapter 15: Regression Models - Theory, Implementation, and Best Practices",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nAn Introduction to Statistical Learning - Chapters 3, 6, 8\nThe Elements of Statistical Learning - Chapters 3, 4, 15\nApplied Predictive Modeling - Chapters 6-8\nRegression and Other Stories - Comprehensive regression guide\nTidy Modeling with R - Chapters 7-11"
  },
  {
    "objectID": "04-data-tidying.html",
    "href": "04-data-tidying.html",
    "title": "Chapter 4: Data Tidying with tidyr",
    "section": "",
    "text": "By the end of this chapter, you will master:\n\nUnderstanding tidy data principles\nPivoting data between wide and long formats\nSeparating and uniting columns\nHandling missing values systematically\nNesting and unnesting data\nWorking with list-columns\nExpanding and completing data\nAdvanced tidying techniques"
  },
  {
    "objectID": "04-data-tidying.html#learning-objectives",
    "href": "04-data-tidying.html#learning-objectives",
    "title": "Chapter 4: Data Tidying with tidyr",
    "section": "",
    "text": "By the end of this chapter, you will master:\n\nUnderstanding tidy data principles\nPivoting data between wide and long formats\nSeparating and uniting columns\nHandling missing values systematically\nNesting and unnesting data\nWorking with list-columns\nExpanding and completing data\nAdvanced tidying techniques"
  },
  {
    "objectID": "04-data-tidying.html#setup",
    "href": "04-data-tidying.html#setup",
    "title": "Chapter 4: Data Tidying with tidyr",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.4     v readr     2.1.5\nv forcats   1.0.0     v stringr   1.5.2\nv ggplot2   4.0.0     v tibble    3.3.0\nv lubridate 1.9.4     v tidyr     1.3.1\nv purrr     1.1.0     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(gapminder)\nlibrary(palmerpenguins)\n\n# We'll create various messy datasets to practice tidying\nset.seed(123)  # For reproducibility"
  },
  {
    "objectID": "04-data-tidying.html#tidy-data-principles",
    "href": "04-data-tidying.html#tidy-data-principles",
    "title": "Chapter 4: Data Tidying with tidyr",
    "section": "Tidy Data Principles",
    "text": "Tidy Data Principles\n\nWhat Makes Data Tidy?\nTidy data follows three interrelated rules:\n\nEach variable forms a column\nEach observation forms a row\nEach type of observational unit forms a table\n\n\n\nWhy Tidy Data?\n\nConsistent structure makes tools easier to learn\nR’s vectorized operations work naturally with tidy data\nSupports the grammar of data manipulation\n\n\n\nCommon Problems with Messy Data\n\n# Problem 1: Column headers are values, not variable names\nmessy1 &lt;- tibble(\n  country = c(\"Afghanistan\", \"Brazil\", \"China\"),\n  `1999` = c(745, 37737, 212258),\n  `2000` = c(2666, 80488, 213766)\n)\nprint(\"Messy data - years as columns:\")\n\n[1] \"Messy data - years as columns:\"\n\nmessy1\n\n# A tibble: 3 x 3\n  country     `1999` `2000`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\n# Tidy version\ntidy1 &lt;- messy1 %&gt;%\n  pivot_longer(\n    cols = `1999`:`2000`,\n    names_to = \"year\",\n    values_to = \"cases\"\n  ) %&gt;%\n  mutate(year = as.integer(year))\nprint(\"Tidy version:\")\n\n[1] \"Tidy version:\"\n\ntidy1\n\n# A tibble: 6 x 3\n  country      year  cases\n  &lt;chr&gt;       &lt;int&gt;  &lt;dbl&gt;\n1 Afghanistan  1999    745\n2 Afghanistan  2000   2666\n3 Brazil       1999  37737\n4 Brazil       2000  80488\n5 China        1999 212258\n6 China        2000 213766\n\n# Problem 2: Multiple variables in one column\nmessy2 &lt;- tibble(\n  country = c(\"Afghanistan\", \"Brazil\", \"China\"),\n  rate = c(\"745/19987071\", \"37737/172006362\", \"212258/1272915272\")\n)\nprint(\"Messy data - multiple values in one column:\")\n\n[1] \"Messy data - multiple values in one column:\"\n\nmessy2\n\n# A tibble: 3 x 2\n  country     rate             \n  &lt;chr&gt;       &lt;chr&gt;            \n1 Afghanistan 745/19987071     \n2 Brazil      37737/172006362  \n3 China       212258/1272915272\n\n# Tidy version\ntidy2 &lt;- messy2 %&gt;%\n  separate(rate, into = c(\"cases\", \"population\"), sep = \"/\", convert = TRUE) %&gt;%\n  mutate(rate_per_10k = cases / population * 10000)\nprint(\"Tidy version:\")\n\n[1] \"Tidy version:\"\n\ntidy2\n\n# A tibble: 3 x 4\n  country      cases population rate_per_10k\n  &lt;chr&gt;        &lt;int&gt;      &lt;int&gt;        &lt;dbl&gt;\n1 Afghanistan    745   19987071        0.373\n2 Brazil       37737  172006362        2.19 \n3 China       212258 1272915272        1.67"
  },
  {
    "objectID": "04-data-tidying.html#pivoting-data",
    "href": "04-data-tidying.html#pivoting-data",
    "title": "Chapter 4: Data Tidying with tidyr",
    "section": "Pivoting Data",
    "text": "Pivoting Data\n\npivot_longer(): Wide to Long\n\n# Example: Wide format data\nwide_data &lt;- tibble(\n  student = c(\"Alice\", \"Bob\", \"Charlie\"),\n  math_score = c(85, 92, 78),\n  science_score = c(90, 88, 85),\n  english_score = c(88, 85, 90),\n  history_score = c(92, 90, 88)\n)\nprint(\"Wide format:\")\n\n[1] \"Wide format:\"\n\nwide_data\n\n# A tibble: 3 x 5\n  student math_score science_score english_score history_score\n  &lt;chr&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 Alice           85            90            88            92\n2 Bob             92            88            85            90\n3 Charlie         78            85            90            88\n\n# Basic pivot_longer\nlong_data &lt;- wide_data %&gt;%\n  pivot_longer(\n    cols = ends_with(\"_score\"),\n    names_to = \"subject\",\n    values_to = \"score\"\n  )\nprint(\"Long format:\")\n\n[1] \"Long format:\"\n\nlong_data\n\n# A tibble: 12 x 3\n   student subject       score\n   &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt;\n 1 Alice   math_score       85\n 2 Alice   science_score    90\n 3 Alice   english_score    88\n 4 Alice   history_score    92\n 5 Bob     math_score       92\n 6 Bob     science_score    88\n 7 Bob     english_score    85\n 8 Bob     history_score    90\n 9 Charlie math_score       78\n10 Charlie science_score    85\n11 Charlie english_score    90\n12 Charlie history_score    88\n\n# Advanced: Extract parts of column names\nlong_data_clean &lt;- wide_data %&gt;%\n  pivot_longer(\n    cols = ends_with(\"_score\"),\n    names_to = \"subject\",\n    names_pattern = \"(.*)_score\",  # Extract subject name without \"_score\"\n    values_to = \"score\"\n  )\nprint(\"Long format with cleaned names:\")\n\n[1] \"Long format with cleaned names:\"\n\nlong_data_clean\n\n# A tibble: 12 x 3\n   student subject score\n   &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;\n 1 Alice   math       85\n 2 Alice   science    90\n 3 Alice   english    88\n 4 Alice   history    92\n 5 Bob     math       92\n 6 Bob     science    88\n 7 Bob     english    85\n 8 Bob     history    90\n 9 Charlie math       78\n10 Charlie science    85\n11 Charlie english    90\n12 Charlie history    88\n\n# Multiple value columns\nquarterly_sales &lt;- tibble(\n  company = c(\"A\", \"B\", \"C\"),\n  revenue_q1 = c(100, 200, 150),\n  revenue_q2 = c(110, 210, 160),\n  costs_q1 = c(80, 150, 120),\n  costs_q2 = c(85, 155, 125)\n)\n\nquarterly_long &lt;- quarterly_sales %&gt;%\n  pivot_longer(\n    cols = -company,\n    names_to = c(\".value\", \"quarter\"),\n    names_sep = \"_\"\n  )\nprint(\"Multiple value columns:\")\n\n[1] \"Multiple value columns:\"\n\nquarterly_long\n\n# A tibble: 6 x 4\n  company quarter revenue costs\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 A       q1          100    80\n2 A       q2          110    85\n3 B       q1          200   150\n4 B       q2          210   155\n5 C       q1          150   120\n6 C       q2          160   125\n\n\n\n\npivot_wider(): Long to Wide\n\n# Basic pivot_wider\nwide_again &lt;- long_data_clean %&gt;%\n  pivot_wider(\n    names_from = subject,\n    values_from = score\n  )\nprint(\"Back to wide format:\")\n\n[1] \"Back to wide format:\"\n\nwide_again\n\n# A tibble: 3 x 5\n  student  math science english history\n  &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Alice      85      90      88      92\n2 Bob        92      88      85      90\n3 Charlie    78      85      90      88\n\n# With custom column names\nwide_custom &lt;- long_data_clean %&gt;%\n  pivot_wider(\n    names_from = subject,\n    values_from = score,\n    names_prefix = \"score_\"\n  )\nprint(\"Wide with custom names:\")\n\n[1] \"Wide with custom names:\"\n\nwide_custom\n\n# A tibble: 3 x 5\n  student score_math score_science score_english score_history\n  &lt;chr&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 Alice           85            90            88            92\n2 Bob             92            88            85            90\n3 Charlie         78            85            90            88\n\n# Handling multiple values\nmulti_observations &lt;- tibble(\n  day = c(1, 1, 2, 2, 3, 3),\n  measurement = c(\"temp\", \"humidity\", \"temp\", \"humidity\", \"temp\", \"humidity\"),\n  value = c(20, 60, 22, 55, 21, 58)\n)\n\nwide_multi &lt;- multi_observations %&gt;%\n  pivot_wider(\n    names_from = measurement,\n    values_from = value\n  )\nprint(\"Multiple measurements to wide:\")\n\n[1] \"Multiple measurements to wide:\"\n\nwide_multi\n\n# A tibble: 3 x 3\n    day  temp humidity\n  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1     1    20       60\n2     2    22       55\n3     3    21       58\n\n\n\n\nComplex Pivoting Examples\n\n# Real-world example with gapminder\ngapminder_wide &lt;- gapminder %&gt;%\n  select(country, continent, year, lifeExp) %&gt;%\n  pivot_wider(\n    names_from = year,\n    names_prefix = \"year_\",\n    values_from = lifeExp\n  ) %&gt;%\n  head()\nprint(\"Gapminder in wide format:\")\n\n[1] \"Gapminder in wide format:\"\n\ngapminder_wide\n\n# A tibble: 6 x 14\n  country  continent year_1952 year_1957 year_1962 year_1967 year_1972 year_1977\n  &lt;fct&gt;    &lt;fct&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Afghani~ Asia           28.8      30.3      32.0      34.0      36.1      38.4\n2 Albania  Europe         55.2      59.3      64.8      66.2      67.7      68.9\n3 Algeria  Africa         43.1      45.7      48.3      51.4      54.5      58.0\n4 Angola   Africa         30.0      32.0      34        36.0      37.9      39.5\n5 Argenti~ Americas       62.5      64.4      65.1      65.6      67.1      68.5\n6 Austral~ Oceania        69.1      70.3      70.9      71.1      71.9      73.5\n# i 6 more variables: year_1982 &lt;dbl&gt;, year_1987 &lt;dbl&gt;, year_1992 &lt;dbl&gt;,\n#   year_1997 &lt;dbl&gt;, year_2002 &lt;dbl&gt;, year_2007 &lt;dbl&gt;\n\n# Creating a correlation matrix format\npenguins_corr &lt;- penguins %&gt;%\n  select(where(is.numeric)) %&gt;%\n  drop_na() %&gt;%\n  cor() %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"var1\") %&gt;%\n  pivot_longer(\n    cols = -var1,\n    names_to = \"var2\",\n    values_to = \"correlation\"\n  ) %&gt;%\n  filter(var1 &lt; var2)  # Keep only upper triangle\nprint(\"Correlation matrix in long format:\")\n\n[1] \"Correlation matrix in long format:\"\n\npenguins_corr\n\n# A tibble: 10 x 3\n   var1              var2              correlation\n   &lt;chr&gt;             &lt;chr&gt;                   &lt;dbl&gt;\n 1 bill_length_mm    flipper_length_mm      0.656 \n 2 bill_length_mm    body_mass_g            0.595 \n 3 bill_length_mm    year                   0.0545\n 4 bill_depth_mm     bill_length_mm        -0.235 \n 5 bill_depth_mm     flipper_length_mm     -0.584 \n 6 bill_depth_mm     body_mass_g           -0.472 \n 7 bill_depth_mm     year                  -0.0604\n 8 flipper_length_mm year                   0.170 \n 9 body_mass_g       flipper_length_mm      0.871 \n10 body_mass_g       year                   0.0422"
  },
  {
    "objectID": "04-data-tidying.html#separating-and-uniting-columns",
    "href": "04-data-tidying.html#separating-and-uniting-columns",
    "title": "Chapter 4: Data Tidying with tidyr",
    "section": "Separating and Uniting Columns",
    "text": "Separating and Uniting Columns\n\nseparate(): Split One Column into Multiple\n\n# Basic separation\ndates_data &lt;- tibble(\n  date = c(\"2023-01-15\", \"2023-02-20\", \"2023-03-25\"),\n  value = c(100, 120, 115)\n)\n\nseparated_dates &lt;- dates_data %&gt;%\n  separate(date, into = c(\"year\", \"month\", \"day\"), sep = \"-\", convert = TRUE)\nprint(\"Separated date components:\")\n\n[1] \"Separated date components:\"\n\nseparated_dates\n\n# A tibble: 3 x 4\n   year month   day value\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1  2023     1    15   100\n2  2023     2    20   120\n3  2023     3    25   115\n\n# Separation with regular expressions\ncomplex_data &lt;- tibble(\n  code = c(\"USA-NY-001\", \"CAN-ON-002\", \"MEX-DF-003\"),\n  sales = c(1000, 1500, 800)\n)\n\nseparated_complex &lt;- complex_data %&gt;%\n  separate(\n    code, \n    into = c(\"country\", \"state\", \"id\"),\n    sep = \"-\"\n  )\nprint(\"Complex separation:\")\n\n[1] \"Complex separation:\"\n\nseparated_complex\n\n# A tibble: 3 x 4\n  country state id    sales\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n1 USA     NY    001    1000\n2 CAN     ON    002    1500\n3 MEX     DF    003     800\n\n# Keep original column\nwith_original &lt;- complex_data %&gt;%\n  separate(\n    code,\n    into = c(\"country\", \"state\", \"id\"),\n    sep = \"-\",\n    remove = FALSE  # Keep original column\n  )\nprint(\"With original column:\")\n\n[1] \"With original column:\"\n\nwith_original\n\n# A tibble: 3 x 5\n  code       country state id    sales\n  &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n1 USA-NY-001 USA     NY    001    1000\n2 CAN-ON-002 CAN     ON    002    1500\n3 MEX-DF-003 MEX     DF    003     800\n\n\n\n\nseparate_rows(): Create Multiple Rows\n\n# Data with multiple values in cells\nmulti_value_data &lt;- tibble(\n  person = c(\"Alice\", \"Bob\", \"Charlie\"),\n  languages = c(\"R,Python,SQL\", \"R,JavaScript\", \"Python,SQL,Julia\"),\n  years_exp = c(5, 3, 7)\n)\n\nseparated_rows &lt;- multi_value_data %&gt;%\n  separate_rows(languages, sep = \",\")\nprint(\"Multiple values to rows:\")\n\n[1] \"Multiple values to rows:\"\n\nseparated_rows\n\n# A tibble: 8 x 3\n  person  languages  years_exp\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 Alice   R                  5\n2 Alice   Python             5\n3 Alice   SQL                5\n4 Bob     R                  3\n5 Bob     JavaScript         3\n6 Charlie Python             7\n7 Charlie SQL                7\n8 Charlie Julia              7\n\n# Count languages per person\nlanguage_counts &lt;- separated_rows %&gt;%\n  group_by(person) %&gt;%\n  summarise(\n    num_languages = n(),\n    languages_list = paste(languages, collapse = \", \")\n  )\nprint(\"Language summary:\")\n\n[1] \"Language summary:\"\n\nlanguage_counts\n\n# A tibble: 3 x 3\n  person  num_languages languages_list    \n  &lt;chr&gt;           &lt;int&gt; &lt;chr&gt;             \n1 Alice               3 R, Python, SQL    \n2 Bob                 2 R, JavaScript     \n3 Charlie             3 Python, SQL, Julia\n\n\n\n\nunite(): Combine Multiple Columns\n\n# Basic unite\naddress_data &lt;- tibble(\n  street_num = c(\"123\", \"456\", \"789\"),\n  street_name = c(\"Main St\", \"Oak Ave\", \"Park Rd\"),\n  city = c(\"Boston\", \"New York\", \"Chicago\"),\n  state = c(\"MA\", \"NY\", \"IL\")\n)\n\nunited_address &lt;- address_data %&gt;%\n  unite(full_address, street_num, street_name, city, state, sep = \", \")\nprint(\"United address:\")\n\n[1] \"United address:\"\n\nunited_address\n\n# A tibble: 3 x 1\n  full_address              \n  &lt;chr&gt;                     \n1 123, Main St, Boston, MA  \n2 456, Oak Ave, New York, NY\n3 789, Park Rd, Chicago, IL \n\n# Unite with custom separator\ndate_parts &lt;- tibble(\n  year = c(2023, 2023, 2023),\n  month = c(1, 2, 3),\n  day = c(15, 20, 25)\n)\n\nunited_dates &lt;- date_parts %&gt;%\n  unite(date, year, month, day, sep = \"-\") %&gt;%\n  mutate(date = as.Date(date))\nprint(\"United dates:\")\n\n[1] \"United dates:\"\n\nunited_dates\n\n# A tibble: 3 x 1\n  date      \n  &lt;date&gt;    \n1 2023-01-15\n2 2023-02-20\n3 2023-03-25"
  },
  {
    "objectID": "04-data-tidying.html#handling-missing-values",
    "href": "04-data-tidying.html#handling-missing-values",
    "title": "Chapter 4: Data Tidying with tidyr",
    "section": "Handling Missing Values",
    "text": "Handling Missing Values\n\nExplicit vs Implicit Missing Values\n\n# Dataset with implicit missing values\nstocks &lt;- tibble(\n  year = c(2020, 2020, 2021, 2022, 2022),\n  quarter = c(1, 4, 2, 1, 3),\n  revenue = c(100, 120, 110, 130, 125)\n)\nprint(\"Data with implicit missing values:\")\n\n[1] \"Data with implicit missing values:\"\n\nstocks\n\n# A tibble: 5 x 3\n   year quarter revenue\n  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1  2020       1     100\n2  2020       4     120\n3  2021       2     110\n4  2022       1     130\n5  2022       3     125\n\n# Make implicit missing values explicit\ncomplete_stocks &lt;- stocks %&gt;%\n  complete(year, quarter)\nprint(\"After complete():\")\n\n[1] \"After complete():\"\n\ncomplete_stocks\n\n# A tibble: 12 x 3\n    year quarter revenue\n   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1  2020       1     100\n 2  2020       2      NA\n 3  2020       3      NA\n 4  2020       4     120\n 5  2021       1      NA\n 6  2021       2     110\n 7  2021       3      NA\n 8  2021       4      NA\n 9  2022       1     130\n10  2022       2      NA\n11  2022       3     125\n12  2022       4      NA\n\n# Fill missing values\nfilled_stocks &lt;- complete_stocks %&gt;%\n  arrange(year, quarter) %&gt;%\n  fill(revenue, .direction = \"down\")\nprint(\"After filling:\")\n\n[1] \"After filling:\"\n\nfilled_stocks\n\n# A tibble: 12 x 3\n    year quarter revenue\n   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1  2020       1     100\n 2  2020       2     100\n 3  2020       3     100\n 4  2020       4     120\n 5  2021       1     120\n 6  2021       2     110\n 7  2021       3     110\n 8  2021       4     110\n 9  2022       1     130\n10  2022       2     130\n11  2022       3     125\n12  2022       4     125\n\n\n\n\ndrop_na() and replace_na()\n\n# Sample data with NAs\nmessy_data &lt;- tibble(\n  x = c(1, 2, NA, 4, 5),\n  y = c(\"a\", NA, \"c\", \"d\", NA),\n  z = c(10, 20, 30, NA, 50)\n)\nprint(\"Data with NAs:\")\n\n[1] \"Data with NAs:\"\n\nmessy_data\n\n# A tibble: 5 x 3\n      x y         z\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 a        10\n2     2 &lt;NA&gt;     20\n3    NA c        30\n4     4 d        NA\n5     5 &lt;NA&gt;     50\n\n# Drop rows with any NA\nclean_all &lt;- messy_data %&gt;%\n  drop_na()\nprint(\"Drop all NAs:\")\n\n[1] \"Drop all NAs:\"\n\nclean_all\n\n# A tibble: 1 x 3\n      x y         z\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 a        10\n\n# Drop NAs from specific columns\nclean_x &lt;- messy_data %&gt;%\n  drop_na(x)\nprint(\"Drop NAs from x only:\")\n\n[1] \"Drop NAs from x only:\"\n\nclean_x\n\n# A tibble: 4 x 3\n      x y         z\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 a        10\n2     2 &lt;NA&gt;     20\n3     4 d        NA\n4     5 &lt;NA&gt;     50\n\n# Replace NAs with specific values\nreplaced_data &lt;- messy_data %&gt;%\n  replace_na(list(\n    x = 0,\n    y = \"missing\",\n    z = mean(messy_data$z, na.rm = TRUE)\n  ))\nprint(\"Replaced NAs:\")\n\n[1] \"Replaced NAs:\"\n\nreplaced_data\n\n# A tibble: 5 x 3\n      x y           z\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1 a        10  \n2     2 missing  20  \n3     0 c        30  \n4     4 d        27.5\n5     5 missing  50  \n\n\n\n\nAdvanced Missing Value Patterns\n\n# Create time series with gaps\ntime_series &lt;- tibble(\n  date = as.Date(c(\"2023-01-01\", \"2023-01-03\", \"2023-01-07\", \"2023-01-08\")),\n  value = c(100, 105, 115, 118)\n)\n\n# Complete the time series\ncomplete_series &lt;- time_series %&gt;%\n  complete(date = seq.Date(min(date), max(date), by = \"day\"))\nprint(\"Complete time series:\")\n\n[1] \"Complete time series:\"\n\ncomplete_series\n\n# A tibble: 8 x 2\n  date       value\n  &lt;date&gt;     &lt;dbl&gt;\n1 2023-01-01   100\n2 2023-01-02    NA\n3 2023-01-03   105\n4 2023-01-04    NA\n5 2023-01-05    NA\n6 2023-01-06    NA\n7 2023-01-07   115\n8 2023-01-08   118\n\n# Forward fill (last observation carried forward)\nfilled_series &lt;- complete_series %&gt;%\n  fill(value, .direction = \"down\")\nprint(\"Forward filled:\")\n\n[1] \"Forward filled:\"\n\nfilled_series\n\n# A tibble: 8 x 2\n  date       value\n  &lt;date&gt;     &lt;dbl&gt;\n1 2023-01-01   100\n2 2023-01-02   100\n3 2023-01-03   105\n4 2023-01-04   105\n5 2023-01-05   105\n6 2023-01-06   105\n7 2023-01-07   115\n8 2023-01-08   118\n\n# Interpolation (requires zoo package)\nif (require(zoo, quietly = TRUE)) {\n  interpolated &lt;- complete_series %&gt;%\n    mutate(value_interpolated = zoo::na.approx(value, na.rm = FALSE))\n  print(\"Linear interpolation:\")\n  interpolated\n}\n\n[1] \"Linear interpolation:\"\n\n\n# A tibble: 8 x 3\n  date       value value_interpolated\n  &lt;date&gt;     &lt;dbl&gt;              &lt;dbl&gt;\n1 2023-01-01   100               100 \n2 2023-01-02    NA               102.\n3 2023-01-03   105               105 \n4 2023-01-04    NA               108.\n5 2023-01-05    NA               110 \n6 2023-01-06    NA               112.\n7 2023-01-07   115               115 \n8 2023-01-08   118               118"
  },
  {
    "objectID": "04-data-tidying.html#nesting-and-unnesting",
    "href": "04-data-tidying.html#nesting-and-unnesting",
    "title": "Chapter 4: Data Tidying with tidyr",
    "section": "Nesting and Unnesting",
    "text": "Nesting and Unnesting\n\nCreating Nested Data\n\n# Group and nest\nnested_penguins &lt;- penguins %&gt;%\n  group_by(species, island) %&gt;%\n  nest()\nprint(\"Nested penguins data:\")\n\n[1] \"Nested penguins data:\"\n\nnested_penguins\n\n# A tibble: 5 x 3\n# Groups:   species, island [5]\n  species   island    data              \n  &lt;fct&gt;     &lt;fct&gt;     &lt;list&gt;            \n1 Adelie    Torgersen &lt;tibble [52 x 6]&gt; \n2 Adelie    Biscoe    &lt;tibble [44 x 6]&gt; \n3 Adelie    Dream     &lt;tibble [56 x 6]&gt; \n4 Gentoo    Biscoe    &lt;tibble [124 x 6]&gt;\n5 Chinstrap Dream     &lt;tibble [68 x 6]&gt; \n\n# Access nested data\nprint(\"First nested tibble:\")\n\n[1] \"First nested tibble:\"\n\nnested_penguins$data[[1]]\n\n# A tibble: 52 x 6\n   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex     year\n            &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt;  &lt;int&gt;\n 1           39.1          18.7               181        3750 male    2007\n 2           39.5          17.4               186        3800 female  2007\n 3           40.3          18                 195        3250 female  2007\n 4           NA            NA                  NA          NA &lt;NA&gt;    2007\n 5           36.7          19.3               193        3450 female  2007\n 6           39.3          20.6               190        3650 male    2007\n 7           38.9          17.8               181        3625 female  2007\n 8           39.2          19.6               195        4675 male    2007\n 9           34.1          18.1               193        3475 &lt;NA&gt;    2007\n10           42            20.2               190        4250 &lt;NA&gt;    2007\n# i 42 more rows\n\n# Nested with custom columns\ncustom_nest &lt;- penguins %&gt;%\n  group_by(species) %&gt;%\n  nest(\n    bill_data = c(bill_length_mm, bill_depth_mm),\n    body_data = c(flipper_length_mm, body_mass_g)\n  )\nprint(\"Custom nested columns:\")\n\n[1] \"Custom nested columns:\"\n\ncustom_nest\n\n# A tibble: 35 x 6\n# Groups:   species [3]\n   species island    sex     year bill_data         body_data        \n   &lt;fct&gt;   &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt; &lt;list&gt;            &lt;list&gt;           \n 1 Adelie  Torgersen male    2007 &lt;tibble [7 x 2]&gt;  &lt;tibble [7 x 2]&gt; \n 2 Adelie  Torgersen female  2007 &lt;tibble [8 x 2]&gt;  &lt;tibble [8 x 2]&gt; \n 3 Adelie  Torgersen &lt;NA&gt;    2007 &lt;tibble [5 x 2]&gt;  &lt;tibble [5 x 2]&gt; \n 4 Adelie  Biscoe    female  2007 &lt;tibble [5 x 2]&gt;  &lt;tibble [5 x 2]&gt; \n 5 Adelie  Biscoe    male    2007 &lt;tibble [5 x 2]&gt;  &lt;tibble [5 x 2]&gt; \n 6 Adelie  Dream     female  2007 &lt;tibble [9 x 2]&gt;  &lt;tibble [9 x 2]&gt; \n 7 Adelie  Dream     male    2007 &lt;tibble [10 x 2]&gt; &lt;tibble [10 x 2]&gt;\n 8 Adelie  Dream     &lt;NA&gt;    2007 &lt;tibble [1 x 2]&gt;  &lt;tibble [1 x 2]&gt; \n 9 Adelie  Biscoe    female  2008 &lt;tibble [9 x 2]&gt;  &lt;tibble [9 x 2]&gt; \n10 Adelie  Biscoe    male    2008 &lt;tibble [9 x 2]&gt;  &lt;tibble [9 x 2]&gt; \n# i 25 more rows\n\n\n\n\nWorking with Nested Data\n\n# Apply functions to nested data\nnested_summary &lt;- nested_penguins %&gt;%\n  mutate(\n    n_obs = map_int(data, nrow),\n    avg_mass = map_dbl(data, ~ mean(.$body_mass_g, na.rm = TRUE)),\n    mass_range = map_dbl(data, ~ diff(range(.$body_mass_g, na.rm = TRUE)))\n  )\nprint(\"Summary of nested data:\")\n\n[1] \"Summary of nested data:\"\n\nnested_summary %&gt;% select(-data)\n\n# A tibble: 5 x 5\n# Groups:   species, island [5]\n  species   island    n_obs avg_mass mass_range\n  &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1 Adelie    Torgersen    52    3706.       1800\n2 Adelie    Biscoe       44    3710.       1925\n3 Adelie    Dream        56    3688.       1750\n4 Gentoo    Biscoe      124    5076.       2350\n5 Chinstrap Dream        68    3733.       2100\n\n# Fit models to nested data\nnested_models &lt;- nested_penguins %&gt;%\n  mutate(\n    model = map(data, ~ lm(body_mass_g ~ flipper_length_mm, data = .)),\n    r_squared = map_dbl(model, ~ summary(.)$r.squared)\n  )\nprint(\"Models fitted to nested data:\")\n\n[1] \"Models fitted to nested data:\"\n\nnested_models %&gt;% select(-data, -model)\n\n# A tibble: 5 x 3\n# Groups:   species, island [5]\n  species   island    r_squared\n  &lt;fct&gt;     &lt;fct&gt;         &lt;dbl&gt;\n1 Adelie    Torgersen     0.190\n2 Adelie    Biscoe        0.277\n3 Adelie    Dream         0.212\n4 Gentoo    Biscoe        0.494\n5 Chinstrap Dream         0.412\n\n\n\n\nUnnesting\n\n# Basic unnest\nunnested &lt;- nested_penguins %&gt;%\n  unnest(data)\nprint(\"Unnested data (first 10 rows):\")\n\n[1] \"Unnested data (first 10 rows):\"\n\nhead(unnested, 10)\n\n# A tibble: 10 x 8\n# Groups:   species, island [1]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# i 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n# Unnest with selection\npartial_unnest &lt;- custom_nest %&gt;%\n  unnest(bill_data) %&gt;%\n  head(10)\nprint(\"Partial unnest:\")\n\n[1] \"Partial unnest:\"\n\npartial_unnest\n\n# A tibble: 10 x 7\n# Groups:   species [1]\n   species island    sex     year bill_length_mm bill_depth_mm body_data       \n   &lt;fct&gt;   &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;          &lt;dbl&gt;         &lt;dbl&gt; &lt;list&gt;          \n 1 Adelie  Torgersen male    2007           39.1          18.7 &lt;tibble [7 x 2]&gt;\n 2 Adelie  Torgersen male    2007           39.3          20.6 &lt;tibble [7 x 2]&gt;\n 3 Adelie  Torgersen male    2007           39.2          19.6 &lt;tibble [7 x 2]&gt;\n 4 Adelie  Torgersen male    2007           38.6          21.2 &lt;tibble [7 x 2]&gt;\n 5 Adelie  Torgersen male    2007           34.6          21.1 &lt;tibble [7 x 2]&gt;\n 6 Adelie  Torgersen male    2007           42.5          20.7 &lt;tibble [7 x 2]&gt;\n 7 Adelie  Torgersen male    2007           46            21.5 &lt;tibble [7 x 2]&gt;\n 8 Adelie  Torgersen female  2007           39.5          17.4 &lt;tibble [8 x 2]&gt;\n 9 Adelie  Torgersen female  2007           40.3          18   &lt;tibble [8 x 2]&gt;\n10 Adelie  Torgersen female  2007           36.7          19.3 &lt;tibble [8 x 2]&gt;\n\n# Unnest wider (for list columns with consistent structure)\nsummary_data &lt;- nested_penguins %&gt;%\n  mutate(\n    summary = map(data, ~ tibble(\n      mean_mass = mean(.$body_mass_g, na.rm = TRUE),\n      sd_mass = sd(.$body_mass_g, na.rm = TRUE),\n      n = n()\n    ))\n  ) %&gt;%\n  select(-data) %&gt;%\n  unnest_wider(summary)\nprint(\"Unnest wider:\")\n\n[1] \"Unnest wider:\"\n\nsummary_data\n\n# A tibble: 5 x 5\n# Groups:   species, island [5]\n  species   island    mean_mass sd_mass     n\n  &lt;fct&gt;     &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;\n1 Adelie    Torgersen     3706.    445.     1\n2 Adelie    Biscoe        3710.    488.     1\n3 Adelie    Dream         3688.    455.     1\n4 Gentoo    Biscoe        5076.    504.     1\n5 Chinstrap Dream         3733.    384.     1"
  },
  {
    "objectID": "04-data-tidying.html#list-columns",
    "href": "04-data-tidying.html#list-columns",
    "title": "Chapter 4: Data Tidying with tidyr",
    "section": "List Columns",
    "text": "List Columns\n\nCreating and Working with List Columns\n\n# Create list columns\nlist_col_data &lt;- tibble(\n  id = 1:3,\n  values = list(\n    c(1, 2, 3),\n    c(4, 5),\n    c(6, 7, 8, 9)\n  ),\n  metadata = list(\n    list(type = \"A\", quality = \"high\"),\n    list(type = \"B\", quality = \"medium\"),\n    list(type = \"A\", quality = \"low\")\n  )\n)\nprint(\"Data with list columns:\")\n\n[1] \"Data with list columns:\"\n\nlist_col_data\n\n# A tibble: 3 x 3\n     id values    metadata        \n  &lt;int&gt; &lt;list&gt;    &lt;list&gt;          \n1     1 &lt;dbl [3]&gt; &lt;named list [2]&gt;\n2     2 &lt;dbl [2]&gt; &lt;named list [2]&gt;\n3     3 &lt;dbl [4]&gt; &lt;named list [2]&gt;\n\n# Extract from list columns\nextracted &lt;- list_col_data %&gt;%\n  mutate(\n    n_values = map_int(values, length),\n    sum_values = map_dbl(values, sum),\n    type = map_chr(metadata, \"type\"),\n    quality = map_chr(metadata, \"quality\")\n  )\nprint(\"Extracted values:\")\n\n[1] \"Extracted values:\"\n\nextracted %&gt;% select(-values, -metadata)\n\n# A tibble: 3 x 5\n     id n_values sum_values type  quality\n  &lt;int&gt;    &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  \n1     1        3          6 A     high   \n2     2        2          9 B     medium \n3     3        4         30 A     low    \n\n# Unnest list columns\nunnested_values &lt;- list_col_data %&gt;%\n  unnest(values) %&gt;%\n  group_by(id) %&gt;%\n  mutate(value_index = row_number())\nprint(\"Unnested values:\")\n\n[1] \"Unnested values:\"\n\nunnested_values\n\n# A tibble: 9 x 4\n# Groups:   id [3]\n     id values metadata         value_index\n  &lt;int&gt;  &lt;dbl&gt; &lt;list&gt;                 &lt;int&gt;\n1     1      1 &lt;named list [2]&gt;           1\n2     1      2 &lt;named list [2]&gt;           2\n3     1      3 &lt;named list [2]&gt;           3\n4     2      4 &lt;named list [2]&gt;           1\n5     2      5 &lt;named list [2]&gt;           2\n6     3      6 &lt;named list [2]&gt;           1\n7     3      7 &lt;named list [2]&gt;           2\n8     3      8 &lt;named list [2]&gt;           3\n9     3      9 &lt;named list [2]&gt;           4\n\n\n\n\nAdvanced List Column Operations\n\n# Store complex objects\nmodel_data &lt;- penguins %&gt;%\n  drop_na() %&gt;%\n  group_by(species) %&gt;%\n  nest() %&gt;%\n  mutate(\n    # Fit different models\n    lm_model = map(data, ~ lm(body_mass_g ~ flipper_length_mm + bill_length_mm, data = .)),\n    # Extract coefficients\n    coefficients = map(lm_model, broom::tidy),\n    # Get predictions\n    predictions = map2(lm_model, data, ~ broom::augment(.x, newdata = .y))\n  )\n\n# View coefficients\ncoefficients_df &lt;- model_data %&gt;%\n  select(species, coefficients) %&gt;%\n  unnest(coefficients)\nprint(\"Model coefficients:\")\n\n[1] \"Model coefficients:\"\n\ncoefficients_df\n\n# A tibble: 9 x 6\n# Groups:   species [3]\n  species   term              estimate std.error statistic       p.value\n  &lt;fct&gt;     &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 Adelie    (Intercept)        -3492.     890.       -3.93 0.000134     \n2 Adelie    flipper_length_mm     22.5      4.88      4.60 0.00000928   \n3 Adelie    bill_length_mm        75.5     12.0       6.31 0.00000000325\n4 Gentoo    (Intercept)        -5524.    1039.       -5.32 0.000000516  \n5 Gentoo    flipper_length_mm     36.6      6.18      5.92 0.0000000331 \n6 Gentoo    bill_length_mm        56.1     13.1       4.29 0.0000378    \n7 Chinstrap (Intercept)        -3212.     957.       -3.36 0.00132      \n8 Chinstrap flipper_length_mm     27.7      5.52      5.01 0.00000444   \n9 Chinstrap bill_length_mm        31.2     11.8       2.65 0.0101       \n\n# Extract R-squared values\nr_squared &lt;- model_data %&gt;%\n  mutate(r_squared = map_dbl(lm_model, ~ summary(.)$r.squared)) %&gt;%\n  select(species, r_squared)\nprint(\"R-squared values:\")\n\n[1] \"R-squared values:\"\n\nr_squared\n\n# A tibble: 3 x 2\n# Groups:   species [3]\n  species   r_squared\n  &lt;fct&gt;         &lt;dbl&gt;\n1 Adelie        0.387\n2 Gentoo        0.573\n3 Chinstrap     0.469"
  },
  {
    "objectID": "04-data-tidying.html#expanding-and-completing-data",
    "href": "04-data-tidying.html#expanding-and-completing-data",
    "title": "Chapter 4: Data Tidying with tidyr",
    "section": "Expanding and Completing Data",
    "text": "Expanding and Completing Data\n\nexpand(): All Combinations\n\n# Create all combinations\nexperiment_design &lt;- expand_grid(\n  treatment = c(\"Control\", \"Drug A\", \"Drug B\"),\n  dose = c(0, 10, 20, 50),\n  replicate = 1:3\n)\nprint(\"Experimental design:\")\n\n[1] \"Experimental design:\"\n\nhead(experiment_design, 12)\n\n# A tibble: 12 x 3\n   treatment  dose replicate\n   &lt;chr&gt;     &lt;dbl&gt;     &lt;int&gt;\n 1 Control       0         1\n 2 Control       0         2\n 3 Control       0         3\n 4 Control      10         1\n 5 Control      10         2\n 6 Control      10         3\n 7 Control      20         1\n 8 Control      20         2\n 9 Control      20         3\n10 Control      50         1\n11 Control      50         2\n12 Control      50         3\n\n# Expand existing data\nsales_data &lt;- tibble(\n  store = c(\"A\", \"A\", \"B\"),\n  product = c(\"X\", \"Y\", \"X\"),\n  sales = c(100, 150, 120)\n)\n\nall_combinations &lt;- sales_data %&gt;%\n  expand(store, product)\nprint(\"All store-product combinations:\")\n\n[1] \"All store-product combinations:\"\n\nall_combinations\n\n# A tibble: 4 x 2\n  store product\n  &lt;chr&gt; &lt;chr&gt;  \n1 A     X      \n2 A     Y      \n3 B     X      \n4 B     Y      \n\n# Complete with defaults\ncomplete_sales &lt;- sales_data %&gt;%\n  complete(store, product, fill = list(sales = 0))\nprint(\"Complete sales data:\")\n\n[1] \"Complete sales data:\"\n\ncomplete_sales\n\n# A tibble: 4 x 3\n  store product sales\n  &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 A     X         100\n2 A     Y         150\n3 B     X         120\n4 B     Y           0\n\n\n\n\ncrossing() and nesting()\n\n# Crossing - all combinations\ncolors &lt;- c(\"red\", \"blue\", \"green\")\nsizes &lt;- c(\"S\", \"M\", \"L\")\n\ninventory &lt;- crossing(\n  color = colors,\n  size = sizes\n) %&gt;%\n  mutate(stock = sample(0:50, n(), replace = TRUE))\nprint(\"Inventory matrix:\")\n\n[1] \"Inventory matrix:\"\n\ninventory\n\n# A tibble: 9 x 3\n  color size  stock\n  &lt;chr&gt; &lt;chr&gt; &lt;int&gt;\n1 blue  L        30\n2 blue  M        14\n3 blue  S        50\n4 green L        13\n5 green M         2\n6 green S        41\n7 red   L        49\n8 red   M        42\n9 red   S        36\n\n# Nesting - only observed combinations\nobserved_data &lt;- tibble(\n  city = c(\"NYC\", \"NYC\", \"LA\", \"LA\", \"Chicago\"),\n  year = c(2020, 2021, 2020, 2021, 2021),\n  value = c(100, 110, 95, 105, 115)\n)\n\nnested_combos &lt;- observed_data %&gt;%\n  expand(nesting(city, year))\nprint(\"Nested combinations (only observed):\")\n\n[1] \"Nested combinations (only observed):\"\n\nnested_combos\n\n# A tibble: 5 x 2\n  city     year\n  &lt;chr&gt;   &lt;dbl&gt;\n1 Chicago  2021\n2 LA       2020\n3 LA       2021\n4 NYC      2020\n5 NYC      2021"
  },
  {
    "objectID": "04-data-tidying.html#real-world-tidying-examples",
    "href": "04-data-tidying.html#real-world-tidying-examples",
    "title": "Chapter 4: Data Tidying with tidyr",
    "section": "Real-World Tidying Examples",
    "text": "Real-World Tidying Examples\n\nExample 1: Survey Data\n\n# Messy survey data\nsurvey_messy &lt;- tibble(\n  respondent = c(\"R001\", \"R002\", \"R003\"),\n  `Q1_satisfied` = c(4, 5, 3),\n  `Q2_satisfied` = c(3, 4, 4),\n  `Q1_importance` = c(5, 5, 4),\n  `Q2_importance` = c(4, 3, 5),\n  age_gender = c(\"25_M\", \"30_F\", \"28_M\")\n)\n\n# Tidy the survey data\nsurvey_tidy &lt;- survey_messy %&gt;%\n  # Separate age and gender\n  separate(age_gender, into = c(\"age\", \"gender\"), sep = \"_\", convert = TRUE) %&gt;%\n  # Pivot questions\n  pivot_longer(\n    cols = starts_with(\"Q\"),\n    names_to = c(\"question\", \"measure\"),\n    names_pattern = \"(.*)_(.*)\",\n    values_to = \"value\"\n  ) %&gt;%\n  # Reshape to have satisfaction and importance as columns\n  pivot_wider(\n    names_from = measure,\n    values_from = value\n  )\n\nprint(\"Tidied survey data:\")\n\n[1] \"Tidied survey data:\"\n\nsurvey_tidy\n\n# A tibble: 6 x 6\n  respondent   age gender question satisfied importance\n  &lt;chr&gt;      &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 R001          25 M      Q1               4          5\n2 R001          25 M      Q2               3          4\n3 R002          30 F      Q1               5          5\n4 R002          30 F      Q2               4          3\n5 R003          28 M      Q1               3          4\n6 R003          28 M      Q2               4          5\n\n\n\n\nExample 2: Time Series Data\n\n# Messy time series\nts_messy &lt;- tibble(\n  date_time = c(\"2023-01-15 10:30\", \"2023-01-15 14:45\", \"2023-01-16 09:15\"),\n  sensor_readings = c(\"temp:22.5;humidity:60\", \"temp:23.1;humidity:58\", \"temp:21.8;humidity:62\")\n)\n\n# Tidy the time series\nts_tidy &lt;- ts_messy %&gt;%\n  # Separate date and time\n  separate(date_time, into = c(\"date\", \"time\"), sep = \" \") %&gt;%\n  # Separate sensor readings\n  separate_rows(sensor_readings, sep = \";\") %&gt;%\n  separate(sensor_readings, into = c(\"sensor\", \"value\"), sep = \":\", convert = TRUE) %&gt;%\n  # Convert date\n  mutate(date = as.Date(date))\n\nprint(\"Tidied time series:\")\n\n[1] \"Tidied time series:\"\n\nts_tidy\n\n# A tibble: 6 x 4\n  date       time  sensor   value\n  &lt;date&gt;     &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;\n1 2023-01-15 10:30 temp      22.5\n2 2023-01-15 10:30 humidity  60  \n3 2023-01-15 14:45 temp      23.1\n4 2023-01-15 14:45 humidity  58  \n5 2023-01-16 09:15 temp      21.8\n6 2023-01-16 09:15 humidity  62  \n\n# Reshape for analysis\nts_wide &lt;- ts_tidy %&gt;%\n  pivot_wider(\n    names_from = sensor,\n    values_from = value\n  )\nprint(\"Wide format for analysis:\")\n\n[1] \"Wide format for analysis:\"\n\nts_wide\n\n# A tibble: 3 x 4\n  date       time   temp humidity\n  &lt;date&gt;     &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 2023-01-15 10:30  22.5       60\n2 2023-01-15 14:45  23.1       58\n3 2023-01-16 09:15  21.8       62\n\n\n\n\nExample 3: Hierarchical Data\n\n# Hierarchical organization data\norg_data &lt;- tibble(\n  employee = c(\"Alice\", \"Bob\", \"Charlie\", \"Diana\"),\n  department_division = c(\"Sales/North\", \"Sales/South\", \"IT/Infrastructure\", \"IT/Development\"),\n  metrics = c(\"revenue:1000000;satisfaction:4.5\", \n              \"revenue:800000;satisfaction:4.2\",\n              \"tickets:450;uptime:99.9\",\n              \"features:12;bugs:3\")\n)\n\n# Tidy hierarchical data\norg_tidy &lt;- org_data %&gt;%\n  # Separate hierarchy\n  separate(department_division, into = c(\"department\", \"division\"), sep = \"/\") %&gt;%\n  # Separate metrics\n  separate_rows(metrics, sep = \";\") %&gt;%\n  separate(metrics, into = c(\"metric\", \"value\"), sep = \":\", convert = TRUE)\n\nprint(\"Tidied organizational data:\")\n\n[1] \"Tidied organizational data:\"\n\norg_tidy\n\n# A tibble: 8 x 5\n  employee department division       metric           value\n  &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt;            &lt;dbl&gt;\n1 Alice    Sales      North          revenue      1000000  \n2 Alice    Sales      North          satisfaction       4.5\n3 Bob      Sales      South          revenue       800000  \n4 Bob      Sales      South          satisfaction       4.2\n5 Charlie  IT         Infrastructure tickets          450  \n6 Charlie  IT         Infrastructure uptime            99.9\n7 Diana    IT         Development    features          12  \n8 Diana    IT         Development    bugs               3  \n\n# Summarize by department\ndept_summary &lt;- org_tidy %&gt;%\n  group_by(department, metric) %&gt;%\n  summarise(\n    total = sum(value),\n    avg = mean(value),\n    .groups = \"drop\"\n  ) %&gt;%\n  pivot_wider(\n    names_from = metric,\n    values_from = c(total, avg),\n    names_glue = \"{metric}_{.value}\"\n  )\n\nprint(\"Department summary:\")\n\n[1] \"Department summary:\"\n\ndept_summary\n\n# A tibble: 2 x 13\n  department bugs_total features_total tickets_total uptime_total revenue_total\n  &lt;chr&gt;           &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;\n1 IT                  3             12           450         99.9            NA\n2 Sales              NA             NA            NA         NA         1800000\n# i 7 more variables: satisfaction_total &lt;dbl&gt;, bugs_avg &lt;dbl&gt;,\n#   features_avg &lt;dbl&gt;, tickets_avg &lt;dbl&gt;, uptime_avg &lt;dbl&gt;, revenue_avg &lt;dbl&gt;,\n#   satisfaction_avg &lt;dbl&gt;"
  },
  {
    "objectID": "04-data-tidying.html#exercises",
    "href": "04-data-tidying.html#exercises",
    "title": "Chapter 4: Data Tidying with tidyr",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Complex Pivoting\nTransform this gradebook data into a tidy format suitable for analysis:\n\n# Given messy gradebook\ngradebook &lt;- tibble(\n  student = c(\"Alice\", \"Bob\", \"Charlie\"),\n  `Math_Midterm` = c(85, 90, 78),\n  `Math_Final` = c(88, 85, 82),\n  `Science_Midterm` = c(92, 88, 90),\n  `Science_Final` = c(90, 92, 88),\n  `attendance_days` = c(\"45/50\", \"48/50\", \"42/50\")\n)\n\n# Your solution\ntidy_gradebook &lt;- gradebook %&gt;%\n  # Separate attendance\n  separate(attendance_days, into = c(\"days_present\", \"total_days\"), \n           sep = \"/\", convert = TRUE) %&gt;%\n  # Calculate attendance rate\n  mutate(attendance_rate = days_present / total_days) %&gt;%\n  # Pivot grades\n  pivot_longer(\n    cols = matches(\"_Midterm|_Final\"),\n    names_to = c(\"subject\", \"exam\"),\n    names_sep = \"_\",\n    values_to = \"score\"\n  ) %&gt;%\n  # Calculate average by subject\n  group_by(student, subject) %&gt;%\n  mutate(subject_avg = mean(score)) %&gt;%\n  ungroup()\n\nprint(\"Tidied gradebook:\")\n\n[1] \"Tidied gradebook:\"\n\ntidy_gradebook\n\n# A tibble: 12 x 8\n   student days_present total_days attendance_rate subject exam    score\n   &lt;chr&gt;          &lt;int&gt;      &lt;int&gt;           &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;\n 1 Alice             45         50            0.9  Math    Midterm    85\n 2 Alice             45         50            0.9  Math    Final      88\n 3 Alice             45         50            0.9  Science Midterm    92\n 4 Alice             45         50            0.9  Science Final      90\n 5 Bob               48         50            0.96 Math    Midterm    90\n 6 Bob               48         50            0.96 Math    Final      85\n 7 Bob               48         50            0.96 Science Midterm    88\n 8 Bob               48         50            0.96 Science Final      92\n 9 Charlie           42         50            0.84 Math    Midterm    78\n10 Charlie           42         50            0.84 Math    Final      82\n11 Charlie           42         50            0.84 Science Midterm    90\n12 Charlie           42         50            0.84 Science Final      88\n# i 1 more variable: subject_avg &lt;dbl&gt;\n\n\n\n\nExercise 2: Nested Data Analysis\nWork with nested gapminder data to calculate decade-wise trends:\n\n# Your solution\ngapminder_nested &lt;- gapminder %&gt;%\n  mutate(decade = floor(year / 10) * 10) %&gt;%\n  group_by(continent, decade) %&gt;%\n  nest() %&gt;%\n  mutate(\n    # Calculate trends within each group\n    avg_lifeExp = map_dbl(data, ~ mean(.$lifeExp)),\n    gdp_growth = map_dbl(data, ~ {\n      if(nrow(.) &gt; 1) {\n        model &lt;- lm(log(gdpPercap) ~ I(year - min(year)), data = .)\n        coef(model)[2] * 100  # Percent growth per year\n      } else {\n        NA_real_\n      }\n    }),\n    pop_total = map_dbl(data, ~ sum(.$pop) / 1e9),  # In billions\n    n_countries = map_int(data, ~ n_distinct(.$country))\n  )\n\ngapminder_summary &lt;- gapminder_nested %&gt;%\n  select(-data) %&gt;%\n  arrange(continent, decade)\n\nprint(\"Decade-wise trends by continent:\")\n\n[1] \"Decade-wise trends by continent:\"\n\ngapminder_summary\n\n# A tibble: 30 x 6\n# Groups:   continent, decade [30]\n   continent decade avg_lifeExp gdp_growth pop_total n_countries\n   &lt;fct&gt;      &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;       &lt;int&gt;\n 1 Africa      1950        40.2     1.88       0.502          52\n 2 Africa      1960        44.3     2.74       0.632          52\n 3 Africa      1970        48.5     0.619      0.813          52\n 4 Africa      1980        52.5    -1.05       1.07           52\n 5 Africa      1990        53.6     0.450      1.40           52\n 6 Africa      2000        54.1     2.38       1.76           52\n 7 Americas    1950        54.6     2.33       0.732          25\n 8 Americas    1960        59.4     2.45       0.914          25\n 9 Americas    1970        63.4     2.65       1.11           25\n10 Americas    1980        67.2    -0.0866     1.31           25\n# i 20 more rows\n\n\n\n\nExercise 3: Complex Missing Data\nHandle this dataset with various types of missing data:\n\n# Dataset with complex missing patterns\ncomplex_missing &lt;- tibble(\n  date = as.Date(c(\"2023-01-01\", \"2023-01-02\", \"2023-01-04\", \"2023-01-07\")),\n  store_a = c(100, NA, 120, 130),\n  store_b = c(200, 210, NA, NA),\n  store_c = c(150, 160, 170, 180)\n)\n\n# Your solution\n# Complete the date sequence\ncomplete_data &lt;- complex_missing %&gt;%\n  complete(date = seq.Date(min(date), max(date), by = \"day\"))\n\n# Different filling strategies\nfilled_data &lt;- complete_data %&gt;%\n  # Forward fill for store_a\n  fill(store_a, .direction = \"down\") %&gt;%\n  # Interpolate store_b\n  mutate(\n    store_b = zoo::na.approx(store_b, na.rm = FALSE)\n  ) %&gt;%\n  # Average fill for store_c\n  mutate(\n    store_c = if_else(is.na(store_c), \n                      mean(store_c, na.rm = TRUE), \n                      store_c)\n  )\n\nprint(\"Original data:\")\n\n[1] \"Original data:\"\n\ncomplex_missing\n\n# A tibble: 4 x 4\n  date       store_a store_b store_c\n  &lt;date&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 2023-01-01     100     200     150\n2 2023-01-02      NA     210     160\n3 2023-01-04     120      NA     170\n4 2023-01-07     130      NA     180\n\nprint(\"After handling missing values:\")\n\n[1] \"After handling missing values:\"\n\nfilled_data\n\n# A tibble: 7 x 4\n  date       store_a store_b store_c\n  &lt;date&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 2023-01-01     100     200     150\n2 2023-01-02     100     210     160\n3 2023-01-03     100      NA     165\n4 2023-01-04     120      NA     170\n5 2023-01-05     120      NA     165\n6 2023-01-06     120      NA     165\n7 2023-01-07     130      NA     180\n\n# Convert to long format for analysis\nlong_filled &lt;- filled_data %&gt;%\n  pivot_longer(\n    cols = starts_with(\"store\"),\n    names_to = \"store\",\n    names_prefix = \"store_\",\n    values_to = \"sales\"\n  )\n\nprint(\"Long format for analysis:\")\n\n[1] \"Long format for analysis:\"\n\nlong_filled\n\n# A tibble: 21 x 3\n   date       store sales\n   &lt;date&gt;     &lt;chr&gt; &lt;dbl&gt;\n 1 2023-01-01 a       100\n 2 2023-01-01 b       200\n 3 2023-01-01 c       150\n 4 2023-01-02 a       100\n 5 2023-01-02 b       210\n 6 2023-01-02 c       160\n 7 2023-01-03 a       100\n 8 2023-01-03 b        NA\n 9 2023-01-03 c       165\n10 2023-01-04 a       120\n# i 11 more rows\n\n\n\n\nExercise 4: Real-World Data Cleaning\nClean and reshape this messy real-world dataset:\n\n# Messy real-world data\nmessy_sales &lt;- tibble(\n  id = c(\"US-2023-001\", \"UK-2023-002\", \"CA-2023-003\"),\n  jan_feb_mar = c(\"100,120,115\", \"200,210,205\", \"150,145,160\"),\n  apr_may_jun = c(\"125,130,135\", \"215,220,225\", \"165,170,175\"),\n  customer_info = c(\"John Doe|Premium\", \"Jane Smith|Standard\", \"Bob Johnson|Premium\"),\n  notes = c(\"Q1: Good performance\", \"Q1: Steady growth\", \"Q1: Meeting targets\")\n)\n\n# Your solution\nclean_sales &lt;- messy_sales %&gt;%\n  # Separate ID components\n  separate(id, into = c(\"country\", \"year\", \"customer_id\"), sep = \"-\", remove = FALSE) %&gt;%\n  # Separate customer info\n  separate(customer_info, into = c(\"customer_name\", \"tier\"), sep = \"\\\\|\") %&gt;%\n  # Pivot quarters\n  pivot_longer(\n    cols = c(jan_feb_mar, apr_may_jun),\n    names_to = \"quarter\",\n    values_to = \"monthly_sales\"\n  ) %&gt;%\n  # Separate monthly sales\n  separate(monthly_sales, into = c(\"month1\", \"month2\", \"month3\"), sep = \",\", convert = TRUE) %&gt;%\n  # Create proper quarter labels\n  mutate(\n    quarter = case_when(\n      quarter == \"jan_feb_mar\" ~ \"Q1\",\n      quarter == \"apr_may_jun\" ~ \"Q2\"\n    )\n  ) %&gt;%\n  # Pivot months to long format\n  pivot_longer(\n    cols = starts_with(\"month\"),\n    names_to = \"month_num\",\n    names_prefix = \"month\",\n    values_to = \"sales\"\n  ) %&gt;%\n  # Calculate month\n  mutate(\n    month_num = as.integer(month_num),\n    month = case_when(\n      quarter == \"Q1\" ~ month_num,\n      quarter == \"Q2\" ~ month_num + 3\n    ),\n    date = as.Date(paste(year, month, \"01\", sep = \"-\"))\n  ) %&gt;%\n  select(id, country, date, customer_name, tier, sales, notes)\n\nprint(\"Cleaned sales data:\")\n\n[1] \"Cleaned sales data:\"\n\nclean_sales\n\n# A tibble: 18 x 7\n   id          country date       customer_name tier     sales notes            \n   &lt;chr&gt;       &lt;chr&gt;   &lt;date&gt;     &lt;chr&gt;         &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;            \n 1 US-2023-001 US      2023-01-01 John Doe      Premium    100 Q1: Good perform~\n 2 US-2023-001 US      2023-02-01 John Doe      Premium    120 Q1: Good perform~\n 3 US-2023-001 US      2023-03-01 John Doe      Premium    115 Q1: Good perform~\n 4 US-2023-001 US      2023-04-01 John Doe      Premium    125 Q1: Good perform~\n 5 US-2023-001 US      2023-05-01 John Doe      Premium    130 Q1: Good perform~\n 6 US-2023-001 US      2023-06-01 John Doe      Premium    135 Q1: Good perform~\n 7 UK-2023-002 UK      2023-01-01 Jane Smith    Standard   200 Q1: Steady growth\n 8 UK-2023-002 UK      2023-02-01 Jane Smith    Standard   210 Q1: Steady growth\n 9 UK-2023-002 UK      2023-03-01 Jane Smith    Standard   205 Q1: Steady growth\n10 UK-2023-002 UK      2023-04-01 Jane Smith    Standard   215 Q1: Steady growth\n11 UK-2023-002 UK      2023-05-01 Jane Smith    Standard   220 Q1: Steady growth\n12 UK-2023-002 UK      2023-06-01 Jane Smith    Standard   225 Q1: Steady growth\n13 CA-2023-003 CA      2023-01-01 Bob Johnson   Premium    150 Q1: Meeting targ~\n14 CA-2023-003 CA      2023-02-01 Bob Johnson   Premium    145 Q1: Meeting targ~\n15 CA-2023-003 CA      2023-03-01 Bob Johnson   Premium    160 Q1: Meeting targ~\n16 CA-2023-003 CA      2023-04-01 Bob Johnson   Premium    165 Q1: Meeting targ~\n17 CA-2023-003 CA      2023-05-01 Bob Johnson   Premium    170 Q1: Meeting targ~\n18 CA-2023-003 CA      2023-06-01 Bob Johnson   Premium    175 Q1: Meeting targ~"
  },
  {
    "objectID": "04-data-tidying.html#summary",
    "href": "04-data-tidying.html#summary",
    "title": "Chapter 4: Data Tidying with tidyr",
    "section": "Summary",
    "text": "Summary\nYou’ve mastered tidyr essentials:\n✅ Understanding tidy data principles\n✅ Pivoting between wide and long formats\n✅ Separating and uniting columns\n✅ Handling missing values systematically\n✅ Working with nested data and list columns\n✅ Expanding and completing datasets\n✅ Real-world data tidying techniques"
  },
  {
    "objectID": "04-data-tidying.html#whats-next",
    "href": "04-data-tidying.html#whats-next",
    "title": "Chapter 4: Data Tidying with tidyr",
    "section": "What’s Next?",
    "text": "What’s Next?\nIn Chapter 5, we’ll explore data visualization with ggplot2, creating beautiful and informative graphics."
  },
  {
    "objectID": "04-data-tidying.html#additional-resources",
    "href": "04-data-tidying.html#additional-resources",
    "title": "Chapter 4: Data Tidying with tidyr",
    "section": "Additional Resources",
    "text": "Additional Resources\n\ntidyr Documentation\nTidy Data Paper\ntidyr Cheat Sheet\nR for Data Science - Tidy Data"
  },
  {
    "objectID": "08-tidymodels-intro.html",
    "href": "08-tidymodels-intro.html",
    "title": "Chapter 8: Introduction to Tidymodels - Theory and Practice",
    "section": "",
    "text": "By the end of this chapter, you will understand:\n\nThe philosophy and structure of tidymodels\nCore machine learning concepts and theory\nThe bias-variance tradeoff\nOverfitting and underfitting\nCross-validation theory\nThe tidymodels workflow\nKey packages in the tidymodels ecosystem"
  },
  {
    "objectID": "08-tidymodels-intro.html#learning-objectives",
    "href": "08-tidymodels-intro.html#learning-objectives",
    "title": "Chapter 8: Introduction to Tidymodels - Theory and Practice",
    "section": "",
    "text": "By the end of this chapter, you will understand:\n\nThe philosophy and structure of tidymodels\nCore machine learning concepts and theory\nThe bias-variance tradeoff\nOverfitting and underfitting\nCross-validation theory\nThe tidymodels workflow\nKey packages in the tidymodels ecosystem"
  },
  {
    "objectID": "08-tidymodels-intro.html#machine-learning-foundations",
    "href": "08-tidymodels-intro.html#machine-learning-foundations",
    "title": "Chapter 8: Introduction to Tidymodels - Theory and Practice",
    "section": "Machine Learning Foundations",
    "text": "Machine Learning Foundations\n\nWhat is Machine Learning?\nMachine learning is the science of getting computers to learn patterns from data without being explicitly programmed. Instead of writing rules, we let algorithms discover patterns.\nKey Concepts:\n\nSupervised Learning: Learning from labeled examples (y is known)\n\nClassification: Predicting categories\nRegression: Predicting continuous values\n\nUnsupervised Learning: Finding patterns without labels\n\nClustering: Grouping similar observations\nDimensionality reduction: Simplifying complex data\n\nFeatures (X): Input variables/predictors\nTarget (y): Output variable we want to predict\nTraining: Process of learning patterns from data\nInference: Making predictions on new data\n\n\n\nThe Learning Process\n\nlibrary(tidymodels)\n\n-- Attaching packages -------------------------------------- tidymodels 1.4.1 --\n\n\nv broom        1.0.10     v recipes      1.3.1 \nv dials        1.4.2      v rsample      1.3.1 \nv dplyr        1.1.4      v tailor       0.1.0 \nv ggplot2      4.0.0      v tidyr        1.3.1 \nv infer        1.0.9      v tune         2.0.0 \nv modeldata    1.5.1      v workflows    1.3.0 \nv parsnip      1.3.3      v workflowsets 1.1.1 \nv purrr        1.1.0      v yardstick    1.3.2 \n\n\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx purrr::discard() masks scales::discard()\nx dplyr::filter()  masks stats::filter()\nx dplyr::lag()     masks stats::lag()\nx recipes::step()  masks stats::step()\n\nlibrary(tidyverse)\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv forcats   1.0.0     v stringr   1.5.2\nv lubridate 1.9.4     v tibble    3.3.0\nv readr     2.1.5     \n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx readr::col_factor() masks scales::col_factor()\nx purrr::discard()    masks scales::discard()\nx dplyr::filter()     masks stats::filter()\nx stringr::fixed()    masks recipes::fixed()\nx dplyr::lag()        masks stats::lag()\nx readr::spec()       masks yardstick::spec()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(palmerpenguins)\n\n\nAdjuntando el paquete: 'palmerpenguins'\n\nThe following object is masked from 'package:modeldata':\n\n    penguins\n\nlibrary(modeldata)\nlibrary(vip)\n\n\nAdjuntando el paquete: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\n# Set theme\ntheme_set(theme_minimal())\n\n# For reproducibility\nset.seed(123)\n\n\n\nMathematical Foundation\nIn supervised learning, we seek to find a function f that maps inputs X to outputs y:\n\\[y = f(X) + \\epsilon\\]\nWhere: - \\(f\\) is the true underlying function - \\(\\epsilon\\) is irreducible error (noise)\nOur goal is to estimate \\(\\hat{f}\\) that minimizes prediction error:\n\\[\\text{Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}\\]"
  },
  {
    "objectID": "08-tidymodels-intro.html#the-bias-variance-tradeoff",
    "href": "08-tidymodels-intro.html#the-bias-variance-tradeoff",
    "title": "Chapter 8: Introduction to Tidymodels - Theory and Practice",
    "section": "The Bias-Variance Tradeoff",
    "text": "The Bias-Variance Tradeoff\n\nUnderstanding Bias and Variance\n\n# Demonstrate bias-variance tradeoff\nn &lt;- 100\nx &lt;- seq(0, 10, length.out = n)\n\n# True function\ntrue_function &lt;- function(x) sin(x) + 0.5 * x\n\n# Generate data with noise\ny &lt;- true_function(x) + rnorm(n, sd = 0.5)\ndata_sim &lt;- tibble(x = x, y = y, y_true = true_function(x))\n\n# Fit models of different complexity\nmodels &lt;- list(\n  \"Underfit (High Bias)\" = lm(y ~ x, data = data_sim),\n  \"Good Fit\" = lm(y ~ poly(x, 3), data = data_sim),\n  \"Overfit (High Variance)\" = lm(y ~ poly(x, 15), data = data_sim)\n)\n\n# Generate predictions\npredictions &lt;- map_df(names(models), function(model_name) {\n  model &lt;- models[[model_name]]\n  tibble(\n    x = x,\n    y_true = true_function(x),\n    y_pred = predict(model),\n    model = model_name\n  )\n})\n\n# Visualize\nggplot() +\n  geom_point(data = data_sim, aes(x = x, y = y), alpha = 0.3) +\n  geom_line(data = data_sim, aes(x = x, y = y_true), \n            color = \"black\", linewidth = 1.5, linetype = \"dashed\") +\n  geom_line(data = predictions, aes(x = x, y = y_pred, color = model), \n            linewidth = 1.2) +\n  facet_wrap(~model, ncol = 3) +\n  scale_color_manual(values = c(\"red\", \"green\", \"blue\")) +\n  labs(\n    title = \"Bias-Variance Tradeoff Demonstration\",\n    subtitle = \"Black dashed line = true function, Points = observed data\",\n    x = \"X\",\n    y = \"Y\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nKey Insights:\n\nBias: Error from overly simplistic assumptions\n\nHigh bias = Underfitting\nModel misses relevant patterns\n\nVariance: Error from sensitivity to small fluctuations\n\nHigh variance = Overfitting\n\nModel learns noise as patterns\n\nGoal: Find the sweet spot balancing bias and variance"
  },
  {
    "objectID": "08-tidymodels-intro.html#overfitting-and-underfitting",
    "href": "08-tidymodels-intro.html#overfitting-and-underfitting",
    "title": "Chapter 8: Introduction to Tidymodels - Theory and Practice",
    "section": "Overfitting and Underfitting",
    "text": "Overfitting and Underfitting\n\nDetecting Overfitting\n\n# Create training and test sets\nset.seed(123)\ntrain_indices &lt;- sample(1:n, size = 0.7 * n)\ntrain_data &lt;- data_sim[train_indices, ]\ntest_data &lt;- data_sim[-train_indices, ]\n\n# Fit models of increasing complexity\ncomplexity_range &lt;- 1:12\ntrain_errors &lt;- numeric(length(complexity_range))\ntest_errors &lt;- numeric(length(complexity_range))\n\nfor (i in complexity_range) {\n  model &lt;- lm(y ~ poly(x, i), data = train_data)\n  \n  train_pred &lt;- predict(model, train_data)\n  test_pred &lt;- predict(model, test_data)\n  \n  train_errors[i] &lt;- mean((train_data$y - train_pred)^2)\n  test_errors[i] &lt;- mean((test_data$y - test_pred)^2)\n}\n\n# Plot training vs test error\nerror_data &lt;- tibble(\n  complexity = rep(complexity_range, 2),\n  error = c(train_errors, test_errors),\n  type = rep(c(\"Training\", \"Test\"), each = length(complexity_range))\n)\n\nggplot(error_data, aes(x = complexity, y = error, color = type)) +\n  geom_line(linewidth = 1.2) +\n  geom_point(size = 3) +\n  scale_color_manual(values = c(\"Training\" = \"blue\", \"Test\" = \"red\")) +\n  labs(\n    title = \"Training vs Test Error: Detecting Overfitting\",\n    subtitle = \"Test error increases while training error decreases = Overfitting\",\n    x = \"Model Complexity (Polynomial Degree)\",\n    y = \"Mean Squared Error\",\n    color = \"Dataset\"\n  ) +\n  geom_vline(xintercept = 3, linetype = \"dashed\", alpha = 0.5) +\n  annotate(\"text\", x = 3, y = max(test_errors) * 0.9, \n           label = \"Optimal\\nComplexity\", hjust = -0.1)"
  },
  {
    "objectID": "08-tidymodels-intro.html#introduction-to-tidymodels",
    "href": "08-tidymodels-intro.html#introduction-to-tidymodels",
    "title": "Chapter 8: Introduction to Tidymodels - Theory and Practice",
    "section": "Introduction to Tidymodels",
    "text": "Introduction to Tidymodels\n\nThe Tidymodels Ecosystem\nTidymodels is a collection of packages for modeling and machine learning using tidyverse principles:\n\n# Core tidymodels packages\ntidymodels_packages &lt;- c(\n  \"rsample\",     # Data splitting and resampling\n  \"parsnip\",     # Model specification\n  \"recipes\",     # Feature engineering\n  \"workflows\",   # Workflow management\n  \"tune\",        # Hyperparameter tuning\n  \"yardstick\",   # Model evaluation metrics\n  \"broom\",       # Tidy model outputs\n  \"dials\"        # Parameter tuning dials\n)\n\n# Display package info\ntibble(\n  Package = tidymodels_packages,\n  Purpose = c(\n    \"Data splitting, cross-validation, bootstrapping\",\n    \"Unified interface for model specification\",\n    \"Feature engineering and preprocessing\",\n    \"Combine preprocessing and modeling\",\n    \"Hyperparameter optimization\",\n    \"Performance metrics and evaluation\",\n    \"Convert model outputs to tidy format\",\n    \"Tools for creating tuning parameter sets\"\n  )\n) %&gt;%\n  knitr::kable()\n\n\n\n\nPackage\nPurpose\n\n\n\n\nrsample\nData splitting, cross-validation, bootstrapping\n\n\nparsnip\nUnified interface for model specification\n\n\nrecipes\nFeature engineering and preprocessing\n\n\nworkflows\nCombine preprocessing and modeling\n\n\ntune\nHyperparameter optimization\n\n\nyardstick\nPerformance metrics and evaluation\n\n\nbroom\nConvert model outputs to tidy format\n\n\ndials\nTools for creating tuning parameter sets\n\n\n\n\n\n\n\nTidymodels Philosophy\n\nConsistency: Same interface across different models\nComposability: Modular components that work together\nReproducibility: Clear, documented workflows\nBest Practices: Built-in safeguards against common mistakes"
  },
  {
    "objectID": "08-tidymodels-intro.html#a-complete-tidymodels-workflow",
    "href": "08-tidymodels-intro.html#a-complete-tidymodels-workflow",
    "title": "Chapter 8: Introduction to Tidymodels - Theory and Practice",
    "section": "A Complete Tidymodels Workflow",
    "text": "A Complete Tidymodels Workflow\nLet’s build a complete machine learning workflow to predict penguin species:\n\n1. Data Exploration\n\n# Load and explore data\npenguins_clean &lt;- penguins %&gt;%\n  drop_na()\n\n# Basic exploration\nglimpse(penguins_clean)\n\nRows: 333\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel~\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse~\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.6~\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.2~\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 18~\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3800~\n$ sex               &lt;fct&gt; male, female, female, female, male, female, male, fe~\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007~\n\n# Class distribution\npenguins_clean %&gt;%\n  count(species) %&gt;%\n  mutate(prop = n / sum(n))\n\n# A tibble: 3 x 3\n  species       n  prop\n  &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt;\n1 Adelie      146 0.438\n2 Chinstrap    68 0.204\n3 Gentoo      119 0.357\n\n# Correlation matrix\npenguins_clean %&gt;%\n  select(where(is.numeric)) %&gt;%\n  cor() %&gt;%\n  corrplot(method = \"color\", type = \"upper\", \n           order = \"hclust\", tl.cex = 0.8,\n           addCoef.col = \"black\", number.cex = 0.7)\n\n\n\n\n\n\n\n# Feature relationships\npenguins_clean %&gt;%\n  select(species, bill_length_mm, bill_depth_mm, \n         flipper_length_mm, body_mass_g) %&gt;%\n  pivot_longer(cols = -species, names_to = \"measurement\", values_to = \"value\") %&gt;%\n  ggplot(aes(x = value, fill = species)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(~measurement, scales = \"free\") +\n  scale_fill_viridis_d() +\n  labs(title = \"Feature Distributions by Species\")\n\n\n\n\n\n\n\n\n\n\n2. Data Splitting\n\n# Initial split (training vs testing)\nset.seed(123)\npenguin_split &lt;- initial_split(penguins_clean, prop = 0.75, strata = species)\n\npenguin_train &lt;- training(penguin_split)\npenguin_test &lt;- testing(penguin_split)\n\n# Check split proportions\ntibble(\n  Dataset = c(\"Training\", \"Testing\"),\n  N = c(nrow(penguin_train), nrow(penguin_test)),\n  Proportion = c(nrow(penguin_train), nrow(penguin_test)) / nrow(penguins_clean)\n) %&gt;%\n  knitr::kable()\n\n\n\n\nDataset\nN\nProportion\n\n\n\n\nTraining\n249\n0.7477477\n\n\nTesting\n84\n0.2522523\n\n\n\n\n# Create cross-validation folds\npenguin_folds &lt;- vfold_cv(penguin_train, v = 5, strata = species)\npenguin_folds\n\n#  5-fold cross-validation using stratification \n# A tibble: 5 x 2\n  splits           id   \n  &lt;list&gt;           &lt;chr&gt;\n1 &lt;split [198/51]&gt; Fold1\n2 &lt;split [199/50]&gt; Fold2\n3 &lt;split [199/50]&gt; Fold3\n4 &lt;split [199/50]&gt; Fold4\n5 &lt;split [201/48]&gt; Fold5\n\n\n\n\n3. Feature Engineering with Recipes\n\n# Create a recipe\npenguin_recipe &lt;- recipe(species ~ ., data = penguin_train) %&gt;%\n  # Remove unnecessary variables\n  step_rm(year) %&gt;%\n  # Convert factors to dummy variables\n  step_dummy(all_nominal_predictors()) %&gt;%\n  # Normalize numeric predictors\n  step_normalize(all_numeric_predictors()) %&gt;%\n  # Remove zero variance predictors\n  step_zv(all_predictors())\n\n# View the recipe\npenguin_recipe\n\n# Prepare and bake to see transformed data\npenguin_prep &lt;- prep(penguin_recipe)\nbake(penguin_prep, new_data = penguin_train %&gt;% head())\n\n# A tibble: 6 x 8\n  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g species\n           &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1         -0.683         0.393            -0.437      -1.21  Adelie \n2         -1.35          1.05             -0.578      -0.963 Adelie \n3         -0.942         0.293            -1.42       -0.751 Adelie \n4         -0.886         1.20             -0.437       0.520 Adelie \n5         -0.535         0.192            -1.35       -1.27  Adelie \n6         -0.997         2.00             -0.719      -0.539 Adelie \n# i 3 more variables: island_Dream &lt;dbl&gt;, island_Torgersen &lt;dbl&gt;,\n#   sex_male &lt;dbl&gt;\n\n\n\n\n4. Model Specification\n\n# Specify different models\n\n# Multinomial regression (for multiclass classification)\nmultinom_spec &lt;- multinom_reg() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"classification\")\n\n# Random forest\nrf_spec &lt;- rand_forest(\n  trees = 100,\n  min_n = 5\n) %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"classification\")\n\n# Support vector machine\nsvm_spec &lt;- svm_rbf(\n  cost = 1,\n  rbf_sigma = 0.01\n) %&gt;%\n  set_engine(\"kernlab\") %&gt;%\n  set_mode(\"classification\")\n\nprint(\"Model specifications created\")\n\n[1] \"Model specifications created\"\n\n\n\n\n5. Creating Workflows\n\n# Combine recipe and model into workflows\nmultinom_workflow &lt;- workflow() %&gt;%\n  add_recipe(penguin_recipe) %&gt;%\n  add_model(multinom_spec)\n\nrf_workflow &lt;- workflow() %&gt;%\n  add_recipe(penguin_recipe) %&gt;%\n  add_model(rf_spec)\n\nsvm_workflow &lt;- workflow() %&gt;%\n  add_recipe(penguin_recipe) %&gt;%\n  add_model(svm_spec)\n\nmultinom_workflow\n\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: multinom_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n4 Recipe Steps\n\n* step_rm()\n* step_dummy()\n* step_normalize()\n* step_zv()\n\n-- Model -----------------------------------------------------------------------\nMultinomial Regression Model Specification (classification)\n\nComputational engine: nnet \n\n\n\n\n6. Model Training and Evaluation\n\n# Fit models using cross-validation\n# For multiclass problems, we'll use accuracy and multiclass AUC\nmultinom_cv &lt;- fit_resamples(\n  multinom_workflow,\n  resamples = penguin_folds,\n  metrics = metric_set(accuracy, roc_auc),\n  control = control_resamples(save_pred = TRUE)\n)\n\nrf_cv &lt;- fit_resamples(\n  rf_workflow,\n  resamples = penguin_folds,\n  metrics = metric_set(accuracy, roc_auc),\n  control = control_resamples(save_pred = TRUE)\n)\n\n# Compare models\nmodel_comparison &lt;- bind_rows(\n  collect_metrics(multinom_cv) %&gt;% mutate(model = \"Multinomial Regression\"),\n  collect_metrics(rf_cv) %&gt;% mutate(model = \"Random Forest\")\n)\n\n# Visualize comparison\nggplot(model_comparison, aes(x = model, y = mean, fill = model)) +\n  geom_col() +\n  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), width = 0.2) +\n  facet_wrap(~.metric, scales = \"free_y\") +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"Model Performance Comparison\",\n    subtitle = \"5-fold cross-validation results\",\n    y = \"Score\"\n  ) +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n7. Final Model Training\n\n# Train final model on full training set\nfinal_model &lt;- rf_workflow %&gt;%\n  fit(penguin_train)\n\n# Make predictions on test set\npredictions &lt;- final_model %&gt;%\n  predict(penguin_test) %&gt;%\n  bind_cols(penguin_test %&gt;% select(species))\n\n# Confusion matrix\nconf_mat &lt;- predictions %&gt;%\n  conf_mat(truth = species, estimate = .pred_class)\n\nconf_mat\n\n           Truth\nPrediction  Adelie Chinstrap Gentoo\n  Adelie        37         0      0\n  Chinstrap      0        17      0\n  Gentoo         0         0     30\n\n# Visualize confusion matrix\nautoplot(conf_mat, type = \"heatmap\") +\n  scale_fill_gradient(low = \"white\", high = \"darkblue\") +\n  labs(title = \"Confusion Matrix - Random Forest\")\n\n\n\n\n\n\n\n\n\n\n8. Model Interpretation\n\n# Feature importance\nfinal_rf &lt;- final_model %&gt;%\n  extract_fit_parsnip()\n\n# Variable importance plot\nif (require(vip, quietly = TRUE)) {\n  vip(final_rf, num_features = 10) +\n    labs(title = \"Feature Importance - Random Forest\")\n}\n\n\n\n\n\n\n\n# Prediction probabilities\nprob_predictions &lt;- final_model %&gt;%\n  predict(penguin_test, type = \"prob\") %&gt;%\n  bind_cols(penguin_test %&gt;% select(species))\n\n# ROC curves for multiclass\nif (require(yardstick, quietly = TRUE)) {\n  roc_data &lt;- prob_predictions %&gt;%\n    roc_curve(truth = species, .pred_Adelie:.pred_Gentoo)\n  \n  autoplot(roc_data) +\n    labs(\n      title = \"ROC Curves by Species\",\n      subtitle = \"One-vs-All approach\"\n    )\n}"
  },
  {
    "objectID": "08-tidymodels-intro.html#cross-validation-theory",
    "href": "08-tidymodels-intro.html#cross-validation-theory",
    "title": "Chapter 8: Introduction to Tidymodels - Theory and Practice",
    "section": "Cross-Validation Theory",
    "text": "Cross-Validation Theory\n\nWhy Cross-Validation?\nCross-validation helps us: 1. Estimate model performance on unseen data 2. Detect overfitting 3. Compare different models fairly 4. Make better use of limited data\n\n\nTypes of Cross-Validation\n\n# Demonstrate different CV strategies\nset.seed(123)\nsample_data &lt;- tibble(\n  id = 1:100,\n  x = rnorm(100),\n  y = 2 * x + rnorm(100, sd = 0.5),\n  group = rep(1:10, each = 10),\n  time = rep(1:10, 10)\n)\n\n# Different CV strategies\ncv_strategies &lt;- list(\n  \"5-Fold CV\" = vfold_cv(sample_data, v = 5),\n  \"10-Fold CV\" = vfold_cv(sample_data, v = 10),\n  \"Leave-One-Out CV\" = loo_cv(sample_data),\n  \"Bootstrap\" = bootstraps(sample_data, times = 5),\n  \"Group CV\" = group_vfold_cv(sample_data, group = group, v = 5)\n)\n\n# Visualize fold assignments\nfold_viz &lt;- vfold_cv(sample_data, v = 5) %&gt;%\n  mutate(fold_data = map(splits, analysis)) %&gt;%\n  unnest(fold_data, names_sep = \"_\") %&gt;%\n  select(obs_id = fold_data_id, Fold = id) %&gt;%\n  distinct()\n\nggplot(fold_viz, aes(x = obs_id, y = 1, fill = Fold)) +\n  geom_tile(height = 0.8) +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"5-Fold Cross-Validation: Data Assignment\",\n    subtitle = \"Each observation appears in exactly one test fold\",\n    x = \"Observation ID\",\n    y = \"\"\n  ) +\n  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\n\n\nMathematical Foundation of CV\nFor k-fold cross-validation, the CV estimate of prediction error is:\n\\[CV_{(k)} = \\frac{1}{k} \\sum_{i=1}^{k} MSE_i\\]\nWhere \\(MSE_i\\) is the mean squared error on fold \\(i\\)."
  },
  {
    "objectID": "08-tidymodels-intro.html#model-selection-theory",
    "href": "08-tidymodels-intro.html#model-selection-theory",
    "title": "Chapter 8: Introduction to Tidymodels - Theory and Practice",
    "section": "Model Selection Theory",
    "text": "Model Selection Theory\n\nInformation Criteria\n\n# Demonstrate AIC/BIC for model selection\nmodels_to_compare &lt;- list(\n  \"Simple\" = lm(body_mass_g ~ bill_length_mm, data = penguin_train),\n  \"Moderate\" = lm(body_mass_g ~ bill_length_mm + flipper_length_mm, data = penguin_train),\n  \"Complex\" = lm(body_mass_g ~ bill_length_mm + flipper_length_mm + \n                 bill_depth_mm + island + sex, data = penguin_train),\n  \"Very Complex\" = lm(body_mass_g ~ .^2, data = penguin_train)  # All interactions\n)\n\nmodel_selection &lt;- map_df(names(models_to_compare), function(name) {\n  model &lt;- models_to_compare[[name]]\n  tibble(\n    Model = name,\n    Parameters = length(coef(model)),\n    AIC = AIC(model),\n    BIC = BIC(model),\n    `Adj R²` = summary(model)$adj.r.squared\n  )\n})\n\nmodel_selection %&gt;%\n  arrange(AIC) %&gt;%\n  knitr::kable(digits = 2)\n\n\n\n\nModel\nParameters\nAIC\nBIC\nAdj R²\n\n\n\n\nVery Complex\n44\n3566.11\n3710.33\n0.88\n\n\nComplex\n7\n3607.11\n3635.25\n0.84\n\n\nModerate\n3\n3699.53\n3713.59\n0.76\n\n\nSimple\n2\n3948.30\n3958.85\n0.35\n\n\n\n\n\n\n\nRegularization Theory\nRegularization adds a penalty term to prevent overfitting:\nRidge Regression (L2): \\[\\min_{\\beta} \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij})^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2\\]\nLasso Regression (L1): \\[\\min_{\\beta} \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij})^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\\]\n\n# Demonstrate regularization\nlibrary(glmnet)\n\n# Prepare data\nX &lt;- model.matrix(body_mass_g ~ . - 1, data = penguin_train %&gt;% select(-species, -year))\ny &lt;- penguin_train$body_mass_g\n\n# Fit ridge and lasso\nridge_fit &lt;- glmnet(X, y, alpha = 0)  # Ridge\nlasso_fit &lt;- glmnet(X, y, alpha = 1)  # Lasso\n\n# Plot coefficient paths\npar(mfrow = c(1, 2))\nplot(ridge_fit, xvar = \"lambda\", main = \"Ridge Regression\")\nplot(lasso_fit, xvar = \"lambda\", main = \"Lasso Regression\")"
  },
  {
    "objectID": "08-tidymodels-intro.html#best-practices-in-machine-learning",
    "href": "08-tidymodels-intro.html#best-practices-in-machine-learning",
    "title": "Chapter 8: Introduction to Tidymodels - Theory and Practice",
    "section": "Best Practices in Machine Learning",
    "text": "Best Practices in Machine Learning\n\n1. Data Leakage Prevention\n\n# WRONG: Preprocessing before splitting\n# This leaks information from test set into training\nwrong_way &lt;- penguins_clean %&gt;%\n  mutate(bill_length_scaled = scale(bill_length_mm)[,1])  # Uses all data!\n\n# RIGHT: Preprocessing within training set only\nright_way &lt;- recipe(species ~ ., data = penguin_train) %&gt;%\n  step_normalize(all_numeric_predictors())  # Only uses training data\n\n\n\n2. Proper Evaluation\nAlways use: - Separate test set (never touched during development) - Cross-validation for model selection - Appropriate metrics for your problem\n\n\n3. Feature Engineering Guidelines\n\nDomain knowledge is crucial\nStart simple, add complexity gradually\nValidate feature importance\nWatch for multicollinearity"
  },
  {
    "objectID": "08-tidymodels-intro.html#exercises",
    "href": "08-tidymodels-intro.html#exercises",
    "title": "Chapter 8: Introduction to Tidymodels - Theory and Practice",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Implement Cross-Validation\nCompare different CV strategies on the penguins dataset:\n\n# Your solution\ncv_comparison &lt;- tibble(\n  strategy = c(\"5-Fold\", \"10-Fold\", \"Bootstrap\", \"Monte Carlo\"),\n  cv_object = list(\n    vfold_cv(penguin_train, v = 5),\n    vfold_cv(penguin_train, v = 10),\n    bootstraps(penguin_train, times = 25),\n    mc_cv(penguin_train, prop = 0.75, times = 25)\n  )\n)\n\n# Fit a simple model with each CV strategy\nsimple_spec &lt;- multinom_reg() %&gt;%\n  set_engine(\"nnet\")\n\nsimple_recipe &lt;- recipe(species ~ bill_length_mm + bill_depth_mm, \n                       data = penguin_train) %&gt;%\n  step_normalize(all_predictors())\n\nsimple_workflow &lt;- workflow() %&gt;%\n  add_recipe(simple_recipe) %&gt;%\n  add_model(simple_spec)\n\n# Compare results\ncv_results &lt;- cv_comparison %&gt;%\n  mutate(\n    fits = map(cv_object, ~ fit_resamples(simple_workflow, resamples = .)),\n    metrics = map(fits, collect_metrics)\n  ) %&gt;%\n  unnest(metrics) %&gt;%\n  filter(.metric == \"accuracy\") %&gt;%\n  select(strategy, mean, std_err)\n\ncv_results %&gt;%\n  ggplot(aes(x = strategy, y = mean)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), width = 0.2) +\n  labs(title = \"Cross-Validation Strategy Comparison\",\n       y = \"Accuracy\")\n\n\n\n\n\n\n\n\n\n\nExercise 2: Bias-Variance Analysis\nCreate models with different complexity levels and analyze their bias-variance tradeoff:\n\n# Your solution\n# Create polynomial features of different degrees\ncomplexity_levels &lt;- 1:5\n\nmodel_fits &lt;- map(complexity_levels, function(degree) {\n  recipe_poly &lt;- recipe(body_mass_g ~ flipper_length_mm, data = penguin_train) %&gt;%\n    step_poly(flipper_length_mm, degree = degree)\n  \n  lm_spec &lt;- linear_reg() %&gt;%\n    set_engine(\"lm\")\n  \n  workflow() %&gt;%\n    add_recipe(recipe_poly) %&gt;%\n    add_model(lm_spec) %&gt;%\n    fit(penguin_train)\n})\n\n# Evaluate on training and test sets\nevaluation &lt;- map_df(1:length(model_fits), function(i) {\n  model &lt;- model_fits[[i]]\n  \n  train_pred &lt;- predict(model, penguin_train)\n  test_pred &lt;- predict(model, penguin_test)\n  \n  tibble(\n    complexity = complexity_levels[i],\n    train_rmse = rmse_vec(penguin_train$body_mass_g, train_pred$.pred),\n    test_rmse = rmse_vec(penguin_test$body_mass_g, test_pred$.pred)\n  )\n})\n\nevaluation %&gt;%\n  pivot_longer(cols = c(train_rmse, test_rmse), \n               names_to = \"dataset\", values_to = \"rmse\") %&gt;%\n  ggplot(aes(x = complexity, y = rmse, color = dataset)) +\n  geom_line(linewidth = 1.2) +\n  geom_point(size = 3) +\n  labs(title = \"Model Complexity vs Error\",\n       x = \"Polynomial Degree\",\n       y = \"RMSE\") +\n  scale_color_manual(values = c(\"train_rmse\" = \"blue\", \"test_rmse\" = \"red\"))\n\n\n\n\n\n\n\n\n\n\nExercise 3: Build a Complete Pipeline\nCreate a complete tidymodels pipeline for a regression problem:\n\n# Your solution\n# Predict penguin body mass\nmass_split &lt;- initial_split(penguins_clean, prop = 0.8)\nmass_train &lt;- training(mass_split)\nmass_test &lt;- testing(mass_split)\n\n# Create recipe with feature engineering\nmass_recipe &lt;- recipe(body_mass_g ~ ., data = mass_train) %&gt;%\n  step_rm(year) %&gt;%\n  step_impute_median(all_numeric_predictors()) %&gt;%\n  step_impute_mode(all_nominal_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_interact(terms = ~ bill_length_mm:bill_depth_mm)\n\n# Specify model\nrf_reg_spec &lt;- rand_forest(\n  trees = 200,\n  min_n = 10\n) %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\n# Create workflow\nmass_workflow &lt;- workflow() %&gt;%\n  add_recipe(mass_recipe) %&gt;%\n  add_model(rf_reg_spec)\n\n# Fit and evaluate\nmass_fit &lt;- mass_workflow %&gt;%\n  fit(mass_train)\n\n# Predictions\nmass_predictions &lt;- mass_fit %&gt;%\n  predict(mass_test) %&gt;%\n  bind_cols(mass_test)\n\n# Evaluate\nmetrics &lt;- mass_predictions %&gt;%\n  metrics(truth = body_mass_g, estimate = .pred)\n\nprint(metrics)\n\n# A tibble: 3 x 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     309.   \n2 rsq     standard       0.843\n3 mae     standard     249.   \n\n# Visualize predictions\nggplot(mass_predictions, aes(x = body_mass_g, y = .pred)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Predicted vs Actual Body Mass\",\n       x = \"Actual Mass (g)\",\n       y = \"Predicted Mass (g)\")"
  },
  {
    "objectID": "08-tidymodels-intro.html#summary",
    "href": "08-tidymodels-intro.html#summary",
    "title": "Chapter 8: Introduction to Tidymodels - Theory and Practice",
    "section": "Summary",
    "text": "Summary\nYou’ve learned the theoretical foundations and practical implementation of:\n✅ Machine learning fundamentals and theory\n✅ Bias-variance tradeoff\n✅ Overfitting and underfitting concepts\n✅ Cross-validation theory and practice\n✅ The tidymodels ecosystem structure\n✅ Complete ML workflow implementation\n✅ Model selection and evaluation\n✅ Best practices in machine learning"
  },
  {
    "objectID": "08-tidymodels-intro.html#whats-next",
    "href": "08-tidymodels-intro.html#whats-next",
    "title": "Chapter 8: Introduction to Tidymodels - Theory and Practice",
    "section": "What’s Next?",
    "text": "What’s Next?\nIn Chapter 9, we’ll dive deep into data splitting strategies and resampling techniques with rsample."
  },
  {
    "objectID": "08-tidymodels-intro.html#additional-resources",
    "href": "08-tidymodels-intro.html#additional-resources",
    "title": "Chapter 8: Introduction to Tidymodels - Theory and Practice",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nTidymodels Documentation\nAn Introduction to Statistical Learning\nThe Elements of Statistical Learning\nTidy Modeling with R\nFeature Engineering and Selection"
  },
  {
    "objectID": "10-feature-engineering.html",
    "href": "10-feature-engineering.html",
    "title": "Chapter 10: Feature Engineering with recipes - The Art and Science of Data Preparation",
    "section": "",
    "text": "By the end of this chapter, you will master:\n\nThe philosophy and importance of feature engineering\nCreating and applying recipes in tidymodels\nNumeric transformations and scaling\nHandling categorical variables\nCreating interaction terms and polynomial features\nDealing with missing data systematically\nFeature selection and dimensionality reduction\nTime-based and text features\nBest practices and common pitfalls"
  },
  {
    "objectID": "10-feature-engineering.html#learning-objectives",
    "href": "10-feature-engineering.html#learning-objectives",
    "title": "Chapter 10: Feature Engineering with recipes - The Art and Science of Data Preparation",
    "section": "",
    "text": "By the end of this chapter, you will master:\n\nThe philosophy and importance of feature engineering\nCreating and applying recipes in tidymodels\nNumeric transformations and scaling\nHandling categorical variables\nCreating interaction terms and polynomial features\nDealing with missing data systematically\nFeature selection and dimensionality reduction\nTime-based and text features\nBest practices and common pitfalls"
  },
  {
    "objectID": "10-feature-engineering.html#what-is-feature-engineering",
    "href": "10-feature-engineering.html#what-is-feature-engineering",
    "title": "Chapter 10: Feature Engineering with recipes - The Art and Science of Data Preparation",
    "section": "What is Feature Engineering?",
    "text": "What is Feature Engineering?\nFeature engineering is the process of transforming raw data into features that better represent the underlying problem to predictive models. It’s often said that “data and features determine the upper limit of machine learning, while models and algorithms only approach this limit.”\n\nWhy Feature Engineering Matters\nThink of feature engineering as translating your data into a language that your model can better understand. Even the most sophisticated algorithm will struggle with poorly prepared data, while a simple model can perform remarkably well with thoughtfully engineered features.\nConsider these scenarios: - Raw timestamps → Extract hour of day, day of week, is_weekend, season - Text addresses → Extract zip code, city, distance from city center - Numerical ratios → Price per square foot instead of just price and area - Domain knowledge → Age of house at sale instead of just year built and sale year\nLet’s see this in action:\n\nlibrary(tidymodels)\n\n-- Attaching packages -------------------------------------- tidymodels 1.4.1 --\n\n\nv broom        1.0.10     v recipes      1.3.1 \nv dials        1.4.2      v rsample      1.3.1 \nv dplyr        1.1.4      v tailor       0.1.0 \nv ggplot2      4.0.0      v tidyr        1.3.1 \nv infer        1.0.9      v tune         2.0.0 \nv modeldata    1.5.1      v workflows    1.3.0 \nv parsnip      1.3.3      v workflowsets 1.1.1 \nv purrr        1.1.0      v yardstick    1.3.2 \n\n\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx purrr::discard() masks scales::discard()\nx dplyr::filter()  masks stats::filter()\nx dplyr::lag()     masks stats::lag()\nx recipes::step()  masks stats::step()\n\nlibrary(tidyverse)\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv forcats   1.0.0     v stringr   1.5.2\nv lubridate 1.9.4     v tibble    3.3.0\nv readr     2.1.5     \n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx readr::col_factor() masks scales::col_factor()\nx purrr::discard()    masks scales::discard()\nx dplyr::filter()     masks stats::filter()\nx stringr::fixed()    masks recipes::fixed()\nx dplyr::lag()        masks stats::lag()\nx readr::spec()       masks yardstick::spec()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(textrecipes)\nlibrary(themis)\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\n# Set theme and seed\ntheme_set(theme_minimal())\nset.seed(123)\n\n# Load example datasets\ndata(ames)\ndata(credit_data)\n\n# Create a simple example to show feature engineering impact\nsimple_data &lt;- tibble(\n  sale_date = seq(as.Date(\"2020-01-01\"), as.Date(\"2022-12-31\"), by = \"day\"),\n  temperature = 50 + 30 * sin(2 * pi * as.numeric(sale_date) / 365) + rnorm(length(sale_date), 0, 5),\n  sales = 1000 + 200 * sin(2 * pi * as.numeric(sale_date) / 365) + \n          100 * (wday(sale_date) %in% c(1, 7)) +  # Weekend boost\n          rnorm(length(sale_date), 0, 50)\n)\n\n# Without feature engineering - just using date as numeric\nbad_model &lt;- lm(sales ~ as.numeric(sale_date), data = simple_data)\n\n# With feature engineering\ngood_data &lt;- simple_data %&gt;%\n  mutate(\n    month = month(sale_date),\n    day_of_week = wday(sale_date, label = TRUE),\n    is_weekend = wday(sale_date) %in% c(1, 7),\n    quarter = quarter(sale_date),\n    days_since_start = as.numeric(sale_date - min(sale_date))\n  )\n\ngood_model &lt;- lm(sales ~ month + is_weekend + temperature + days_since_start, \n                 data = good_data)\n\n# Compare R-squared\ntibble(\n  Model = c(\"Without Feature Engineering\", \"With Feature Engineering\"),\n  `R-squared` = c(summary(bad_model)$r.squared, summary(good_model)$r.squared)\n) %&gt;%\n  knitr::kable(digits = 3)\n\n\n\n\nModel\nR-squared\n\n\n\n\nWithout Feature Engineering\n0.055\n\n\nWith Feature Engineering\n0.869\n\n\n\n\n\nNotice the dramatic improvement! The model with engineered features captures patterns that the raw date couldn’t represent."
  },
  {
    "objectID": "10-feature-engineering.html#the-recipes-package-philosophy",
    "href": "10-feature-engineering.html#the-recipes-package-philosophy",
    "title": "Chapter 10: Feature Engineering with recipes - The Art and Science of Data Preparation",
    "section": "The recipes Package Philosophy",
    "text": "The recipes Package Philosophy\nThe recipes package provides a domain-specific language for feature engineering. Think of it like writing a recipe for a meal:\n\nIngredients (raw data): What you start with\nInstructions (steps): How to transform the ingredients\nPreparation (prep): Getting everything ready with your training data\nBaking (bake): Applying the recipe to new data\n\nThis approach ensures: - Reproducibility: The same transformations applied consistently - Modularity: Easy to add, remove, or modify steps - Prevention of data leakage: Transformations learned only from training data"
  },
  {
    "objectID": "10-feature-engineering.html#creating-your-first-recipe",
    "href": "10-feature-engineering.html#creating-your-first-recipe",
    "title": "Chapter 10: Feature Engineering with recipes - The Art and Science of Data Preparation",
    "section": "Creating Your First Recipe",
    "text": "Creating Your First Recipe\nLet’s start with a basic recipe and build complexity gradually:\n\n# Prepare the Ames housing data\names_train &lt;- ames %&gt;%\n  filter(Sale_Price &gt; 0) %&gt;%\n  sample_frac(0.8)\n\names_test &lt;- ames %&gt;%\n  filter(Sale_Price &gt; 0) %&gt;%\n  anti_join(ames_train)\n\n# Create a basic recipe\nbasic_recipe &lt;- recipe(Sale_Price ~ Lot_Area + Year_Built + Overall_Cond, \n                       data = ames_train)\n\n# View the recipe\nbasic_recipe\n\nAt this point, the recipe is just a specification - it hasn’t done anything yet. It’s like having a recipe card but not having cooked the meal.\n\nAdding Steps to the Recipe\nNow let’s add transformation steps. Each step transforms the data in a specific way:\n\n# Enhanced recipe with multiple steps\nenhanced_recipe &lt;- recipe(Sale_Price ~ Lot_Area + Year_Built + Overall_Cond + \n                          Neighborhood + Gr_Liv_Area, \n                          data = ames_train) %&gt;%\n  # Step 1: Log transform the outcome\n  step_log(Sale_Price) %&gt;%\n  # Step 2: Create a new feature\n  step_mutate(House_Age = 2010 - Year_Built) %&gt;%\n  # Step 3: Remove the original Year_Built\n  step_rm(Year_Built) %&gt;%\n  # Step 4: Normalize numeric predictors\n  step_normalize(all_numeric_predictors()) %&gt;%\n  # Step 5: Create dummy variables for categorical predictors\n  step_dummy(all_nominal_predictors())\n\nenhanced_recipe\n\nEach step is performed in order, and the output of one step becomes the input to the next. This is crucial to understand - order matters!\n\n\nPreparing and Baking the Recipe\nNow we need to “prepare” the recipe using the training data, then “bake” it to apply the transformations:\n\n# Prepare the recipe (learn parameters from training data)\nprepped_recipe &lt;- prep(enhanced_recipe, training = ames_train)\n\n# See what was learned\nprepped_recipe\n\n# Apply to training data\nbaked_train &lt;- bake(prepped_recipe, new_data = NULL)  # NULL means use training data\nglimpse(baked_train)\n\nRows: 2,344\nColumns: 41\n$ Lot_Area                                             &lt;dbl&gt; -0.07145116, -0.1~\n$ Gr_Liv_Area                                          &lt;dbl&gt; -0.88264922, -0.3~\n$ Sale_Price                                           &lt;dbl&gt; 11.86358, 12.2496~\n$ House_Age                                            &lt;dbl&gt; 0.10653670, -1.19~\n$ Overall_Cond_Poor                                    &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Overall_Cond_Fair                                    &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Overall_Cond_Below_Average                           &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Overall_Cond_Average                                 &lt;dbl&gt; 1, 1, 1, 0, 1, 1,~\n$ Overall_Cond_Above_Average                           &lt;dbl&gt; 0, 0, 0, 1, 0, 0,~\n$ Overall_Cond_Good                                    &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Overall_Cond_Very_Good                               &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Overall_Cond_Excellent                               &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Overall_Cond_Very_Excellent                          &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_College_Creek                           &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Old_Town                                &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Edwards                                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Somerset                                &lt;dbl&gt; 0, 1, 1, 0, 0, 0,~\n$ Neighborhood_Northridge_Heights                      &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Gilbert                                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Sawyer                                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Northwest_Ames                          &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Sawyer_West                             &lt;dbl&gt; 0, 0, 0, 0, 1, 0,~\n$ Neighborhood_Mitchell                                &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Brookside                               &lt;dbl&gt; 0, 0, 0, 1, 0, 0,~\n$ Neighborhood_Crawford                                &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Iowa_DOT_and_Rail_Road                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Timberland                              &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Northridge                              &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Stone_Brook                             &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_South_and_West_of_Iowa_State_University &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Clear_Creek                             &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Meadow_Village                          &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Briardale                               &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Bloomington_Heights                     &lt;dbl&gt; 0, 0, 0, 0, 0, 1,~\n$ Neighborhood_Veenker                                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Northpark_Villa                         &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Blueste                                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Greens                                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Green_Hills                             &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Landmark                                &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Hayden_Lake                             &lt;dbl&gt; 0, 0, 0, 0, 0, 0,~\n\n# Apply to test data\nbaked_test &lt;- bake(prepped_recipe, new_data = ames_test)\n\n# Check that dimensions match (except for rows)\ntibble(\n  Dataset = c(\"Training\", \"Test\"),\n  Rows = c(nrow(baked_train), nrow(baked_test)),\n  Columns = c(ncol(baked_train), ncol(baked_test))\n) %&gt;%\n  knitr::kable()\n\n\n\n\nDataset\nRows\nColumns\n\n\n\n\nTraining\n2344\n41\n\n\nTest\n586\n41\n\n\n\n\n\nThe key insight: prep() learns any necessary parameters (like mean and SD for normalization) from the training data, and bake() applies these learned transformations to any dataset."
  },
  {
    "objectID": "10-feature-engineering.html#numeric-transformations",
    "href": "10-feature-engineering.html#numeric-transformations",
    "title": "Chapter 10: Feature Engineering with recipes - The Art and Science of Data Preparation",
    "section": "Numeric Transformations",
    "text": "Numeric Transformations\nNumeric features often need transformation to work well with models. Let’s explore the most important transformations:\n\nScaling and Normalization\nDifferent scaling methods serve different purposes:\n\n# Create example data with different scales\nscaling_demo &lt;- tibble(\n  feature_A = rnorm(1000, mean = 100, sd = 15),      # Normal, mean=100\n  feature_B = rexp(1000, rate = 0.01),               # Exponential, right-skewed\n  feature_C = runif(1000, min = 0, max = 1),         # Uniform, 0-1 range\n  feature_D = rlnorm(1000, meanlog = 10, sdlog = 2)  # Log-normal, very large\n)\n\n# Different scaling recipes\nscaling_recipes &lt;- list(\n  original = recipe(~ ., data = scaling_demo),\n  \n  normalized = recipe(~ ., data = scaling_demo) %&gt;%\n    step_normalize(all_predictors()),\n  \n  range = recipe(~ ., data = scaling_demo) %&gt;%\n    step_range(all_predictors(), min = 0, max = 1),\n  \n  robust = recipe(~ ., data = scaling_demo) %&gt;%\n    step_center(all_predictors()) %&gt;%  # Center using median\n    step_scale(all_predictors())       # Scale using standard deviation\n)\n\n# Apply each recipe\nscaled_data &lt;- map_df(names(scaling_recipes), function(name) {\n  scaling_recipes[[name]] %&gt;%\n    prep() %&gt;%\n    bake(new_data = NULL) %&gt;%\n    mutate(scaling_method = name) %&gt;%\n    pivot_longer(cols = -scaling_method, \n                 names_to = \"feature\", \n                 values_to = \"value\")\n})\n\n# Visualize the effects\nggplot(scaled_data, aes(x = value, fill = scaling_method)) +\n  geom_histogram(bins = 30, alpha = 0.6, position = \"identity\") +\n  facet_grid(scaling_method ~ feature, scales = \"free\") +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"Effects of Different Scaling Methods\",\n    subtitle = \"Each method has different properties and use cases\",\n    x = \"Scaled Value\",\n    y = \"Count\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nKey insights about scaling: - Normalization (z-score): Centers at 0, scales by standard deviation. Good for normally distributed features. - Range scaling: Forces values between min and max. Preserves shape but sensitive to outliers. - Robust scaling: Uses median and MAD, resistant to outliers.\n\n\nTransformations for Skewed Data\nMany real-world variables are skewed. Let’s handle them properly:\n\n# Create skewed data\nskewed_data &lt;- tibble(\n  mild_skew = rgamma(1000, shape = 2, rate = 0.5),\n  moderate_skew = rlnorm(1000, meanlog = 0, sdlog = 1),\n  severe_skew = rexp(1000, rate = 0.1),\n  outcome = rnorm(1000)\n)\n\n# Different transformation recipes\ntransform_recipes &lt;- list(\n  original = recipe(outcome ~ ., data = skewed_data),\n  \n  log = recipe(outcome ~ ., data = skewed_data) %&gt;%\n    step_log(all_predictors(), offset = 1),  # offset prevents log(0)\n  \n  sqrt = recipe(outcome ~ ., data = skewed_data) %&gt;%\n    step_sqrt(all_predictors()),\n  \n  yeo_johnson = recipe(outcome ~ ., data = skewed_data) %&gt;%\n    step_YeoJohnson(all_predictors()),  # Automatic optimal transformation\n  \n  box_cox = recipe(outcome ~ ., data = skewed_data) %&gt;%\n    step_BoxCox(all_predictors())  # Requires positive values\n)\n\n# Apply transformations and calculate skewness\nskewness_comparison &lt;- map_df(names(transform_recipes), function(name) {\n  transformed &lt;- transform_recipes[[name]] %&gt;%\n    prep() %&gt;%\n    bake(new_data = NULL)\n  \n  tibble(\n    method = name,\n    mild_skew = moments::skewness(transformed$mild_skew),\n    moderate_skew = moments::skewness(transformed$moderate_skew),\n    severe_skew = moments::skewness(transformed$severe_skew)\n  )\n})\n\n# Display results\nskewness_comparison %&gt;%\n  pivot_longer(cols = -method, names_to = \"feature\", values_to = \"skewness\") %&gt;%\n  ggplot(aes(x = method, y = abs(skewness), fill = method)) +\n  geom_col() +\n  facet_wrap(~feature) +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"Effectiveness of Different Transformations on Skewed Data\",\n    subtitle = \"Lower absolute skewness is better (closer to normal distribution)\",\n    x = \"Transformation Method\",\n    y = \"Absolute Skewness\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\nThe Yeo-Johnson transformation is particularly useful because it: - Automatically finds the optimal transformation parameter - Handles both positive and negative values - Often achieves near-normal distributions"
  },
  {
    "objectID": "10-feature-engineering.html#handling-categorical-variables",
    "href": "10-feature-engineering.html#handling-categorical-variables",
    "title": "Chapter 10: Feature Engineering with recipes - The Art and Science of Data Preparation",
    "section": "Handling Categorical Variables",
    "text": "Handling Categorical Variables\nCategorical variables require special treatment. The approach depends on the model type and the nature of the categories.\n\nDummy Variables (One-Hot Encoding)\nThis is the most common approach for linear models:\n\n# Example with different types of categorical variables\ncat_data &lt;- tibble(\n  color = factor(c(\"red\", \"blue\", \"green\", \"red\", \"blue\")),\n  size = factor(c(\"S\", \"M\", \"L\", \"XL\", \"M\"), \n                levels = c(\"S\", \"M\", \"L\", \"XL\"), ordered = TRUE),\n  quality = factor(c(\"good\", \"bad\", \"excellent\", \"good\", \"bad\")),\n  outcome = c(10, 15, 20, 12, 14)\n)\n\n# Basic dummy encoding\ndummy_recipe &lt;- recipe(outcome ~ ., data = cat_data) %&gt;%\n  step_dummy(all_nominal_predictors())\n\ndummy_result &lt;- dummy_recipe %&gt;%\n  prep() %&gt;%\n  bake(new_data = NULL)\n\ndummy_result\n\n# A tibble: 5 x 8\n  outcome color_green color_red size_1 size_2 size_3 quality_excellent\n    &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;             &lt;dbl&gt;\n1      10           0         1 -0.671    0.5 -0.224                 0\n2      15           0         0 -0.224   -0.5  0.671                 0\n3      20           1         0  0.224   -0.5 -0.671                 1\n4      12           0         1  0.671    0.5  0.224                 0\n5      14           0         0 -0.224   -0.5  0.671                 0\n# i 1 more variable: quality_good &lt;dbl&gt;\n\n\nNotice how each category becomes its own binary column, except one category is dropped (reference level) to avoid perfect multicollinearity.\n\n\nAdvanced Categorical Encoding\nFor high-cardinality categorical variables (many unique values), simple dummy encoding can create too many features:\n\n# Create high-cardinality example\nhigh_card_data &lt;- ames_train %&gt;%\n  select(Sale_Price, Neighborhood, MS_SubClass) %&gt;%\n  mutate(\n    Neighborhood_Freq = n(),\n    .by = Neighborhood\n  )\n\n# Different encoding strategies\nencoding_recipes &lt;- list(\n  # Standard dummy encoding\n  dummy = recipe(Sale_Price ~ Neighborhood, data = high_card_data) %&gt;%\n    step_dummy(Neighborhood),\n  \n  # Frequency encoding\n  frequency = recipe(Sale_Price ~ Neighborhood, data = high_card_data) %&gt;%\n    step_mutate(Neighborhood_Freq = n(), .by = Neighborhood) %&gt;%\n    step_rm(Neighborhood),\n  \n  # Target encoding (mean of target for each category)\n  target = recipe(Sale_Price ~ Neighborhood, data = high_card_data) %&gt;%\n    step_mutate(\n      Neighborhood_Mean = mean(Sale_Price, na.rm = TRUE),\n      .by = Neighborhood\n    ) %&gt;%\n    step_rm(Neighborhood),\n  \n  # Lumping rare categories\n  lumped = recipe(Sale_Price ~ Neighborhood, data = high_card_data) %&gt;%\n    step_other(Neighborhood, threshold = 0.05) %&gt;%  # Combine rare levels\n    step_dummy(Neighborhood)\n)\n\n# Compare number of features created\nfeature_counts &lt;- map_df(names(encoding_recipes), function(name) {\n  n_features &lt;- encoding_recipes[[name]] %&gt;%\n    prep() %&gt;%\n    bake(new_data = NULL) %&gt;%\n    select(-Sale_Price) %&gt;%\n    ncol()\n  \n  tibble(\n    method = name,\n    n_features = n_features\n  )\n})\n\nfeature_counts %&gt;%\n  ggplot(aes(x = method, y = n_features, fill = method)) +\n  geom_col() +\n  geom_text(aes(label = n_features), vjust = -0.5) +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"Feature Count with Different Encoding Methods\",\n    subtitle = \"High-cardinality categorical variables can create many features\",\n    x = \"Encoding Method\",\n    y = \"Number of Features\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nEach method has trade-offs: - Dummy encoding: Simple but creates many features - Frequency encoding: Single feature but loses category identity - Target encoding: Powerful but risks overfitting - Lumping: Reduces features while preserving main categories"
  },
  {
    "objectID": "10-feature-engineering.html#creating-interaction-terms",
    "href": "10-feature-engineering.html#creating-interaction-terms",
    "title": "Chapter 10: Feature Engineering with recipes - The Art and Science of Data Preparation",
    "section": "Creating Interaction Terms",
    "text": "Creating Interaction Terms\nInteractions capture relationships between features that aren’t additive:\n\n# Generate data with interaction effect\nset.seed(123)\ninteraction_data &lt;- tibble(\n  x1 = runif(500, 0, 10),\n  x2 = runif(500, 0, 10),\n  # True relationship includes interaction\n  y = 10 + 2*x1 + 3*x2 + 0.5*x1*x2 + rnorm(500, 0, 2)\n)\n\n# Models with and without interaction\nno_interaction_recipe &lt;- recipe(y ~ x1 + x2, data = interaction_data)\n\nwith_interaction_recipe &lt;- recipe(y ~ x1 + x2, data = interaction_data) %&gt;%\n  step_interact(terms = ~ x1:x2)\n\n# Fit both models\nno_int_fit &lt;- workflow() %&gt;%\n  add_recipe(no_interaction_recipe) %&gt;%\n  add_model(linear_reg()) %&gt;%\n  fit(interaction_data)\n\nwith_int_fit &lt;- workflow() %&gt;%\n  add_recipe(with_interaction_recipe) %&gt;%\n  add_model(linear_reg()) %&gt;%\n  fit(interaction_data)\n\n# Create prediction surface\ngrid &lt;- expand_grid(\n  x1 = seq(0, 10, length.out = 50),\n  x2 = seq(0, 10, length.out = 50)\n)\n\ngrid_no_int &lt;- grid %&gt;%\n  mutate(\n    prediction = predict(no_int_fit, grid)$.pred,\n    model = \"Without Interaction\"\n  )\n\ngrid_with_int &lt;- grid %&gt;%\n  mutate(\n    prediction = predict(with_int_fit, grid)$.pred,\n    model = \"With Interaction\"\n  )\n\n# Visualize the difference\nbind_rows(grid_no_int, grid_with_int) %&gt;%\n  ggplot(aes(x = x1, y = x2, fill = prediction)) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  facet_wrap(~model) +\n  labs(\n    title = \"Effect of Including Interaction Terms\",\n    subtitle = \"Interaction allows the effect of x1 to depend on x2\",\n    x = \"Feature 1\",\n    y = \"Feature 2\",\n    fill = \"Predicted\\nValue\"\n  )\n\n\n\n\n\n\n\n# Compare model performance\ntibble(\n  Model = c(\"Without Interaction\", \"With Interaction\"),\n  RMSE = c(\n    sqrt(mean((interaction_data$y - predict(no_int_fit, interaction_data)$.pred)^2)),\n    sqrt(mean((interaction_data$y - predict(with_int_fit, interaction_data)$.pred)^2))\n  )\n) %&gt;%\n  knitr::kable(digits = 3)\n\n\n\n\nModel\nRMSE\n\n\n\n\nWithout Interaction\n4.511\n\n\nWith Interaction\n2.018\n\n\n\n\n\nThe interaction term allows the model to capture how the effect of one variable depends on another. This is crucial in many real-world scenarios: - Price elasticity depending on income level - Drug effectiveness depending on patient age - Marketing response depending on customer segment"
  },
  {
    "objectID": "10-feature-engineering.html#handling-missing-data",
    "href": "10-feature-engineering.html#handling-missing-data",
    "title": "Chapter 10: Feature Engineering with recipes - The Art and Science of Data Preparation",
    "section": "Handling Missing Data",
    "text": "Handling Missing Data\nMissing data is ubiquitous in real-world datasets. The strategy depends on why data is missing:\n\nTypes of Missingness\n\nMissing Completely at Random (MCAR): Missingness is independent of all variables\nMissing at Random (MAR): Missingness depends on observed variables\nMissing Not at Random (MNAR): Missingness depends on the missing value itself\n\nLet’s explore different imputation strategies:\n\n# Create data with different missing patterns\nset.seed(123)\nmissing_data &lt;- tibble(\n  x1 = rnorm(1000),\n  x2 = rnorm(1000),\n  x3 = x1 + x2 + rnorm(1000, 0, 0.5),\n  y = 2*x1 + 3*x2 + x3 + rnorm(1000)\n)\n\n# Introduce different missing patterns\nmissing_data &lt;- missing_data %&gt;%\n  mutate(\n    # MCAR: Random 20% missing\n    x1_mcar = ifelse(runif(n()) &lt; 0.2, NA, x1),\n    # MAR: Missing depends on x2\n    x2_mar = ifelse(x2 &lt; quantile(x2, 0.2), NA, x2),\n    # MNAR: Large values more likely missing\n    x3_mnar = ifelse(x3 &gt; quantile(x3, 0.8) & runif(n()) &lt; 0.5, NA, x3)\n  )\n\n# Different imputation strategies\nimputation_recipes &lt;- list(\n  # Remove rows with missing data\n  complete_case = recipe(y ~ x1_mcar + x2_mar + x3_mnar, data = missing_data) %&gt;%\n    step_naomit(all_predictors()),\n  \n  # Mean imputation\n  mean_imp = recipe(y ~ x1_mcar + x2_mar + x3_mnar, data = missing_data) %&gt;%\n    step_impute_mean(all_predictors()),\n  \n  # Median imputation (robust to outliers)\n  median_imp = recipe(y ~ x1_mcar + x2_mar + x3_mnar, data = missing_data) %&gt;%\n    step_impute_median(all_predictors()),\n  \n  # K-nearest neighbors imputation\n  knn_imp = recipe(y ~ x1_mcar + x2_mar + x3_mnar, data = missing_data) %&gt;%\n    step_impute_knn(all_predictors(), neighbors = 5),\n  \n  # Linear imputation (using other variables)\n  linear_imp = recipe(y ~ x1_mcar + x2_mar + x3_mnar, data = missing_data) %&gt;%\n    step_impute_linear(x1_mcar, impute_with = imp_vars(x2_mar, x3_mnar)) %&gt;%\n    step_impute_linear(x2_mar, impute_with = imp_vars(x1_mcar, x3_mnar)) %&gt;%\n    step_impute_linear(x3_mnar, impute_with = imp_vars(x1_mcar, x2_mar))\n)\n\n# Apply imputation and evaluate\nimputation_results &lt;- map_df(names(imputation_recipes), function(name) {\n  imputed &lt;- imputation_recipes[[name]] %&gt;%\n    prep() %&gt;%\n    bake(new_data = NULL)\n  \n  # Calculate statistics\n  tibble(\n    method = name,\n    n_rows = nrow(imputed),\n    x1_mean_error = mean(imputed$x1_mcar - missing_data$x1[!is.na(imputed$x1_mcar)], \n                         na.rm = TRUE),\n    x2_mean_error = mean(imputed$x2_mar - missing_data$x2[!is.na(imputed$x2_mar)], \n                         na.rm = TRUE),\n    x3_mean_error = mean(imputed$x3_mnar - missing_data$x3[!is.na(imputed$x3_mnar)], \n                         na.rm = TRUE)\n  )\n})\n\n# Visualize imputation quality\nimputation_results %&gt;%\n  pivot_longer(cols = contains(\"error\"), \n               names_to = \"variable\", \n               values_to = \"error\") %&gt;%\n  ggplot(aes(x = method, y = abs(error), fill = variable)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"Imputation Error by Method\",\n    subtitle = \"Lower is better - comparing imputed values to true values\",\n    x = \"Imputation Method\",\n    y = \"Absolute Mean Error\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n# Show data loss with complete case\ntibble(\n  Method = c(\"Complete Case\", \"Imputation Methods\"),\n  `Rows Retained` = c(\n    imputation_results %&gt;% filter(method == \"complete_case\") %&gt;% pull(n_rows),\n    1000\n  ),\n  `Percentage` = c(\n    imputation_results %&gt;% filter(method == \"complete_case\") %&gt;% pull(n_rows) / 10,\n    100\n  )\n) %&gt;%\n  knitr::kable()\n\n\n\n\nMethod\nRows Retained\nPercentage\n\n\n\n\nComplete Case\n551\n55.1\n\n\nImputation Methods\n1000\n100.0\n\n\n\n\n\nKey insights about imputation: - Complete case analysis loses a lot of data - Mean/median imputation is simple but ignores relationships - KNN imputation uses similar observations - Linear imputation preserves linear relationships - Choice depends on missing mechanism and data structure"
  },
  {
    "objectID": "10-feature-engineering.html#feature-selection-and-dimensionality-reduction",
    "href": "10-feature-engineering.html#feature-selection-and-dimensionality-reduction",
    "title": "Chapter 10: Feature Engineering with recipes - The Art and Science of Data Preparation",
    "section": "Feature Selection and Dimensionality Reduction",
    "text": "Feature Selection and Dimensionality Reduction\nToo many features can lead to overfitting and computational issues. Let’s explore methods to reduce dimensionality:\n\nFilter Methods\nFilter methods select features based on statistical tests:\n\n# Create data with relevant and irrelevant features\nset.seed(123)\nfeature_data &lt;- tibble(\n  # Relevant features\n  relevant_1 = rnorm(500),\n  relevant_2 = rnorm(500),\n  relevant_3 = rnorm(500),\n  # Irrelevant features\n  noise_1 = rnorm(500),\n  noise_2 = rnorm(500),\n  noise_3 = rnorm(500),\n  noise_4 = rnorm(500),\n  # Target depends only on relevant features\n  target = 2*relevant_1 + 3*relevant_2 - relevant_3 + rnorm(500, 0, 0.5)\n)\n\n# Calculate correlations with target\ncorrelations &lt;- feature_data %&gt;%\n  select(-target) %&gt;%\n  map_dbl(~ cor(., feature_data$target, use = \"complete.obs\")) %&gt;%\n  enframe(name = \"feature\", value = \"correlation\") %&gt;%\n  mutate(\n    abs_correlation = abs(correlation),\n    feature_type = ifelse(str_detect(feature, \"relevant\"), \"Relevant\", \"Noise\")\n  )\n\n# Visualize correlations\nggplot(correlations, aes(x = reorder(feature, abs_correlation), \n                         y = abs_correlation, \n                         fill = feature_type)) +\n  geom_col() +\n  coord_flip() +\n  scale_fill_manual(values = c(\"Relevant\" = \"darkgreen\", \"Noise\" = \"gray50\")) +\n  geom_hline(yintercept = 0.1, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Feature Selection Using Correlation Filter\",\n    subtitle = \"Red line shows potential threshold for feature selection\",\n    x = \"Feature\",\n    y = \"Absolute Correlation with Target\",\n    fill = \"True Feature Type\"\n  )\n\n\n\n\n\n\n\n# Recipe with correlation filter\nfiltered_recipe &lt;- recipe(target ~ ., data = feature_data) %&gt;%\n  step_corr(all_predictors(), threshold = 0.9) %&gt;%  # Remove highly correlated features\n  step_rm(all_predictors(), -all_outcomes(), \n          skip = FALSE,\n          threshold = 0.1)  # This would remove low correlation features\n\n# Near-zero variance filter\nnzv_recipe &lt;- recipe(target ~ ., data = feature_data) %&gt;%\n  step_nzv(all_predictors())  # Remove features with near-zero variance\n\n\n\nPrincipal Component Analysis (PCA)\nPCA creates new features that are linear combinations of original features:\n\n# Create correlated features for PCA demonstration\npca_data &lt;- tibble(\n  x1 = rnorm(500),\n  x2 = x1 + rnorm(500, 0, 0.5),  # Correlated with x1\n  x3 = rnorm(500),\n  x4 = x3 + rnorm(500, 0, 0.5),  # Correlated with x3\n  x5 = rnorm(500),\n  y = x1 + x3 + rnorm(500, 0, 0.5)\n)\n\n# PCA recipe\npca_recipe &lt;- recipe(y ~ ., data = pca_data) %&gt;%\n  step_normalize(all_predictors()) %&gt;%  # Important: normalize before PCA\n  step_pca(all_predictors(), num_comp = 3)  # Keep 3 components\n\n# Prepare and examine\npca_prep &lt;- prep(pca_recipe)\npca_result &lt;- bake(pca_prep, new_data = NULL)\n\n# Extract loadings\npca_loadings &lt;- tidy(pca_prep, 2) %&gt;%  # 2nd step is PCA\n  filter(component %in% paste0(\"PC\", 1:3)) %&gt;%\n  mutate(\n    component = factor(component, levels = paste0(\"PC\", 1:3))\n  )\n\n# Visualize loadings\nggplot(pca_loadings, aes(x = terms, y = value, fill = value &gt; 0)) +\n  geom_col() +\n  facet_wrap(~component) +\n  coord_flip() +\n  scale_fill_manual(values = c(\"FALSE\" = \"red\", \"TRUE\" = \"blue\")) +\n  labs(\n    title = \"PCA Loadings\",\n    subtitle = \"How original features contribute to each principal component\",\n    x = \"Original Feature\",\n    y = \"Loading\",\n    fill = \"Sign\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n# Variance explained\npca_variance &lt;- pca_prep$steps[[2]]$res$sdev^2\nvariance_explained &lt;- tibble(\n  PC = paste0(\"PC\", 1:length(pca_variance)),\n  Variance = pca_variance,\n  `Proportion Explained` = Variance / sum(Variance),\n  `Cumulative Proportion` = cumsum(`Proportion Explained`)\n)\n\n# Scree plot\nggplot(variance_explained %&gt;% head(5), \n       aes(x = PC, y = `Proportion Explained`)) +\n  geom_col(fill = \"steelblue\") +\n  geom_line(aes(group = 1), color = \"red\", linewidth = 1) +\n  geom_point(size = 3, color = \"red\") +\n  geom_text(aes(label = round(`Cumulative Proportion`, 2)), \n            vjust = -1, size = 3) +\n  labs(\n    title = \"PCA Scree Plot\",\n    subtitle = \"Shows variance explained by each component\",\n    x = \"Principal Component\",\n    y = \"Proportion of Variance Explained\"\n  )\n\n\n\n\n\n\n\n\nPCA is powerful for: - Reducing dimensionality while preserving variance - Removing multicollinearity - Visualization (first 2-3 components) - Noise reduction"
  },
  {
    "objectID": "10-feature-engineering.html#time-based-features",
    "href": "10-feature-engineering.html#time-based-features",
    "title": "Chapter 10: Feature Engineering with recipes - The Art and Science of Data Preparation",
    "section": "Time-Based Features",
    "text": "Time-Based Features\nTime series data requires special feature engineering:\n\n# Create time series data\ntime_data &lt;- tibble(\n  date = seq(as.Date(\"2020-01-01\"), as.Date(\"2022-12-31\"), by = \"day\"),\n  base_value = 1000,\n  trend = seq(0, 200, length.out = length(date)),\n  seasonal = 100 * sin(2 * pi * as.numeric(date) / 365),\n  weekly = 50 * sin(2 * pi * wday(date) / 7),\n  noise = rnorm(length(date), 0, 30),\n  sales = base_value + trend + seasonal + weekly + noise\n)\n\n# Time-based feature engineering\ntime_features &lt;- time_data %&gt;%\n  mutate(\n    # Basic time components\n    year = year(date),\n    month = month(date),\n    day = day(date),\n    day_of_week = wday(date, label = TRUE),\n    day_of_year = yday(date),\n    week_of_year = week(date),\n    quarter = quarter(date),\n    \n    # Cyclical encoding (preserves continuity)\n    month_sin = sin(2 * pi * month / 12),\n    month_cos = cos(2 * pi * month / 12),\n    day_sin = sin(2 * pi * day / 31),\n    day_cos = cos(2 * pi * day / 31),\n    \n    # Binary indicators\n    is_weekend = wday(date) %in% c(1, 7),\n    is_month_start = day &lt;= 7,\n    is_month_end = day &gt;= day(ceiling_date(date, \"month\") - days(7)),\n    \n    # Lag features\n    sales_lag_1 = lag(sales, 1),\n    sales_lag_7 = lag(sales, 7),\n    sales_lag_30 = lag(sales, 30),\n    \n    # Rolling statistics\n    sales_ma_7 = zoo::rollmean(sales, 7, fill = NA, align = \"right\"),\n    sales_ma_30 = zoo::rollmean(sales, 30, fill = NA, align = \"right\"),\n    sales_std_7 = zoo::rollapply(sales, 7, sd, fill = NA, align = \"right\")\n  )\n\n# Visualize some engineered features\nfeature_importance &lt;- time_features %&gt;%\n  drop_na() %&gt;%\n  select(sales, month_sin, month_cos, is_weekend, sales_lag_7, sales_ma_30) %&gt;%\n  cor() %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"feature\") %&gt;%\n  filter(feature != \"sales\") %&gt;%\n  select(feature, correlation = sales) %&gt;%\n  arrange(desc(abs(correlation)))\n\nggplot(feature_importance, aes(x = reorder(feature, abs(correlation)), \n                               y = abs(correlation))) +\n  geom_col(fill = \"darkblue\") +\n  coord_flip() +\n  labs(\n    title = \"Importance of Time-Based Features\",\n    subtitle = \"Correlation with sales\",\n    x = \"Feature\",\n    y = \"Absolute Correlation\"\n  )\n\n\n\n\n\n\n\n# Show cyclical encoding\ncyclical_demo &lt;- time_features %&gt;%\n  select(month, month_sin, month_cos) %&gt;%\n  distinct() %&gt;%\n  arrange(month)\n\nggplot(cyclical_demo, aes(x = month_cos, y = month_sin)) +\n  geom_path(color = \"blue\", linewidth = 1) +\n  geom_point(aes(color = factor(month)), size = 3) +\n  geom_text(aes(label = month), vjust = -1) +\n  coord_equal() +\n  scale_color_viridis_d() +\n  labs(\n    title = \"Cyclical Encoding of Months\",\n    subtitle = \"Preserves continuity: December is close to January\",\n    x = \"Cosine Component\",\n    y = \"Sine Component\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nTime features are crucial for: - Capturing seasonality and trends - Accounting for day-of-week effects - Incorporating historical information (lags) - Smoothing noisy signals (moving averages)"
  },
  {
    "objectID": "10-feature-engineering.html#text-features-brief-introduction",
    "href": "10-feature-engineering.html#text-features-brief-introduction",
    "title": "Chapter 10: Feature Engineering with recipes - The Art and Science of Data Preparation",
    "section": "Text Features (Brief Introduction)",
    "text": "Text Features (Brief Introduction)\nText data requires specialized preprocessing:\n\n# Simple text feature engineering example\ntext_data &lt;- tibble(\n  id = 1:5,\n  review = c(\n    \"This product is absolutely amazing! Best purchase ever!\",\n    \"Terrible quality. Very disappointed. Would not recommend.\",\n    \"Good value for money. Satisfied with the purchase.\",\n    \"Excellent service and fast delivery. Five stars!\",\n    \"Product broke after one day. Complete waste of money.\"\n  ),\n  rating = c(5, 1, 4, 5, 1)\n)\n\n# Text feature recipe using textrecipes\ntext_recipe &lt;- recipe(rating ~ review, data = text_data) %&gt;%\n  # Tokenize text\n  step_tokenize(review) %&gt;%\n  # Remove stop words\n  step_stopwords(review) %&gt;%\n  # Create n-grams\n  step_ngram(review, num_tokens = 2) %&gt;%\n  # Convert to term frequency\n  step_tf(review, weight_scheme = \"binary\") %&gt;%\n  # Optionally: TF-IDF weighting\n  # step_tfidf(review) %&gt;%\n  # Keep only most common terms\n  step_tokenfilter(review, max_tokens = 20)\n\n# Note: This is just a demonstration - real text processing needs more data\n\nText features can include: - Word counts and frequencies - N-grams (sequences of words) - TF-IDF weights - Sentiment scores - Word embeddings"
  },
  {
    "objectID": "10-feature-engineering.html#best-practices-and-common-pitfalls",
    "href": "10-feature-engineering.html#best-practices-and-common-pitfalls",
    "title": "Chapter 10: Feature Engineering with recipes - The Art and Science of Data Preparation",
    "section": "Best Practices and Common Pitfalls",
    "text": "Best Practices and Common Pitfalls\n\nBest Practices\n\nAlways split before feature engineering\n\nPrevents data leakage\nEnsures fair evaluation\n\nOrder of operations matters\n\nImpute missing values before normalization\nCreate interactions after dummy encoding\nNormalize after all transformations\n\nKeep it simple initially\n\nStart with basic features\nAdd complexity gradually\nValidate improvements\n\nDocument your choices\n\nWhy each transformation?\nWhat domain knowledge informed decisions?\n\n\n\n\nCommon Pitfalls to Avoid\nLet’s demonstrate some common mistakes:\n\n# Create split if not already done\nif (!exists(\"ames_split\")) {\n  set.seed(123)\n  ames_split &lt;- initial_split(ames, prop = 0.75, strata = Sale_Price)\n  ames_train &lt;- training(ames_split)\n  ames_test &lt;- testing(ames_split)\n}\n\n# WRONG: Normalizing before splitting\n# This leaks information from test set into training\nwrong_approach &lt;- ames %&gt;%\n  mutate(Gr_Liv_Area_scaled = scale(Gr_Liv_Area)[,1]) %&gt;%  # Uses ALL data!\n  initial_split()\n\n# RIGHT: Normalize within recipe\nright_recipe &lt;- recipe(Sale_Price ~ Gr_Liv_Area, data = training(ames_split)) %&gt;%\n  step_normalize(Gr_Liv_Area)  # Will use only training data statistics\n\n# WRONG: Creating too many features\noverengineered_recipe &lt;- recipe(Sale_Price ~ ., data = ames_train) %&gt;%\n  step_poly(all_numeric_predictors(), degree = 5) %&gt;%  # Too many polynomial terms\n  step_interact(terms = ~ all_numeric_predictors()^2)  # All 2-way interactions\n\n# RIGHT: Thoughtful feature engineering\nthoughtful_recipe &lt;- recipe(Sale_Price ~ ., data = ames_train) %&gt;%\n  step_log(Sale_Price) %&gt;%\n  step_poly(Gr_Liv_Area, degree = 2) %&gt;%  # Only where needed\n  step_interact(terms = ~ Gr_Liv_Area:Overall_Cond)  # Specific, meaningful interaction"
  },
  {
    "objectID": "10-feature-engineering.html#complete-example-putting-it-all-together",
    "href": "10-feature-engineering.html#complete-example-putting-it-all-together",
    "title": "Chapter 10: Feature Engineering with recipes - The Art and Science of Data Preparation",
    "section": "Complete Example: Putting It All Together",
    "text": "Complete Example: Putting It All Together\nLet’s create a comprehensive feature engineering pipeline:\n\n# Use credit data for a complete example\ncredit_split &lt;- initial_split(credit_data, prop = 0.75, strata = Status)\ncredit_train &lt;- training(credit_split)\ncredit_test &lt;- testing(credit_split)\n\n# Comprehensive recipe\ncomprehensive_recipe &lt;- recipe(Status ~ ., data = credit_train) %&gt;%\n  # 1. Handle missing values\n  step_impute_median(all_numeric_predictors()) %&gt;%\n  step_impute_mode(all_nominal_predictors()) %&gt;%\n  \n  # 2. Feature creation\n  step_mutate(\n    debt_to_income = Expenses / Income,\n    savings_rate = (Income - Expenses) / Income,\n    has_records = !is.na(Records)\n  ) %&gt;%\n  \n  # 3. Handle skewness\n  step_YeoJohnson(Income, Amount) %&gt;%\n  \n  # 4. Create dummy variables\n  step_dummy(all_nominal_predictors(), -has_records) %&gt;%\n  \n  # 5. Remove near-zero variance\n  step_nzv(all_predictors()) %&gt;%\n  \n  # 6. Normalize\n  step_normalize(all_numeric_predictors()) %&gt;%\n  \n  # 7. Remove highly correlated features\n  step_corr(all_numeric_predictors(), threshold = 0.9) %&gt;%\n  \n  # 8. PCA for dimensionality reduction (optional)\n  # step_pca(all_numeric_predictors(), threshold = 0.95) %&gt;%\n  \n  # 9. Balance classes (for classification)\n  step_smote(Status)  # Synthetic minority oversampling\n\n# Examine the recipe\ncomprehensive_recipe\n\n# Prepare and check results\ncomprehensive_prep &lt;- prep(comprehensive_recipe)\ncomprehensive_baked &lt;- bake(comprehensive_prep, new_data = NULL)\n\n# Summary of transformations\ntibble(\n  Stage = c(\"Original\", \"After Engineering\"),\n  `N Features` = c(ncol(credit_train) - 1, ncol(comprehensive_baked) - 1),\n  `N Observations` = c(nrow(credit_train), nrow(comprehensive_baked)),\n  `Class Balance` = c(\n    sum(credit_train$Status == \"good\") / nrow(credit_train),\n    sum(comprehensive_baked$Status == \"good\") / nrow(comprehensive_baked)\n  )\n) %&gt;%\n  knitr::kable(digits = 3)\n\n\n\n\nStage\nN Features\nN Observations\nClass Balance\n\n\n\n\nOriginal\n13\n3340\n0.719\n\n\nAfter Engineering\n19\n4800\n0.500\n\n\n\n\n# Fit a model with the engineered features\nrf_spec &lt;- rand_forest(trees = 100) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\nworkflow() %&gt;%\n  add_recipe(comprehensive_recipe) %&gt;%\n  add_model(rf_spec) %&gt;%\n  fit(credit_train) %&gt;%\n  predict(credit_test) %&gt;%\n  bind_cols(credit_test) %&gt;%\n  accuracy(truth = Status, estimate = .pred_class)\n\n# A tibble: 1 x 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.777"
  },
  {
    "objectID": "10-feature-engineering.html#exercises",
    "href": "10-feature-engineering.html#exercises",
    "title": "Chapter 10: Feature Engineering with recipes - The Art and Science of Data Preparation",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Engineer Features for House Prices\nCreate a comprehensive feature engineering pipeline for the Ames housing data:\n\n# Your solution\nexercise_recipe &lt;- recipe(Sale_Price ~ ., data = ames_train) %&gt;%\n  # Transform outcome\n  step_log(Sale_Price) %&gt;%\n  \n  # Create meaningful features\n  step_mutate(\n    House_Age = 2010 - Year_Built,\n    Remod_Age = 2010 - Year_Remod_Add,\n    Has_Garage = !is.na(Garage_Type),\n    Total_Bathrooms = Full_Bath + Half_Bath * 0.5,\n    Total_SF = Gr_Liv_Area + Total_Bsmt_SF,\n    Quality_x_Condition = Overall_Cond * Overall_Cond\n  ) %&gt;%\n  \n  # Remove original year variables\n  step_rm(Year_Built, Year_Remod_Add, Mo_Sold, Year_Sold) %&gt;%\n  \n  # Handle missing values\n  step_impute_median(all_numeric_predictors()) %&gt;%\n  step_impute_mode(all_nominal_predictors()) %&gt;%\n  \n  # Rare categories\n  step_other(all_nominal_predictors(), threshold = 0.05) %&gt;%\n  \n  # Transform skewed variables (before creating dummies)\n  step_YeoJohnson(Lot_Area, Gr_Liv_Area, Total_SF) %&gt;%\n  \n  # Interactions (before creating dummies)\n  step_interact(terms = ~ Total_SF:Overall_Cond) %&gt;%\n  \n  # Create dummies\n  step_dummy(all_nominal_predictors()) %&gt;%\n  \n  # Scale\n  step_normalize(all_numeric_predictors()) %&gt;%\n  \n  # Remove zero variance\n  step_nzv(all_predictors())\n\n# Test the recipe\nexercise_prep &lt;- prep(exercise_recipe)\nexercise_baked &lt;- bake(exercise_prep, new_data = NULL)\n\nprint(paste(\"Created\", ncol(exercise_baked) - 1, \"features from\", \n            ncol(ames_train) - 1, \"original features\"))\n\n[1] \"Created 103 features from 73 original features\"\n\n\n\n\nExercise 2: Handle High-Cardinality Categorical\nWork with a high-cardinality categorical variable:\n\n# Your solution\n# Create synthetic high-cardinality data\nhigh_card_ex &lt;- tibble(\n  category = sample(paste0(\"Cat_\", 1:100), 1000, replace = TRUE),\n  value = rnorm(1000),\n  target = rnorm(1000)\n) %&gt;%\n  mutate(\n    # Make target depend somewhat on category\n    target = target + as.numeric(factor(category)) / 20\n  )\n\n# Try different encoding strategies\nstrategies &lt;- list(\n  # Frequency encoding\n  frequency = recipe(target ~ ., data = high_card_ex) %&gt;%\n    step_mutate(cat_freq = n(), .by = category) %&gt;%\n    step_rm(category),\n  \n  # Target encoding with smoothing\n  target_enc = recipe(target ~ ., data = high_card_ex) %&gt;%\n    step_mutate(\n      cat_mean = mean(target),\n      cat_count = n(),\n      .by = category\n    ) %&gt;%\n    step_mutate(\n      # Smooth with global mean for rare categories\n      cat_smooth = (cat_mean * cat_count + mean(target) * 10) / (cat_count + 10)\n    ) %&gt;%\n    step_rm(category, cat_mean, cat_count),\n  \n  # Embedding-like (PCA on dummies)\n  embedding = recipe(target ~ ., data = high_card_ex) %&gt;%\n    step_dummy(category) %&gt;%\n    step_pca(starts_with(\"category_\"), num_comp = 10)\n)\n\n# Compare approaches\ncomparison &lt;- map_df(names(strategies), function(name) {\n  prepped &lt;- prep(strategies[[name]])\n  baked &lt;- bake(prepped, new_data = NULL)\n  \n  tibble(\n    method = name,\n    n_features = ncol(baked) - 1\n  )\n})\n\nprint(comparison)\n\n# A tibble: 3 x 2\n  method     n_features\n  &lt;chr&gt;           &lt;dbl&gt;\n1 frequency           3\n2 target_enc          3\n3 embedding          11\n\n\n\n\nExercise 3: Time Series Feature Engineering\nCreate features for time series prediction:\n\n# Your solution\n# Generate time series data\nts_exercise &lt;- tibble(\n  date = seq(as.Date(\"2021-01-01\"), as.Date(\"2023-12-31\"), by = \"day\")\n) %&gt;%\n  mutate(\n    trend = row_number() / n() * 100,\n    seasonal = 50 * sin(2 * pi * yday(date) / 365),\n    weekly = 20 * sin(2 * pi * wday(date) / 7),\n    noise = rnorm(n(), 0, 10),\n    sales = 1000 + trend + seasonal + weekly + noise\n  )\n\n# Create time features\nts_features &lt;- ts_exercise %&gt;%\n  mutate(\n    # Calendar features\n    year = year(date),\n    month = month(date),\n    week = week(date),\n    day_of_week = wday(date),\n    day_of_month = day(date),\n    day_of_year = yday(date),\n    quarter = quarter(date),\n    \n    # Cyclical encoding\n    month_sin = sin(2 * pi * month / 12),\n    month_cos = cos(2 * pi * month / 12),\n    dow_sin = sin(2 * pi * day_of_week / 7),\n    dow_cos = cos(2 * pi * day_of_week / 7),\n    \n    # Indicators\n    is_weekend = day_of_week %in% c(1, 7),\n    is_month_start = day_of_month &lt;= 3,\n    is_month_end = day_of_month &gt;= 28,\n    \n    # Lag features\n    sales_lag_1 = lag(sales, 1),\n    sales_lag_7 = lag(sales, 7),\n    sales_lag_30 = lag(sales, 30),\n    sales_lag_365 = lag(sales, 365),\n    \n    # Rolling statistics\n    sales_ma_7 = zoo::rollmean(sales, 7, fill = NA, align = \"right\"),\n    sales_ma_30 = zoo::rollmean(sales, 30, fill = NA, align = \"right\"),\n    sales_std_7 = zoo::rollapply(sales, 7, sd, fill = NA, align = \"right\"),\n    sales_std_30 = zoo::rollapply(sales, 30, sd, fill = NA, align = \"right\"),\n    \n    # Differences\n    sales_diff_1 = sales - lag(sales, 1),\n    sales_diff_7 = sales - lag(sales, 7)\n  ) %&gt;%\n  drop_na()\n\n# Evaluate feature importance\nfeature_cors &lt;- ts_features %&gt;%\n  select(-date, -trend, -seasonal, -weekly, -noise) %&gt;%\n  select(-sales) %&gt;%\n  map_dbl(~ cor(., ts_features$sales)) %&gt;%\n  enframe(name = \"feature\", value = \"correlation\") %&gt;%\n  arrange(desc(abs(correlation))) %&gt;%\n  head(15)\n\nggplot(feature_cors, aes(x = reorder(feature, abs(correlation)), \n                         y = abs(correlation))) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Top Time Series Features\",\n    subtitle = \"Absolute correlation with sales\",\n    x = \"Feature\",\n    y = \"|Correlation|\"\n  )"
  },
  {
    "objectID": "10-feature-engineering.html#summary",
    "href": "10-feature-engineering.html#summary",
    "title": "Chapter 10: Feature Engineering with recipes - The Art and Science of Data Preparation",
    "section": "Summary",
    "text": "Summary\nFeature engineering is both an art and a science. You’ve learned:\n✅ Core concepts: Why feature engineering matters and how it works\n✅ Numeric transformations: Scaling, normalization, handling skewness\n✅ Categorical encoding: Dummies, target encoding, handling high cardinality\n✅ Interaction terms: Capturing non-additive relationships\n✅ Missing data strategies: Various imputation methods and when to use them\n✅ Dimensionality reduction: PCA and feature selection\n✅ Time features: Extracting temporal patterns\n✅ Best practices: Avoiding leakage, proper ordering, validation\nRemember: - Feature engineering often has more impact than model selection - Domain knowledge is invaluable for creating meaningful features - Always validate that engineered features improve performance - Keep transformations in recipes for reproducibility - Start simple and add complexity gradually"
  },
  {
    "objectID": "10-feature-engineering.html#whats-next",
    "href": "10-feature-engineering.html#whats-next",
    "title": "Chapter 10: Feature Engineering with recipes - The Art and Science of Data Preparation",
    "section": "What’s Next?",
    "text": "What’s Next?\nIn Chapter 11, we’ll explore parsnip for unified model specification across different engines."
  },
  {
    "objectID": "10-feature-engineering.html#additional-resources",
    "href": "10-feature-engineering.html#additional-resources",
    "title": "Chapter 10: Feature Engineering with recipes - The Art and Science of Data Preparation",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nFeature Engineering and Selection\nRecipes Documentation\nFeature Engineering for Machine Learning\nTidy Modeling with R - Recipes Chapter"
  },
  {
    "objectID": "03-data-wrangling.html",
    "href": "03-data-wrangling.html",
    "title": "Chapter 3: Data Wrangling with dplyr",
    "section": "",
    "text": "By the end of this chapter, you will master:\n\nThe five key dplyr verbs: filter(), select(), mutate(), arrange(), summarise()\nGrouped operations with group_by()\nJoining tables with *_join() functions\nWindow functions and ranking\nAdvanced selection helpers\nRow-wise operations\nWorking with databases using dplyr"
  },
  {
    "objectID": "03-data-wrangling.html#learning-objectives",
    "href": "03-data-wrangling.html#learning-objectives",
    "title": "Chapter 3: Data Wrangling with dplyr",
    "section": "",
    "text": "By the end of this chapter, you will master:\n\nThe five key dplyr verbs: filter(), select(), mutate(), arrange(), summarise()\nGrouped operations with group_by()\nJoining tables with *_join() functions\nWindow functions and ranking\nAdvanced selection helpers\nRow-wise operations\nWorking with databases using dplyr"
  },
  {
    "objectID": "03-data-wrangling.html#setup",
    "href": "03-data-wrangling.html#setup",
    "title": "Chapter 3: Data Wrangling with dplyr",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.4     v readr     2.1.5\nv forcats   1.0.0     v stringr   1.5.2\nv ggplot2   4.0.0     v tibble    3.3.0\nv lubridate 1.9.4     v tidyr     1.3.1\nv purrr     1.1.0     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(palmerpenguins)\nlibrary(nycflights13)\nlibrary(gapminder)\n\n# We'll use these datasets throughout\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel~\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse~\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, ~\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, ~\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186~\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, ~\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male~\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007~\n\nglimpse(flights)\n\nRows: 336,776\nColumns: 19\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2~\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~\n$ dep_time       &lt;int&gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, ~\n$ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600, ~\n$ dep_delay      &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -1~\n$ arr_time       &lt;int&gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849,~\n$ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851,~\n$ arr_delay      &lt;dbl&gt; 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -1~\n$ carrier        &lt;chr&gt; \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV\", \"B6\", \"~\n$ flight         &lt;int&gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 4~\n$ tailnum        &lt;chr&gt; \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668DN\", \"N394~\n$ origin         &lt;chr&gt; \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EWR\", \"LGA\",~\n$ dest           &lt;chr&gt; \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FLL\", \"IAD\",~\n$ air_time       &lt;dbl&gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 1~\n$ distance       &lt;dbl&gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, ~\n$ hour           &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6~\n$ minute         &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 0~\n$ time_hour      &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 0~\n\nglimpse(gapminder)\n\nRows: 1,704\nColumns: 6\n$ country   &lt;fct&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", ~\n$ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, ~\n$ year      &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, ~\n$ lifeExp   &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8~\n$ pop       &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12~\n$ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, ~"
  },
  {
    "objectID": "03-data-wrangling.html#the-core-dplyr-verbs",
    "href": "03-data-wrangling.html#the-core-dplyr-verbs",
    "title": "Chapter 3: Data Wrangling with dplyr",
    "section": "The Core dplyr Verbs",
    "text": "The Core dplyr Verbs\n\n1. filter() - Keep Rows That Match Conditions\n\n# Simple filtering\npenguins %&gt;%\n  filter(species == \"Adelie\")\n\n# A tibble: 152 x 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# i 142 more rows\n# i 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n# Multiple conditions (AND)\npenguins %&gt;%\n  filter(species == \"Adelie\", bill_length_mm &gt; 40)\n\n# A tibble: 51 x 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           40.3          18                 195        3250\n 2 Adelie  Torgersen           42            20.2               190        4250\n 3 Adelie  Torgersen           41.1          17.6               182        3200\n 4 Adelie  Torgersen           42.5          20.7               197        4500\n 5 Adelie  Torgersen           46            21.5               194        4200\n 6 Adelie  Biscoe              40.6          18.6               183        3550\n 7 Adelie  Biscoe              40.5          17.9               187        3200\n 8 Adelie  Biscoe              40.5          18.9               180        3950\n 9 Adelie  Dream               40.9          18.9               184        3900\n10 Adelie  Dream               42.2          18.5               180        3550\n# i 41 more rows\n# i 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n# OR conditions\npenguins %&gt;%\n  filter(species == \"Adelie\" | species == \"Gentoo\")\n\n# A tibble: 276 x 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# i 266 more rows\n# i 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n# Using %in% for multiple values\npenguins %&gt;%\n  filter(species %in% c(\"Adelie\", \"Gentoo\"))\n\n# A tibble: 276 x 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# i 266 more rows\n# i 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n# Complex conditions\npenguins %&gt;%\n  filter(\n    bill_length_mm &gt; 45,\n    flipper_length_mm &lt; 210,\n    !is.na(sex)\n  )\n\n# A tibble: 68 x 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           46            21.5               194        4200\n 2 Adelie  Torgersen           45.8          18.9               197        4150\n 3 Adelie  Biscoe              45.6          20.3               191        4600\n 4 Gentoo  Biscoe              46.2          14.5               209        4800\n 5 Gentoo  Biscoe              45.1          14.5               207        5050\n 6 Gentoo  Biscoe              48.7          15.7               208        5350\n 7 Gentoo  Biscoe              45.3          13.8               208        4200\n 8 Gentoo  Biscoe              47.5          14.2               209        4600\n 9 Gentoo  Biscoe              48.4          14.4               203        4625\n10 Gentoo  Biscoe              48.1          15.1               209        5500\n# i 58 more rows\n# i 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n# Filter with between\nflights %&gt;%\n  filter(between(dep_delay, 10, 30)) %&gt;%\n  select(dep_delay, carrier, flight) %&gt;%\n  head(10)\n\n# A tibble: 10 x 3\n   dep_delay carrier flight\n       &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt;\n 1        11 UA         303\n 2        13 AA        1837\n 3        24 EV        4144\n 4        13 AA          33\n 5        13 DL         495\n 6        11 UA        1626\n 7        23 UA        1643\n 8        12 AA         647\n 9        14 UA         783\n10        15 B6        1305\n\n\n\n\n2. select() - Choose Columns\n\n# Select specific columns\npenguins %&gt;%\n  select(species, island, bill_length_mm)\n\n# A tibble: 344 x 3\n   species island    bill_length_mm\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;\n 1 Adelie  Torgersen           39.1\n 2 Adelie  Torgersen           39.5\n 3 Adelie  Torgersen           40.3\n 4 Adelie  Torgersen           NA  \n 5 Adelie  Torgersen           36.7\n 6 Adelie  Torgersen           39.3\n 7 Adelie  Torgersen           38.9\n 8 Adelie  Torgersen           39.2\n 9 Adelie  Torgersen           34.1\n10 Adelie  Torgersen           42  \n# i 334 more rows\n\n# Select range of columns\npenguins %&gt;%\n  select(species:body_mass_g) %&gt;%\n  head(3)\n\n# A tibble: 3 x 6\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n\n# Exclude columns\npenguins %&gt;%\n  select(-year, -sex) %&gt;%\n  head(3)\n\n# A tibble: 3 x 6\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n\n# Select with helpers\npenguins %&gt;%\n  select(starts_with(\"bill\")) %&gt;%\n  head(3)\n\n# A tibble: 3 x 2\n  bill_length_mm bill_depth_mm\n           &lt;dbl&gt;         &lt;dbl&gt;\n1           39.1          18.7\n2           39.5          17.4\n3           40.3          18  \n\npenguins %&gt;%\n  select(ends_with(\"mm\")) %&gt;%\n  head(3)\n\n# A tibble: 3 x 3\n  bill_length_mm bill_depth_mm flipper_length_mm\n           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;\n1           39.1          18.7               181\n2           39.5          17.4               186\n3           40.3          18                 195\n\npenguins %&gt;%\n  select(contains(\"length\")) %&gt;%\n  head(3)\n\n# A tibble: 3 x 2\n  bill_length_mm flipper_length_mm\n           &lt;dbl&gt;             &lt;int&gt;\n1           39.1               181\n2           39.5               186\n3           40.3               195\n\n# Select and rename\npenguins %&gt;%\n  select(\n    penguin_species = species,\n    island_name = island,\n    bill_len = bill_length_mm\n  ) %&gt;%\n  head(3)\n\n# A tibble: 3 x 3\n  penguin_species island_name bill_len\n  &lt;fct&gt;           &lt;fct&gt;          &lt;dbl&gt;\n1 Adelie          Torgersen       39.1\n2 Adelie          Torgersen       39.5\n3 Adelie          Torgersen       40.3\n\n# Reorder columns\npenguins %&gt;%\n  select(island, species, everything()) %&gt;%\n  head(3)\n\n# A tibble: 3 x 8\n  island    species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;     &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Torgersen Adelie            39.1          18.7               181        3750\n2 Torgersen Adelie            39.5          17.4               186        3800\n3 Torgersen Adelie            40.3          18                 195        3250\n# i 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n3. mutate() - Create or Transform Columns\n\n# Create new columns\npenguins %&gt;%\n  mutate(\n    bill_ratio = bill_length_mm / bill_depth_mm,\n    body_mass_kg = body_mass_g / 1000,\n    size_category = case_when(\n      body_mass_g &lt; 3500 ~ \"Small\",\n      body_mass_g &lt; 4500 ~ \"Medium\",\n      TRUE ~ \"Large\"\n    )\n  ) %&gt;%\n  select(species, bill_ratio, body_mass_kg, size_category) %&gt;%\n  head(10)\n\n# A tibble: 10 x 4\n   species bill_ratio body_mass_kg size_category\n   &lt;fct&gt;        &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;        \n 1 Adelie        2.09         3.75 Medium       \n 2 Adelie        2.27         3.8  Medium       \n 3 Adelie        2.24         3.25 Small        \n 4 Adelie       NA           NA    Large        \n 5 Adelie        1.90         3.45 Small        \n 6 Adelie        1.91         3.65 Medium       \n 7 Adelie        2.19         3.62 Medium       \n 8 Adelie        2            4.68 Large        \n 9 Adelie        1.88         3.48 Small        \n10 Adelie        2.08         4.25 Medium       \n\n# Modify existing columns\npenguins %&gt;%\n  mutate(\n    species = str_to_upper(species),\n    island = factor(island, levels = c(\"Biscoe\", \"Dream\", \"Torgersen\"))\n  ) %&gt;%\n  head(3)\n\n# A tibble: 3 x 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 ADELIE  Torgersen           39.1          18.7               181        3750\n2 ADELIE  Torgersen           39.5          17.4               186        3800\n3 ADELIE  Torgersen           40.3          18                 195        3250\n# i 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n# Multiple transformations\nflights %&gt;%\n  mutate(\n    speed = distance / air_time * 60,  # miles per hour\n    dep_delay_hours = dep_delay / 60,\n    on_time = if_else(dep_delay &lt;= 0, \"On Time\", \"Delayed\"),\n    delay_category = cut(\n      dep_delay,\n      breaks = c(-Inf, 0, 15, 60, Inf),\n      labels = c(\"Early\", \"On Time\", \"Minor Delay\", \"Major Delay\")\n    )\n  ) %&gt;%\n  select(carrier, flight, speed, on_time, delay_category) %&gt;%\n  drop_na() %&gt;%\n  head(10)\n\n# A tibble: 10 x 5\n   carrier flight speed on_time delay_category\n   &lt;chr&gt;    &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;fct&gt;         \n 1 UA        1545  370. Delayed On Time       \n 2 UA        1714  374. Delayed On Time       \n 3 AA        1141  408. Delayed On Time       \n 4 B6         725  517. On Time Early         \n 5 DL         461  394. On Time Early         \n 6 UA        1696  288. On Time Early         \n 7 B6         507  404. On Time Early         \n 8 EV        5708  259. On Time Early         \n 9 B6          79  405. On Time Early         \n10 AA         301  319. On Time Early         \n\n\n\n\n4. arrange() - Sort Rows\n\n# Simple sorting\npenguins %&gt;%\n  arrange(bill_length_mm) %&gt;%\n  select(species, bill_length_mm) %&gt;%\n  head(5)\n\n# A tibble: 5 x 2\n  species bill_length_mm\n  &lt;fct&gt;            &lt;dbl&gt;\n1 Adelie            32.1\n2 Adelie            33.1\n3 Adelie            33.5\n4 Adelie            34  \n5 Adelie            34.1\n\n# Descending order\npenguins %&gt;%\n  arrange(desc(body_mass_g)) %&gt;%\n  select(species, body_mass_g) %&gt;%\n  head(5)\n\n# A tibble: 5 x 2\n  species body_mass_g\n  &lt;fct&gt;         &lt;int&gt;\n1 Gentoo         6300\n2 Gentoo         6050\n3 Gentoo         6000\n4 Gentoo         6000\n5 Gentoo         5950\n\n# Multiple columns\npenguins %&gt;%\n  arrange(species, desc(body_mass_g)) %&gt;%\n  select(species, island, body_mass_g) %&gt;%\n  head(10)\n\n# A tibble: 10 x 3\n   species island    body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;int&gt;\n 1 Adelie  Biscoe           4775\n 2 Adelie  Biscoe           4725\n 3 Adelie  Torgersen        4700\n 4 Adelie  Torgersen        4675\n 5 Adelie  Dream            4650\n 6 Adelie  Dream            4600\n 7 Adelie  Biscoe           4600\n 8 Adelie  Torgersen        4500\n 9 Adelie  Dream            4475\n10 Adelie  Torgersen        4450\n\n# Handle NAs\npenguins %&gt;%\n  arrange(desc(is.na(sex)), sex, body_mass_g) %&gt;%\n  select(sex, body_mass_g) %&gt;%\n  head(10)\n\n# A tibble: 10 x 2\n   sex   body_mass_g\n   &lt;fct&gt;       &lt;int&gt;\n 1 &lt;NA&gt;         2975\n 2 &lt;NA&gt;         3300\n 3 &lt;NA&gt;         3475\n 4 &lt;NA&gt;         3700\n 5 &lt;NA&gt;         4100\n 6 &lt;NA&gt;         4250\n 7 &lt;NA&gt;         4650\n 8 &lt;NA&gt;         4725\n 9 &lt;NA&gt;         4875\n10 &lt;NA&gt;           NA\n\n\n\n\n5. summarise() - Aggregate Data\n\n# Basic summary\npenguins %&gt;%\n  summarise(\n    count = n(),\n    avg_mass = mean(body_mass_g, na.rm = TRUE),\n    sd_mass = sd(body_mass_g, na.rm = TRUE),\n    min_mass = min(body_mass_g, na.rm = TRUE),\n    max_mass = max(body_mass_g, na.rm = TRUE)\n  )\n\n# A tibble: 1 x 5\n  count avg_mass sd_mass min_mass max_mass\n  &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;int&gt;    &lt;int&gt;\n1   344    4202.    802.     2700     6300\n\n# Multiple statistics\nflights %&gt;%\n  summarise(\n    total_flights = n(),\n    total_distance = sum(distance, na.rm = TRUE),\n    avg_delay = mean(dep_delay, na.rm = TRUE),\n    median_delay = median(dep_delay, na.rm = TRUE),\n    pct_delayed = mean(dep_delay &gt; 0, na.rm = TRUE) * 100\n  )\n\n# A tibble: 1 x 5\n  total_flights total_distance avg_delay median_delay pct_delayed\n          &lt;int&gt;          &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1        336776      350217607      12.6           -2        39.1"
  },
  {
    "objectID": "03-data-wrangling.html#grouped-operations",
    "href": "03-data-wrangling.html#grouped-operations",
    "title": "Chapter 3: Data Wrangling with dplyr",
    "section": "Grouped Operations",
    "text": "Grouped Operations\n\ngroup_by() - The Power of Groups\n\n# Group by single variable\npenguins %&gt;%\n  group_by(species) %&gt;%\n  summarise(\n    count = n(),\n    avg_bill_length = mean(bill_length_mm, na.rm = TRUE),\n    avg_body_mass = mean(body_mass_g, na.rm = TRUE)\n  )\n\n# A tibble: 3 x 4\n  species   count avg_bill_length avg_body_mass\n  &lt;fct&gt;     &lt;int&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n1 Adelie      152            38.8         3701.\n2 Chinstrap    68            48.8         3733.\n3 Gentoo      124            47.5         5076.\n\n# Group by multiple variables\npenguins %&gt;%\n  group_by(species, island) %&gt;%\n  summarise(\n    count = n(),\n    avg_flipper_length = mean(flipper_length_mm, na.rm = TRUE),\n    .groups = \"drop\"  # Avoid warning about grouping\n  ) %&gt;%\n  arrange(species, island)\n\n# A tibble: 5 x 4\n  species   island    count avg_flipper_length\n  &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt;              &lt;dbl&gt;\n1 Adelie    Biscoe       44               189.\n2 Adelie    Dream        56               190.\n3 Adelie    Torgersen    52               191.\n4 Chinstrap Dream        68               196.\n5 Gentoo    Biscoe      124               217.\n\n# Grouped mutate (keep all rows)\npenguins %&gt;%\n  group_by(species) %&gt;%\n  mutate(\n    mass_z_score = (body_mass_g - mean(body_mass_g, na.rm = TRUE)) / \n                   sd(body_mass_g, na.rm = TRUE),\n    mass_pct_rank = percent_rank(body_mass_g)\n  ) %&gt;%\n  select(species, body_mass_g, mass_z_score, mass_pct_rank) %&gt;%\n  arrange(species, desc(body_mass_g)) %&gt;%\n  head(10)\n\n# A tibble: 10 x 4\n# Groups:   species [1]\n   species body_mass_g mass_z_score mass_pct_rank\n   &lt;fct&gt;         &lt;int&gt;        &lt;dbl&gt;         &lt;dbl&gt;\n 1 Adelie         4775         2.34         1    \n 2 Adelie         4725         2.23         0.993\n 3 Adelie         4700         2.18         0.987\n 4 Adelie         4675         2.12         0.98 \n 5 Adelie         4650         2.07         0.973\n 6 Adelie         4600         1.96         0.96 \n 7 Adelie         4600         1.96         0.96 \n 8 Adelie         4500         1.74         0.953\n 9 Adelie         4475         1.69         0.947\n10 Adelie         4450         1.63         0.933\n\n# Complex grouped operations\nflights %&gt;%\n  group_by(carrier, month) %&gt;%\n  summarise(\n    flights = n(),\n    avg_delay = mean(dep_delay, na.rm = TRUE),\n    pct_on_time = mean(dep_delay &lt;= 0, na.rm = TRUE) * 100,\n    .groups = \"drop\"\n  ) %&gt;%\n  filter(flights &gt;= 100) %&gt;%  # Only carriers with 100+ flights per month\n  arrange(desc(pct_on_time)) %&gt;%\n  head(10)\n\n# A tibble: 10 x 5\n   carrier month flights avg_delay pct_on_time\n   &lt;chr&gt;   &lt;int&gt;   &lt;int&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n 1 US         10    1846     0.141        81.7\n 2 US          9    1698     1.96         81.5\n 3 US          2    1552     0.980        80.2\n 4 DL          1    3690     3.85         78.2\n 5 US          1    1602     1.82         77.6\n 6 US         11    1699     0.576        77.4\n 7 DL         11    3849     2.85         77.1\n 8 FL          1     328     1.97         76.5\n 9 US          3    1721     2.72         76.4\n10 DL          9    3883     5.53         76.3\n\n\n\n\nAdvanced Grouping Techniques\n\n# Group by computed values\npenguins %&gt;%\n  group_by(\n    size_class = cut(body_mass_g, \n                     breaks = quantile(body_mass_g, probs = 0:4/4, na.rm = TRUE),\n                     labels = c(\"Q1\", \"Q2\", \"Q3\", \"Q4\"),\n                     include.lowest = TRUE)\n  ) %&gt;%\n  summarise(\n    count = n(),\n    species_diversity = n_distinct(species),\n    avg_bill_length = mean(bill_length_mm, na.rm = TRUE)\n  )\n\n# A tibble: 5 x 4\n  size_class count species_diversity avg_bill_length\n  &lt;fct&gt;      &lt;int&gt;             &lt;int&gt;           &lt;dbl&gt;\n1 Q1            89                 2            39.9\n2 Q2            87                 3            43.2\n3 Q3            81                 3            44.4\n4 Q4            85                 3            48.5\n5 &lt;NA&gt;           2                 2           NaN  \n\n# Multiple grouping sets\ngapminder %&gt;%\n  filter(year %in% c(1952, 2007)) %&gt;%\n  group_by(continent, year) %&gt;%\n  summarise(\n    countries = n(),\n    total_pop = sum(pop) / 1e9,  # in billions\n    avg_lifeExp = weighted.mean(lifeExp, pop),\n    median_gdpPercap = median(gdpPercap),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(continent, year)\n\n# A tibble: 10 x 6\n   continent  year countries total_pop avg_lifeExp median_gdpPercap\n   &lt;fct&gt;     &lt;int&gt;     &lt;int&gt;     &lt;dbl&gt;       &lt;dbl&gt;            &lt;dbl&gt;\n 1 Africa     1952        52    0.238         38.8             987.\n 2 Africa     2007        52    0.930         54.6            1452.\n 3 Americas   1952        25    0.345         60.2            3048.\n 4 Americas   2007        25    0.899         75.4            8948.\n 5 Asia       1952        33    1.40          42.9            1207.\n 6 Asia       2007        33    3.81          69.4            4471.\n 7 Europe     1952        30    0.418         64.9            5142.\n 8 Europe     2007        30    0.586         77.9           28054.\n 9 Oceania    1952         2    0.0107        69.2           10298.\n10 Oceania    2007         2    0.0245        81.1           29810."
  },
  {
    "objectID": "03-data-wrangling.html#joining-tables",
    "href": "03-data-wrangling.html#joining-tables",
    "title": "Chapter 3: Data Wrangling with dplyr",
    "section": "Joining Tables",
    "text": "Joining Tables\n\nCreating Sample Data for Joins\n\n# Create sample datasets\ncustomers &lt;- tibble(\n  customer_id = 1:5,\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\"),\n  city = c(\"NYC\", \"LA\", \"Chicago\", \"Houston\", \"Phoenix\")\n)\n\norders &lt;- tibble(\n  order_id = 101:108,\n  customer_id = c(1, 2, 1, 3, 4, 2, 5, 6),  # Note: customer 6 doesn't exist\n  product = c(\"Widget\", \"Gadget\", \"Widget\", \"Doohickey\", \n              \"Gadget\", \"Widget\", \"Doohickey\", \"Widget\"),\n  amount = c(100, 200, 150, 300, 250, 175, 400, 125)\n)\n\ncustomer_categories &lt;- tibble(\n  customer_id = c(1, 2, 3, 5),  # Note: missing customer 4\n  category = c(\"Premium\", \"Standard\", \"Premium\", \"Standard\")\n)\n\nprint(\"Customers:\")\n\n[1] \"Customers:\"\n\ncustomers\n\n# A tibble: 5 x 3\n  customer_id name    city   \n        &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;  \n1           1 Alice   NYC    \n2           2 Bob     LA     \n3           3 Charlie Chicago\n4           4 Diana   Houston\n5           5 Eve     Phoenix\n\nprint(\"Orders:\")\n\n[1] \"Orders:\"\n\norders\n\n# A tibble: 8 x 4\n  order_id customer_id product   amount\n     &lt;int&gt;       &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n1      101           1 Widget       100\n2      102           2 Gadget       200\n3      103           1 Widget       150\n4      104           3 Doohickey    300\n5      105           4 Gadget       250\n6      106           2 Widget       175\n7      107           5 Doohickey    400\n8      108           6 Widget       125\n\nprint(\"Categories:\")\n\n[1] \"Categories:\"\n\ncustomer_categories\n\n# A tibble: 4 x 2\n  customer_id category\n        &lt;dbl&gt; &lt;chr&gt;   \n1           1 Premium \n2           2 Standard\n3           3 Premium \n4           5 Standard\n\n\n\n\nTypes of Joins\n\n# Inner join - only matching records\ninner_join(customers, orders, by = \"customer_id\")\n\n# A tibble: 7 x 6\n  customer_id name    city    order_id product   amount\n        &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;\n1           1 Alice   NYC          101 Widget       100\n2           1 Alice   NYC          103 Widget       150\n3           2 Bob     LA           102 Gadget       200\n4           2 Bob     LA           106 Widget       175\n5           3 Charlie Chicago      104 Doohickey    300\n6           4 Diana   Houston      105 Gadget       250\n7           5 Eve     Phoenix      107 Doohickey    400\n\n# Left join - all from left, matching from right\nleft_join(customers, orders, by = \"customer_id\")\n\n# A tibble: 7 x 6\n  customer_id name    city    order_id product   amount\n        &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;\n1           1 Alice   NYC          101 Widget       100\n2           1 Alice   NYC          103 Widget       150\n3           2 Bob     LA           102 Gadget       200\n4           2 Bob     LA           106 Widget       175\n5           3 Charlie Chicago      104 Doohickey    300\n6           4 Diana   Houston      105 Gadget       250\n7           5 Eve     Phoenix      107 Doohickey    400\n\n# Right join - all from right, matching from left\nright_join(customers, orders, by = \"customer_id\")\n\n# A tibble: 8 x 6\n  customer_id name    city    order_id product   amount\n        &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;\n1           1 Alice   NYC          101 Widget       100\n2           1 Alice   NYC          103 Widget       150\n3           2 Bob     LA           102 Gadget       200\n4           2 Bob     LA           106 Widget       175\n5           3 Charlie Chicago      104 Doohickey    300\n6           4 Diana   Houston      105 Gadget       250\n7           5 Eve     Phoenix      107 Doohickey    400\n8           6 &lt;NA&gt;    &lt;NA&gt;         108 Widget       125\n\n# Full join - all records from both\nfull_join(customers, orders, by = \"customer_id\")\n\n# A tibble: 8 x 6\n  customer_id name    city    order_id product   amount\n        &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;\n1           1 Alice   NYC          101 Widget       100\n2           1 Alice   NYC          103 Widget       150\n3           2 Bob     LA           102 Gadget       200\n4           2 Bob     LA           106 Widget       175\n5           3 Charlie Chicago      104 Doohickey    300\n6           4 Diana   Houston      105 Gadget       250\n7           5 Eve     Phoenix      107 Doohickey    400\n8           6 &lt;NA&gt;    &lt;NA&gt;         108 Widget       125\n\n# Semi join - filter left table to matching records\nsemi_join(customers, orders, by = \"customer_id\")\n\n# A tibble: 5 x 3\n  customer_id name    city   \n        &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;  \n1           1 Alice   NYC    \n2           2 Bob     LA     \n3           3 Charlie Chicago\n4           4 Diana   Houston\n5           5 Eve     Phoenix\n\n# Anti join - filter left table to non-matching records\nanti_join(customers, orders, by = \"customer_id\")\n\n# A tibble: 0 x 3\n# i 3 variables: customer_id &lt;int&gt;, name &lt;chr&gt;, city &lt;chr&gt;\n\n\n\n\nComplex Joins\n\n# Multiple join keys\nflights_weather &lt;- flights %&gt;%\n  select(year, month, day, hour, origin, dep_delay) %&gt;%\n  left_join(\n    weather %&gt;% select(year, month, day, hour, origin, temp, wind_speed),\n    by = c(\"year\", \"month\", \"day\", \"hour\", \"origin\")\n  ) %&gt;%\n  drop_na() %&gt;%\n  head(10)\n\nflights_weather\n\n# A tibble: 10 x 8\n    year month   day  hour origin dep_delay  temp wind_speed\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n 1  2013     1     1     5 EWR            2  39.0       12.7\n 2  2013     1     1     5 LGA            4  39.9       15.0\n 3  2013     1     1     5 JFK            2  39.0       15.0\n 4  2013     1     1     5 JFK           -1  39.0       15.0\n 5  2013     1     1     6 LGA           -6  39.9       16.1\n 6  2013     1     1     5 EWR           -4  39.0       12.7\n 7  2013     1     1     6 EWR           -5  37.9       11.5\n 8  2013     1     1     6 LGA           -3  39.9       16.1\n 9  2013     1     1     6 JFK           -3  37.9       13.8\n10  2013     1     1     6 LGA           -2  39.9       16.1\n\n# Chain multiple joins\ncustomer_summary &lt;- customers %&gt;%\n  left_join(orders, by = \"customer_id\") %&gt;%\n  left_join(customer_categories, by = \"customer_id\") %&gt;%\n  group_by(customer_id, name, city, category) %&gt;%\n  summarise(\n    total_orders = sum(!is.na(order_id)),\n    total_amount = sum(amount, na.rm = TRUE),\n    avg_order_value = mean(amount, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(\n    category = replace_na(category, \"Unassigned\"),\n    customer_value = case_when(\n      total_amount &gt;= 500 ~ \"High\",\n      total_amount &gt;= 200 ~ \"Medium\",\n      total_amount &gt; 0 ~ \"Low\",\n      TRUE ~ \"No Orders\"\n    )\n  )\n\ncustomer_summary\n\n# A tibble: 5 x 8\n  customer_id name    city    category total_orders total_amount avg_order_value\n        &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;           &lt;int&gt;        &lt;dbl&gt;           &lt;dbl&gt;\n1           1 Alice   NYC     Premium             2          250            125 \n2           2 Bob     LA      Standard            2          375            188.\n3           3 Charlie Chicago Premium             1          300            300 \n4           4 Diana   Houston Unassig~            1          250            250 \n5           5 Eve     Phoenix Standard            1          400            400 \n# i 1 more variable: customer_value &lt;chr&gt;"
  },
  {
    "objectID": "03-data-wrangling.html#window-functions",
    "href": "03-data-wrangling.html#window-functions",
    "title": "Chapter 3: Data Wrangling with dplyr",
    "section": "Window Functions",
    "text": "Window Functions\n\n# Ranking functions\npenguins %&gt;%\n  drop_na() %&gt;%\n  group_by(species) %&gt;%\n  mutate(\n    mass_rank = rank(body_mass_g),\n    mass_dense_rank = dense_rank(body_mass_g),\n    mass_row_number = row_number(body_mass_g),\n    mass_percent = percent_rank(body_mass_g),\n    mass_ntile = ntile(body_mass_g, 4)\n  ) %&gt;%\n  select(species, body_mass_g, starts_with(\"mass_\")) %&gt;%\n  arrange(species, body_mass_g) %&gt;%\n  head(15)\n\n# A tibble: 15 x 7\n# Groups:   species [1]\n   species body_mass_g mass_rank mass_dense_rank mass_row_number mass_percent\n   &lt;fct&gt;         &lt;int&gt;     &lt;dbl&gt;           &lt;int&gt;           &lt;int&gt;        &lt;dbl&gt;\n 1 Adelie         2850       1.5               1               1       0     \n 2 Adelie         2850       1.5               1               2       0     \n 3 Adelie         2900       4                 2               3       0.0138\n 4 Adelie         2900       4                 2               4       0.0138\n 5 Adelie         2900       4                 2               5       0.0138\n 6 Adelie         2925       6                 3               6       0.0345\n 7 Adelie         3000       7.5               4               7       0.0414\n 8 Adelie         3000       7.5               4               8       0.0414\n 9 Adelie         3050      10.5               5               9       0.0552\n10 Adelie         3050      10.5               5              10       0.0552\n11 Adelie         3050      10.5               5              11       0.0552\n12 Adelie         3050      10.5               5              12       0.0552\n13 Adelie         3075      13                 6              13       0.0828\n14 Adelie         3100      14                 7              14       0.0897\n15 Adelie         3150      16.5               8              15       0.0966\n# i 1 more variable: mass_ntile &lt;int&gt;\n\n# Lead and lag\ngapminder %&gt;%\n  filter(country == \"United States\") %&gt;%\n  arrange(year) %&gt;%\n  mutate(\n    prev_gdp = lag(gdpPercap),\n    next_gdp = lead(gdpPercap),\n    gdp_growth = (gdpPercap - prev_gdp) / prev_gdp * 100,\n    gdp_change_5yr = gdpPercap - lag(gdpPercap, 2)  # 10 year change (data is every 5 years)\n  ) %&gt;%\n  select(year, gdpPercap, prev_gdp, gdp_growth, gdp_change_5yr)\n\n# A tibble: 12 x 5\n    year gdpPercap prev_gdp gdp_growth gdp_change_5yr\n   &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;\n 1  1952    13990.      NA       NA               NA \n 2  1957    14847.   13990.       6.12            NA \n 3  1962    16173.   14847.       8.93          2183.\n 4  1967    19530.   16173.      20.8           4683.\n 5  1972    21806.   19530.      11.7           5633.\n 6  1977    24073.   21806.      10.4           4542.\n 7  1982    25010.   24073.       3.89          3204.\n 8  1987    29884.   25010.      19.5           5812.\n 9  1992    32004.   29884.       7.09          6994.\n10  1997    35767.   32004.      11.8           5883.\n11  2002    39097.   35767.       9.31          7093.\n12  2007    42952.   39097.       9.86          7184.\n\n# Cumulative functions\nflights %&gt;%\n  filter(carrier == \"AA\", month == 1, day == 1) %&gt;%\n  arrange(dep_time) %&gt;%\n  mutate(\n    cumulative_flights = row_number(),\n    cumulative_distance = cumsum(distance),\n    cumulative_avg_distance = cummean(distance),\n    cumulative_max_delay = cummax(coalesce(dep_delay, 0))\n  ) %&gt;%\n  select(dep_time, distance, dep_delay, starts_with(\"cumulative\")) %&gt;%\n  head(10)\n\n# A tibble: 10 x 7\n   dep_time distance dep_delay cumulative_flights cumulative_distance\n      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;              &lt;int&gt;               &lt;dbl&gt;\n 1      542     1089         2                  1                1089\n 2      558      733        -2                  2                1822\n 3      559     1389        -1                  3                3211\n 4      606     1085        -4                  4                4296\n 5      623     1096        13                  5                5392\n 6      628     1598        -2                  6                6990\n 7      629      733        -1                  7                7723\n 8      635     1389         0                  8                9112\n 9      656      733        -4                  9                9845\n10      656      944        -3                 10               10789\n# i 2 more variables: cumulative_avg_distance &lt;dbl&gt;, cumulative_max_delay &lt;dbl&gt;"
  },
  {
    "objectID": "03-data-wrangling.html#advanced-selection-techniques",
    "href": "03-data-wrangling.html#advanced-selection-techniques",
    "title": "Chapter 3: Data Wrangling with dplyr",
    "section": "Advanced Selection Techniques",
    "text": "Advanced Selection Techniques\n\n# Using where() for type-based selection\npenguins %&gt;%\n  select(where(is.numeric)) %&gt;%\n  head(3)\n\n# A tibble: 3 x 5\n  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g  year\n           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;int&gt;\n1           39.1          18.7               181        3750  2007\n2           39.5          17.4               186        3800  2007\n3           40.3          18                 195        3250  2007\n\npenguins %&gt;%\n  select(where(is.factor)) %&gt;%\n  head(3)\n\n# A tibble: 3 x 3\n  species island    sex   \n  &lt;fct&gt;   &lt;fct&gt;     &lt;fct&gt; \n1 Adelie  Torgersen male  \n2 Adelie  Torgersen female\n3 Adelie  Torgersen female\n\n# Custom selection functions\npenguins %&gt;%\n  select(where(~ is.numeric(.) && mean(., na.rm = TRUE) &gt; 100)) %&gt;%\n  head(3)\n\n# A tibble: 3 x 3\n  flipper_length_mm body_mass_g  year\n              &lt;int&gt;       &lt;int&gt; &lt;int&gt;\n1               181        3750  2007\n2               186        3800  2007\n3               195        3250  2007\n\n# Combining selection helpers\npenguins %&gt;%\n  select(\n    species,  # Specific column\n    starts_with(\"bill\"),  # Pattern matching\n    where(is.numeric) & contains(\"mm\")  # Combination\n  ) %&gt;%\n  head(3)\n\n# A tibble: 3 x 4\n  species bill_length_mm bill_depth_mm flipper_length_mm\n  &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;\n1 Adelie            39.1          18.7               181\n2 Adelie            39.5          17.4               186\n3 Adelie            40.3          18                 195\n\n# Using all_of() and any_of()\nimportant_cols &lt;- c(\"species\", \"island\", \"body_mass_g\")\noptional_cols &lt;- c(\"sex\", \"year\", \"nonexistent_col\")\n\npenguins %&gt;%\n  select(all_of(important_cols)) %&gt;%  # Will error if column doesn't exist\n  head(3)\n\n# A tibble: 3 x 3\n  species island    body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;           &lt;int&gt;\n1 Adelie  Torgersen        3750\n2 Adelie  Torgersen        3800\n3 Adelie  Torgersen        3250\n\npenguins %&gt;%\n  select(any_of(optional_cols)) %&gt;%  # Won't error for missing columns\n  head(3)\n\n# A tibble: 3 x 2\n  sex     year\n  &lt;fct&gt;  &lt;int&gt;\n1 male    2007\n2 female  2007\n3 female  2007"
  },
  {
    "objectID": "03-data-wrangling.html#row-wise-operations",
    "href": "03-data-wrangling.html#row-wise-operations",
    "title": "Chapter 3: Data Wrangling with dplyr",
    "section": "Row-wise Operations",
    "text": "Row-wise Operations\n\n# Create sample data with list columns\nstudent_scores &lt;- tibble(\n  student = c(\"Alice\", \"Bob\", \"Charlie\"),\n  math = c(85, 92, 78),\n  science = c(90, 88, 85),\n  english = c(88, 85, 90)\n)\n\n# Row-wise operations\nstudent_scores %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    avg_score = mean(c(math, science, english)),\n    max_score = max(c(math, science, english)),\n    min_score = min(c(math, science, english)),\n    score_range = max_score - min_score\n  ) %&gt;%\n  ungroup()  # Important to ungroup after rowwise\n\n# A tibble: 3 x 8\n  student  math science english avg_score max_score min_score score_range\n  &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 Alice      85      90      88      87.7        90        85           5\n2 Bob        92      88      85      88.3        92        85           7\n3 Charlie    78      85      90      84.3        90        78          12\n\n# Working with list columns\nnested_data &lt;- tibble(\n  group = c(\"A\", \"B\", \"C\"),\n  values = list(\n    c(1, 2, 3, 4, 5),\n    c(10, 20, 30),\n    c(100, 200, 300, 400)\n  )\n)\n\nnested_data %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    mean_value = mean(values),\n    sum_value = sum(values),\n    n_values = length(values)\n  ) %&gt;%\n  ungroup()\n\n# A tibble: 3 x 5\n  group values    mean_value sum_value n_values\n  &lt;chr&gt; &lt;list&gt;         &lt;dbl&gt;     &lt;dbl&gt;    &lt;int&gt;\n1 A     &lt;dbl [5]&gt;          3        15        5\n2 B     &lt;dbl [3]&gt;         20        60        3\n3 C     &lt;dbl [4]&gt;        250      1000        4"
  },
  {
    "objectID": "03-data-wrangling.html#combining-multiple-operations",
    "href": "03-data-wrangling.html#combining-multiple-operations",
    "title": "Chapter 3: Data Wrangling with dplyr",
    "section": "Combining Multiple Operations",
    "text": "Combining Multiple Operations\n\n# Complex data pipeline\npenguins %&gt;%\n  # Remove missing values\n  drop_na() %&gt;%\n  # Add computed columns\n  mutate(\n    bill_ratio = bill_length_mm / bill_depth_mm,\n    flipper_body_ratio = flipper_length_mm / (body_mass_g / 100)\n  ) %&gt;%\n  # Group and summarize\n  group_by(species, island) %&gt;%\n  summarise(\n    n_penguins = n(),\n    avg_bill_ratio = mean(bill_ratio),\n    sd_bill_ratio = sd(bill_ratio),\n    avg_flipper_ratio = mean(flipper_body_ratio),\n    .groups = \"drop\"\n  ) %&gt;%\n  # Filter results\n  filter(n_penguins &gt;= 5) %&gt;%\n  # Add rankings\n  mutate(\n    bill_ratio_rank = dense_rank(desc(avg_bill_ratio)),\n    flipper_ratio_rank = dense_rank(desc(avg_flipper_ratio))\n  ) %&gt;%\n  # Sort results\n  arrange(bill_ratio_rank)\n\n# A tibble: 5 x 8\n  species   island    n_penguins avg_bill_ratio sd_bill_ratio avg_flipper_ratio\n  &lt;fct&gt;     &lt;fct&gt;          &lt;int&gt;          &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;\n1 Gentoo    Biscoe           119           3.18         0.169              4.30\n2 Chinstrap Dream             68           2.65         0.147              5.29\n3 Adelie    Biscoe            44           2.13         0.139              5.17\n4 Adelie    Torgersen         47           2.12         0.194              5.23\n5 Adelie    Dream             55           2.12         0.132              5.20\n# i 2 more variables: bill_ratio_rank &lt;int&gt;, flipper_ratio_rank &lt;int&gt;"
  },
  {
    "objectID": "03-data-wrangling.html#working-with-databases",
    "href": "03-data-wrangling.html#working-with-databases",
    "title": "Chapter 3: Data Wrangling with dplyr",
    "section": "Working with Databases",
    "text": "Working with Databases\n\n# Create in-memory SQLite database\nlibrary(DBI)\nlibrary(RSQLite)\n\ncon &lt;- dbConnect(RSQLite::SQLite(), \":memory:\")\n\n# Copy data to database\ncopy_to(con, penguins, \"penguins\", overwrite = TRUE)\ncopy_to(con, flights, \"flights\", overwrite = TRUE)\n\n# Create table references\npenguins_db &lt;- tbl(con, \"penguins\")\nflights_db &lt;- tbl(con, \"flights\")\n\n# dplyr operations are translated to SQL\npenguins_db %&gt;%\n  filter(species == \"Adelie\") %&gt;%\n  group_by(island) %&gt;%\n  summarise(\n    count = n(),\n    avg_mass = mean(body_mass_g, na.rm = TRUE)\n  ) %&gt;%\n  show_query()  # Show the SQL query\n\n&lt;SQL&gt;\nSELECT `island`, COUNT(*) AS `count`, AVG(`body_mass_g`) AS `avg_mass`\nFROM (\n  SELECT `penguins`.*\n  FROM `penguins`\n  WHERE (`species` = 'Adelie')\n) AS `q01`\nGROUP BY `island`\n\n# Execute and collect results\nresult &lt;- penguins_db %&gt;%\n  filter(bill_length_mm &gt; 45) %&gt;%\n  select(species, island, bill_length_mm, body_mass_g) %&gt;%\n  collect()  # Bring into R\n\nresult\n\n# A tibble: 165 x 4\n   species island    bill_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           46          4200\n 2 Adelie  Torgersen           45.8        4150\n 3 Adelie  Biscoe              45.6        4600\n 4 Gentoo  Biscoe              46.1        4500\n 5 Gentoo  Biscoe              50          5700\n 6 Gentoo  Biscoe              48.7        4450\n 7 Gentoo  Biscoe              50          5700\n 8 Gentoo  Biscoe              47.6        5400\n 9 Gentoo  Biscoe              46.5        4550\n10 Gentoo  Biscoe              45.4        4800\n# i 155 more rows\n\n# Disconnect\ndbDisconnect(con)"
  },
  {
    "objectID": "03-data-wrangling.html#performance-tips",
    "href": "03-data-wrangling.html#performance-tips",
    "title": "Chapter 3: Data Wrangling with dplyr",
    "section": "Performance Tips",
    "text": "Performance Tips\n\n# Use data.table for very large datasets\n# install.packages(\"dtplyr\")\nlibrary(dtplyr)\n\n# Convert to data.table backend\npenguins_dt &lt;- lazy_dt(penguins)\n\n# Operations are optimized for speed\nresult &lt;- penguins_dt %&gt;%\n  filter(species == \"Adelie\") %&gt;%\n  group_by(island) %&gt;%\n  summarise(avg_mass = mean(body_mass_g, na.rm = TRUE)) %&gt;%\n  as_tibble()  # Convert back to tibble\n\nresult\n\n# A tibble: 3 x 2\n  island    avg_mass\n  &lt;fct&gt;        &lt;dbl&gt;\n1 Biscoe       3710.\n2 Dream        3688.\n3 Torgersen    3706.\n\n# Benchmarking example\nif (require(microbenchmark, quietly = TRUE)) {\n  # Create larger dataset\n  big_data &lt;- penguins %&gt;%\n    slice_sample(n = 10000, replace = TRUE)\n  \n  microbenchmark(\n    dplyr = big_data %&gt;%\n      group_by(species) %&gt;%\n      summarise(mean_mass = mean(body_mass_g, na.rm = TRUE)),\n    \n    dtplyr = lazy_dt(big_data) %&gt;%\n      group_by(species) %&gt;%\n      summarise(mean_mass = mean(body_mass_g, na.rm = TRUE)) %&gt;%\n      as_tibble(),\n    \n    times = 10\n  )\n}\n\nUnit: milliseconds\n   expr      min       lq     mean   median       uq      max neval\n  dplyr 1.022499 1.048903 1.168574 1.084286 1.157102 1.857669    10\n dtplyr 1.844754 1.881162 2.014654 1.939095 2.098380 2.406905    10"
  },
  {
    "objectID": "03-data-wrangling.html#exercises",
    "href": "03-data-wrangling.html#exercises",
    "title": "Chapter 3: Data Wrangling with dplyr",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Complex Filtering\nUsing the flights dataset, find all flights that: - Departed from JFK - Were delayed by more than 30 minutes - Flew more than 1000 miles - Occurred in summer months (June, July, August)\n\n# Your solution\nflights %&gt;%\n  filter(\n    origin == \"JFK\",\n    dep_delay &gt; 30,\n    distance &gt; 1000,\n    month %in% c(6, 7, 8)\n  ) %&gt;%\n  select(carrier, flight, dest, dep_delay, distance, month) %&gt;%\n  arrange(desc(dep_delay)) %&gt;%\n  head(10)\n\n# A tibble: 10 x 6\n   carrier flight dest  dep_delay distance month\n   &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n 1 DL        2007 PDX         899     2454     6\n 2 DL         503 SAN         790     2446     6\n 3 VX         411 LAX         634     2475     7\n 4 VX          23 SFO         629     2586     7\n 5 DL         141 SFO         589     2586     7\n 6 DL        1543 SEA         504     2422     6\n 7 DL        1643 SEA         471     2422     7\n 8 B6         415 SFO         453     2586     7\n 9 DL         503 SAN         452     2446     7\n10 DL        1373 MIA         436     1089     8\n\n\n\n\nExercise 2: Advanced Grouping\nUsing gapminder data, calculate for each continent and decade: - Number of countries - Total population - Average life expectancy (weighted by population) - GDP per capita range (max - min)\n\n# Your solution\ngapminder %&gt;%\n  mutate(decade = floor(year / 10) * 10) %&gt;%\n  group_by(continent, decade) %&gt;%\n  summarise(\n    n_countries = n_distinct(country),\n    total_pop = sum(pop) / 1e9,  # in billions\n    avg_lifeExp = weighted.mean(lifeExp, pop),\n    gdp_range = max(gdpPercap) - min(gdpPercap),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(continent, decade)\n\n# A tibble: 30 x 6\n   continent decade n_countries total_pop avg_lifeExp gdp_range\n   &lt;fct&gt;      &lt;dbl&gt;       &lt;int&gt;     &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n 1 Africa      1950          52     0.502        39.9     5188.\n 2 Africa      1960          52     0.632        44.2    18418.\n 3 Africa      1970          52     0.813        48.3    21487.\n 4 Africa      1980          52     1.07         52.0    16974.\n 5 Africa      1990          52     1.40         53.3    14411.\n 6 Africa      2000          52     1.76         54.0    12965.\n 7 Americas    1950          25     0.732        61.2    13449.\n 8 Americas    1960          25     0.914        64.0    18078.\n 9 Americas    1970          25     1.11         66.7    22418.\n10 Americas    1980          25     1.31         69.8    28061.\n# i 20 more rows\n\n\n\n\nExercise 3: Window Functions\nFor each penguin species, identify the top 3 heaviest penguins and show their rank within their species:\n\n# Your solution\npenguins %&gt;%\n  drop_na(body_mass_g) %&gt;%\n  group_by(species) %&gt;%\n  mutate(\n    mass_rank = dense_rank(desc(body_mass_g))\n  ) %&gt;%\n  filter(mass_rank &lt;= 3) %&gt;%\n  select(species, island, body_mass_g, mass_rank) %&gt;%\n  arrange(species, mass_rank)\n\n# A tibble: 10 x 4\n# Groups:   species [3]\n   species   island    body_mass_g mass_rank\n   &lt;fct&gt;     &lt;fct&gt;           &lt;int&gt;     &lt;int&gt;\n 1 Adelie    Biscoe           4775         1\n 2 Adelie    Biscoe           4725         2\n 3 Adelie    Torgersen        4700         3\n 4 Chinstrap Dream            4800         1\n 5 Chinstrap Dream            4550         2\n 6 Chinstrap Dream            4500         3\n 7 Gentoo    Biscoe           6300         1\n 8 Gentoo    Biscoe           6050         2\n 9 Gentoo    Biscoe           6000         3\n10 Gentoo    Biscoe           6000         3\n\n\n\n\nExercise 4: Complex Joins\nCreate a summary showing: - Each airline carrier - Total number of flights - Average delay - Most common destination - Weather conditions for their most delayed flight\n\n# Your solution\ncarrier_summary &lt;- flights %&gt;%\n  group_by(carrier) %&gt;%\n  summarise(\n    total_flights = n(),\n    avg_delay = mean(dep_delay, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\nmost_common_dest &lt;- flights %&gt;%\n  count(carrier, dest) %&gt;%\n  group_by(carrier) %&gt;%\n  slice_max(n, n = 1) %&gt;%\n  select(carrier, most_common_dest = dest)\n\nmost_delayed &lt;- flights %&gt;%\n  group_by(carrier) %&gt;%\n  slice_max(dep_delay, n = 1) %&gt;%\n  select(carrier, year, month, day, hour, origin, max_delay = dep_delay)\n\n# Combine all information\nfinal_summary &lt;- carrier_summary %&gt;%\n  left_join(most_common_dest, by = \"carrier\") %&gt;%\n  left_join(most_delayed, by = \"carrier\") %&gt;%\n  left_join(\n    weather %&gt;% \n      select(year, month, day, hour, origin, temp, wind_speed, precip),\n    by = c(\"year\", \"month\", \"day\", \"hour\", \"origin\")\n  ) %&gt;%\n  select(carrier, total_flights, avg_delay, most_common_dest, \n         max_delay, temp, wind_speed, precip) %&gt;%\n  arrange(desc(avg_delay))\n\nhead(final_summary, 10)\n\n# A tibble: 10 x 8\n   carrier total_flights avg_delay most_common_dest max_delay  temp wind_speed\n   &lt;chr&gt;           &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n 1 F9                685      20.2 DEN                    853  23         8.06\n 2 EV              54173      20.0 IAD                    548  53.6       0   \n 3 YV                601      19.0 IAD                    387  64.0       9.21\n 4 FL               3260      18.7 ATL                    602  78.8      13.8 \n 5 WN              12275      17.7 MDW                    471  66.9      10.4 \n 6 9E              18460      16.7 CVG                    747  34.0      15.0 \n 7 B6              54635      13.0 FLL                    502  33.8       8.06\n 8 VX               5162      12.9 LAX                    653  89.1       9.21\n 9 OO                 32      12.6 CLE                    154  77         0   \n10 UA              58665      12.1 ORD                    483  81.0      11.5 \n# i 1 more variable: precip &lt;dbl&gt;"
  },
  {
    "objectID": "03-data-wrangling.html#summary",
    "href": "03-data-wrangling.html#summary",
    "title": "Chapter 3: Data Wrangling with dplyr",
    "section": "Summary",
    "text": "Summary\nYou’ve mastered the essential dplyr functions:\n✅ Core verbs: filter, select, mutate, arrange, summarise\n✅ Grouped operations with group_by\n✅ All types of joins\n✅ Window functions for advanced calculations\n✅ Row-wise operations\n✅ Database connections with dplyr"
  },
  {
    "objectID": "03-data-wrangling.html#whats-next",
    "href": "03-data-wrangling.html#whats-next",
    "title": "Chapter 3: Data Wrangling with dplyr",
    "section": "What’s Next?",
    "text": "What’s Next?\nIn Chapter 4, we’ll learn how to reshape and tidy messy data using tidyr, including pivoting, nesting, and handling missing values."
  },
  {
    "objectID": "03-data-wrangling.html#additional-resources",
    "href": "03-data-wrangling.html#additional-resources",
    "title": "Chapter 3: Data Wrangling with dplyr",
    "section": "Additional Resources",
    "text": "Additional Resources\n\ndplyr Documentation\ndplyr Cheat Sheet\nR for Data Science - Data Transformation\nWindow Functions Vignette"
  },
  {
    "objectID": "18-model-deployment.html",
    "href": "18-model-deployment.html",
    "title": "Chapter 18: Model Deployment and Production - From Prototype to Product",
    "section": "",
    "text": "By the end of this chapter, you will master:\n\nThe journey from development to production\nModel serialization and versioning\nCreating APIs with plumber\nBuilding Shiny applications for model deployment\nDocker containerization for R models\nModel monitoring and maintenance\nA/B testing and gradual rollouts\nBest practices for production ML systems"
  },
  {
    "objectID": "18-model-deployment.html#learning-objectives",
    "href": "18-model-deployment.html#learning-objectives",
    "title": "Chapter 18: Model Deployment and Production - From Prototype to Product",
    "section": "",
    "text": "By the end of this chapter, you will master:\n\nThe journey from development to production\nModel serialization and versioning\nCreating APIs with plumber\nBuilding Shiny applications for model deployment\nDocker containerization for R models\nModel monitoring and maintenance\nA/B testing and gradual rollouts\nBest practices for production ML systems"
  },
  {
    "objectID": "18-model-deployment.html#the-production-challenge",
    "href": "18-model-deployment.html#the-production-challenge",
    "title": "Chapter 18: Model Deployment and Production - From Prototype to Product",
    "section": "The Production Challenge",
    "text": "The Production Challenge\nBuilding a great model is only the beginning. The real challenge lies in deploying it to production where it can deliver value. Studies show that 87% of data science projects never make it to production. This chapter will ensure your models are in the successful 13%.\nThe journey from Jupyter notebook to production system involves many considerations: - Reliability: Will it work 24/7? - Scalability: Can it handle production load? - Maintainability: Can others understand and update it? - Monitoring: How do we know if it’s working correctly? - Versioning: How do we update without breaking things?\n\nlibrary(tidymodels)\n\n-- Attaching packages -------------------------------------- tidymodels 1.4.1 --\n\n\nv broom        1.0.10     v recipes      1.3.1 \nv dials        1.4.2      v rsample      1.3.1 \nv dplyr        1.1.4      v tailor       0.1.0 \nv ggplot2      4.0.0      v tidyr        1.3.1 \nv infer        1.0.9      v tune         2.0.0 \nv modeldata    1.5.1      v workflows    1.3.0 \nv parsnip      1.3.3      v workflowsets 1.1.1 \nv purrr        1.1.0      v yardstick    1.3.2 \n\n\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx purrr::discard() masks scales::discard()\nx dplyr::filter()  masks stats::filter()\nx dplyr::lag()     masks stats::lag()\nx recipes::step()  masks stats::step()\n\nlibrary(tidyverse)\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv forcats   1.0.0     v stringr   1.5.2\nv lubridate 1.9.4     v tibble    3.3.0\nv readr     2.1.5     \n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx readr::col_factor() masks scales::col_factor()\nx purrr::discard()    masks scales::discard()\nx dplyr::filter()     masks stats::filter()\nx stringr::fixed()    masks recipes::fixed()\nx dplyr::lag()        masks stats::lag()\nx readr::spec()       masks yardstick::spec()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(plumber)\nlibrary(pins)\nlibrary(vetiver)\n\n\nAdjuntando el paquete: 'vetiver'\n\nThe following object is masked from 'package:tune':\n\n    load_pkgs\n\nlibrary(jsonlite)\n\n\nAdjuntando el paquete: 'jsonlite'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(httr)\n\n\nAdjuntando el paquete: 'httr'\n\nThe following object is masked from 'package:pins':\n\n    cache_info\n\nlibrary(shiny)\n\n\nAdjuntando el paquete: 'shiny'\n\nThe following object is masked from 'package:jsonlite':\n\n    validate\n\nThe following object is masked from 'package:infer':\n\n    observe\n\nlibrary(shinydashboard)\n\n\nAdjuntando el paquete: 'shinydashboard'\n\nThe following object is masked from 'package:graphics':\n\n    box\n\nlibrary(DT)\n\n\nAdjuntando el paquete: 'DT'\n\nThe following objects are masked from 'package:shiny':\n\n    dataTableOutput, renderDataTable\n\n# Set theme and seed\ntheme_set(theme_minimal())\nset.seed(123)\n\n# Load and prepare example model\ndata(ames)\names_split &lt;- initial_split(ames, prop = 0.75, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test &lt;- testing(ames_split)\n\n# Create a production-ready model\n# Transform outcome before recipe\names_train_log &lt;- ames_train %&gt;%\n  mutate(Sale_Price = log(Sale_Price))\n\names_test_log &lt;- ames_test %&gt;%\n  mutate(Sale_Price = log(Sale_Price))\n\nproduction_recipe &lt;- recipe(Sale_Price ~ Gr_Liv_Area + Overall_Cond + \n                          Year_Built + Neighborhood + Total_Bsmt_SF, \n                          data = ames_train_log) %&gt;%\n  step_impute_median(all_numeric_predictors()) %&gt;%\n  step_novel(all_nominal_predictors()) %&gt;%  # Handle new categories\n  step_unknown(all_nominal_predictors()) %&gt;%  # Handle missing categories\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\nproduction_model &lt;- linear_reg(penalty = 0.01, mixture = 0.5) %&gt;%\n  set_engine(\"glmnet\")\n\nproduction_workflow &lt;- workflow() %&gt;%\n  add_recipe(production_recipe) %&gt;%\n  add_model(production_model)\n\n# Fit the model\nproduction_fit &lt;- production_workflow %&gt;%\n  fit(ames_train_log)\n\n# Evaluate\ntest_predictions &lt;- predict(production_fit, ames_test_log) %&gt;%\n  bind_cols(ames_test_log %&gt;% select(Sale_Price))\n\ntest_metrics &lt;- test_predictions %&gt;%\n  metrics(Sale_Price, .pred)\n\ncat(\"Model Performance:\\n\")\n\nModel Performance:\n\nprint(test_metrics)\n\n# A tibble: 3 x 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.147\n2 rsq     standard       0.870\n3 mae     standard       0.107"
  },
  {
    "objectID": "18-model-deployment.html#model-serialization-and-storage",
    "href": "18-model-deployment.html#model-serialization-and-storage",
    "title": "Chapter 18: Model Deployment and Production - From Prototype to Product",
    "section": "Model Serialization and Storage",
    "text": "Model Serialization and Storage\n\nSaving Models Locally\n\n# Method 1: Base R serialization\n# Create directory if it doesn't exist\ndir.create(\"models\", showWarnings = FALSE)\nsaveRDS(production_fit, \"models/ames_model_v1.rds\")\n\n# Load it back\nloaded_model &lt;- readRDS(\"models/ames_model_v1.rds\")\n\n# Verify it works\ntest_pred &lt;- predict(loaded_model, ames_test_log %&gt;% slice(1:5))\nprint(test_pred)\n\n# A tibble: 5 x 1\n  .pred\n  &lt;dbl&gt;\n1  11.7\n2  11.9\n3  12.1\n4  12.1\n5  12.3\n\n# Method 2: Using bundle package for better portability\nlibrary(bundle)\n\n# Bundle the model (includes necessary metadata)\nmodel_bundle &lt;- bundle(production_fit)\nsaveRDS(model_bundle, \"models/ames_model_bundle_v1.rds\")\n\n# Unbundle when loading\nloaded_bundle &lt;- readRDS(\"models/ames_model_bundle_v1.rds\")\nunbundled_model &lt;- unbundle(loaded_bundle)\n\n\n\nVersion Control with pins\n\n# Create a board for model storage\nlibrary(pins)\n\n# Local board (can also use S3, Azure, etc.)\nmodel_board &lt;- board_folder(\"models/pins\", versioned = TRUE)\n\n# Pin the model with metadata\nmodel_board %&gt;%\n  pin_write(\n    production_fit,\n    name = \"ames_price_model\",\n    type = \"rds\",\n    title = \"Ames Housing Price Model\",\n    description = \"Elastic net model for predicting house prices\",\n    metadata = list(\n      metrics = test_metrics,\n      training_date = Sys.Date(),\n      features = c(\"Gr_Liv_Area\", \"Overall_Cond\", \"Year_Built\", \n                  \"Neighborhood\", \"Total_Bsmt_SF\"),\n      model_type = \"elastic_net\",\n      package_versions = sessionInfo()\n    )\n  )\n\n# List available versions\nmodel_board %&gt;%\n  pin_versions(\"ames_price_model\")\n\n# A tibble: 5 x 3\n  version                created             hash \n  &lt;chr&gt;                  &lt;dttm&gt;              &lt;chr&gt;\n1 20250927T131955Z-95ee1 2025-09-27 15:19:55 95ee1\n2 20250927T143618Z-06c96 2025-09-27 16:36:18 06c96\n3 20251001T111314Z-b12c5 2025-10-01 13:13:14 b12c5\n4 20251001T112218Z-76001 2025-10-01 13:22:18 76001\n5 20251001T112538Z-3e8e8 2025-10-01 13:25:38 3e8e8\n\n# Load specific version\nretrieved_model &lt;- model_board %&gt;%\n  pin_read(\"ames_price_model\")\n\n\n\nUsing vetiver for Model Deployment\n\nlibrary(vetiver)\n\n# Create a vetiver model\nv &lt;- vetiver_model(\n  production_fit,\n  \"ames_price_predictor\",\n  metadata = list(\n    description = \"Predicts house prices in Ames, Iowa\",\n    features = c(\"Gr_Liv_Area\", \"Overall_Cond\", \"Year_Built\", \n                \"Neighborhood\", \"Total_Bsmt_SF\"),\n    target = \"Sale_Price\"\n  )\n)\n\n# Store with version\nmodel_board %&gt;%\n  vetiver_pin_write(v)\n\n# Create model card documentation\nmodel_card &lt;- \"\n# Ames Housing Price Model\n\n## Model Details\n- **Type**: Elastic Net Regression\n- **Version**: 1.0.0\n- **Training Date**: 2024-01-01\n- **Author**: Data Science Team\n\n## Intended Use\nPredicts sale prices for residential properties in Ames, Iowa.\n\n## Training Data\n- **Source**: Ames Housing Dataset\n- **Size**: 2,930 properties\n- **Time Period**: 2006-2010\n\n## Performance Metrics\n- **RMSE**: $25,432\n- **R-squared**: 0.89\n- **MAE**: $18,234\n\n## Limitations\n- Only applies to Ames, Iowa market\n- May not generalize to luxury homes (&gt;$500k)\n- Requires neighborhood information\n\n## Ethical Considerations\n- Model should not be used for discriminatory pricing\n- Regular audits for fairness across neighborhoods\n\"\n\ncat(model_card)\n\n\n# Ames Housing Price Model\n\n## Model Details\n- **Type**: Elastic Net Regression\n- **Version**: 1.0.0\n- **Training Date**: 2024-01-01\n- **Author**: Data Science Team\n\n## Intended Use\nPredicts sale prices for residential properties in Ames, Iowa.\n\n## Training Data\n- **Source**: Ames Housing Dataset\n- **Size**: 2,930 properties\n- **Time Period**: 2006-2010\n\n## Performance Metrics\n- **RMSE**: $25,432\n- **R-squared**: 0.89\n- **MAE**: $18,234\n\n## Limitations\n- Only applies to Ames, Iowa market\n- May not generalize to luxury homes (&gt;$500k)\n- Requires neighborhood information\n\n## Ethical Considerations\n- Model should not be used for discriminatory pricing\n- Regular audits for fairness across neighborhoods"
  },
  {
    "objectID": "18-model-deployment.html#creating-rest-apis-with-plumber",
    "href": "18-model-deployment.html#creating-rest-apis-with-plumber",
    "title": "Chapter 18: Model Deployment and Production - From Prototype to Product",
    "section": "Creating REST APIs with Plumber",
    "text": "Creating REST APIs with Plumber\n\nBasic API Setup\n\n# Create plumber API file\napi_code &lt;- '\n#* @apiTitle Ames House Price Prediction API\n#* @apiDescription Predicts house prices using machine learning\n#* @apiVersion 1.0.0\n\nlibrary(tidymodels)\nlibrary(vetiver)\n\n# Load model\nmodel &lt;- readRDS(\"models/ames_model_v1.rds\")\n\n#* Health check endpoint\n#* @get /health\nfunction() {\n  list(\n    status = \"healthy\",\n    timestamp = Sys.time(),\n    model_version = \"1.0.0\"\n  )\n}\n\n#* Predict house price\n#* @param Gr_Liv_Area:numeric Living area in square feet\n#* @param Overall_Cond:numeric Overall quality (1-10)\n#* @param Year_Built:numeric Year built\n#* @param Neighborhood:character Neighborhood name\n#* @param Total_Bsmt_SF:numeric Basement square feet\n#* @post /predict\nfunction(Gr_Liv_Area, Overall_Cond, Year_Built, Neighborhood, Total_Bsmt_SF) {\n  \n  # Input validation\n  if (is.na(as.numeric(Gr_Liv_Area))) {\n    stop(\"Gr_Liv_Area must be numeric\")\n  }\n  \n  # Create input data frame\n  input_data &lt;- data.frame(\n    Gr_Liv_Area = as.numeric(Gr_Liv_Area),\n    Overall_Cond = as.numeric(Overall_Cond),\n    Year_Built = as.numeric(Year_Built),\n    Neighborhood = as.character(Neighborhood),\n    Total_Bsmt_SF = as.numeric(Total_Bsmt_SF)\n  )\n  \n  # Make prediction\n  prediction &lt;- predict(model, input_data)\n  \n  # Return result\n  list(\n    predicted_price = prediction$.pred,\n    input = input_data,\n    model_version = \"1.0.0\",\n    timestamp = Sys.time()\n  )\n}\n\n#* Batch predictions\n#* @param data:character JSON string of multiple houses\n#* @post /predict_batch\nfunction(data) {\n  # Parse JSON\n  input_data &lt;- jsonlite::fromJSON(data)\n  \n  # Make predictions\n  predictions &lt;- predict(model, input_data)\n  \n  # Return results\n  list(\n    predictions = predictions$.pred,\n    n_predictions = nrow(predictions),\n    timestamp = Sys.time()\n  )\n}\n\n#* Model information\n#* @get /model_info\nfunction() {\n  list(\n    model_type = \"elastic_net\",\n    features = c(\"Gr_Liv_Area\", \"Overall_Cond\", \"Year_Built\", \n                \"Neighborhood\", \"Total_Bsmt_SF\"),\n    target = \"Sale_Price\",\n    training_date = \"2024-01-01\",\n    performance = list(\n      rmse = 25432,\n      r_squared = 0.89\n    )\n  )\n}\n'\n\n# Save API file\ndir.create(\"api\", showWarnings = FALSE)\nwriteLines(api_code, \"api/model_api.R\")\n\n# To run the API (don't run in notebook):\n# library(plumber)\n# pr(\"api/model_api.R\") %&gt;%\n#   pr_run(port = 8000)\n\n\n\nAdvanced API with Authentication and Logging\n\nadvanced_api &lt;- '\n#* @apiTitle Production House Price API\n#* @apiDescription Enterprise-grade prediction service\n\nlibrary(tidymodels)\nlibrary(logger)\nlibrary(jose)\n\n# Setup logging\nlog_appender(appender_file(\"logs/api.log\"))\n\n# Load model and config\nmodel &lt;- readRDS(\"models/ames_model_v1.rds\")\napi_keys &lt;- readRDS(\"config/api_keys.rds\")  # In production, use env variables\n\n# Request counter for rate limiting\nrequest_counts &lt;- new.env()\n\n#* @filter cors\ncors &lt;- function(req, res) {\n  res$setHeader(\"Access-Control-Allow-Origin\", \"*\")\n  res$setHeader(\"Access-Control-Allow-Methods\", \"GET, POST\")\n  res$setHeader(\"Access-Control-Allow-Headers\", \"Content-Type, X-API-Key\")\n  plumber::forward()\n}\n\n#* @filter authenticate\nfunction(req, res) {\n  # Check API key\n  api_key &lt;- req$HTTP_X_API_KEY\n  \n  if (is.null(api_key) || !api_key %in% api_keys) {\n    res$status &lt;- 401\n    log_warn(\"Unauthorized access attempt\")\n    return(list(error = \"Invalid or missing API key\"))\n  }\n  \n  # Rate limiting\n  if (!exists(api_key, envir = request_counts)) {\n    assign(api_key, list(count = 1, reset_time = Sys.time() + 3600), \n           envir = request_counts)\n  } else {\n    rate_info &lt;- get(api_key, envir = request_counts)\n    if (Sys.time() &gt; rate_info$reset_time) {\n      rate_info &lt;- list(count = 1, reset_time = Sys.time() + 3600)\n    } else if (rate_info$count &gt; 100) {  # 100 requests per hour\n      res$status &lt;- 429\n      return(list(error = \"Rate limit exceeded\"))\n    } else {\n      rate_info$count &lt;- rate_info$count + 1\n    }\n    assign(api_key, rate_info, envir = request_counts)\n  }\n  \n  plumber::forward()\n}\n\n#* Predict with comprehensive logging\n#* @post /predict\nfunction(req, Gr_Liv_Area, Overall_Cond, Year_Built, Neighborhood, Total_Bsmt_SF) {\n  \n  request_id &lt;- uuid::UUIDgenerate()\n  log_info(\"Prediction request\", request_id = request_id)\n  \n  tryCatch({\n    # Input validation\n    input_data &lt;- data.frame(\n      Gr_Liv_Area = as.numeric(Gr_Liv_Area),\n      Overall_Cond = as.numeric(Overall_Cond),\n      Year_Built = as.numeric(Year_Built),\n      Neighborhood = as.character(Neighborhood),\n      Total_Bsmt_SF = as.numeric(Total_Bsmt_SF)\n    )\n    \n    # Data quality checks\n    if (input_data$Gr_Liv_Area &lt; 0 || input_data$Gr_Liv_Area &gt; 10000) {\n      stop(\"Gr_Liv_Area out of valid range\")\n    }\n    \n    if (input_data$Overall_Cond &lt; 1 || input_data$Overall_Cond &gt; 10) {\n      stop(\"Overall_Cond must be between 1 and 10\")\n    }\n    \n    # Make prediction\n    start_time &lt;- Sys.time()\n    prediction &lt;- predict(model, input_data)\n    inference_time &lt;- as.numeric(Sys.time() - start_time, units = \"secs\")\n    \n    # Log successful prediction\n    log_info(\"Prediction successful\", \n             request_id = request_id,\n             predicted_value = prediction$.pred,\n             inference_time = inference_time)\n    \n    # Return result\n    list(\n      request_id = request_id,\n      predicted_price = prediction$.pred,\n      confidence_interval = c(\n        lower = prediction$.pred * 0.9,  # Simplified CI\n        upper = prediction$.pred * 1.1\n      ),\n      input = input_data,\n      inference_time_ms = round(inference_time * 1000, 2),\n      model_version = \"1.0.0\",\n      timestamp = Sys.time()\n    )\n    \n  }, error = function(e) {\n    log_error(\"Prediction failed\", \n              request_id = request_id, \n              error = e$message)\n    res$status &lt;- 400\n    list(\n      request_id = request_id,\n      error = e$message\n    )\n  })\n}\n'\n\nwriteLines(advanced_api, \"api/advanced_api.R\")"
  },
  {
    "objectID": "18-model-deployment.html#building-shiny-applications",
    "href": "18-model-deployment.html#building-shiny-applications",
    "title": "Chapter 18: Model Deployment and Production - From Prototype to Product",
    "section": "Building Shiny Applications",
    "text": "Building Shiny Applications\n\nBasic Prediction App\n\n# Create Shiny app for model deployment\nshiny_app &lt;- '\nlibrary(shiny)\nlibrary(shinydashboard)\nlibrary(tidymodels)\nlibrary(ggplot2)\nlibrary(DT)\n\n# Load model\nmodel &lt;- readRDS(\"models/ames_model_v1.rds\")\n\n# UI\nui &lt;- dashboardPage(\n  dashboardHeader(title = \"House Price Predictor\"),\n  \n  dashboardSidebar(\n    sidebarMenu(\n      menuItem(\"Predict\", tabName = \"predict\", icon = icon(\"calculator\")),\n      menuItem(\"Batch Upload\", tabName = \"batch\", icon = icon(\"upload\")),\n      menuItem(\"Model Info\", tabName = \"info\", icon = icon(\"info-circle\")),\n      menuItem(\"Performance\", tabName = \"performance\", icon = icon(\"chart-line\"))\n    )\n  ),\n  \n  dashboardBody(\n    tabItems(\n      # Prediction tab\n      tabItem(\n        tabName = \"predict\",\n        fluidRow(\n          box(\n            title = \"Input Features\",\n            status = \"primary\",\n            solidHeader = TRUE,\n            width = 6,\n            \n            numericInput(\"gr_liv_area\", \"Living Area (sq ft)\", \n                        value = 1500, min = 0, max = 10000),\n            \n            sliderInput(\"overall_qual\", \"Overall Quality\", \n                       min = 1, max = 10, value = 5),\n            \n            numericInput(\"year_built\", \"Year Built\", \n                        value = 2000, min = 1900, max = 2024),\n            \n            selectInput(\"neighborhood\", \"Neighborhood\",\n                       choices = unique(ames_train$Neighborhood),\n                       selected = \"NAmes\"),\n            \n            numericInput(\"total_bsmt_sf\", \"Basement Area (sq ft)\", \n                        value = 1000, min = 0, max = 5000),\n            \n            actionButton(\"predict\", \"Predict Price\", \n                        class = \"btn-primary btn-lg\")\n          ),\n          \n          box(\n            title = \"Prediction Result\",\n            status = \"success\",\n            solidHeader = TRUE,\n            width = 6,\n            \n            h2(textOutput(\"predicted_price\")),\n            \n            plotOutput(\"confidence_plot\", height = 200),\n            \n            br(),\n            \n            h4(\"Input Summary:\"),\n            tableOutput(\"input_summary\")\n          )\n        )\n      ),\n      \n      # Batch upload tab\n      tabItem(\n        tabName = \"batch\",\n        fluidRow(\n          box(\n            title = \"Upload CSV File\",\n            status = \"primary\",\n            solidHeader = TRUE,\n            width = 12,\n            \n            fileInput(\"file\", \"Choose CSV File\",\n                     accept = c(\".csv\")),\n            \n            actionButton(\"predict_batch\", \"Predict All\", \n                        class = \"btn-success\"),\n            \n            br(), br(),\n            \n            DTOutput(\"batch_results\")\n          )\n        )\n      ),\n      \n      # Model info tab\n      tabItem(\n        tabName = \"info\",\n        fluidRow(\n          box(\n            title = \"Model Information\",\n            status = \"info\",\n            solidHeader = TRUE,\n            width = 12,\n            \n            h3(\"Model Type: Elastic Net Regression\"),\n            \n            h4(\"Features Used:\"),\n            tags$ul(\n              tags$li(\"Living Area (Gr_Liv_Area)\"),\n              tags$li(\"Overall Quality (Overall_Cond)\"),\n              tags$li(\"Year Built (Year_Built)\"),\n              tags$li(\"Neighborhood\"),\n              tags$li(\"Basement Area (Total_Bsmt_SF)\")\n            ),\n            \n            h4(\"Model Performance:\"),\n            tags$ul(\n              tags$li(\"RMSE: $25,432\"),\n              tags$li(\"R-squared: 0.89\"),\n              tags$li(\"MAE: $18,234\")\n            ),\n            \n            h4(\"Training Information:\"),\n            tags$ul(\n              tags$li(\"Training Date: 2024-01-01\"),\n              tags$li(\"Training Samples: 2,197\"),\n              tags$li(\"Validation Method: 5-fold CV\")\n            )\n          )\n        )\n      ),\n      \n      # Performance monitoring tab\n      tabItem(\n        tabName = \"performance\",\n        fluidRow(\n          box(\n            title = \"Model Performance Monitoring\",\n            status = \"warning\",\n            solidHeader = TRUE,\n            width = 12,\n            \n            plotOutput(\"performance_plot\", height = 400),\n            \n            br(),\n            \n            h4(\"Recent Predictions:\"),\n            DTOutput(\"recent_predictions\")\n          )\n        )\n      )\n    )\n  )\n)\n\n# Server\nserver &lt;- function(input, output, session) {\n  \n  # Store predictions for monitoring\n  predictions_log &lt;- reactiveVal(data.frame())\n  \n  # Single prediction\n  observeEvent(input$predict, {\n    \n    # Create input data\n    input_data &lt;- data.frame(\n      Gr_Liv_Area = input$gr_liv_area,\n      Overall_Cond = input$overall_qual,\n      Year_Built = input$year_built,\n      Neighborhood = input$neighborhood,\n      Total_Bsmt_SF = input$total_bsmt_sf\n    )\n    \n    # Make prediction\n    prediction &lt;- predict(model, input_data)\n    \n    # Update predictions log\n    new_log &lt;- rbind(\n      predictions_log(),\n      data.frame(\n        timestamp = Sys.time(),\n        predicted = prediction$.pred,\n        living_area = input$gr_liv_area,\n        quality = input$overall_qual\n      )\n    )\n    predictions_log(new_log)\n    \n    # Display prediction\n    output$predicted_price &lt;- renderText({\n      paste0(\"$\", format(round(prediction$.pred), big.mark = \",\"))\n    })\n    \n    # Confidence interval plot\n    output$confidence_plot &lt;- renderPlot({\n      ci_lower &lt;- prediction$.pred * 0.9\n      ci_upper &lt;- prediction$.pred * 1.1\n      \n      ggplot(data.frame(x = c(ci_lower, prediction$.pred, ci_upper))) +\n        geom_segment(aes(x = ci_lower, xend = ci_upper, y = 0, yend = 0),\n                    size = 2, color = \"steelblue\") +\n        geom_point(aes(x = prediction$.pred, y = 0), \n                  size = 5, color = \"darkblue\") +\n        scale_x_continuous(labels = scales::dollar) +\n        theme_minimal() +\n        theme(axis.text.y = element_blank(),\n              axis.title.y = element_blank()) +\n        labs(x = \"Predicted Price\",\n             title = \"90% Confidence Interval\")\n    })\n    \n    # Input summary\n    output$input_summary &lt;- renderTable({\n      data.frame(\n        Feature = c(\"Living Area\", \"Quality\", \"Year Built\", \n                   \"Neighborhood\", \"Basement\"),\n        Value = c(\n          paste(input$gr_liv_area, \"sq ft\"),\n          paste(input$overall_qual, \"/ 10\"),\n          input$year_built,\n          input$neighborhood,\n          paste(input$total_bsmt_sf, \"sq ft\")\n        )\n      )\n    })\n  })\n  \n  # Batch predictions\n  observeEvent(input$predict_batch, {\n    req(input$file)\n    \n    # Read uploaded file\n    batch_data &lt;- read.csv(input$file$datapath)\n    \n    # Make predictions\n    predictions &lt;- predict(model, batch_data)\n    \n    # Combine with input\n    results &lt;- cbind(batch_data, Predicted_Price = predictions$.pred)\n    \n    # Display results\n    output$batch_results &lt;- renderDT({\n      datatable(results, options = list(pageLength = 10))\n    })\n  })\n  \n  # Performance monitoring\n  output$performance_plot &lt;- renderPlot({\n    if (nrow(predictions_log()) &gt; 0) {\n      ggplot(predictions_log(), aes(x = timestamp, y = predicted)) +\n        geom_line() +\n        geom_point() +\n        scale_y_continuous(labels = scales::dollar) +\n        theme_minimal() +\n        labs(title = \"Predictions Over Time\",\n             x = \"Time\",\n             y = \"Predicted Price\")\n    }\n  })\n  \n  output$recent_predictions &lt;- renderDT({\n    if (nrow(predictions_log()) &gt; 0) {\n      recent &lt;- tail(predictions_log(), 10)\n      recent$predicted &lt;- scales::dollar(round(recent$predicted))\n      datatable(recent, options = list(pageLength = 5))\n    }\n  })\n}\n\n# Run app\nshinyApp(ui = ui, server = server)\n'\n\n# Save Shiny app\ndir.create(\"apps\", showWarnings = FALSE)\nwriteLines(shiny_app, \"apps/prediction_app.R\")"
  },
  {
    "objectID": "18-model-deployment.html#docker-containerization",
    "href": "18-model-deployment.html#docker-containerization",
    "title": "Chapter 18: Model Deployment and Production - From Prototype to Product",
    "section": "Docker Containerization",
    "text": "Docker Containerization\n\nCreating a Dockerfile\n\ndockerfile_content &lt;- '\n# Base R image\nFROM rocker/r-ver:4.3.0\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\\\n    libcurl4-openssl-dev \\\\\n    libssl-dev \\\\\n    libxml2-dev \\\\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install R packages\nRUN R -e \"install.packages(c(\\'tidymodels\\', \\'plumber\\', \\'jsonlite\\'), repos=\\'https://cloud.r-project.org/\\')\"\n\n# Create app directory\nWORKDIR /app\n\n# Copy model and API files\nCOPY models/ames_model_v1.rds /app/model.rds\nCOPY api/model_api.R /app/api.R\n\n# Expose port\nEXPOSE 8000\n\n# Run API\nCMD [\"R\", \"-e\", \"plumber::pr(\\'api.R\\') %&gt;% plumber::pr_run(host=\\'0.0.0.0\\', port=8000)\"]\n'\n\ndir.create(\"docker\", showWarnings = FALSE)\nwriteLines(dockerfile_content, \"docker/Dockerfile\")\n\n# Docker compose for multi-container deployment\ndocker_compose &lt;- '\nversion: \"3.8\"\n\nservices:\n  api:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      - MODEL_VERSION=1.0.0\n    volumes:\n      - ./models:/app/models\n      - ./logs:/app/logs\n    restart: unless-stopped\n    \n  shiny:\n    image: rocker/shiny:4.3.0\n    ports:\n      - \"3838:3838\"\n    volumes:\n      - ./apps:/srv/shiny-server/\n      - ./models:/srv/shiny-server/models\n    restart: unless-stopped\n    \n  nginx:\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf\n    depends_on:\n      - api\n      - shiny\n    restart: unless-stopped\n'\n\nwriteLines(docker_compose, \"docker/docker-compose.yml\")"
  },
  {
    "objectID": "18-model-deployment.html#model-monitoring-and-maintenance",
    "href": "18-model-deployment.html#model-monitoring-and-maintenance",
    "title": "Chapter 18: Model Deployment and Production - From Prototype to Product",
    "section": "Model Monitoring and Maintenance",
    "text": "Model Monitoring and Maintenance\n\nPerformance Tracking\n\n# Create monitoring system\ncreate_monitoring_system &lt;- function(model, production_data) {\n  \n  # Calculate current metrics\n  current_predictions &lt;- predict(model, production_data)\n  current_metrics &lt;- production_data %&gt;%\n    bind_cols(current_predictions) %&gt;%\n    metrics(truth = Sale_Price, estimate = .pred)\n  \n  # Compare with baseline\n  baseline_metrics &lt;- tibble(\n    .metric = c(\"rmse\", \"rsq\", \"mae\"),\n    baseline = c(25432, 0.89, 18234)\n  )\n  \n  # Join metrics\n  comparison &lt;- current_metrics %&gt;%\n    left_join(baseline_metrics, by = \".metric\") %&gt;%\n    mutate(\n      degradation = (.estimate - baseline) / baseline * 100,\n      alert = abs(degradation) &gt; 10  # Alert if &gt;10% degradation\n    )\n  \n  return(comparison)\n}\n\n# Simulate production data with drift\nproduction_sample &lt;- ames_test_log %&gt;%\n  mutate(\n    # Simulate data drift - keep as integer\n    Gr_Liv_Area = as.integer(Gr_Liv_Area * runif(n(), 0.9, 1.1)),\n    Year_Built = as.integer(Year_Built + sample(-5:5, n(), replace = TRUE))\n  )\n\nmonitoring_results &lt;- create_monitoring_system(production_fit, production_sample)\nknitr::kable(monitoring_results, digits = 2)\n\n\n\n\n.metric\n.estimator\n.estimate\nbaseline\ndegradation\nalert\n\n\n\n\nrmse\nstandard\n0.15\n25432.00\n-100.0\nTRUE\n\n\nrsq\nstandard\n0.86\n0.89\n-3.2\nFALSE\n\n\nmae\nstandard\n0.11\n18234.00\n-100.0\nTRUE\n\n\n\n\n# Visualize model performance over time\nsimulate_performance_timeline &lt;- function(n_days = 30) {\n  timeline &lt;- map_df(1:n_days, function(day) {\n    # Simulate daily performance with random variation\n    daily_rmse &lt;- 25432 + rnorm(1, mean = day * 50, sd = 1000)  # Gradual degradation\n    daily_requests &lt;- rpois(1, lambda = 1000)\n    \n    tibble(\n      date = Sys.Date() - n_days + day,\n      rmse = daily_rmse,\n      requests = daily_requests,\n      alert = daily_rmse &gt; 28000  # Alert threshold\n    )\n  })\n  \n  return(timeline)\n}\n\nperformance_timeline &lt;- simulate_performance_timeline()\n\n# Plot monitoring dashboard\np1 &lt;- ggplot(performance_timeline, aes(x = date, y = rmse)) +\n  geom_line(linewidth = 1) +\n  geom_point(aes(color = alert), size = 2) +\n  geom_hline(yintercept = 28000, linetype = \"dashed\", color = \"red\") +\n  scale_color_manual(values = c(\"FALSE\" = \"steelblue\", \"TRUE\" = \"red\")) +\n  labs(title = \"Model RMSE Over Time\",\n       subtitle = \"Red line indicates alert threshold\",\n       y = \"RMSE\") +\n  theme(legend.position = \"none\")\n\np2 &lt;- ggplot(performance_timeline, aes(x = date, y = requests)) +\n  geom_col(fill = \"steelblue\", alpha = 0.7) +\n  labs(title = \"Daily Request Volume\",\n       y = \"Requests\")\n\nlibrary(patchwork)\np1 / p2\n\n\n\n\n\n\n\n\n\n\nData Drift Detection\n\n# Detect feature drift\ndetect_drift &lt;- function(training_data, production_data, threshold = 0.1) {\n  \n  drift_results &lt;- map_df(names(training_data), function(col) {\n    if (is.numeric(training_data[[col]])) {\n      # Kolmogorov-Smirnov test for continuous variables\n      ks_test &lt;- ks.test(training_data[[col]], production_data[[col]])\n      \n      tibble(\n        feature = col,\n        test_type = \"KS\",\n        p_value = ks_test$p.value,\n        drift_detected = ks_test$p.value &lt; threshold,\n        train_mean = mean(training_data[[col]], na.rm = TRUE),\n        prod_mean = mean(production_data[[col]], na.rm = TRUE),\n        mean_shift = abs(train_mean - prod_mean) / train_mean\n      )\n    } else {\n      # Chi-square test for categorical variables\n      train_prop &lt;- table(training_data[[col]]) / nrow(training_data)\n      prod_prop &lt;- table(production_data[[col]]) / nrow(production_data)\n      \n      # Ensure same categories\n      all_cats &lt;- union(names(train_prop), names(prod_prop))\n      train_prop &lt;- train_prop[all_cats]\n      prod_prop &lt;- prod_prop[all_cats]\n      train_prop[is.na(train_prop)] &lt;- 0\n      prod_prop[is.na(prod_prop)] &lt;- 0\n      \n      chi_test &lt;- chisq.test(rbind(train_prop, prod_prop))\n      \n      tibble(\n        feature = col,\n        test_type = \"Chi-square\",\n        p_value = chi_test$p.value,\n        drift_detected = chi_test$p.value &lt; threshold,\n        train_mean = NA_real_,\n        prod_mean = NA_real_,\n        mean_shift = NA_real_\n      )\n    }\n  })\n  \n  return(drift_results)\n}\n\n# Test drift detection\ntraining_sample &lt;- ames_train %&gt;%\n  select(Gr_Liv_Area, Overall_Cond, Year_Built, Neighborhood, Total_Bsmt_SF) %&gt;%\n  slice_sample(n = 200)\n\nproduction_sample_drift &lt;- training_sample %&gt;%\n  mutate(\n    Gr_Liv_Area = Gr_Liv_Area * 1.2,  # Introduce drift\n    Year_Built = Year_Built + 10       # Houses are newer\n  )\n\ndrift_analysis &lt;- detect_drift(training_sample, production_sample_drift)\nknitr::kable(drift_analysis, digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nfeature\ntest_type\np_value\ndrift_detected\ntrain_mean\nprod_mean\nmean_shift\n\n\n\n\nGr_Liv_Area\nKS\n0\nTRUE\n1496.695\n1796.034\n0.200\n\n\nOverall_Cond\nChi-square\nNaN\nNA\nNA\nNA\nNA\n\n\nYear_Built\nKS\n0\nTRUE\n1970.110\n1980.110\n0.005\n\n\nNeighborhood\nChi-square\nNaN\nNA\nNA\nNA\nNA\n\n\nTotal_Bsmt_SF\nKS\n1\nFALSE\n1008.070\n1008.070\n0.000\n\n\n\n\n# Visualize drift\ndrift_viz &lt;- training_sample %&gt;%\n  mutate(dataset = \"Training\") %&gt;%\n  bind_rows(production_sample_drift %&gt;% mutate(dataset = \"Production\"))\n\nggplot(drift_viz, aes(x = Gr_Liv_Area, fill = dataset)) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Feature Drift: Living Area\",\n       subtitle = \"Distribution shift between training and production\")"
  },
  {
    "objectID": "18-model-deployment.html#ab-testing-and-gradual-rollouts",
    "href": "18-model-deployment.html#ab-testing-and-gradual-rollouts",
    "title": "Chapter 18: Model Deployment and Production - From Prototype to Product",
    "section": "A/B Testing and Gradual Rollouts",
    "text": "A/B Testing and Gradual Rollouts\n\n# Implement A/B testing framework\nab_test_framework &lt;- function(model_a, model_b, test_data, split_ratio = 0.5) {\n  \n  n &lt;- nrow(test_data)\n  \n  # Random assignment to groups\n  assignment &lt;- sample(c(\"A\", \"B\"), n, replace = TRUE, \n                      prob = c(split_ratio, 1 - split_ratio))\n  \n  # Make predictions with each model\n  results &lt;- test_data %&gt;%\n    mutate(\n      group = assignment,\n      prediction = if_else(\n        group == \"A\",\n        predict(model_a, test_data)$.pred,\n        predict(model_b, test_data)$.pred\n      ),\n      error = abs(Sale_Price - prediction)\n    )\n  \n  # Calculate metrics per group\n  group_metrics &lt;- results %&gt;%\n    group_by(group) %&gt;%\n    summarise(\n      n = n(),\n      rmse = sqrt(mean(error^2)),\n      mae = mean(error),\n      median_error = median(error),\n      .groups = \"drop\"\n    )\n  \n  # Statistical test\n  t_test &lt;- t.test(error ~ group, data = results)\n  \n  return(list(\n    metrics = group_metrics,\n    test = t_test,\n    winner = if_else(t_test$p.value &lt; 0.05,\n                    if_else(group_metrics$rmse[1] &lt; group_metrics$rmse[2], \n                           \"Model A\", \"Model B\"),\n                    \"No significant difference\")\n  ))\n}\n\n# Create alternative model for testing\nalternative_model &lt;- workflow() %&gt;%\n  add_recipe(production_recipe) %&gt;%\n  add_model(rand_forest(trees = 100) %&gt;% \n           set_engine(\"ranger\") %&gt;% \n           set_mode(\"regression\")) %&gt;%\n  fit(ames_train_log)\n\n# Run A/B test\nab_results &lt;- ab_test_framework(production_fit, alternative_model, ames_test_log)\n\ncat(\"A/B Test Results:\\n\")\n\nA/B Test Results:\n\nprint(ab_results$metrics)\n\n# A tibble: 2 x 5\n  group     n  rmse   mae median_error\n  &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;\n1 A       368 0.152 0.109       0.0757\n2 B       365 0.151 0.109       0.0838\n\ncat(\"\\nWinner:\", ab_results$winner, \"\\n\")\n\n\nWinner: No significant difference \n\ncat(\"P-value:\", ab_results$test$p.value, \"\\n\")\n\nP-value: 0.9844975 \n\n# Gradual rollout simulation\nsimulate_gradual_rollout &lt;- function(days = 14) {\n  rollout_schedule &lt;- tibble(\n    day = 1:days,\n    traffic_percentage = pmin(100, 5 * 1.5^(day - 1)),  # Exponential increase\n    errors_detected = rpois(days, lambda = 100 - traffic_percentage) / 10,\n    rollback = errors_detected &gt; 5\n  )\n  \n  # Find rollback point if any\n  rollback_day &lt;- which(rollout_schedule$rollback)[1]\n  if (!is.na(rollback_day)) {\n    rollout_schedule$traffic_percentage[rollback_day:days] &lt;- 0\n  }\n  \n  return(rollout_schedule)\n}\n\nrollout &lt;- simulate_gradual_rollout()\n\nggplot(rollout, aes(x = day)) +\n  geom_area(aes(y = traffic_percentage), fill = \"steelblue\", alpha = 0.7) +\n  geom_point(aes(y = errors_detected * 10), color = \"red\", size = 2) +\n  scale_y_continuous(\n    name = \"Traffic Percentage\",\n    sec.axis = sec_axis(~./10, name = \"Errors Detected\")\n  ) +\n  labs(title = \"Gradual Model Rollout\",\n       subtitle = \"Blue area shows traffic %, red points show errors\")"
  },
  {
    "objectID": "18-model-deployment.html#best-practices-checklist",
    "href": "18-model-deployment.html#best-practices-checklist",
    "title": "Chapter 18: Model Deployment and Production - From Prototype to Product",
    "section": "Best Practices Checklist",
    "text": "Best Practices Checklist\n\n# Production readiness checklist\nproduction_checklist &lt;- tibble(\n  Category = c(\n    rep(\"Model\", 4),\n    rep(\"Code\", 4),\n    rep(\"Infrastructure\", 4),\n    rep(\"Monitoring\", 4),\n    rep(\"Documentation\", 3)\n  ),\n  Item = c(\n    # Model\n    \"Model validated on holdout set\",\n    \"Handles missing values gracefully\",\n    \"Handles new categories\",\n    \"Performance meets requirements\",\n    \n    # Code\n    \"Code review completed\",\n    \"Unit tests written\",\n    \"Integration tests passed\",\n    \"Error handling implemented\",\n    \n    # Infrastructure\n    \"API endpoints defined\",\n    \"Load testing completed\",\n    \"Backup strategy in place\",\n    \"Rollback plan documented\",\n    \n    # Monitoring\n    \"Logging configured\",\n    \"Alerts set up\",\n    \"Performance metrics tracked\",\n    \"Data drift detection enabled\",\n    \n    # Documentation\n    \"API documentation complete\",\n    \"Model card created\",\n    \"Runbook available\"\n  ),\n  Status = c(\n    \"✅\", \"✅\", \"✅\", \"✅\",  # Model\n    \"✅\", \"✅\", \"⚠️\", \"✅\",  # Code\n    \"✅\", \"⚠️\", \"✅\", \"✅\",  # Infrastructure\n    \"✅\", \"✅\", \"⚠️\", \"❌\",  # Monitoring\n    \"✅\", \"✅\", \"⚠️\"        # Documentation\n  ),\n  Priority = c(\n    \"High\", \"High\", \"High\", \"High\",\n    \"High\", \"High\", \"Medium\", \"High\",\n    \"High\", \"Medium\", \"High\", \"High\",\n    \"High\", \"High\", \"Medium\", \"Low\",\n    \"Medium\", \"High\", \"Medium\"\n  )\n)\n\nknitr::kable(production_checklist)\n\n\n\n\n\n\n\n\n\n\nCategory\nItem\nStatus\nPriority\n\n\n\n\nModel\nModel validated on holdout set\n&lt;U+2705&gt;\nHigh\n\n\nModel\nHandles missing values gracefully\n&lt;U+2705&gt;\nHigh\n\n\nModel\nHandles new categories\n&lt;U+2705&gt;\nHigh\n\n\nModel\nPerformance meets requirements\n&lt;U+2705&gt;\nHigh\n\n\nCode\nCode review completed\n&lt;U+2705&gt;\nHigh\n\n\nCode\nUnit tests written\n&lt;U+2705&gt;\nHigh\n\n\nCode\nIntegration tests passed\n&lt;U+26A0&gt;&lt;U+FE0F&gt;\nMedium\n\n\nCode\nError handling implemented\n&lt;U+2705&gt;\nHigh\n\n\nInfrastructure\nAPI endpoints defined\n&lt;U+2705&gt;\nHigh\n\n\nInfrastructure\nLoad testing completed\n&lt;U+26A0&gt;&lt;U+FE0F&gt;\nMedium\n\n\nInfrastructure\nBackup strategy in place\n&lt;U+2705&gt;\nHigh\n\n\nInfrastructure\nRollback plan documented\n&lt;U+2705&gt;\nHigh\n\n\nMonitoring\nLogging configured\n&lt;U+2705&gt;\nHigh\n\n\nMonitoring\nAlerts set up\n&lt;U+2705&gt;\nHigh\n\n\nMonitoring\nPerformance metrics tracked\n&lt;U+26A0&gt;&lt;U+FE0F&gt;\nMedium\n\n\nMonitoring\nData drift detection enabled\n&lt;U+274C&gt;\nLow\n\n\nDocumentation\nAPI documentation complete\n&lt;U+2705&gt;\nMedium\n\n\nDocumentation\nModel card created\n&lt;U+2705&gt;\nHigh\n\n\nDocumentation\nRunbook available\n&lt;U+26A0&gt;&lt;U+FE0F&gt;\nMedium\n\n\n\n\n# Calculate readiness score\nreadiness_score &lt;- sum(production_checklist$Status == \"✅\") / \n                  nrow(production_checklist) * 100\n\ncat(\"\\nProduction Readiness Score:\", round(readiness_score, 1), \"%\\n\")\n\n\nProduction Readiness Score: 73.7 %\n\n# Priority action items\naction_items &lt;- production_checklist %&gt;%\n  filter(Status != \"✅\") %&gt;%\n  arrange(match(Priority, c(\"High\", \"Medium\", \"Low\")))\n\ncat(\"\\nAction Items:\\n\")\n\n\nAction Items:\n\nprint(action_items)\n\n# A tibble: 5 x 4\n  Category       Item                         Status           Priority\n  &lt;chr&gt;          &lt;chr&gt;                        &lt;chr&gt;            &lt;chr&gt;   \n1 Code           Integration tests passed     &lt;U+26A0&gt;&lt;U+FE0F&gt; Medium  \n2 Infrastructure Load testing completed       &lt;U+26A0&gt;&lt;U+FE0F&gt; Medium  \n3 Monitoring     Performance metrics tracked  &lt;U+26A0&gt;&lt;U+FE0F&gt; Medium  \n4 Documentation  Runbook available            &lt;U+26A0&gt;&lt;U+FE0F&gt; Medium  \n5 Monitoring     Data drift detection enabled &lt;U+274C&gt;         Low"
  },
  {
    "objectID": "18-model-deployment.html#exercises",
    "href": "18-model-deployment.html#exercises",
    "title": "Chapter 18: Model Deployment and Production - From Prototype to Product",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Create a Model Registry\n\n# Your solution\n# Implement a model registry system\ncreate_model_registry &lt;- function() {\n  \n  # Registry structure\n  registry &lt;- list(\n    models = list(),\n    metadata = list(),\n    performance = list()\n  )\n  \n  # Register model function\n  register_model &lt;- function(model, name, version, metrics, metadata = list()) {\n    model_id &lt;- paste0(name, \"_v\", version)\n    \n    registry$models[[model_id]] &lt;&lt;- model\n    registry$metadata[[model_id]] &lt;&lt;- c(\n      list(\n        name = name,\n        version = version,\n        registration_date = Sys.Date()\n      ),\n      metadata\n    )\n    registry$performance[[model_id]] &lt;&lt;- metrics\n    \n    cat(\"Model\", model_id, \"registered successfully\\n\")\n  }\n  \n  # Get model function\n  get_model &lt;- function(name, version = \"latest\") {\n    if (version == \"latest\") {\n      # Find latest version\n      all_versions &lt;- grep(paste0(\"^\", name, \"_v\"), \n                          names(registry$models), value = TRUE)\n      if (length(all_versions) == 0) {\n        stop(\"Model not found\")\n      }\n      model_id &lt;- all_versions[length(all_versions)]\n    } else {\n      model_id &lt;- paste0(name, \"_v\", version)\n    }\n    \n    if (!model_id %in% names(registry$models)) {\n      stop(\"Model version not found\")\n    }\n    \n    return(list(\n      model = registry$models[[model_id]],\n      metadata = registry$metadata[[model_id]],\n      performance = registry$performance[[model_id]]\n    ))\n  }\n  \n  # Compare models function\n  compare_models &lt;- function(name) {\n    all_versions &lt;- grep(paste0(\"^\", name, \"_v\"), \n                        names(registry$models), value = TRUE)\n    \n    comparison &lt;- map_df(all_versions, function(model_id) {\n      perf &lt;- registry$performance[[model_id]]\n      meta &lt;- registry$metadata[[model_id]]\n      \n      tibble(\n        version = meta$version,\n        date = meta$registration_date,\n        rmse = perf$rmse,\n        rsq = perf$rsq\n      )\n    })\n    \n    return(comparison)\n  }\n  \n  # Return registry functions\n  list(\n    register = register_model,\n    get = get_model,\n    compare = compare_models\n  )\n}\n\n# Use the registry\nregistry &lt;- create_model_registry()\n\n# Register models\nregistry$register(\n  production_fit, \n  \"ames_predictor\", \n  \"1.0.0\",\n  list(rmse = 25432, rsq = 0.89),\n  list(algorithm = \"elastic_net\")\n)\n\nModel ames_predictor_v1.0.0 registered successfully\n\nregistry$register(\n  alternative_model, \n  \"ames_predictor\", \n  \"1.1.0\",\n  list(rmse = 24800, rsq = 0.91),\n  list(algorithm = \"random_forest\")\n)\n\nModel ames_predictor_v1.1.0 registered successfully\n\n# Compare versions\ncomparison &lt;- registry$compare(\"ames_predictor\")\nprint(comparison)\n\n# A tibble: 2 x 4\n  version date        rmse   rsq\n  &lt;chr&gt;   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 1.0.0   2025-10-01 25432  0.89\n2 1.1.0   2025-10-01 24800  0.91\n\n\n\n\nExercise 2: Implement Model Retraining Pipeline\n\n# Your solution\n# Automated retraining pipeline\ncreate_retraining_pipeline &lt;- function(current_model, retraining_threshold = 0.1) {\n  \n  # Monitor performance\n  monitor &lt;- function(model, new_data) {\n    predictions &lt;- predict(model, new_data)\n    metrics &lt;- new_data %&gt;%\n      bind_cols(predictions) %&gt;%\n      metrics(truth = Sale_Price, estimate = .pred)\n    \n    return(metrics)\n  }\n  \n  # Check if retraining needed\n  should_retrain &lt;- function(current_metrics, baseline_metrics) {\n    rmse_degradation &lt;- (current_metrics$rmse - baseline_metrics$rmse) / \n                       baseline_metrics$rmse\n    \n    return(rmse_degradation &gt; retraining_threshold)\n  }\n  \n  # Retrain model\n  retrain &lt;- function(new_data) {\n    cat(\"Retraining model with\", nrow(new_data), \"samples\\n\")\n    \n    # Create new workflow\n    new_workflow &lt;- workflow() %&gt;%\n      add_recipe(production_recipe) %&gt;%\n      add_model(production_model)\n    \n    # Fit on new data\n    new_fit &lt;- new_workflow %&gt;%\n      fit(new_data)\n    \n    # Validate on holdout\n    validation_split &lt;- initial_split(new_data, prop = 0.8)\n    validation_metrics &lt;- new_fit %&gt;%\n      predict(testing(validation_split)) %&gt;%\n      bind_cols(testing(validation_split)) %&gt;%\n      metrics(truth = Sale_Price, estimate = .pred)\n    \n    return(list(\n      model = new_fit,\n      metrics = validation_metrics\n    ))\n  }\n  \n  # Pipeline execution\n  execute &lt;- function(new_data) {\n    # Current performance\n    current_metrics &lt;- monitor(current_model, new_data)\n    baseline_metrics &lt;- list(rmse = 25432, rsq = 0.89)\n    \n    cat(\"Current RMSE:\", current_metrics %&gt;% \n        filter(.metric == \"rmse\") %&gt;% \n        pull(.estimate), \"\\n\")\n    \n    if (should_retrain(\n      list(rmse = current_metrics %&gt;% \n             filter(.metric == \"rmse\") %&gt;% \n             pull(.estimate)),\n      baseline_metrics)) {\n      \n      cat(\"Performance degradation detected. Initiating retraining...\\n\")\n      retrain_result &lt;- retrain(new_data)\n      \n      cat(\"New model RMSE:\", retrain_result$metrics %&gt;% \n          filter(.metric == \"rmse\") %&gt;% \n          pull(.estimate), \"\\n\")\n      \n      return(retrain_result)\n    } else {\n      cat(\"Model performance acceptable. No retraining needed.\\n\")\n      return(NULL)\n    }\n  }\n  \n  return(execute)\n}\n\n# Test the pipeline\npipeline &lt;- create_retraining_pipeline(production_fit)\n\n# Simulate degraded performance data\ndegraded_data &lt;- ames_test_log %&gt;%\n  mutate(Sale_Price = Sale_Price * runif(n(), 0.7, 1.3))  # Add noise\n\nresult &lt;- pipeline(degraded_data)\n\nCurrent RMSE: 2.037085 \nModel performance acceptable. No retraining needed.\n\n\n\n\nExercise 3: Create Performance Dashboard\n\n# Your solution\n# Create monitoring dashboard data\ncreate_dashboard_data &lt;- function(n_days = 30) {\n  \n  # Simulate daily metrics\n  daily_data &lt;- map_df(1:n_days, function(day) {\n    tibble(\n      date = Sys.Date() - n_days + day,\n      \n      # Model metrics\n      predictions = rpois(1, 1000 + day * 10),\n      avg_response_time = rnorm(1, 50, 10),\n      error_rate = rbeta(1, 1, 100),\n      \n      # Business metrics  \n      conversion_rate = rbeta(1, 10, 90),\n      revenue_impact = rnorm(1, 10000, 2000),\n      \n      # System metrics\n      cpu_usage = rbeta(1, 20, 80),\n      memory_usage = rbeta(1, 30, 70),\n      \n      # Data quality\n      missing_features = rpois(1, 5),\n      out_of_range = rpois(1, 3)\n    )\n  })\n  \n  return(daily_data)\n}\n\ndashboard_data &lt;- create_dashboard_data()\n\n# Create dashboard visualizations\np1 &lt;- ggplot(dashboard_data, aes(x = date, y = predictions)) +\n  geom_line(color = \"steelblue\", linewidth = 1) +\n  geom_smooth(se = FALSE, color = \"red\") +\n  labs(title = \"Daily Predictions\", y = \"Count\")\n\np2 &lt;- ggplot(dashboard_data, aes(x = date, y = avg_response_time)) +\n  geom_line(color = \"darkgreen\", linewidth = 1) +\n  geom_hline(yintercept = 100, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Response Time\", y = \"Milliseconds\")\n\np3 &lt;- ggplot(dashboard_data, aes(x = date)) +\n  geom_ribbon(aes(ymin = 0, ymax = cpu_usage), fill = \"blue\", alpha = 0.3) +\n  geom_ribbon(aes(ymin = 0, ymax = memory_usage), fill = \"red\", alpha = 0.3) +\n  labs(title = \"Resource Usage\", y = \"Percentage\") +\n  scale_y_continuous(labels = scales::percent)\n\np4 &lt;- ggplot(dashboard_data, aes(x = date, y = revenue_impact)) +\n  geom_col(fill = \"gold\", alpha = 0.7) +\n  scale_y_continuous(labels = scales::dollar) +\n  labs(title = \"Revenue Impact\", y = \"Daily Revenue\")\n\n(p1 + p2) / (p3 + p4)\n\n\n\n\n\n\n\n# Summary statistics\nsummary_stats &lt;- dashboard_data %&gt;%\n  summarise(\n    total_predictions = sum(predictions),\n    avg_response_time = mean(avg_response_time),\n    total_revenue = sum(revenue_impact),\n    avg_cpu = mean(cpu_usage),\n    total_errors = sum(missing_features + out_of_range)\n  )\n\ncat(\"\\nDashboard Summary (Last 30 Days):\\n\")\n\n\nDashboard Summary (Last 30 Days):\n\ncat(\"Total Predictions:\", format(summary_stats$total_predictions, big.mark = \",\"), \"\\n\")\n\nTotal Predictions: 34,662 \n\ncat(\"Avg Response Time:\", round(summary_stats$avg_response_time, 1), \"ms\\n\")\n\nAvg Response Time: 46.7 ms\n\ncat(\"Total Revenue Impact: $\", format(round(summary_stats$total_revenue), big.mark = \",\"), \"\\n\")\n\nTotal Revenue Impact: $ 290,824 \n\ncat(\"Average CPU Usage:\", round(summary_stats$avg_cpu * 100, 1), \"%\\n\")\n\nAverage CPU Usage: 18.9 %\n\ncat(\"Total Data Errors:\", summary_stats$total_errors, \"\\n\")\n\nTotal Data Errors: 222"
  },
  {
    "objectID": "18-model-deployment.html#summary",
    "href": "18-model-deployment.html#summary",
    "title": "Chapter 18: Model Deployment and Production - From Prototype to Product",
    "section": "Summary",
    "text": "Summary\nIn this comprehensive final chapter, you’ve mastered:\n✅ Model deployment fundamentals\n\nSerialization and versioning\nModel registries\nProduction readiness\n\n✅ API development\n\nREST APIs with plumber\nAuthentication and rate limiting\nError handling and logging\n\n✅ Application development\n\nShiny dashboards\nUser interfaces\nBatch processing\n\n✅ Containerization\n\nDocker for R models\nMulti-container orchestration\nEnvironment consistency\n\n✅ Monitoring and maintenance\n\nPerformance tracking\nData drift detection\nAutomated retraining\n\n✅ Production best practices\n\nA/B testing\nGradual rollouts\nProduction checklists\n\nKey takeaways:\n\nDeployment is as important as model development\nMonitor everything in production\nVersion control is crucial for models\nAutomate retraining pipelines\nPlan for failure and rollback\nDocumentation is essential"
  },
  {
    "objectID": "18-model-deployment.html#final-assessment",
    "href": "18-model-deployment.html#final-assessment",
    "title": "Chapter 18: Model Deployment and Production - From Prototype to Product",
    "section": "Final Assessment",
    "text": "Final Assessment\nCongratulations on completing Block 3! Before we conclude this comprehensive workshop, we recommend taking our Block 3 Assessment to test your mastery of advanced machine learning concepts including classification, regression, ensemble methods, unsupervised learning, and model deployment.\nThis final assessment will help consolidate your understanding of the sophisticated techniques you’ve learned and prepare you for real-world machine learning applications."
  },
  {
    "objectID": "18-model-deployment.html#course-conclusion",
    "href": "18-model-deployment.html#course-conclusion",
    "title": "Chapter 18: Model Deployment and Production - From Prototype to Product",
    "section": "Course Conclusion",
    "text": "Course Conclusion\nCongratulations! You’ve completed a comprehensive journey through:\n\nTidyverse fundamentals and advanced techniques\nTidymodels framework for machine learning\nProduction deployment of ML systems\n\nYou now have the skills to:\n\nWrangle and visualize data efficiently\nBuild and evaluate machine learning models\nDeploy models to production systems\nMonitor and maintain ML applications\n\nRemember: The journey doesn’t end here. Continue practicing, stay curious, and keep learning!"
  },
  {
    "objectID": "18-model-deployment.html#additional-resources",
    "href": "18-model-deployment.html#additional-resources",
    "title": "Chapter 18: Model Deployment and Production - From Prototype to Product",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nEngineering Production Machine Learning Systems\nplumber Documentation\nShiny Documentation\nDocker for Data Science\nMLOps: Continuous Delivery for ML"
  },
  {
    "objectID": "11-model-specification.html",
    "href": "11-model-specification.html",
    "title": "Chapter 11: Model Specification with parsnip - A Unified Interface for Models",
    "section": "",
    "text": "By the end of this chapter, you will master:\n\nThe parsnip philosophy and design principles\nSpecifying models across different engines\nSetting modes and engines appropriately\nModel arguments and hyperparameters\nTranslating between different modeling packages\nCreating custom model specifications\nUnderstanding computational engines\nBest practices for model specification"
  },
  {
    "objectID": "11-model-specification.html#learning-objectives",
    "href": "11-model-specification.html#learning-objectives",
    "title": "Chapter 11: Model Specification with parsnip - A Unified Interface for Models",
    "section": "",
    "text": "By the end of this chapter, you will master:\n\nThe parsnip philosophy and design principles\nSpecifying models across different engines\nSetting modes and engines appropriately\nModel arguments and hyperparameters\nTranslating between different modeling packages\nCreating custom model specifications\nUnderstanding computational engines\nBest practices for model specification"
  },
  {
    "objectID": "11-model-specification.html#the-problem-parsnip-solves",
    "href": "11-model-specification.html#the-problem-parsnip-solves",
    "title": "Chapter 11: Model Specification with parsnip - A Unified Interface for Models",
    "section": "The Problem parsnip Solves",
    "text": "The Problem parsnip Solves\nOne of the most frustrating aspects of machine learning in R is the inconsistency across modeling packages. Each package has its own syntax, arguments, and output format. Let’s see this problem in action:\n\nlibrary(tidymodels)\n\n-- Attaching packages -------------------------------------- tidymodels 1.4.1 --\n\n\nv broom        1.0.10     v recipes      1.3.1 \nv dials        1.4.2      v rsample      1.3.1 \nv dplyr        1.1.4      v tailor       0.1.0 \nv ggplot2      4.0.0      v tidyr        1.3.1 \nv infer        1.0.9      v tune         2.0.0 \nv modeldata    1.5.1      v workflows    1.3.0 \nv parsnip      1.3.3      v workflowsets 1.1.1 \nv purrr        1.1.0      v yardstick    1.3.2 \n\n\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx purrr::discard() masks scales::discard()\nx dplyr::filter()  masks stats::filter()\nx dplyr::lag()     masks stats::lag()\nx recipes::step()  masks stats::step()\n\nlibrary(tidyverse)\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv forcats   1.0.0     v stringr   1.5.2\nv lubridate 1.9.4     v tibble    3.3.0\nv readr     2.1.5     \n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx readr::col_factor() masks scales::col_factor()\nx purrr::discard()    masks scales::discard()\nx dplyr::filter()     masks stats::filter()\nx stringr::fixed()    masks recipes::fixed()\nx dplyr::lag()        masks stats::lag()\nx readr::spec()       masks yardstick::spec()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(modeldata)\nlibrary(ranger)     # For random forests\nlibrary(glmnet)     # For regularized regression\n\nCargando paquete requerido: Matrix\n\nAdjuntando el paquete: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoaded glmnet 4.1-10\n\nlibrary(kknn)       # For k-nearest neighbors\nlibrary(kernlab)    # For support vector machines\n\n\nAdjuntando el paquete: 'kernlab'\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    alpha\n\nThe following object is masked from 'package:dials':\n\n    buffer\n\nThe following object is masked from 'package:scales':\n\n    alpha\n\nlibrary(xgboost)    # For gradient boosting\n\n\nAdjuntando el paquete: 'xgboost'\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\n# Set theme and seed\ntheme_set(theme_minimal())\nset.seed(123)\n\n# Load data\ndata(ames)\names_small &lt;- ames %&gt;%\n  select(Sale_Price, Gr_Liv_Area, Year_Built, Overall_Cond, Neighborhood) %&gt;%\n  slice_sample(n = 500)  # Smaller sample for demonstration\n\n# The chaos of different interfaces\n# Linear regression with lm()\nlm_fit &lt;- lm(Sale_Price ~ ., data = ames_small)\n\n# Random forest with ranger()\nrf_fit &lt;- ranger(Sale_Price ~ ., data = ames_small, num.trees = 100)\n\n# Elastic net with glmnet() - requires matrix input!\nx_matrix &lt;- model.matrix(Sale_Price ~ . - 1, data = ames_small)\ny_vector &lt;- ames_small$Sale_Price\nglmnet_fit &lt;- glmnet(x_matrix, y_vector, alpha = 0.5)\n\n# Different prediction methods\nlm_pred &lt;- predict(lm_fit, ames_small)  # Returns vector\nrf_pred &lt;- predict(rf_fit, ames_small)$predictions  # Returns list with $predictions\nglmnet_pred &lt;- predict(glmnet_fit, x_matrix, s = 0.01)  # Requires matrix and lambda\n\n# The outputs are all different!\nstr(lm_pred)\n\n Named num [1:500] 222479 219821 228298 259642 129456 ...\n - attr(*, \"names\")= chr [1:500] \"1\" \"2\" \"3\" \"4\" ...\n\nstr(rf_pred)\n\n num [1:500] 234231 188176 197626 263308 124126 ...\n\nstr(glmnet_pred)\n\n num [1:500, 1] 222179 219528 228247 259319 128907 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:500] \"1\" \"2\" \"3\" \"4\" ...\n  ..$ : chr \"s=0.01\"\n\n\nNotice the problems: - Different function names and arguments - Different data format requirements (data frame vs matrix) - Different prediction interfaces - Different output structures\nThis inconsistency makes it hard to: - Switch between models - Compare different approaches - Build reproducible workflows - Write general-purpose code"
  },
  {
    "objectID": "11-model-specification.html#enter-parsnip-one-interface-to-rule-them-all",
    "href": "11-model-specification.html#enter-parsnip-one-interface-to-rule-them-all",
    "title": "Chapter 11: Model Specification with parsnip - A Unified Interface for Models",
    "section": "Enter parsnip: One Interface to Rule Them All",
    "text": "Enter parsnip: One Interface to Rule Them All\nParsnip provides a unified interface for model specification. The philosophy is simple but powerful: 1. Separate model specification from model fitting 2. Use consistent naming across all models 3. Provide a common interface for predictions 4. Allow engine flexibility while maintaining consistency\n\nThe parsnip Workflow\n\n# Step 1: Specify the model type\nlinear_spec &lt;- linear_reg()\nprint(linear_spec)\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n# Step 2: Set the engine (implementation)\nlinear_spec &lt;- linear_spec %&gt;%\n  set_engine(\"lm\")\nprint(linear_spec)\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n# Step 3: Set the mode (if needed - regression vs classification)\n# For linear_reg, mode is always regression, so this is automatic\n\n# Step 4: Fit the model\nlinear_fit &lt;- linear_spec %&gt;%\n  fit(Sale_Price ~ ., data = ames_small)\n\n# Consistent prediction interface\nlinear_pred &lt;- predict(linear_fit, ames_small)\nstr(linear_pred)  # Always returns a tibble with .pred column\n\ntibble [500 x 1] (S3: tbl_df/tbl/data.frame)\n $ .pred: num [1:500] 222479 219821 228298 259642 129456 ...\n\n\nThe beauty of this approach: - Model specification is separate from fitting - Consistent interface across all models - Predictable output format - Easy to swap different implementations"
  },
  {
    "objectID": "11-model-specification.html#model-types-in-parsnip",
    "href": "11-model-specification.html#model-types-in-parsnip",
    "title": "Chapter 11: Model Specification with parsnip - A Unified Interface for Models",
    "section": "Model Types in parsnip",
    "text": "Model Types in parsnip\nParsnip supports a wide variety of model types. Let’s explore the main categories:\n\nRegression Models\n\n# Linear regression\nlinear_reg_spec &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\n\n# Decision tree\ntree_reg_spec &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"regression\")\n\n# Random forest\nrf_reg_spec &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\n# Support vector machine\nsvm_reg_spec &lt;- svm_rbf() %&gt;%\n  set_engine(\"kernlab\") %&gt;%\n  set_mode(\"regression\")\n\n# Neural network\nnn_reg_spec &lt;- mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\n# Show info for linear regression\nparsnip::show_model_info(\"linear_reg\")\n\nInformation for `linear_reg`\n modes: unknown, regression, quantile regression \n\n engines: \n   quantile regression: quantreg1\n   regression:          brulee, glm1, glmnet1, keras, lm1, spark1, stan1\n\n1The model can use case weights.\n\n arguments: \n   glmnet: \n      penalty --&gt; lambda\n      mixture --&gt; alpha\n   spark:  \n      penalty --&gt; reg_param\n      mixture --&gt; elastic_net_param\n   keras:  \n      penalty --&gt; penalty\n   brulee: \n      penalty --&gt; penalty\n      mixture --&gt; mixture\n\n fit modules:\n     engine                mode\n         lm          regression\n        glm          regression\n     glmnet          regression\n       stan          regression\n      spark          regression\n      keras          regression\n     brulee          regression\n   quantreg quantile regression\n\n prediction modules:\n                  mode   engine                          methods\n   quantile regression quantreg                         quantile\n            regression   brulee                          numeric\n            regression      glm           conf_int, numeric, raw\n            regression   glmnet                     numeric, raw\n            regression    keras                          numeric\n            regression       lm conf_int, numeric, pred_int, raw\n            regression    spark                          numeric\n            regression     stan conf_int, numeric, pred_int, raw\n\n\n\n\nClassification Models\n\n# Create classification data\names_class &lt;- ames %&gt;%\n  mutate(expensive = factor(if_else(Sale_Price &gt; median(Sale_Price), \n                                    \"yes\", \"no\"))) %&gt;%\n  select(expensive, Gr_Liv_Area, Year_Built, Overall_Cond) %&gt;%\n  slice_sample(n = 500)\n\n# Logistic regression\nlogistic_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\")\n\n# Random forest for classification\nrf_class_spec &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n# Support vector machine for classification\nsvm_class_spec &lt;- svm_rbf() %&gt;%\n  set_engine(\"kernlab\") %&gt;%\n  set_mode(\"classification\")\n\n# K-nearest neighbors\nknn_spec &lt;- nearest_neighbor() %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"classification\")\n\n# Naive Bayes\nnb_spec &lt;- naive_Bayes() %&gt;%\n  set_engine(\"naivebayes\") %&gt;%\n  set_mode(\"classification\")"
  },
  {
    "objectID": "11-model-specification.html#understanding-engines",
    "href": "11-model-specification.html#understanding-engines",
    "title": "Chapter 11: Model Specification with parsnip - A Unified Interface for Models",
    "section": "Understanding Engines",
    "text": "Understanding Engines\nEach model type can have multiple engines (implementations). The choice of engine affects: - Available hyperparameters - Computational efficiency - Additional features - Required packages\n\nExploring Available Engines\n\n# See all engines for a model type\nshow_engines(\"rand_forest\")\n\n# A tibble: 6 x 2\n  engine       mode          \n  &lt;chr&gt;        &lt;chr&gt;         \n1 ranger       classification\n2 ranger       regression    \n3 randomForest classification\n4 randomForest regression    \n5 spark        classification\n6 spark        regression    \n\n# Different engines for linear regression\nlinear_engines &lt;- show_engines(\"linear_reg\")\nprint(linear_engines)\n\n# A tibble: 8 x 2\n  engine   mode               \n  &lt;chr&gt;    &lt;chr&gt;              \n1 lm       regression         \n2 glm      regression         \n3 glmnet   regression         \n4 stan     regression         \n5 spark    regression         \n6 keras    regression         \n7 brulee   regression         \n8 quantreg quantile regression\n\n# Let's compare different engines for the same model\nengines_to_compare &lt;- c(\"lm\", \"glm\", \"glmnet\")\n\nlinear_comparisons &lt;- map(engines_to_compare, function(eng) {\n  # Special handling for glmnet\n  if (eng == \"glmnet\") {\n    spec &lt;- linear_reg(penalty = 0) %&gt;% set_engine(eng)  # No regularization\n  } else {\n    spec &lt;- linear_reg() %&gt;% set_engine(eng)\n  }\n  \n  # Fit the model\n  fit &lt;- spec %&gt;% fit(Sale_Price ~ Gr_Liv_Area + Overall_Cond, data = ames_small)\n  \n  # Get predictions\n  preds &lt;- predict(fit, ames_small)\n  \n  # Return summary\n  tibble(\n    engine = eng,\n    rmse = sqrt(mean((ames_small$Sale_Price - preds$.pred)^2))\n  )\n})\n\nbind_rows(linear_comparisons) %&gt;%\n  knitr::kable(digits = 2)\n\n\n\n\nengine\nrmse\n\n\n\n\nlm\n53749.53\n\n\nglm\n53749.53\n\n\nglmnet\n53751.01\n\n\n\n\n\n\n\nEngine-Specific Arguments\nDifferent engines support different arguments:\n\n# Random forest with ranger engine\nrf_ranger &lt;- rand_forest(trees = 500) %&gt;%\n  set_engine(\"ranger\",\n             importance = \"impurity\",  # ranger-specific\n             num.threads = 2)          # ranger-specific\n\n# Random forest with randomForest engine\nrf_randomForest &lt;- rand_forest(trees = 500) %&gt;%\n  set_engine(\"randomForest\",\n             nodesize = 5,             # randomForest-specific\n             maxnodes = 100)           # randomForest-specific\n\n# The model specification is the same, but engine arguments differ\nprint(rf_ranger)\n\nRandom Forest Model Specification (unknown mode)\n\nMain Arguments:\n  trees = 500\n\nEngine-Specific Arguments:\n  importance = impurity\n  num.threads = 2\n\nComputational engine: ranger \n\nprint(rf_randomForest)\n\nRandom Forest Model Specification (unknown mode)\n\nMain Arguments:\n  trees = 500\n\nEngine-Specific Arguments:\n  nodesize = 5\n  maxnodes = 100\n\nComputational engine: randomForest"
  },
  {
    "objectID": "11-model-specification.html#model-arguments-and-hyperparameters",
    "href": "11-model-specification.html#model-arguments-and-hyperparameters",
    "title": "Chapter 11: Model Specification with parsnip - A Unified Interface for Models",
    "section": "Model Arguments and Hyperparameters",
    "text": "Model Arguments and Hyperparameters\nParsnip distinguishes between: - Main arguments: Common across engines (e.g., trees for random forests) - Engine arguments: Specific to an implementation\n\nMain Arguments\n\n# Main arguments are specified in the model function\nrf_with_args &lt;- rand_forest(\n  trees = 1000,      # Number of trees\n  mtry = 3,          # Variables per split\n  min_n = 10         # Minimum node size\n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\nprint(rf_with_args)\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 3\n  trees = 1000\n  min_n = 10\n\nComputational engine: ranger \n\n# These translate to engine-specific names\ntranslate(rf_with_args)  # See the actual ranger call\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 3\n  trees = 1000\n  min_n = 10\n\nComputational engine: ranger \n\nModel fit template:\nranger::ranger(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    mtry = min_cols(~3, x), num.trees = 1000, min.node.size = min_rows(~10, \n        x), num.threads = 1, verbose = FALSE, seed = sample.int(10^5, \n        1))\n\n\n\n\nUpdating Model Specifications\nModel specifications can be updated dynamically:\n\n# Start with a basic specification\nbase_rf &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\n# Update with new values\nupdated_rf &lt;- base_rf %&gt;%\n  set_args(trees = 2000, mtry = 5)\n\nprint(updated_rf)\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 5\n  trees = 2000\n\nComputational engine: ranger \n\n# This is useful for tuning\ntunable_rf &lt;- rand_forest(\n  trees = tune(),    # Mark for tuning\n  mtry = tune(),\n  min_n = tune()\n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\nprint(tunable_rf)\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  trees = tune()\n  min_n = tune()\n\nComputational engine: ranger"
  },
  {
    "objectID": "11-model-specification.html#consistent-prediction-interface",
    "href": "11-model-specification.html#consistent-prediction-interface",
    "title": "Chapter 11: Model Specification with parsnip - A Unified Interface for Models",
    "section": "Consistent Prediction Interface",
    "text": "Consistent Prediction Interface\nOne of parsnip’s greatest strengths is consistent predictions:\n\n# Fit different models\nmodels &lt;- list(\n  linear = linear_reg() %&gt;% \n    set_engine(\"lm\") %&gt;%\n    fit(Sale_Price ~ Gr_Liv_Area + Overall_Cond, data = ames_small),\n  \n  tree = decision_tree() %&gt;%\n    set_engine(\"rpart\") %&gt;%\n    set_mode(\"regression\") %&gt;%\n    fit(Sale_Price ~ Gr_Liv_Area + Overall_Cond, data = ames_small),\n  \n  knn = nearest_neighbor(neighbors = 5) %&gt;%\n    set_engine(\"kknn\") %&gt;%\n    set_mode(\"regression\") %&gt;%\n    fit(Sale_Price ~ Gr_Liv_Area + Overall_Cond, data = ames_small)\n)\n\n# All predictions have the same format\npredictions &lt;- map(models, ~ predict(., ames_small))\n\n# Check structure - all identical!\nmap(predictions, str)\n\ntibble [500 x 1] (S3: tbl_df/tbl/data.frame)\n $ .pred: num [1:500] 190481 194350 211597 241778 121628 ...\ntibble [500 x 1] (S3: tbl_df/tbl/data.frame)\n $ .pred: num [1:500] 212497 212497 212497 212497 118782 ...\ntibble [500 x 1] (S3: tbl_df/tbl/data.frame)\n $ .pred: num [1:500] 232004 183235 186429 243400 129300 ...\n\n\n$linear\nNULL\n\n$tree\nNULL\n\n$knn\nNULL\n\n# For classification, we can get probabilities consistently\nclass_model &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  fit(expensive ~ Gr_Liv_Area + Overall_Cond, data = ames_class)\n\n# Class predictions\nclass_preds &lt;- predict(class_model, ames_class)\nhead(class_preds)\n\n# A tibble: 6 x 1\n  .pred_class\n  &lt;fct&gt;      \n1 no         \n2 yes        \n3 yes        \n4 yes        \n5 no         \n6 yes        \n\n# Probability predictions\nprob_preds &lt;- predict(class_model, ames_class, type = \"prob\")\nhead(prob_preds)\n\n# A tibble: 6 x 2\n  .pred_no .pred_yes\n     &lt;dbl&gt;     &lt;dbl&gt;\n1   0.734      0.266\n2   0.465      0.535\n3   0.0965     0.904\n4   0.0685     0.932\n5   0.859      0.141\n6   0.118      0.882"
  },
  {
    "objectID": "11-model-specification.html#advanced-model-specifications",
    "href": "11-model-specification.html#advanced-model-specifications",
    "title": "Chapter 11: Model Specification with parsnip - A Unified Interface for Models",
    "section": "Advanced Model Specifications",
    "text": "Advanced Model Specifications\n\nRegularized Regression\nRegularized models require special handling for the penalty parameter:\n\n# Ridge regression\nridge_spec &lt;- linear_reg(penalty = 0.1, mixture = 0) %&gt;%\n  set_engine(\"glmnet\")\n\n# Lasso regression  \nlasso_spec &lt;- linear_reg(penalty = 0.1, mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\n\n# Elastic net\nelastic_spec &lt;- linear_reg(penalty = 0.1, mixture = 0.5) %&gt;%\n  set_engine(\"glmnet\")\n\n# Fit and compare\nregularized_models &lt;- list(\n  ridge = ridge_spec,\n  lasso = lasso_spec,\n  elastic = elastic_spec\n) %&gt;%\n  map(~ fit(., Sale_Price ~ ., data = ames_small))\n\n# Extract coefficients\ncoef_comparison &lt;- map_df(names(regularized_models), function(model_name) {\n  coefs &lt;- regularized_models[[model_name]] %&gt;%\n    tidy() %&gt;%\n    filter(term != \"(Intercept)\") %&gt;%\n    mutate(model = model_name)\n})\n\n# Visualize coefficient shrinkage\nggplot(coef_comparison, aes(x = term, y = estimate, fill = model)) +\n  geom_col(position = \"dodge\") +\n  coord_flip() +\n  labs(\n    title = \"Coefficient Comparison: Ridge vs Lasso vs Elastic Net\",\n    subtitle = \"Notice how Lasso sets some coefficients to exactly zero\",\n    x = \"Variable\", y = \"Coefficient\"\n  ) +\n  theme(axis.text.y = element_text(size = 8))\n\n\n\n\n\n\n\n\n\n\nBoosted Trees\nGradient boosting has many hyperparameters:\n\n# XGBoost specification\nxgb_spec &lt;- boost_tree(\n  trees = 1000,           # Number of trees\n  tree_depth = 6,         # Maximum tree depth\n  min_n = 10,             # Minimum node size\n  loss_reduction = 0.01,  # Minimum loss reduction\n  sample_size = 0.8,      # Subsample ratio\n  learn_rate = 0.01       # Learning rate\n) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\nprint(xgb_spec)\n\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n  min_n = 10\n  tree_depth = 6\n  learn_rate = 0.01\n  loss_reduction = 0.01\n  sample_size = 0.8\n\nComputational engine: xgboost \n\n# Fit the model\nxgb_fit &lt;- xgb_spec %&gt;%\n  fit(Sale_Price ~ ., data = ames_small)\n\n# Feature importance\nxgb_importance &lt;- xgb_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  xgboost::xgb.importance(model = .) %&gt;%\n  as_tibble()\n\nggplot(xgb_importance %&gt;% head(10), \n       aes(x = reorder(Feature, Gain), y = Gain)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"XGBoost Feature Importance\",\n    subtitle = \"Top 10 most important features\",\n    x = \"Feature\", y = \"Importance (Gain)\"\n  )"
  },
  {
    "objectID": "11-model-specification.html#model-comparison-framework",
    "href": "11-model-specification.html#model-comparison-framework",
    "title": "Chapter 11: Model Specification with parsnip - A Unified Interface for Models",
    "section": "Model Comparison Framework",
    "text": "Model Comparison Framework\nParsnip makes it easy to compare different models systematically:\n\n# Define multiple model specifications\nmodel_specs &lt;- list(\n  linear = linear_reg() %&gt;% \n    set_engine(\"lm\"),\n  \n  ridge = linear_reg(penalty = 0.1, mixture = 0) %&gt;%\n    set_engine(\"glmnet\"),\n  \n  tree = decision_tree(tree_depth = 10) %&gt;%\n    set_engine(\"rpart\") %&gt;%\n    set_mode(\"regression\"),\n  \n  rf = rand_forest(trees = 100) %&gt;%\n    set_engine(\"ranger\") %&gt;%\n    set_mode(\"regression\"),\n  \n  knn = nearest_neighbor(neighbors = 10) %&gt;%\n    set_engine(\"kknn\") %&gt;%\n    set_mode(\"regression\")\n)\n\n# Fit all models\nfitted_models &lt;- map(model_specs, ~ fit(., Sale_Price ~ ., data = ames_small))\n\n# Evaluate all models\nmodel_evaluation &lt;- map_df(names(fitted_models), function(model_name) {\n  model &lt;- fitted_models[[model_name]]\n  \n  # Get predictions\n  preds &lt;- predict(model, ames_small)$.pred\n  \n  # Calculate metrics\n  tibble(\n    model = model_name,\n    rmse = sqrt(mean((ames_small$Sale_Price - preds)^2)),\n    mae = mean(abs(ames_small$Sale_Price - preds)),\n    r_squared = cor(ames_small$Sale_Price, preds)^2\n  )\n})\n\n# Visualize comparison\nmodel_evaluation %&gt;%\n  pivot_longer(cols = c(rmse, mae, r_squared), \n               names_to = \"metric\", values_to = \"value\") %&gt;%\n  ggplot(aes(x = model, y = value, fill = model)) +\n  geom_col() +\n  facet_wrap(~metric, scales = \"free_y\") +\n  labs(\n    title = \"Model Performance Comparison\",\n    subtitle = \"Different metrics across model types\",\n    x = \"Model\", y = \"Value\"\n  ) +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "11-model-specification.html#creating-model-specifications-programmatically",
    "href": "11-model-specification.html#creating-model-specifications-programmatically",
    "title": "Chapter 11: Model Specification with parsnip - A Unified Interface for Models",
    "section": "Creating Model Specifications Programmatically",
    "text": "Creating Model Specifications Programmatically\nSometimes we need to create model specifications dynamically:\n\n# Function to create model specs with different hyperparameters\ncreate_rf_spec &lt;- function(n_trees, mtry_prop, min_node) {\n  rand_forest(\n    trees = n_trees,\n    mtry = floor(mtry_prop * ncol(ames_small) - 1),  # Convert to integer\n    min_n = min_node\n  ) %&gt;%\n    set_engine(\"ranger\") %&gt;%\n    set_mode(\"regression\")\n}\n\n# Create a grid of specifications\nrf_grid &lt;- expand_grid(\n  n_trees = c(100, 500, 1000),\n  mtry_prop = c(0.3, 0.5, 0.7),\n  min_node = c(5, 10, 20)\n)\n\n# Create specifications\nrf_specs &lt;- pmap(rf_grid, create_rf_spec)\n\n# Fit a subset and compare\nsubset_specs &lt;- rf_specs[c(1, 14, 27)]  # Low, medium, high complexity\nsubset_fits &lt;- map(subset_specs, ~ fit(., Sale_Price ~ ., data = ames_small))\n\n# Evaluate\nsubset_evaluation &lt;- map_df(1:3, function(i) {\n  preds &lt;- predict(subset_fits[[i]], ames_small)$.pred\n  tibble(\n    config = c(\"Low\", \"Medium\", \"High\")[i],\n    trees = rf_grid$n_trees[c(1, 14, 27)][i],\n    mtry_prop = rf_grid$mtry_prop[c(1, 14, 27)][i],\n    min_node = rf_grid$min_node[c(1, 14, 27)][i],\n    rmse = sqrt(mean((ames_small$Sale_Price - preds)^2))\n  )\n})\n\nknitr::kable(subset_evaluation, digits = 2)\n\n\n\n\nconfig\ntrees\nmtry_prop\nmin_node\nrmse\n\n\n\n\nLow\n100\n0.3\n5\n20494.26\n\n\nMedium\n500\n0.5\n10\n33261.85\n\n\nHigh\n1000\n0.7\n20\n30141.50"
  },
  {
    "objectID": "11-model-specification.html#understanding-model-translations",
    "href": "11-model-specification.html#understanding-model-translations",
    "title": "Chapter 11: Model Specification with parsnip - A Unified Interface for Models",
    "section": "Understanding Model Translations",
    "text": "Understanding Model Translations\nParsnip translates your specifications to the underlying engine calls:\n\n# See how parsnip translates to different engines\nrf_spec &lt;- rand_forest(trees = 500, mtry = 5, min_n = 10) %&gt;%\n  set_mode(\"regression\")\n\n# Translation for ranger\nrf_spec %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  translate()\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 5\n  trees = 500\n  min_n = 10\n\nComputational engine: ranger \n\nModel fit template:\nranger::ranger(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    mtry = min_cols(~5, x), num.trees = 500, min.node.size = min_rows(~10, \n        x), num.threads = 1, verbose = FALSE, seed = sample.int(10^5, \n        1))\n\n# Translation for randomForest\nrf_spec %&gt;%\n  set_engine(\"randomForest\") %&gt;%\n  translate()\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 5\n  trees = 500\n  min_n = 10\n\nComputational engine: randomForest \n\nModel fit template:\nrandomForest::randomForest(x = missing_arg(), y = missing_arg(), \n    mtry = min_cols(~5, x), ntree = 500, nodesize = min_rows(~10, \n        x))\n\n# The arguments are mapped appropriately!\n# trees -&gt; num.trees (ranger) or ntree (randomForest)\n# mtry -&gt; mtry (both)\n# min_n -&gt; min.node.size (ranger) or nodesize (randomForest)"
  },
  {
    "objectID": "11-model-specification.html#model-specification-best-practices",
    "href": "11-model-specification.html#model-specification-best-practices",
    "title": "Chapter 11: Model Specification with parsnip - A Unified Interface for Models",
    "section": "Model Specification Best Practices",
    "text": "Model Specification Best Practices\n\n1. Start Simple, Add Complexity\n\n# Start with default values\nsimple_rf &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\n# Add complexity as needed\ncomplex_rf &lt;- rand_forest(\n  trees = 1000,\n  mtry = tune(),\n  min_n = tune()\n) %&gt;%\n  set_engine(\"ranger\",\n             importance = \"impurity\",\n             num.threads = parallel::detectCores() - 1) %&gt;%\n  set_mode(\"regression\")\n\n\n\n2. Use Consistent Naming\n\n# Create a naming convention for your specifications\nmodels &lt;- list(\n  # Baseline models\n  baseline_mean = null_model() %&gt;% \n    set_engine(\"parsnip\") %&gt;%\n    set_mode(\"regression\"),\n  \n  baseline_linear = linear_reg() %&gt;%\n    set_engine(\"lm\"),\n  \n  # Regularized models\n  reg_ridge = linear_reg(penalty = 0.1, mixture = 0) %&gt;%\n    set_engine(\"glmnet\"),\n  \n  reg_lasso = linear_reg(penalty = 0.1, mixture = 1) %&gt;%\n    set_engine(\"glmnet\"),\n  \n  # Tree models\n  tree_single = decision_tree() %&gt;%\n    set_engine(\"rpart\") %&gt;%\n    set_mode(\"regression\"),\n  \n  tree_rf = rand_forest() %&gt;%\n    set_engine(\"ranger\") %&gt;%\n    set_mode(\"regression\"),\n  \n  tree_boost = boost_tree() %&gt;%\n    set_engine(\"xgboost\") %&gt;%\n    set_mode(\"regression\")\n)\n\n\n\n3. Document Your Choices\n\n# Document why you chose specific values\nproduction_rf &lt;- rand_forest(\n  trees = 500,        # Balanced accuracy vs training time\n  mtry = 5,          # sqrt(p) rule for regression\n  min_n = 20         # Prevent overfitting on small samples\n) %&gt;%\n  set_engine(\"ranger\",\n             importance = \"impurity\",  # Need feature importance\n             seed = 123,               # Reproducibility\n             num.threads = 4) %&gt;%      # Server has 8 cores, use half\n  set_mode(\"regression\")"
  },
  {
    "objectID": "11-model-specification.html#troubleshooting-common-issues",
    "href": "11-model-specification.html#troubleshooting-common-issues",
    "title": "Chapter 11: Model Specification with parsnip - A Unified Interface for Models",
    "section": "Troubleshooting Common Issues",
    "text": "Troubleshooting Common Issues\n\nIssue 1: Missing Required Packages\n\n# Check if required package is installed\ncheck_model_package &lt;- function(model_spec) {\n  required_pkg &lt;- model_spec$engine\n  \n  if (!requireNamespace(required_pkg, quietly = TRUE)) {\n    cat(\"Package\", required_pkg, \"is not installed.\\n\")\n    cat(\"Install with: install.packages('\", required_pkg, \"')\\n\", sep = \"\")\n    return(FALSE)\n  }\n  \n  cat(\"Package\", required_pkg, \"is available.\\n\")\n  return(TRUE)\n}\n\n# Test\nsvm_spec &lt;- svm_rbf() %&gt;% set_engine(\"kernlab\")\ncheck_model_package(svm_spec)\n\nPackage kernlab is available.\n\n\n[1] TRUE\n\n\n\n\nIssue 2: Incompatible Mode/Engine Combinations\n\n# Not all combinations work\ntryCatch({\n  bad_spec &lt;- linear_reg() %&gt;%\n    set_engine(\"rpart\")  # Decision tree engine for linear regression?\n}, error = function(e) {\n  cat(\"Error:\", e$message, \"\\n\")\n})\n\nError: Engine \"rpart\" is not supported for `linear_reg()` \n\n# Check valid combinations\nshow_engines(\"linear_reg\")\n\n# A tibble: 8 x 2\n  engine   mode               \n  &lt;chr&gt;    &lt;chr&gt;              \n1 lm       regression         \n2 glm      regression         \n3 glmnet   regression         \n4 stan     regression         \n5 spark    regression         \n6 keras    regression         \n7 brulee   regression         \n8 quantreg quantile regression\n\n\n\n\nIssue 3: Missing Arguments\n\n# Some engines require specific arguments\ntryCatch({\n  bad_glmnet &lt;- linear_reg() %&gt;%\n    set_engine(\"glmnet\")  # Missing penalty!\n  \n  fit(bad_glmnet, Sale_Price ~ ., data = ames_small)\n}, error = function(e) {\n  cat(\"Error: glmnet requires penalty argument\\n\")\n})\n\nError: glmnet requires penalty argument\n\n# Correct specification\ngood_glmnet &lt;- linear_reg(penalty = 0.1) %&gt;%\n  set_engine(\"glmnet\")"
  },
  {
    "objectID": "11-model-specification.html#exercises",
    "href": "11-model-specification.html#exercises",
    "title": "Chapter 11: Model Specification with parsnip - A Unified Interface for Models",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Multi-Engine Comparison\nCompare the same model type across different engines:\n\n# Your solution\n# Compare random forest implementations\nengines &lt;- c(\"ranger\", \"randomForest\")\n\nrf_comparison &lt;- map_df(engines, function(eng) {\n  # Skip if package not available\n  if (!requireNamespace(eng, quietly = TRUE)) {\n    return(tibble(engine = eng, status = \"Package not installed\"))\n  }\n  \n  # Create specification\n  spec &lt;- rand_forest(trees = 100) %&gt;%\n    set_engine(eng) %&gt;%\n    set_mode(\"regression\")\n  \n  # Time the fitting\n  start_time &lt;- Sys.time()\n  fit &lt;- spec %&gt;% fit(Sale_Price ~ ., data = ames_small)\n  fit_time &lt;- as.numeric(Sys.time() - start_time, units = \"secs\")\n  \n  # Get predictions\n  preds &lt;- predict(fit, ames_small)$.pred\n  \n  # Return metrics\n  tibble(\n    engine = eng,\n    fit_time = fit_time,\n    rmse = sqrt(mean((ames_small$Sale_Price - preds)^2)),\n    status = \"Success\"\n  )\n})\n\nknitr::kable(rf_comparison, digits = 2)\n\n\n\n\nengine\nfit_time\nrmse\nstatus\n\n\n\n\nranger\n0.02\n20461.06\nSuccess\n\n\nrandomForest\nNA\nNA\nPackage not installed\n\n\n\n\n\n\n\nExercise 2: Custom Model Grid\nCreate a grid of model specifications for comparison:\n\n# Your solution\n# Create diverse model specifications\nmodel_grid &lt;- tribble(\n  ~name, ~type, ~engine, ~hyperparams,\n  \"linear_basic\", \"linear_reg\", \"lm\", list(),\n  \"linear_ridge\", \"linear_reg\", \"glmnet\", list(penalty = 0.01, mixture = 0),\n  \"tree_shallow\", \"decision_tree\", \"rpart\", list(tree_depth = 5),\n  \"tree_deep\", \"decision_tree\", \"rpart\", list(tree_depth = 15),\n  \"rf_small\", \"rand_forest\", \"ranger\", list(trees = 50),\n  \"rf_large\", \"rand_forest\", \"ranger\", list(trees = 500),\n  \"knn_few\", \"nearest_neighbor\", \"kknn\", list(neighbors = 3),\n  \"knn_many\", \"nearest_neighbor\", \"kknn\", list(neighbors = 20)\n)\n\n# Function to create spec from grid row\ncreate_spec &lt;- function(type, engine, hyperparams) {\n  # Get base specification\n  spec &lt;- switch(type,\n    linear_reg = linear_reg(),\n    decision_tree = decision_tree(),\n    rand_forest = rand_forest(),\n    nearest_neighbor = nearest_neighbor()\n  )\n  \n  # Add hyperparameters\n  if (length(hyperparams) &gt; 0) {\n    spec &lt;- do.call(set_args, c(list(spec), hyperparams))\n  }\n  \n  # Set engine and mode\n  spec %&gt;%\n    set_engine(engine) %&gt;%\n    set_mode(\"regression\")\n}\n\n# Create all specifications\nall_specs &lt;- pmap(model_grid %&gt;% select(-name), create_spec)\nnames(all_specs) &lt;- model_grid$name\n\n# Fit and evaluate a few\nsample_models &lt;- all_specs[c(\"linear_basic\", \"tree_deep\", \"rf_large\")]\nsample_fits &lt;- map(sample_models, ~ fit(., Sale_Price ~ ., data = ames_small))\n\n# Quick evaluation\nmap_dbl(sample_fits, function(fit) {\n  preds &lt;- predict(fit, ames_small)$.pred\n  sqrt(mean((ames_small$Sale_Price - preds)^2))\n})\n\nlinear_basic    tree_deep     rf_large \n    37041.70     40841.91     20588.47 \n\n\n\n\nExercise 3: Engine-Specific Features\nExplore engine-specific features for the same model:\n\n# Your solution\n# Random forest with different engine features\n\n# Ranger with importance\nrf_ranger_imp &lt;- rand_forest(trees = 100) %&gt;%\n  set_engine(\"ranger\", \n             importance = \"permutation\",\n             keep.inbag = TRUE) %&gt;%  # For prediction intervals\n  set_mode(\"regression\") %&gt;%\n  fit(Sale_Price ~ ., data = ames_small)\n\n# Extract ranger-specific features\nranger_importance &lt;- rf_ranger_imp %&gt;%\n  extract_fit_engine() %&gt;%\n  .$variable.importance %&gt;%\n  sort(decreasing = TRUE) %&gt;%\n  head(10)\n\nprint(\"Ranger Variable Importance:\")\n\n[1] \"Ranger Variable Importance:\"\n\nprint(ranger_importance)\n\n Gr_Liv_Area   Year_Built Neighborhood Overall_Cond \n  4515460893   3479724041    689627617    215089951 \n\n# RandomForest with proximity\nif (requireNamespace(\"randomForest\", quietly = TRUE)) {\n  rf_rf_prox &lt;- rand_forest(trees = 100) %&gt;%\n    set_engine(\"randomForest\",\n               proximity = TRUE,  # Calculate proximity matrix\n               keep.forest = TRUE) %&gt;%\n    set_mode(\"regression\") %&gt;%\n    fit(Sale_Price ~ ., data = ames_small)\n  \n  # The proximity matrix shows similarity between observations\n  # This is useful for clustering and outlier detection\n  cat(\"\\nrandomForest can provide proximity matrix for clustering\\n\")\n}"
  },
  {
    "objectID": "11-model-specification.html#summary",
    "href": "11-model-specification.html#summary",
    "title": "Chapter 11: Model Specification with parsnip - A Unified Interface for Models",
    "section": "Summary",
    "text": "Summary\nIn this comprehensive chapter, you’ve mastered:\n✅ Core parsnip concepts - Unified model interface philosophy - Separation of specification and fitting - Consistent prediction interface\n✅ Model specifications - Different model types - Setting engines and modes - Main vs engine arguments\n✅ Advanced techniques - Multi-engine comparisons - Programmatic model creation - Model translations\n✅ Best practices - Starting simple and adding complexity - Consistent naming conventions - Proper documentation\nKey takeaways: - Parsnip provides consistency across diverse models - Same interface whether using lm or xgboost - Easy to swap and compare different approaches - Engine flexibility with common interface - Predictable, tidy outputs"
  },
  {
    "objectID": "11-model-specification.html#whats-next",
    "href": "11-model-specification.html#whats-next",
    "title": "Chapter 11: Model Specification with parsnip - A Unified Interface for Models",
    "section": "What’s Next?",
    "text": "What’s Next?\nIn Chapter 12, we’ll learn about workflows that combine models with preprocessing and evaluation metrics."
  },
  {
    "objectID": "11-model-specification.html#additional-resources",
    "href": "11-model-specification.html#additional-resources",
    "title": "Chapter 11: Model Specification with parsnip - A Unified Interface for Models",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nparsnip Documentation\nList of Available Models\nAdding New Models to parsnip\nTidy Modeling with R - Models Chapter"
  },
  {
    "objectID": "01-introduction.html",
    "href": "01-introduction.html",
    "title": "Chapter 1: Introduction to R and the Tidyverse Ecosystem",
    "section": "",
    "text": "By the end of this chapter, you will:\n\nUnderstand the philosophy and principles of the tidyverse\nKnow the core tidyverse packages and their purposes\nBe able to install and load tidyverse packages\nUnderstand the pipe operator (%&gt;% and |&gt;)\nWork with tibbles, the tidyverse’s modern data frames\nUnderstand tidy data principles"
  },
  {
    "objectID": "01-introduction.html#learning-objectives",
    "href": "01-introduction.html#learning-objectives",
    "title": "Chapter 1: Introduction to R and the Tidyverse Ecosystem",
    "section": "",
    "text": "By the end of this chapter, you will:\n\nUnderstand the philosophy and principles of the tidyverse\nKnow the core tidyverse packages and their purposes\nBe able to install and load tidyverse packages\nUnderstand the pipe operator (%&gt;% and |&gt;)\nWork with tibbles, the tidyverse’s modern data frames\nUnderstand tidy data principles"
  },
  {
    "objectID": "01-introduction.html#what-is-the-tidyverse",
    "href": "01-introduction.html#what-is-the-tidyverse",
    "title": "Chapter 1: Introduction to R and the Tidyverse Ecosystem",
    "section": "What is the Tidyverse?",
    "text": "What is the Tidyverse?\nThe tidyverse is a collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. The tidyverse makes data manipulation, exploration, and visualization faster and more intuitive.\n\nCore Tidyverse Packages\n\n# Install tidyverse if you haven't already\n# install.packages(\"tidyverse\")\n\n# Load the tidyverse\nlibrary(tidyverse)\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.4     v readr     2.1.5\nv forcats   1.0.0     v stringr   1.5.2\nv ggplot2   4.0.0     v tibble    3.3.0\nv lubridate 1.9.4     v tidyr     1.3.1\nv purrr     1.1.0     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nThe tidyverse includes these core packages:\n\nggplot2: Data visualization\ndplyr: Data manipulation\ntidyr: Data tidying\nreadr: Data import\npurrr: Functional programming\ntibble: Modern data frames\nstringr: String manipulation\nforcats: Factor handling"
  },
  {
    "objectID": "01-introduction.html#the-pipe-operator",
    "href": "01-introduction.html#the-pipe-operator",
    "title": "Chapter 1: Introduction to R and the Tidyverse Ecosystem",
    "section": "The Pipe Operator",
    "text": "The Pipe Operator\nThe pipe operator is fundamental to tidyverse workflows. It allows you to chain operations together in a readable way.\n\nUnderstanding the Problem with Nested Functions\nIn traditional R programming, when you need to apply multiple functions to data, you end up with deeply nested function calls that are read from the inside out. This creates several problems:\n\nReadability: The order of operations is reversed from how we naturally think about them\nDebugging: It’s hard to inspect intermediate results\nModification: Adding or removing steps requires careful parenthesis management\n\n\n\nTraditional R vs Piped Approach\nLet’s look at a simple example where we want to: 1. Take a vector of numbers 2. Calculate the square root of each 3. Find the mean 4. Round to 2 decimal places\n\n# Traditional nested approach (hard to read)\n# We read this from inside-out: first sqrt, then mean, then round\n# But we write it outside-in!\nround(mean(sqrt(c(1, 4, 9, 16, 25))), 2)\n\n[1] 3\n\n# With pipes (much clearer!)\n# We read AND write this in the order operations happen\nc(1, 4, 9, 16, 25) %&gt;%\n  sqrt() %&gt;%         # First: take square root\n  mean() %&gt;%         # Then: calculate mean\n  round(2)           # Finally: round to 2 decimals\n\n[1] 3\n\n# Using the native R pipe (R 4.1+)\n# The |&gt; operator is built into base R as of version 4.1\n# It works similarly to %&gt;% but with some subtle differences\nc(1, 4, 9, 16, 25) |&gt;\n  sqrt() |&gt;\n  mean() |&gt;\n  round(2)\n\n[1] 3\n\n\nNotice how the piped version reads like a recipe: “Take these numbers, THEN calculate square root, THEN find the mean, THEN round.” This matches our mental model of the data transformation process.\n\n\nHow the Pipe Actually Works\nThe pipe operator takes the output of the expression on its left and passes it as the first argument to the function on its right. So x %&gt;% f() is equivalent to f(x), and x %&gt;% f() %&gt;% g() is equivalent to g(f(x)).\nYou can also use the . placeholder to specify where the piped value should go if it’s not the first argument:\n\n# Using the dot placeholder for custom positioning\n10 %&gt;% \n  `/`(2) %&gt;%      # 10 / 2 = 5\n  `+`(3) %&gt;%      # 5 + 3 = 8  \n  `^`(2)          # 8^2 = 64\n\n[1] 64\n\n# When the piped value isn't the first argument\n5 %&gt;% \n  seq(from = 1, to = .)  # Creates sequence from 1 to 5\n\n[1] 1 2 3 4 5\n\n# More practical example with data\nmtcars %&gt;%\n  lm(mpg ~ cyl + wt, data = .)  # data = . puts the piped data in the right place\n\n\nCall:\nlm(formula = mpg ~ cyl + wt, data = .)\n\nCoefficients:\n(Intercept)          cyl           wt  \n     39.686       -1.508       -3.191  \n\n\n\n\nPractical Example with Data\n\n# Load a built-in dataset\ndata(mtcars)\n\n# Without pipes - nested and hard to read\nhead(arrange(filter(mtcars, cyl == 6), desc(mpg)), 5)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nHornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nMazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nFerrari Dino   19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMerc 280       19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\n\n# With pipes - clear and sequential\nmtcars %&gt;%\n  filter(cyl == 6) %&gt;%\n  arrange(desc(mpg)) %&gt;%\n  head(5)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nHornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nMazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nFerrari Dino   19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMerc 280       19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4"
  },
  {
    "objectID": "01-introduction.html#tibbles-modern-data-frames",
    "href": "01-introduction.html#tibbles-modern-data-frames",
    "title": "Chapter 1: Introduction to R and the Tidyverse Ecosystem",
    "section": "Tibbles: Modern Data Frames",
    "text": "Tibbles: Modern Data Frames\nTibbles are the tidyverse’s enhanced version of data frames.\n\nCreating Tibbles\n\n# Create a tibble from scratch\nmy_tibble &lt;- tibble(\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"Diana\"),\n  age = c(25, 30, 35, 28),\n  score = c(85.5, 92.3, 78.9, 88.1),\n  passed = c(TRUE, TRUE, FALSE, TRUE)\n)\n\nmy_tibble\n\n# A tibble: 4 x 4\n  name      age score passed\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; \n1 Alice      25  85.5 TRUE  \n2 Bob        30  92.3 TRUE  \n3 Charlie    35  78.9 FALSE \n4 Diana      28  88.1 TRUE  \n\n\n\n\nTibble vs Data Frame\n\n# Convert data frame to tibble\nmtcars_tibble &lt;- as_tibble(mtcars)\n\n# Compare printing\nprint(\"Data frame (first 6 rows shown by default):\")\n\n[1] \"Data frame (first 6 rows shown by default):\"\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nprint(\"Tibble (shows what fits on screen):\")\n\n[1] \"Tibble (shows what fits on screen):\"\n\nmtcars_tibble\n\n# A tibble: 32 x 11\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n 2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n 3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n 4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n 5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n 6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n 7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n 8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n 9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n# i 22 more rows\n\n\n\n\nKey Advantages of Tibbles\n\n# Tibbles preserve data types\ndf &lt;- data.frame(x = 1:3, y = c(\"a\", \"b\", \"c\"))\ntb &lt;- tibble(x = 1:3, y = c(\"a\", \"b\", \"c\"))\n\n# Data frame converts strings to factors (in older R versions)\nstr(df)\n\n'data.frame':   3 obs. of  2 variables:\n $ x: int  1 2 3\n $ y: chr  \"a\" \"b\" \"c\"\n\nstr(tb)\n\ntibble [3 x 2] (S3: tbl_df/tbl/data.frame)\n $ x: int [1:3] 1 2 3\n $ y: chr [1:3] \"a\" \"b\" \"c\"\n\n# Tibbles handle column names better\nweird_tb &lt;- tibble(\n  `First Name` = c(\"John\", \"Jane\"),\n  `2020` = c(100, 200),\n  `:)` = c(\"happy\", \"sad\")\n)\nweird_tb\n\n# A tibble: 2 x 3\n  `First Name` `2020` `:)` \n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;\n1 John            100 happy\n2 Jane            200 sad"
  },
  {
    "objectID": "01-introduction.html#tidy-data-principles",
    "href": "01-introduction.html#tidy-data-principles",
    "title": "Chapter 1: Introduction to R and the Tidyverse Ecosystem",
    "section": "Tidy Data Principles",
    "text": "Tidy Data Principles\nTidy data is a standard way of organizing data values within a dataset.\n\nThe Three Rules of Tidy Data\n\nEach variable must have its own column\nEach observation must have its own row\nEach value must have its own cell\n\n\n\nExample: Messy vs Tidy Data\n\n# Messy data\nmessy_data &lt;- tibble(\n  student = c(\"Alice\", \"Bob\", \"Charlie\"),\n  midterm = c(85, 90, 78),\n  final = c(88, 85, 92)\n)\n\nprint(\"Messy data (wide format):\")\n\n[1] \"Messy data (wide format):\"\n\nmessy_data\n\n# A tibble: 3 x 3\n  student midterm final\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 Alice        85    88\n2 Bob          90    85\n3 Charlie      78    92\n\n# Tidy data\ntidy_data &lt;- messy_data %&gt;%\n  pivot_longer(\n    cols = c(midterm, final),\n    names_to = \"exam\",\n    values_to = \"score\"\n  )\n\nprint(\"Tidy data (long format):\")\n\n[1] \"Tidy data (long format):\"\n\ntidy_data\n\n# A tibble: 6 x 3\n  student exam    score\n  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;\n1 Alice   midterm    85\n2 Alice   final      88\n3 Bob     midterm    90\n4 Bob     final      85\n5 Charlie midterm    78\n6 Charlie final      92"
  },
  {
    "objectID": "01-introduction.html#working-with-real-data",
    "href": "01-introduction.html#working-with-real-data",
    "title": "Chapter 1: Introduction to R and the Tidyverse Ecosystem",
    "section": "Working with Real Data",
    "text": "Working with Real Data\nLet’s practice with a real dataset:\n\n# Load the palmerpenguins package for example data\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\n\n# Explore the penguins dataset\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel~\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse~\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, ~\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, ~\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186~\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, ~\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male~\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007~\n\n# Basic exploration\npenguins %&gt;%\n  summary()\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\n\n\nData Exploration Pipeline\n\n# Complete analysis pipeline\npenguins %&gt;%\n  drop_na() %&gt;%  # Remove missing values\n  group_by(species, island) %&gt;%\n  summarise(\n    count = n(),\n    avg_bill_length = mean(bill_length_mm),\n    avg_body_mass = mean(body_mass_g),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(avg_body_mass))\n\n# A tibble: 5 x 5\n  species   island    count avg_bill_length avg_body_mass\n  &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n1 Gentoo    Biscoe      119            47.6         5092.\n2 Chinstrap Dream        68            48.8         3733.\n3 Adelie    Biscoe       44            39.0         3710.\n4 Adelie    Torgersen    47            39.0         3709.\n5 Adelie    Dream        55            38.5         3701."
  },
  {
    "objectID": "01-introduction.html#visualization-preview",
    "href": "01-introduction.html#visualization-preview",
    "title": "Chapter 1: Introduction to R and the Tidyverse Ecosystem",
    "section": "Visualization Preview",
    "text": "Visualization Preview\nA quick taste of ggplot2 (covered in detail in Chapter 5):\n\npenguins %&gt;%\n  drop_na() %&gt;%\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = \"Penguin Body Mass vs Flipper Length\",\n    x = \"Flipper Length (mm)\",\n    y = \"Body Mass (g)\",\n    color = \"Species\"\n  ) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set2\")"
  },
  {
    "objectID": "01-introduction.html#common-tidyverse-patterns",
    "href": "01-introduction.html#common-tidyverse-patterns",
    "title": "Chapter 1: Introduction to R and the Tidyverse Ecosystem",
    "section": "Common Tidyverse Patterns",
    "text": "Common Tidyverse Patterns\n\nPattern 1: Read, Clean, Transform, Visualize\n\n# Typical workflow\npenguins %&gt;%\n  # Clean\n  drop_na() %&gt;%\n  filter(year == 2008) %&gt;%\n  # Transform\n  mutate(\n    body_mass_kg = body_mass_g / 1000,\n    size_category = case_when(\n      body_mass_kg &lt; 3.5 ~ \"Small\",\n      body_mass_kg &lt; 4.5 ~ \"Medium\",\n      TRUE ~ \"Large\"\n    )\n  ) %&gt;%\n  # Summarize\n  group_by(species, size_category) %&gt;%\n  summarise(count = n(), .groups = \"drop\") %&gt;%\n  # Visualize\n  ggplot(aes(x = species, y = count, fill = size_category)) +\n  geom_col(position = \"dodge\") +\n  theme_minimal() +\n  labs(title = \"Penguin Size Distribution by Species (2008)\")\n\n\n\n\n\n\n\n\n\n\nPattern 2: Multiple Operations with Groups\n\n# Complex grouped operations\npenguins %&gt;%\n  drop_na() %&gt;%\n  group_by(species) %&gt;%\n  mutate(\n    bill_length_z = (bill_length_mm - mean(bill_length_mm)) / sd(bill_length_mm),\n    bill_depth_z = (bill_depth_mm - mean(bill_depth_mm)) / sd(bill_depth_mm)\n  ) %&gt;%\n  filter(abs(bill_length_z) &lt; 2 & abs(bill_depth_z) &lt; 2) %&gt;%  # Remove outliers\n  summarise(\n    n = n(),\n    correlation = cor(bill_length_mm, bill_depth_mm),\n    .groups = \"drop\"\n  )\n\n# A tibble: 3 x 3\n  species       n correlation\n  &lt;fct&gt;     &lt;int&gt;       &lt;dbl&gt;\n1 Adelie      135       0.290\n2 Chinstrap    63       0.673\n3 Gentoo      110       0.627"
  },
  {
    "objectID": "01-introduction.html#exercises",
    "href": "01-introduction.html#exercises",
    "title": "Chapter 1: Introduction to R and the Tidyverse Ecosystem",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Basic Pipe Operations\nCreate a pipeline that: 1. Takes the numbers 1 to 100 2. Keeps only even numbers 3. Squares each number 4. Calculates the mean 5. Takes the square root of the result\n\n# Your code here\n1:100 %&gt;%\n  keep(~ . %% 2 == 0) %&gt;%  # Keep even numbers\n  map_dbl(~ .^2) %&gt;%        # Square each\n  mean() %&gt;%                # Calculate mean\n  sqrt()                    # Take square root\n\n[1] 58.60034\n\n\n\n\nExercise 2: Tibble Creation and Manipulation\nCreate a tibble with information about 5 books (title, author, year, pages, rating). Then: 1. Filter books published after 2000 2. Add a column for reading_time (assuming 1 page per minute) 3. Arrange by rating (descending)\n\n# Your code here\nbooks &lt;- tibble(\n  title = c(\"The Great Gatsby\", \"1984\", \"The Hunger Games\", \"Dune\", \"Project Hail Mary\"),\n  author = c(\"F. Scott Fitzgerald\", \"George Orwell\", \"Suzanne Collins\", \"Frank Herbert\", \"Andy Weir\"),\n  year = c(1925, 1949, 2008, 1965, 2021),\n  pages = c(180, 328, 374, 688, 476),\n  rating = c(4.5, 4.8, 4.3, 4.7, 4.9)\n)\n\nbooks %&gt;%\n  filter(year &gt; 2000) %&gt;%\n  mutate(reading_time_hours = pages / 60) %&gt;%\n  arrange(desc(rating))\n\n# A tibble: 2 x 6\n  title             author           year pages rating reading_time_hours\n  &lt;chr&gt;             &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;              &lt;dbl&gt;\n1 Project Hail Mary Andy Weir        2021   476    4.9               7.93\n2 The Hunger Games  Suzanne Collins  2008   374    4.3               6.23\n\n\n\n\nExercise 3: Working with Penguins Data\nUsing the penguins dataset: 1. Calculate the average bill length for each species on each island 2. Find which species-island combination has the longest average bill 3. Create a summary showing min, max, and mean body mass for each species\n\n# Your code here\n# Part 1 & 2\npenguins %&gt;%\n  drop_na(bill_length_mm) %&gt;%\n  group_by(species, island) %&gt;%\n  summarise(\n    avg_bill_length = mean(bill_length_mm),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(avg_bill_length))\n\n# A tibble: 5 x 3\n  species   island    avg_bill_length\n  &lt;fct&gt;     &lt;fct&gt;               &lt;dbl&gt;\n1 Chinstrap Dream                48.8\n2 Gentoo    Biscoe               47.5\n3 Adelie    Biscoe               39.0\n4 Adelie    Torgersen            39.0\n5 Adelie    Dream                38.5\n\n# Part 3\npenguins %&gt;%\n  drop_na(body_mass_g) %&gt;%\n  group_by(species) %&gt;%\n  summarise(\n    min_mass = min(body_mass_g),\n    mean_mass = mean(body_mass_g),\n    max_mass = max(body_mass_g),\n    .groups = \"drop\"\n  )\n\n# A tibble: 3 x 4\n  species   min_mass mean_mass max_mass\n  &lt;fct&gt;        &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n1 Adelie        2850     3701.     4775\n2 Chinstrap     2700     3733.     4800\n3 Gentoo        3950     5076.     6300\n\n\n\n\nExercise 4: Tidy Data Challenge\nConvert this wide dataset to tidy format and calculate the average score for each subject:\n\n# Given data\nstudent_scores &lt;- tibble(\n  student_id = 1:5,\n  math_score = c(85, 92, 78, 95, 88),\n  science_score = c(90, 88, 85, 92, 91),\n  english_score = c(88, 85, 90, 87, 89)\n)\n\n# Your code here\nstudent_scores %&gt;%\n  pivot_longer(\n    cols = ends_with(\"_score\"),\n    names_to = \"subject\",\n    values_to = \"score\",\n    names_pattern = \"(.*)_score\"\n  ) %&gt;%\n  group_by(subject) %&gt;%\n  summarise(\n    avg_score = mean(score),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(avg_score))\n\n# A tibble: 3 x 2\n  subject avg_score\n  &lt;chr&gt;       &lt;dbl&gt;\n1 science      89.2\n2 english      87.8\n3 math         87.6"
  },
  {
    "objectID": "01-introduction.html#summary",
    "href": "01-introduction.html#summary",
    "title": "Chapter 1: Introduction to R and the Tidyverse Ecosystem",
    "section": "Summary",
    "text": "Summary\nIn this chapter, you learned:\n✅ The tidyverse philosophy and ecosystem\n✅ How to use the pipe operator for readable code\n✅ Working with tibbles instead of data frames\n✅ Tidy data principles\n✅ Basic data manipulation patterns"
  },
  {
    "objectID": "01-introduction.html#whats-next",
    "href": "01-introduction.html#whats-next",
    "title": "Chapter 1: Introduction to R and the Tidyverse Ecosystem",
    "section": "What’s Next?",
    "text": "What’s Next?\nIn Chapter 2, we’ll dive deep into importing data from various sources using readr and other packages. You’ll learn to read CSV, Excel, JSON, and database files efficiently."
  },
  {
    "objectID": "01-introduction.html#additional-resources",
    "href": "01-introduction.html#additional-resources",
    "title": "Chapter 1: Introduction to R and the Tidyverse Ecosystem",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nR for Data Science (2e)\nTidyverse Style Guide\nTidyverse Cheat Sheets\nAdvanced R"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "quiz-block-3.html",
    "href": "quiz-block-3.html",
    "title": "Block 3 Assessment: Advanced Machine Learning and Deployment",
    "section": "",
    "text": "Progress\n    Score: 0/15\n    \n      \n    \n    \n      0 questions answered\n    \n  \n\n  \n    Block 3 Assessment: Advanced Machine Learning and Deployment\n  \n\n  \n    Instructions\n    This quiz covers advanced machine learning concepts from Block 3 (Chapters 14-18). Click on your chosen answer for each question. You'll receive immediate feedback with explanations for incorrect options.\n  \n\n  \n    1. What function does logistic regression use to map linear combinations to probabilities?\n    \n      Square root function\n      Exponential function\n      Sigmoid (logistic) function\n      Linear function\n    \n    \n  \n\n  \n    2. Which metric is most appropriate for evaluating a highly imbalanced binary classification problem?\n    \n      Accuracy\n      AUC-ROC\n      Mean Squared Error\n      R-squared\n    \n    \n  \n\n  \n    3. What is the main purpose of regularization in regression models?\n    \n      To handle missing values\n      To speed up training\n      To increase model complexity\n      To prevent overfitting by penalizing large coefficients\n    \n    \n  \n\n  \n    4. What is the key difference between Ridge and Lasso regularization?\n    \n      They are identical methods\n      Ridge uses L2 penalty, Lasso uses L1 penalty\n      Ridge is faster, Lasso is more accurate\n      Ridge is for classification, Lasso is for regression\n    \n    \n  \n\n  \n    5. What is bagging in ensemble methods?\n    \n      Training models sequentially, each correcting previous errors\n      Training multiple models on different bootstrap samples of the data\n      Combining predictions using weighted voting\n      Using different features for each model\n    \n    \n  \n\n  \n    6. What is the main difference between bagging and boosting?\n    \n      There is no difference\n      Bagging trains models in parallel, boosting trains them sequentially\n      Bagging uses decision trees, boosting uses linear models\n      Bagging is for regression, boosting is for classification\n    \n    \n  \n\n  \n    7. What is the primary goal of K-means clustering?\n    \n      To minimize within-cluster variance\n      To reduce the number of features\n      To maximize between-cluster variance\n      To predict class labels for new data\n    \n    \n  \n\n  \n    8. Why is data scaling important before applying K-means clustering?\n    \n      Scaling is not necessary for K-means\n      To reduce memory usage\n      K-means uses distance measures that can be dominated by variables with large scales\n      To improve algorithm speed\n    \n    \n  \n\n  \n    9. What does PCA (Principal Component Analysis) accomplish?\n    \n      It clusters similar observations together\n      It improves classification accuracy\n      It handles missing values in datasets\n      It reduces dimensionality while preserving variance\n    \n    \n  \n\n  \n    10. What is the \"elbow method\" used for in clustering?\n    \n      Scaling the features appropriately\n      Determining outliers in the data\n      Selecting the best clustering algorithm\n      Choosing the optimal number of clusters\n    \n    \n  \n\n  \n    11. What is model stacking in ensemble methods?\n    \n      Training models on different datasets\n      Storing models in a hierarchical structure\n      Using a meta-model to combine predictions from multiple base models\n      Training multiple models with identical parameters\n    \n    \n  \n\n  \n    12. What is the main challenge with deploying machine learning models to production?\n    \n      Ensuring reliability, scalability, and maintainability\n      Models become obsolete immediately\n      Production environments don't support R\n      Models always perform worse in production\n    \n    \n  \n\n  \n    13. What is the purpose of the plumber package in R?\n    \n      To manage package dependencies\n      To deploy Shiny applications\n      To create REST APIs from R functions\n      To fix data pipeline errors\n    \n    \n  \n\n  \n    14. What is Docker containerization useful for in model deployment?\n    \n      It ensures consistent environments across development and production\n      It improves model accuracy\n      It automatically tunes hyperparameters\n      It makes models run faster\n    \n    \n  \n\n  \n    15. Why is model monitoring important in production systems?\n    \n      To track model performance and detect data drift over time\n      To automatically improve model accuracy\n      To reduce computational costs\n      To comply with data privacy regulations\n    \n    \n  \n\n  \n    Quiz Complete!\n    \n    \n    \n    \n      Retake Quiz\n      \n        Return to Workshop Home"
  },
  {
    "objectID": "14-classification.html",
    "href": "14-classification.html",
    "title": "Chapter 14: Classification Models - Theory and Implementation",
    "section": "",
    "text": "By the end of this chapter, you will understand:\n\nClassification theory and concepts\nBinary vs multiclass classification\nLogistic regression mathematics\nDecision trees and random forests theory\nSupport vector machines (SVM) concepts\nEvaluation metrics for classification\nClass imbalance handling\nPractical implementation with tidymodels"
  },
  {
    "objectID": "14-classification.html#learning-objectives",
    "href": "14-classification.html#learning-objectives",
    "title": "Chapter 14: Classification Models - Theory and Implementation",
    "section": "",
    "text": "By the end of this chapter, you will understand:\n\nClassification theory and concepts\nBinary vs multiclass classification\nLogistic regression mathematics\nDecision trees and random forests theory\nSupport vector machines (SVM) concepts\nEvaluation metrics for classification\nClass imbalance handling\nPractical implementation with tidymodels"
  },
  {
    "objectID": "14-classification.html#classification-theory",
    "href": "14-classification.html#classification-theory",
    "title": "Chapter 14: Classification Models - Theory and Implementation",
    "section": "Classification Theory",
    "text": "Classification Theory\n\nWhat is Classification?\nClassification is a supervised learning task where we predict discrete class labels. Unlike regression (continuous outputs), classification assigns observations to categories.\nMathematical Framework:\nGiven features \\(X = (x_1, x_2, ..., x_p)\\) and classes \\(Y \\in \\{C_1, C_2, ..., C_k\\}\\), we seek:\n\\[P(Y = C_k | X)\\]\nThe predicted class is: \\[\\hat{y} = \\arg\\max_{k} P(Y = C_k | X)\\]"
  },
  {
    "objectID": "14-classification.html#setup",
    "href": "14-classification.html#setup",
    "title": "Chapter 14: Classification Models - Theory and Implementation",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidymodels)\n\n-- Attaching packages -------------------------------------- tidymodels 1.4.1 --\n\n\nv broom        1.0.10     v recipes      1.3.1 \nv dials        1.4.2      v rsample      1.3.1 \nv dplyr        1.1.4      v tailor       0.1.0 \nv ggplot2      4.0.0      v tidyr        1.3.1 \nv infer        1.0.9      v tune         2.0.0 \nv modeldata    1.5.1      v workflows    1.3.0 \nv parsnip      1.3.3      v workflowsets 1.1.1 \nv purrr        1.1.0      v yardstick    1.3.2 \n\n\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx purrr::discard() masks scales::discard()\nx dplyr::filter()  masks stats::filter()\nx dplyr::lag()     masks stats::lag()\nx recipes::step()  masks stats::step()\n\nlibrary(tidyverse)\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv forcats   1.0.0     v stringr   1.5.2\nv lubridate 1.9.4     v tibble    3.3.0\nv readr     2.1.5     \n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx readr::col_factor() masks scales::col_factor()\nx purrr::discard()    masks scales::discard()\nx dplyr::filter()     masks stats::filter()\nx stringr::fixed()    masks recipes::fixed()\nx dplyr::lag()        masks stats::lag()\nx readr::spec()       masks yardstick::spec()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(modeldata)\nlibrary(vip)\n\n\nAdjuntando el paquete: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(probably)\n\n\nAdjuntando el paquete: 'probably'\n\nThe following objects are masked from 'package:base':\n\n    as.factor, as.ordered\n\nlibrary(discrim)\n\n\nAdjuntando el paquete: 'discrim'\n\nThe following object is masked from 'package:dials':\n\n    smoothness\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\nlibrary(patchwork)\n\n# For reproducibility\nset.seed(123)\ntheme_set(theme_minimal())\n\n# Load datasets\ndata(credit_data)  # Credit default dataset\ndata(cells)        # Cell segmentation dataset"
  },
  {
    "objectID": "14-classification.html#logistic-regression-theory",
    "href": "14-classification.html#logistic-regression-theory",
    "title": "Chapter 14: Classification Models - Theory and Implementation",
    "section": "Logistic Regression Theory",
    "text": "Logistic Regression Theory\n\nBinary Classification\nFor binary classification with classes 0 and 1, logistic regression models:\n\\[P(Y = 1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + ... + \\beta_px_p)}}\\]\nThis is the logistic (sigmoid) function:\n\n# Visualize sigmoid function\nx &lt;- seq(-10, 10, 0.1)\nsigmoid &lt;- function(x) 1 / (1 + exp(-x))\n\nsigmoid_plot &lt;- tibble(\n  x = x,\n  probability = sigmoid(x)\n) %&gt;%\n  ggplot(aes(x = x, y = probability)) +\n  geom_line(linewidth = 1.5, color = \"darkblue\") +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  labs(\n    title = \"Logistic (Sigmoid) Function\",\n    subtitle = \"Maps linear combinations to probabilities [0,1]\",\n    x = \"Linear Combination (β₀ + β₁x₁ + ... + βₚxₚ)\",\n    y = \"P(Y = 1)\"\n  ) +\n  annotate(\"text\", x = 5, y = 0.25, label = \"Class 0 likely\", size = 5, color = \"blue\") +\n  annotate(\"text\", x = 5, y = 0.75, label = \"Class 1 likely\", size = 5, color = \"blue\")\n\n# Decision boundary illustration\nset.seed(123)\nbinary_data &lt;- tibble(\n  x1 = c(rnorm(50, -1, 1), rnorm(50, 1, 1)),\n  x2 = c(rnorm(50, -1, 1), rnorm(50, 1, 1)),\n  class = factor(rep(c(\"A\", \"B\"), each = 50))\n)\n\nboundary_plot &lt;- ggplot(binary_data, aes(x = x1, y = x2, color = class)) +\n  geom_point(size = 3, alpha = 0.7) +\n  stat_smooth(method = \"glm\", method.args = list(family = \"binomial\"), \n              se = FALSE, color = \"black\", linewidth = 1) +\n  scale_color_manual(values = c(\"A\" = \"red\", \"B\" = \"blue\")) +\n  labs(\n    title = \"Logistic Regression Decision Boundary\",\n    subtitle = \"Linear boundary in feature space\",\n    x = \"Feature 1\",\n    y = \"Feature 2\"\n  )\n\nsigmoid_plot + boundary_plot\n\n\n\n\n\n\n\n\n\n\nOdds and Log-Odds\nThe odds of an event: \\[\\text{Odds} = \\frac{P(Y = 1)}{P(Y = 0)} = \\frac{p}{1-p}\\]\nThe log-odds (logit): \\[\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1x_1 + ... + \\beta_px_p\\]\n\n\nMaximum Likelihood Estimation\nParameters are estimated by maximizing the likelihood:\n\\[L(\\beta) = \\prod_{i=1}^{n} p_i^{y_i}(1-p_i)^{1-y_i}\\]\nWhere \\(p_i = P(Y_i = 1 | X_i)\\)"
  },
  {
    "objectID": "14-classification.html#implementing-logistic-regression",
    "href": "14-classification.html#implementing-logistic-regression",
    "title": "Chapter 14: Classification Models - Theory and Implementation",
    "section": "Implementing Logistic Regression",
    "text": "Implementing Logistic Regression\n\n# Prepare credit data\ncredit_clean &lt;- credit_data %&gt;%\n  drop_na() %&gt;%\n  mutate(Status = factor(Status))\n\n# Split data\ncredit_split &lt;- initial_split(credit_clean, prop = 0.75, strata = Status)\ncredit_train &lt;- training(credit_split)\ncredit_test &lt;- testing(credit_split)\n\n# Create recipe\ncredit_recipe &lt;- recipe(Status ~ ., data = credit_train) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors())\n\n# Logistic regression specification\nlogistic_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  set_mode(\"classification\")\n\n# Workflow\nlogistic_wf &lt;- workflow() %&gt;%\n  add_recipe(credit_recipe) %&gt;%\n  add_model(logistic_spec)\n\n# Fit model\nlogistic_fit &lt;- logistic_wf %&gt;%\n  fit(credit_train)\n\n# Extract coefficients\nlogistic_coefs &lt;- logistic_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  tidy() %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  arrange(desc(abs(estimate)))\n\n# Visualize coefficients\nggplot(logistic_coefs %&gt;% head(10), \n       aes(x = reorder(term, estimate), y = estimate)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Logistic Regression Coefficients\",\n    subtitle = \"Top 10 most influential features\",\n    x = \"Feature\",\n    y = \"Coefficient (log-odds)\"\n  )"
  },
  {
    "objectID": "14-classification.html#decision-trees-theory",
    "href": "14-classification.html#decision-trees-theory",
    "title": "Chapter 14: Classification Models - Theory and Implementation",
    "section": "Decision Trees Theory",
    "text": "Decision Trees Theory\n\nHow Decision Trees Work\nDecision trees recursively partition the feature space using binary splits.\nSplitting Criteria:\nFor classification, common criteria include:\n\nGini Impurity: \\[G = \\sum_{k=1}^{K} p_k(1 - p_k)\\]\nEntropy (Information Gain): \\[H = -\\sum_{k=1}^{K} p_k \\log_2(p_k)\\]\n\nWhere \\(p_k\\) is the proportion of samples in class \\(k\\).\n\n# Visualize decision tree concepts\n# Create sample data for visualization\ntree_data &lt;- tibble(\n  x1 = runif(200, 0, 10),\n  x2 = runif(200, 0, 10),\n  class = factor(case_when(\n    x1 &lt; 3 & x2 &lt; 5 ~ \"A\",\n    x1 &lt; 3 & x2 &gt;= 5 ~ \"B\",\n    x1 &gt;= 3 & x1 &lt; 7 ~ \"C\",\n    TRUE ~ \"D\"\n  ))\n)\n\n# Decision tree boundaries\ntree_boundary_plot &lt;- ggplot(tree_data, aes(x = x1, y = x2, color = class)) +\n  geom_point(size = 2, alpha = 0.6) +\n  geom_vline(xintercept = c(3, 7), linetype = \"dashed\", linewidth = 1) +\n  geom_hline(yintercept = 5, linetype = \"dashed\", linewidth = 1) +\n  scale_color_viridis_d() +\n  labs(\n    title = \"Decision Tree Partitioning\",\n    subtitle = \"Recursive binary splits create rectangular regions\",\n    x = \"Feature 1\",\n    y = \"Feature 2\"\n  ) +\n  annotate(\"text\", x = 1.5, y = 2.5, label = \"Region A\", size = 4) +\n  annotate(\"text\", x = 1.5, y = 7.5, label = \"Region B\", size = 4) +\n  annotate(\"text\", x = 5, y = 5, label = \"Region C\", size = 4) +\n  annotate(\"text\", x = 8.5, y = 5, label = \"Region D\", size = 4)\n\n# Gini vs Entropy\np_range &lt;- seq(0.01, 0.99, 0.01)\nimpurity_data &lt;- tibble(\n  p = p_range,\n  Gini = 2 * p * (1 - p),\n  Entropy = -p * log2(p) - (1 - p) * log2(1 - p)\n) %&gt;%\n  pivot_longer(cols = c(Gini, Entropy), names_to = \"Measure\", values_to = \"Impurity\")\n\nimpurity_plot &lt;- ggplot(impurity_data, aes(x = p, y = Impurity, color = Measure)) +\n  geom_line(linewidth = 1.5) +\n  scale_color_manual(values = c(\"Gini\" = \"blue\", \"Entropy\" = \"red\")) +\n  labs(\n    title = \"Impurity Measures for Binary Classification\",\n    subtitle = \"Both measures peak at p = 0.5 (maximum uncertainty)\",\n    x = \"Proportion of Class 1\",\n    y = \"Impurity\"\n  )\n\ntree_boundary_plot + impurity_plot\n\n\n\n\n\n\n\n\n\n\nImplementing Decision Trees\n\n# Decision tree specification\ntree_spec &lt;- decision_tree(\n  cost_complexity = 0.01,\n  tree_depth = 10,\n  min_n = 20\n) %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\n# Workflow\ntree_wf &lt;- workflow() %&gt;%\n  add_recipe(credit_recipe) %&gt;%\n  add_model(tree_spec)\n\n# Fit model\ntree_fit &lt;- tree_wf %&gt;%\n  fit(credit_train)\n\n# Visualize tree (if rpart.plot is available)\nif (require(rpart.plot, quietly = TRUE)) {\n  tree_fit %&gt;%\n    extract_fit_engine() %&gt;%\n    rpart.plot(roundint = FALSE, type = 4, extra = 101)\n}\n\n# Feature importance\ntree_imp &lt;- tree_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  vip(num_features = 10)\n\ntree_imp + labs(title = \"Decision Tree Feature Importance\")"
  },
  {
    "objectID": "14-classification.html#random-forests-theory",
    "href": "14-classification.html#random-forests-theory",
    "title": "Chapter 14: Classification Models - Theory and Implementation",
    "section": "Random Forests Theory",
    "text": "Random Forests Theory\n\nEnsemble Learning\nRandom Forests combine multiple decision trees through:\n\nBootstrap Aggregating (Bagging): Train each tree on a bootstrap sample\nFeature Randomness: Consider random subset of features at each split\nVoting: Aggregate predictions (majority vote for classification)\n\nWhy it works: - Reduces overfitting through averaging - Decorrelates trees through randomness - Maintains low bias while reducing variance\n\n# Random Forest implementation\nrf_spec &lt;- rand_forest(\n  trees = 500,\n  mtry = 3,\n  min_n = 10\n) %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"classification\")\n\nrf_wf &lt;- workflow() %&gt;%\n  add_recipe(credit_recipe) %&gt;%\n  add_model(rf_spec)\n\n# Fit with cross-validation\nset.seed(123)\ncredit_folds &lt;- vfold_cv(credit_train, v = 5, strata = Status)\n\nrf_cv &lt;- rf_wf %&gt;%\n  fit_resamples(\n    resamples = credit_folds,\n    metrics = metric_set(roc_auc, accuracy, precision, recall),\n    control = control_resamples(save_pred = TRUE)\n  )\n\n# Performance metrics\ncollect_metrics(rf_cv) %&gt;%\n  knitr::kable(digits = 3)\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\naccuracy\nbinary\n0.796\n5\n0.004\npre0_mod0_post0\n\n\nprecision\nbinary\n0.693\n5\n0.018\npre0_mod0_post0\n\n\nrecall\nbinary\n0.354\n5\n0.019\npre0_mod0_post0\n\n\nroc_auc\nbinary\n0.824\n5\n0.009\npre0_mod0_post0\n\n\n\n\n# ROC curve\nrf_roc &lt;- rf_cv %&gt;%\n  collect_predictions() %&gt;%\n  roc_curve(Status, .pred_bad)\n\nautoplot(rf_roc) +\n  labs(title = \"Random Forest ROC Curve\",\n       subtitle = \"5-fold cross-validation results\")"
  },
  {
    "objectID": "14-classification.html#support-vector-machines-theory",
    "href": "14-classification.html#support-vector-machines-theory",
    "title": "Chapter 14: Classification Models - Theory and Implementation",
    "section": "Support Vector Machines Theory",
    "text": "Support Vector Machines Theory\n\nMaximum Margin Classifier\nSVMs find the hyperplane that maximizes the margin between classes.\nLinear SVM Optimization: \\[\\min_{w,b} \\frac{1}{2}||w||^2\\] Subject to: \\(y_i(w^Tx_i + b) \\geq 1\\) for all \\(i\\)\n\n\nThe Kernel Trick\nFor non-linear boundaries, SVMs use kernel functions to map data to higher dimensions:\nCommon Kernels: 1. Linear: \\(K(x_i, x_j) = x_i^T x_j\\) 2. Polynomial: \\(K(x_i, x_j) = (x_i^T x_j + c)^d\\) 3. RBF (Gaussian): \\(K(x_i, x_j) = \\exp(-\\gamma||x_i - x_j||^2)\\)\n\n# Demonstrate SVM with different kernels\n# Create non-linear data\nset.seed(123)\nspiral_data &lt;- tibble(\n  angle = runif(200, 0, 4 * pi),\n  radius = runif(200, 0.5, 2),\n  class = factor(rep(c(\"A\", \"B\"), each = 100))\n) %&gt;%\n  mutate(\n    x1 = radius * cos(angle) + ifelse(class == \"A\", 0, 0.5) + rnorm(200, 0, 0.2),\n    x2 = radius * sin(angle) + ifelse(class == \"A\", 0, 0.5) + rnorm(200, 0, 0.2)\n  )\n\n# Linear SVM\nsvm_linear_spec &lt;- svm_linear(cost = 1) %&gt;%\n  set_engine(\"kernlab\") %&gt;%\n  set_mode(\"classification\")\n\n# RBF SVM\nsvm_rbf_spec &lt;- svm_rbf(cost = 1, rbf_sigma = 0.1) %&gt;%\n  set_engine(\"kernlab\") %&gt;%\n  set_mode(\"classification\")\n\n# Fit both models\nsvm_linear_fit &lt;- svm_linear_spec %&gt;%\n  fit(class ~ x1 + x2, data = spiral_data)\n\nsvm_rbf_fit &lt;- svm_rbf_spec %&gt;%\n  fit(class ~ x1 + x2, data = spiral_data)\n\n# Create prediction grid\ngrid &lt;- expand_grid(\n  x1 = seq(min(spiral_data$x1), max(spiral_data$x1), length.out = 100),\n  x2 = seq(min(spiral_data$x2), max(spiral_data$x2), length.out = 100)\n)\n\n# Get predictions\ngrid_linear &lt;- grid %&gt;%\n  bind_cols(predict(svm_linear_fit, grid, type = \"prob\"))\n\ngrid_rbf &lt;- grid %&gt;%\n  bind_cols(predict(svm_rbf_fit, grid, type = \"prob\"))\n\n# Visualize\np_linear &lt;- ggplot() +\n  geom_tile(data = grid_linear, aes(x = x1, y = x2, fill = .pred_A), alpha = 0.3) +\n  geom_point(data = spiral_data, aes(x = x1, y = x2, color = class), size = 2) +\n  scale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\", midpoint = 0.5) +\n  scale_color_manual(values = c(\"A\" = \"red\", \"B\" = \"blue\")) +\n  labs(title = \"Linear SVM\", subtitle = \"Linear decision boundary\") +\n  theme(legend.position = \"none\")\n\np_rbf &lt;- ggplot() +\n  geom_tile(data = grid_rbf, aes(x = x1, y = x2, fill = .pred_A), alpha = 0.3) +\n  geom_point(data = spiral_data, aes(x = x1, y = x2, color = class), size = 2) +\n  scale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\", midpoint = 0.5) +\n  scale_color_manual(values = c(\"A\" = \"red\", \"B\" = \"blue\")) +\n  labs(title = \"RBF SVM\", subtitle = \"Non-linear decision boundary\") +\n  theme(legend.position = \"none\")\n\np_linear + p_rbf"
  },
  {
    "objectID": "14-classification.html#classification-metrics",
    "href": "14-classification.html#classification-metrics",
    "title": "Chapter 14: Classification Models - Theory and Implementation",
    "section": "Classification Metrics",
    "text": "Classification Metrics\n\nUnderstanding Different Metrics\n\n# Fit final model\nfinal_rf &lt;- rf_wf %&gt;%\n  fit(credit_train)\n\n# Get predictions\ncredit_pred &lt;- final_rf %&gt;%\n  predict(credit_test) %&gt;%\n  bind_cols(\n    final_rf %&gt;% predict(credit_test, type = \"prob\")\n  ) %&gt;%\n  bind_cols(credit_test %&gt;% select(Status))\n\n# Confusion matrix\nconf_mat &lt;- credit_pred %&gt;%\n  conf_mat(truth = Status, estimate = .pred_class)\n\nautoplot(conf_mat, type = \"heatmap\") +\n  scale_fill_gradient(low = \"white\", high = \"darkblue\") +\n  labs(title = \"Confusion Matrix\")\n\n\n\n\n\n\n\n# Calculate various metrics\nmetrics_summary &lt;- credit_pred %&gt;%\n  metrics(truth = Status, estimate = .pred_class, .pred_bad) %&gt;%\n  bind_rows(\n    credit_pred %&gt;% roc_auc(truth = Status, .pred_bad),\n    credit_pred %&gt;% pr_auc(truth = Status, .pred_bad)\n  )\n\nmetrics_summary %&gt;%\n  knitr::kable(digits = 3)\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n0.817\n\n\nkap\nbinary\n0.441\n\n\nmn_log_loss\nbinary\n0.416\n\n\nroc_auc\nbinary\n0.853\n\n\nroc_auc\nbinary\n0.853\n\n\npr_auc\nbinary\n0.663\n\n\n\n\n\n\n\nMetrics Explained\nConfusion Matrix Terms: - True Positives (TP): Correctly predicted positive - True Negatives (TN): Correctly predicted negative - False Positives (FP): Type I error - False Negatives (FN): Type II error\nKey Metrics:\n\\[\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\\]\n\\[\\text{Precision} = \\frac{TP}{TP + FP}\\]\n\\[\\text{Recall (Sensitivity)} = \\frac{TP}{TP + FN}\\]\n\\[\\text{Specificity} = \\frac{TN}{TN + FP}\\]\n\\[\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\]"
  },
  {
    "objectID": "14-classification.html#handling-class-imbalance",
    "href": "14-classification.html#handling-class-imbalance",
    "title": "Chapter 14: Classification Models - Theory and Implementation",
    "section": "Handling Class Imbalance",
    "text": "Handling Class Imbalance\n\nThe Problem\nClass imbalance occurs when one class is much more frequent than others.\n\n# Create imbalanced dataset\nset.seed(123)\nimbalanced_data &lt;- tibble(\n  x1 = c(rnorm(950, 0, 1), rnorm(50, 3, 1)),\n  x2 = c(rnorm(950, 0, 1), rnorm(50, 3, 1)),\n  class = factor(c(rep(\"Majority\", 950), rep(\"Minority\", 50)))\n)\n\n# Show class distribution\nclass_dist &lt;- imbalanced_data %&gt;%\n  count(class) %&gt;%\n  mutate(prop = n / sum(n))\n\nggplot(class_dist, aes(x = class, y = n, fill = class)) +\n  geom_col() +\n  geom_text(aes(label = paste0(n, \"\\n(\", round(prop * 100, 1), \"%)\")), \n            vjust = -0.5, size = 5) +\n  scale_fill_manual(values = c(\"Majority\" = \"steelblue\", \"Minority\" = \"coral\")) +\n  labs(\n    title = \"Class Imbalance Example\",\n    subtitle = \"95% Majority vs 5% Minority\",\n    y = \"Count\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nSolutions for Class Imbalance\n\n# 1. Class weights\nweighted_spec &lt;- logistic_reg(penalty = 0.01) %&gt;%\n  set_engine(\"glmnet\", \n             weights = ifelse(imbalanced_data$class == \"Minority\", 19, 1)) %&gt;%\n  set_mode(\"classification\")\n\n# 2. SMOTE (Synthetic Minority Over-sampling)\nlibrary(themis)  # For sampling methods\n\nimbalanced_recipe &lt;- recipe(class ~ ., data = imbalanced_data) %&gt;%\n  step_smote(class, over_ratio = 0.5) %&gt;%  # Create synthetic minority samples\n  step_normalize(all_predictors())\n\n# 3. Downsampling\ndownsample_recipe &lt;- recipe(class ~ ., data = imbalanced_data) %&gt;%\n  step_downsample(class, under_ratio = 1) %&gt;%\n  step_normalize(all_predictors())\n\n# Compare approaches\nsampling_comparison &lt;- tibble(\n  Method = c(\"Original\", \"SMOTE\", \"Downsampling\"),\n  Recipe = list(\n    recipe(class ~ ., data = imbalanced_data),\n    imbalanced_recipe,\n    downsample_recipe\n  )\n) %&gt;%\n  mutate(\n    prepped = map(Recipe, ~ prep(., training = imbalanced_data)),\n    baked = map(prepped, ~ bake(., new_data = NULL)),\n    class_counts = map(baked, ~ count(., class))\n  )\n\n# Show results\nsampling_comparison %&gt;%\n  select(Method, class_counts) %&gt;%\n  unnest(class_counts) %&gt;%\n  ggplot(aes(x = Method, y = n, fill = class)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Majority\" = \"steelblue\", \"Minority\" = \"coral\")) +\n  labs(\n    title = \"Effect of Different Sampling Strategies\",\n    y = \"Count\"\n  )"
  },
  {
    "objectID": "14-classification.html#multiclass-classification",
    "href": "14-classification.html#multiclass-classification",
    "title": "Chapter 14: Classification Models - Theory and Implementation",
    "section": "Multiclass Classification",
    "text": "Multiclass Classification\n\nOne-vs-Rest and One-vs-One\nFor K classes: - One-vs-Rest: Train K binary classifiers - One-vs-One: Train K(K-1)/2 binary classifiers\n\n# Multiclass example with penguins\npenguins_clean &lt;- palmerpenguins::penguins %&gt;%\n  drop_na()\n\n# Split data\npenguin_split &lt;- initial_split(penguins_clean, prop = 0.75, strata = species)\npenguin_train &lt;- training(penguin_split)\npenguin_test &lt;- testing(penguin_split)\n\n# Recipe\npenguin_recipe &lt;- recipe(species ~ ., data = penguin_train) %&gt;%\n  step_rm(year) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\n# Multinomial regression\nmultinom_spec &lt;- multinom_reg() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"classification\")\n\n# Workflow\nmultinom_wf &lt;- workflow() %&gt;%\n  add_recipe(penguin_recipe) %&gt;%\n  add_model(multinom_spec)\n\n# Fit model\nmultinom_fit &lt;- multinom_wf %&gt;%\n  fit(penguin_train)\n\n# Predictions\npenguin_pred &lt;- multinom_fit %&gt;%\n  predict(penguin_test) %&gt;%\n  bind_cols(\n    multinom_fit %&gt;% predict(penguin_test, type = \"prob\")\n  ) %&gt;%\n  bind_cols(penguin_test %&gt;% select(species))\n\n# Multiclass confusion matrix\nmulticlass_conf &lt;- penguin_pred %&gt;%\n  conf_mat(truth = species, estimate = .pred_class)\n\nautoplot(multiclass_conf, type = \"heatmap\") +\n  scale_fill_gradient(low = \"white\", high = \"darkgreen\") +\n  labs(title = \"Multiclass Confusion Matrix\")\n\n\n\n\n\n\n\n# Per-class metrics\npenguin_pred %&gt;%\n  accuracy(truth = species, estimate = .pred_class) %&gt;%\n  bind_rows(\n    penguin_pred %&gt;% \n      roc_auc(truth = species, .pred_Adelie:.pred_Gentoo)\n  ) %&gt;%\n  knitr::kable(digits = 3)\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nmulticlass\n1\n\n\nroc_auc\nhand_till\n1"
  },
  {
    "objectID": "14-classification.html#calibration-and-probability-thresholds",
    "href": "14-classification.html#calibration-and-probability-thresholds",
    "title": "Chapter 14: Classification Models - Theory and Implementation",
    "section": "Calibration and Probability Thresholds",
    "text": "Calibration and Probability Thresholds\n\nProbability Calibration\nWell-calibrated models produce probabilities that match actual frequencies.\n\n# Calibration plot\ncalibration_data &lt;- credit_pred %&gt;%\n  mutate(\n    prob_bin = cut(.pred_bad, breaks = seq(0, 1, 0.1), include.lowest = TRUE)\n  ) %&gt;%\n  group_by(prob_bin) %&gt;%\n  summarise(\n    mean_predicted = mean(.pred_bad),\n    fraction_positive = mean(Status == \"bad\"),\n    n = n()\n  ) %&gt;%\n  drop_na()\n\ncalib_plot &lt;- ggplot(calibration_data, aes(x = mean_predicted, y = fraction_positive)) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_point(aes(size = n), color = \"darkblue\") +\n  geom_line(color = \"darkblue\") +\n  scale_size_continuous(range = c(2, 8)) +\n  labs(\n    title = \"Calibration Plot\",\n    subtitle = \"Well-calibrated models follow the diagonal\",\n    x = \"Mean Predicted Probability\",\n    y = \"Fraction of Positives\",\n    size = \"Count\"\n  ) +\n  coord_equal()\n\n# Threshold optimization\nthresholds &lt;- seq(0.1, 0.9, 0.05)\nthreshold_metrics &lt;- map_df(thresholds, function(thresh) {\n  credit_pred %&gt;%\n    mutate(.pred_class_adj = factor(ifelse(.pred_bad &gt; thresh, \"bad\", \"good\"),\n                                    levels = c(\"bad\", \"good\"))) %&gt;%\n    metrics(truth = Status, estimate = .pred_class_adj) %&gt;%\n    mutate(threshold = thresh)\n})\n\nthresh_plot &lt;- threshold_metrics %&gt;%\n  filter(.metric %in% c(\"accuracy\", \"precision\", \"recall\")) %&gt;%\n  ggplot(aes(x = threshold, y = .estimate, color = .metric)) +\n  geom_line(linewidth = 1.2) +\n  geom_point(size = 2) +\n  scale_color_viridis_d() +\n  labs(\n    title = \"Metrics vs Classification Threshold\",\n    subtitle = \"Trade-off between different metrics\",\n    x = \"Probability Threshold\",\n    y = \"Metric Value\",\n    color = \"Metric\"\n  )\n\ncalib_plot + thresh_plot"
  },
  {
    "objectID": "14-classification.html#model-comparison",
    "href": "14-classification.html#model-comparison",
    "title": "Chapter 14: Classification Models - Theory and Implementation",
    "section": "Model Comparison",
    "text": "Model Comparison\n\n# Compare multiple models\nmodels &lt;- list(\n  \"Logistic Regression\" = logistic_spec,\n  \"Decision Tree\" = tree_spec,\n  \"Random Forest\" = rf_spec,\n  \"SVM (RBF)\" = svm_rbf(cost = 1, rbf_sigma = 0.01) %&gt;%\n    set_engine(\"kernlab\") %&gt;%\n    set_mode(\"classification\")\n)\n\n# Fit all models\nmodel_fits &lt;- map(models, function(spec) {\n  workflow() %&gt;%\n    add_recipe(credit_recipe) %&gt;%\n    add_model(spec) %&gt;%\n    fit_resamples(\n      resamples = credit_folds,\n      metrics = metric_set(accuracy, roc_auc, precision, recall),\n      control = control_resamples(save_pred = TRUE)\n    )\n})\n\n# Collect metrics\nmodel_metrics &lt;- map2_df(model_fits, names(model_fits), function(fit, name) {\n  collect_metrics(fit) %&gt;%\n    mutate(model = name)\n})\n\n# Visualize comparison\nggplot(model_metrics, aes(x = model, y = mean, fill = model)) +\n  geom_col() +\n  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), width = 0.2) +\n  facet_wrap(~.metric, scales = \"free_y\") +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"Model Performance Comparison\",\n    subtitle = \"5-fold cross-validation results\",\n    y = \"Score\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n# ROC curves comparison\nroc_data &lt;- map2_df(model_fits, names(model_fits), function(fit, name) {\n  collect_predictions(fit) %&gt;%\n    roc_curve(Status, .pred_bad) %&gt;%\n    mutate(model = name)\n})\n\nggplot(roc_data, aes(x = 1 - specificity, y = sensitivity, color = model)) +\n  geom_line(linewidth = 1.2) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  scale_color_viridis_d() +\n  labs(\n    title = \"ROC Curves Comparison\",\n    subtitle = \"All models performance\",\n    x = \"False Positive Rate (1 - Specificity)\",\n    y = \"True Positive Rate (Sensitivity)\"\n  ) +\n  coord_equal()"
  },
  {
    "objectID": "14-classification.html#exercises",
    "href": "14-classification.html#exercises",
    "title": "Chapter 14: Classification Models - Theory and Implementation",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Implement Different Classifiers\nBuild and compare three different classifiers on the cells dataset:\n\n# Your solution\n# Prepare data\ncells_clean &lt;- cells %&gt;%\n  select(-case) %&gt;%\n  drop_na()\n\ncells_split &lt;- initial_split(cells_clean, prop = 0.75, strata = class)\ncells_train &lt;- training(cells_split)\ncells_test &lt;- testing(cells_split)\n\n# Recipe\ncells_recipe &lt;- recipe(class ~ ., data = cells_train) %&gt;%\n  step_normalize(all_numeric_predictors())\n\n# Models\nmodels_ex1 &lt;- list(\n  knn = nearest_neighbor(neighbors = 5) %&gt;%\n    set_engine(\"kknn\") %&gt;%\n    set_mode(\"classification\"),\n  \n  lda = discrim_linear() %&gt;%\n    set_engine(\"MASS\") %&gt;%\n    set_mode(\"classification\"),\n  \n  nb = naive_Bayes() %&gt;%\n    set_engine(\"naivebayes\") %&gt;%\n    set_mode(\"classification\")\n)\n\n# Fit and evaluate\ncells_folds &lt;- vfold_cv(cells_train, v = 5, strata = class)\n\nresults_ex1 &lt;- map_df(names(models_ex1), function(model_name) {\n  wf &lt;- workflow() %&gt;%\n    add_recipe(cells_recipe) %&gt;%\n    add_model(models_ex1[[model_name]])\n  \n  fit_resamples(\n    wf,\n    resamples = cells_folds,\n    metrics = metric_set(accuracy, roc_auc)\n  ) %&gt;%\n    collect_metrics() %&gt;%\n    mutate(model = model_name)\n})\n\n# Visualize results\nggplot(results_ex1, aes(x = model, y = mean, fill = .metric)) +\n  geom_col(position = \"dodge\") +\n  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err),\n                position = position_dodge(0.9), width = 0.2) +\n  scale_fill_viridis_d() +\n  labs(title = \"Model Comparison on Cells Dataset\",\n       y = \"Score\")\n\n\n\n\n\n\n\n\n\n\nExercise 2: Handle Class Imbalance\nWork with an imbalanced dataset and apply different techniques:\n\n# Your solution\n# Create severely imbalanced data\nset.seed(456)\nsevere_imbalanced &lt;- tibble(\n  x1 = c(rnorm(980, 0, 1), rnorm(20, 2, 0.5)),\n  x2 = c(rnorm(980, 0, 1), rnorm(20, 2, 0.5)),\n  x3 = c(rnorm(980, 0, 1), rnorm(20, 2, 0.5)),\n  class = factor(c(rep(\"common\", 980), rep(\"rare\", 20)))\n)\n\n# Split data\nimb_split &lt;- initial_split(severe_imbalanced, prop = 0.75, strata = class)\nimb_train &lt;- training(imb_split)\nimb_test &lt;- testing(imb_split)\n\n# Different recipes\nrecipe_baseline &lt;- recipe(class ~ ., data = imb_train) %&gt;%\n  step_normalize(all_predictors())\n\nrecipe_upsample &lt;- recipe(class ~ ., data = imb_train) %&gt;%\n  step_upsample(class, over_ratio = 1) %&gt;%\n  step_normalize(all_predictors())\n\nrecipe_rose &lt;- recipe(class ~ ., data = imb_train) %&gt;%\n  step_rose(class) %&gt;%\n  step_normalize(all_predictors())\n\n# Compare approaches\nrecipes_list &lt;- list(\n  baseline = recipe_baseline,\n  upsample = recipe_upsample,\n  rose = recipe_rose\n)\n\n# Use same model for all\nrf_imb_spec &lt;- rand_forest(trees = 100) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n# Evaluate each approach\nimb_results &lt;- map_df(names(recipes_list), function(recipe_name) {\n  wf &lt;- workflow() %&gt;%\n    add_recipe(recipes_list[[recipe_name]]) %&gt;%\n    add_model(rf_imb_spec)\n  \n  fit &lt;- wf %&gt;% fit(imb_train)\n  \n  pred &lt;- fit %&gt;%\n    predict(imb_test) %&gt;%\n    bind_cols(imb_test %&gt;% select(class))\n  \n  pred %&gt;%\n    metrics(truth = class, estimate = .pred_class) %&gt;%\n    mutate(method = recipe_name)\n})\n\n# Plot results\nggplot(imb_results, aes(x = method, y = .estimate, fill = .metric)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_viridis_d() +\n  labs(title = \"Handling Class Imbalance: Method Comparison\",\n       y = \"Score\") +\n  facet_wrap(~.metric, scales = \"free_y\")\n\n\n\n\n\n\n\n\n\n\nExercise 3: Optimize Classification Threshold\nFind the optimal threshold for a specific business objective:\n\n# Your solution\n# Business scenario: False negatives cost 5x more than false positives\ncost_fn &lt;- 5  # Cost of false negative\ncost_fp &lt;- 1  # Cost of false positive\n\n# Get probabilities\nfinal_rf_refit &lt;- rf_wf %&gt;% fit(credit_train)\nprob_preds &lt;- final_rf_refit %&gt;%\n  predict(credit_test, type = \"prob\") %&gt;%\n  bind_cols(credit_test %&gt;% select(Status))\n\n# Calculate cost for different thresholds\nthreshold_costs &lt;- map_df(seq(0.1, 0.9, 0.01), function(thresh) {\n  preds &lt;- prob_preds %&gt;%\n    mutate(\n      pred_class = factor(ifelse(.pred_bad &gt; thresh, \"bad\", \"good\"),\n                         levels = c(\"bad\", \"good\"))\n    )\n  \n  cm &lt;- preds %&gt;%\n    conf_mat(truth = Status, estimate = pred_class)\n  \n  # Extract values from confusion matrix\n  tn &lt;- cm$table[1,1]\n  fp &lt;- cm$table[1,2]\n  fn &lt;- cm$table[2,1]\n  tp &lt;- cm$table[2,2]\n  \n  tibble(\n    threshold = thresh,\n    total_cost = fp * cost_fp + fn * cost_fn,\n    accuracy = (tp + tn) / (tp + tn + fp + fn),\n    precision = tp / (tp + fp),\n    recall = tp / (tp + fn)\n  )\n})\n\n# Find optimal threshold\noptimal_thresh &lt;- threshold_costs %&gt;%\n  arrange(total_cost) %&gt;%\n  slice(1)\n\n# Visualize\nggplot(threshold_costs, aes(x = threshold)) +\n  geom_line(aes(y = total_cost), color = \"red\", linewidth = 1.2) +\n  geom_line(aes(y = accuracy * max(total_cost)), color = \"blue\", linewidth = 1.2) +\n  geom_vline(xintercept = optimal_thresh$threshold, \n             linetype = \"dashed\", color = \"green\", linewidth = 1) +\n  scale_y_continuous(\n    name = \"Total Cost\",\n    sec.axis = sec_axis(~./max(threshold_costs$total_cost), name = \"Accuracy\")\n  ) +\n  labs(\n    title = \"Cost-Sensitive Threshold Optimization\",\n    subtitle = paste(\"Optimal threshold:\", round(optimal_thresh$threshold, 3)),\n    x = \"Classification Threshold\"\n  ) +\n  theme(axis.title.y.left = element_text(color = \"red\"),\n        axis.title.y.right = element_text(color = \"blue\"))\n\n\n\n\n\n\n\nprint(paste(\"Optimal threshold:\", optimal_thresh$threshold))\n\n[1] \"Optimal threshold: 0.23\"\n\nprint(paste(\"Total cost at optimal threshold:\", optimal_thresh$total_cost))\n\n[1] \"Total cost at optimal threshold: 399\""
  },
  {
    "objectID": "14-classification.html#summary",
    "href": "14-classification.html#summary",
    "title": "Chapter 14: Classification Models - Theory and Implementation",
    "section": "Summary",
    "text": "Summary\nYou’ve mastered classification theory and practice:\n✅ Logistic regression mathematics and implementation\n✅ Decision trees and splitting criteria\n✅ Random forests and ensemble methods\n✅ Support vector machines and kernel trick\n✅ Classification metrics and their interpretation\n✅ Handling class imbalance\n✅ Multiclass classification strategies\n✅ Probability calibration and threshold optimization"
  },
  {
    "objectID": "14-classification.html#whats-next",
    "href": "14-classification.html#whats-next",
    "title": "Chapter 14: Classification Models - Theory and Implementation",
    "section": "What’s Next?",
    "text": "What’s Next?\nIn Chapter 15, we’ll explore regression models with similar depth in theory and practice."
  },
  {
    "objectID": "14-classification.html#additional-resources",
    "href": "14-classification.html#additional-resources",
    "title": "Chapter 14: Classification Models - Theory and Implementation",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nThe Elements of Statistical Learning\nPattern Recognition and Machine Learning\nApplied Predictive Modeling\nClassification and Regression Trees"
  },
  {
    "objectID": "17-unsupervised-learning.html",
    "href": "17-unsupervised-learning.html",
    "title": "Chapter 17: Unsupervised Learning - Discovering Hidden Patterns",
    "section": "",
    "text": "By the end of this chapter, you will master:\n\nThe fundamentals of unsupervised learning\nClustering algorithms (K-means, hierarchical, DBSCAN)\nDimensionality reduction (PCA, t-SNE, UMAP)\nAnomaly detection techniques\nMarket basket analysis and association rules\nPractical implementation with tidymodels\nEvaluating unsupervised models\nReal-world applications"
  },
  {
    "objectID": "17-unsupervised-learning.html#learning-objectives",
    "href": "17-unsupervised-learning.html#learning-objectives",
    "title": "Chapter 17: Unsupervised Learning - Discovering Hidden Patterns",
    "section": "",
    "text": "By the end of this chapter, you will master:\n\nThe fundamentals of unsupervised learning\nClustering algorithms (K-means, hierarchical, DBSCAN)\nDimensionality reduction (PCA, t-SNE, UMAP)\nAnomaly detection techniques\nMarket basket analysis and association rules\nPractical implementation with tidymodels\nEvaluating unsupervised models\nReal-world applications"
  },
  {
    "objectID": "17-unsupervised-learning.html#the-art-of-finding-structure-without-labels",
    "href": "17-unsupervised-learning.html#the-art-of-finding-structure-without-labels",
    "title": "Chapter 17: Unsupervised Learning - Discovering Hidden Patterns",
    "section": "The Art of Finding Structure Without Labels",
    "text": "The Art of Finding Structure Without Labels\nImagine you’re an explorer discovering a new continent. You have no map, no guide, just raw observations. Your task is to identify natural groupings - mountain ranges, river systems, ecosystems. This is the essence of unsupervised learning: finding structure in data without predefined categories.\nUnlike supervised learning where we have labels to guide us, unsupervised learning must discover patterns purely from the data itself. This makes it both challenging and powerful - we can uncover insights we didn’t even know to look for.\n\nlibrary(tidymodels)\n\n-- Attaching packages -------------------------------------- tidymodels 1.4.1 --\n\n\nv broom        1.0.10     v recipes      1.3.1 \nv dials        1.4.2      v rsample      1.3.1 \nv dplyr        1.1.4      v tailor       0.1.0 \nv ggplot2      4.0.0      v tidyr        1.3.1 \nv infer        1.0.9      v tune         2.0.0 \nv modeldata    1.5.1      v workflows    1.3.0 \nv parsnip      1.3.3      v workflowsets 1.1.1 \nv purrr        1.1.0      v yardstick    1.3.2 \n\n\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx purrr::discard() masks scales::discard()\nx dplyr::filter()  masks stats::filter()\nx dplyr::lag()     masks stats::lag()\nx recipes::step()  masks stats::step()\n\nlibrary(tidyverse)\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv forcats   1.0.0     v stringr   1.5.2\nv lubridate 1.9.4     v tibble    3.3.0\nv readr     2.1.5     \n\n\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx readr::col_factor() masks scales::col_factor()\nx purrr::discard()    masks scales::discard()\nx dplyr::filter()     masks stats::filter()\nx stringr::fixed()    masks recipes::fixed()\nx dplyr::lag()        masks stats::lag()\nx readr::spec()       masks yardstick::spec()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidyclust)  # For clustering with tidymodels\n\n\nAdjuntando el paquete: 'tidyclust'\n\nThe following objects are masked from 'package:parsnip':\n\n    knit_engine_docs, list_md_problems\n\nlibrary(cluster)    # For clustering algorithms\n\n\nAdjuntando el paquete: 'cluster'\n\nThe following object is masked from 'package:tidyclust':\n\n    silhouette\n\nlibrary(factoextra) # For visualization\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nlibrary(dendextend) # For dendrograms\n\n\n---------------------\nWelcome to dendextend version 1.19.1\nType citation('dendextend') for how to cite the package.\n\nType browseVignettes(package = 'dendextend') for the package vignette.\nThe github page is: https://github.com/talgalili/dendextend/\n\nSuggestions and bug-reports can be submitted at: https://github.com/talgalili/dendextend/issues\nYou may ask questions at stackoverflow, use the r and dendextend tags: \n     https://stackoverflow.com/questions/tagged/dendextend\n\n    To suppress this message use:  suppressPackageStartupMessages(library(dendextend))\n---------------------\n\n\nAdjuntando el paquete: 'dendextend'\n\nThe following object is masked from 'package:dials':\n\n    prune\n\nThe following object is masked from 'package:stats':\n\n    cutree\n\nlibrary(dbscan)     # For DBSCAN\n\n\nAdjuntando el paquete: 'dbscan'\n\nThe following object is masked from 'package:stats':\n\n    as.dendrogram\n\nlibrary(ggfortify)  # For PCA plots\n\nRegistered S3 method overwritten by 'ggfortify':\n  method          from   \n  autoplot.glmnet parsnip\n\nlibrary(plotly)     # For 3D visualizations\n\n\nAdjuntando el paquete: 'plotly'\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nlibrary(corrplot)   # For correlation plots\n\ncorrplot 0.95 loaded\n\n# Set theme and seed\ntheme_set(theme_minimal())\nset.seed(123)\n\n# Load data\ndata(ames)\n\n# Prepare data for unsupervised learning\names_numeric &lt;- ames %&gt;%\n  select(where(is.numeric)) %&gt;%\n  select(-Sale_Price) %&gt;%  # Remove target for unsupervised learning\n  na.omit() %&gt;%\n  slice_sample(n = 500)  # Sample for computational efficiency"
  },
  {
    "objectID": "17-unsupervised-learning.html#clustering-finding-natural-groups",
    "href": "17-unsupervised-learning.html#clustering-finding-natural-groups",
    "title": "Chapter 17: Unsupervised Learning - Discovering Hidden Patterns",
    "section": "Clustering: Finding Natural Groups",
    "text": "Clustering: Finding Natural Groups\n\nK-Means Clustering\nK-means is the workhorse of clustering algorithms. It partitions data into K clusters by minimizing within-cluster variance:\n\n# Prepare data for clustering\nclustering_data &lt;- ames_numeric %&gt;%\n  select(Gr_Liv_Area, Lot_Area, Year_Built, Total_Bsmt_SF, Garage_Area)\n\n# Scale the data (crucial for distance-based methods!)\nscaled_data &lt;- scale(clustering_data)\n\n# Determine optimal number of clusters using elbow method\nwss &lt;- map_dbl(1:10, function(k) {\n  kmeans(scaled_data, centers = k, nstart = 25)$tot.withinss\n})\n\nelbow_data &lt;- tibble(k = 1:10, wss = wss)\n\nggplot(elbow_data, aes(x = k, y = wss)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 3) +\n  geom_vline(xintercept = 4, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Elbow Method for Optimal K\",\n    subtitle = \"Look for the 'elbow' where WSS decrease slows\",\n    x = \"Number of Clusters (k)\",\n    y = \"Total Within-Cluster Sum of Squares\"\n  )\n\n\n\n\n\n\n\n# Silhouette analysis for cluster validation\nsilhouette_scores &lt;- map_dbl(2:10, function(k) {\n  km &lt;- kmeans(scaled_data, centers = k, nstart = 25)\n  ss &lt;- silhouette(km$cluster, dist(scaled_data))\n  mean(ss[, 3])\n})\n\nsilhouette_data &lt;- tibble(k = 2:10, silhouette = silhouette_scores)\n\nggplot(silhouette_data, aes(x = k, y = silhouette)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 3) +\n  geom_vline(xintercept = which.max(silhouette_scores) + 1, \n             linetype = \"dashed\", color = \"blue\") +\n  labs(\n    title = \"Silhouette Analysis\",\n    subtitle = \"Higher values indicate better-defined clusters\",\n    x = \"Number of Clusters (k)\",\n    y = \"Average Silhouette Score\"\n  )\n\n\n\n\n\n\n\n# Perform K-means with optimal k=4\nkm_result &lt;- kmeans(scaled_data, centers = 4, nstart = 25)\n\n# Add cluster assignments back to data\nclustered_data &lt;- clustering_data %&gt;%\n  mutate(cluster = factor(km_result$cluster))\n\n# Visualize clusters\np1 &lt;- ggplot(clustered_data, aes(x = Gr_Liv_Area, y = Lot_Area, \n                                  color = cluster)) +\n  geom_point(size = 2, alpha = 0.7) +\n  labs(title = \"Clusters: Living Area vs Quality\")\n\np2 &lt;- ggplot(clustered_data, aes(x = Year_Built, y = Total_Bsmt_SF, \n                                  color = cluster)) +\n  geom_point(size = 2, alpha = 0.7) +\n  labs(title = \"Clusters: Year vs Basement\")\n\nlibrary(patchwork)\np1 + p2\n\n\n\n\n\n\n\n# Cluster characteristics\ncluster_profiles &lt;- clustered_data %&gt;%\n  group_by(cluster) %&gt;%\n  summarise(across(everything(), list(mean = mean, sd = sd)), \n            count = n()) %&gt;%\n  pivot_longer(cols = -c(cluster, count),\n               names_to = c(\"variable\", \"stat\"),\n               names_sep = \"_(?=mean|sd)\") %&gt;%\n  pivot_wider(names_from = stat, values_from = value)\n\nknitr::kable(cluster_profiles, digits = 2)\n\n\n\n\ncluster\ncount\nvariable\nmean\nsd\n\n\n\n\n1\n8\nGr_Liv_Area\n2513.38\n925.13\n\n\n1\n8\nLot_Area\n56046.50\n24669.55\n\n\n1\n8\nYear_Built\n1984.50\n17.14\n\n\n1\n8\nTotal_Bsmt_SF\n1550.62\n764.04\n\n\n1\n8\nGarage_Area\n793.00\n181.72\n\n\n2\n175\nGr_Liv_Area\n1296.11\n409.47\n\n\n2\n175\nLot_Area\n8466.14\n3142.00\n\n\n2\n175\nYear_Built\n1937.54\n20.35\n\n\n2\n175\nTotal_Bsmt_SF\n784.01\n293.47\n\n\n2\n175\nGarage_Area\n294.43\n166.78\n\n\n3\n236\nGr_Liv_Area\n1397.47\n306.68\n\n\n3\n236\nLot_Area\n9413.75\n4132.14\n\n\n3\n236\nYear_Built\n1986.24\n17.40\n\n\n3\n236\nTotal_Bsmt_SF\n1065.54\n295.28\n\n\n3\n236\nGarage_Area\n487.47\n131.59\n\n\n4\n81\nGr_Liv_Area\n2146.53\n465.84\n\n\n4\n81\nLot_Area\n13104.25\n4393.41\n\n\n4\n81\nYear_Built\n1991.11\n21.57\n\n\n4\n81\nTotal_Bsmt_SF\n1577.49\n436.29\n\n\n4\n81\nGarage_Area\n752.70\n171.67\n\n\n\n\n\nThe mathematics of K-means: - Objective: Minimize \\(\\sum_{i=1}^{k} \\sum_{x \\in C_i} ||x - \\mu_i||^2\\) - Algorithm: Iteratively assign points and update centroids - Assumptions: Spherical clusters, similar sizes, similar densities\n\n\nHierarchical Clustering\nHierarchical clustering builds a tree of clusters, allowing exploration at different granularities:\n\n# Calculate distance matrix\ndist_matrix &lt;- dist(scaled_data, method = \"euclidean\")\n\n# Perform hierarchical clustering with different linkage methods\nhc_complete &lt;- hclust(dist_matrix, method = \"complete\")\nhc_average &lt;- hclust(dist_matrix, method = \"average\")\nhc_ward &lt;- hclust(dist_matrix, method = \"ward.D2\")\n\n# Compare dendrograms\npar(mfrow = c(1, 3))\nplot(hc_complete, main = \"Complete Linkage\", labels = FALSE, xlab = \"\")\nplot(hc_average, main = \"Average Linkage\", labels = FALSE, xlab = \"\")\nplot(hc_ward, main = \"Ward's Method\", labels = FALSE, xlab = \"\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n# Use Ward's method (often gives best results)\nhc_clusters &lt;- cutree(hc_ward, k = 4)\n\n# Create a prettier dendrogram\ndend &lt;- as.dendrogram(hc_ward)\ndend &lt;- color_branches(dend, k = 4)\n\nplot(dend, main = \"Hierarchical Clustering Dendrogram\",\n     leaflab = \"none\", ylab = \"Height\")\n\n\n\n\n\n\n\n# Compare with K-means\nclustering_comparison &lt;- tibble(\n  kmeans = km_result$cluster,\n  hierarchical = hc_clusters\n)\n\n# Confusion matrix between methods\ntable(clustering_comparison$kmeans, clustering_comparison$hierarchical)\n\n   \n      1   2   3   4\n  1   0   0   0   8\n  2   0  63 110   2\n  3   3 229   1   3\n  4  66  12   1   2\n\n# Visualize both clustering results\ncomparison_data &lt;- clustering_data %&gt;%\n  mutate(\n    kmeans = factor(km_result$cluster),\n    hierarchical = factor(hc_clusters)\n  )\n\np1 &lt;- ggplot(comparison_data, aes(x = Gr_Liv_Area, y = Lot_Area, \n                                   color = kmeans)) +\n  geom_point(size = 2) +\n  labs(title = \"K-Means Clustering\")\n\np2 &lt;- ggplot(comparison_data, aes(x = Gr_Liv_Area, y = Lot_Area, \n                                   color = hierarchical)) +\n  geom_point(size = 2) +\n  labs(title = \"Hierarchical Clustering\")\n\np1 + p2\n\n\n\n\n\n\n\n\n\n\nDBSCAN: Density-Based Clustering\nDBSCAN finds clusters of arbitrary shape and identifies outliers:\n\n# Create data with non-spherical clusters\nset.seed(456)\nmoon_data &lt;- rbind(\n  # First crescent\n  tibble(\n    x = cos(seq(0, pi, length.out = 100)) + rnorm(100, sd = 0.1),\n    y = sin(seq(0, pi, length.out = 100)) + rnorm(100, sd = 0.1),\n    true_cluster = 1\n  ),\n  # Second crescent (shifted and flipped)\n  tibble(\n    x = 1 - cos(seq(0, pi, length.out = 100)) + rnorm(100, sd = 0.1),\n    y = -0.5 - sin(seq(0, pi, length.out = 100)) + rnorm(100, sd = 0.1),\n    true_cluster = 2\n  ),\n  # Noise points\n  tibble(\n    x = runif(20, -1, 2),\n    y = runif(20, -2, 1),\n    true_cluster = 0\n  )\n)\n\n# Try K-means (will fail on non-spherical clusters)\nkm_moon &lt;- kmeans(moon_data[, c(\"x\", \"y\")], centers = 2)\n\n# DBSCAN\ndb_result &lt;- dbscan(moon_data[, c(\"x\", \"y\")], eps = 0.3, minPts = 5)\n\n# Compare results\nmoon_results &lt;- moon_data %&gt;%\n  mutate(\n    kmeans = factor(km_moon$cluster),\n    dbscan = factor(db_result$cluster)\n  )\n\np1 &lt;- ggplot(moon_results, aes(x = x, y = y, color = factor(true_cluster))) +\n  geom_point(size = 2) +\n  labs(title = \"True Clusters\", color = \"Cluster\") +\n  scale_color_manual(values = c(\"0\" = \"gray\", \"1\" = \"blue\", \"2\" = \"red\"))\n\np2 &lt;- ggplot(moon_results, aes(x = x, y = y, color = kmeans)) +\n  geom_point(size = 2) +\n  labs(title = \"K-Means Result\", color = \"Cluster\")\n\np3 &lt;- ggplot(moon_results, aes(x = x, y = y, color = dbscan)) +\n  geom_point(size = 2) +\n  labs(title = \"DBSCAN Result\", color = \"Cluster\") +\n  scale_color_manual(values = c(\"0\" = \"gray\", \"1\" = \"blue\", \"2\" = \"red\"))\n\np1 + p2 + p3\n\n\n\n\n\n\n\n# Apply DBSCAN to housing data\n# Find optimal eps using k-nearest neighbor distance\nknn_dist &lt;- kNNdist(scaled_data, k = 5)\nknn_dist_sorted &lt;- sort(knn_dist)\n\nplot(knn_dist_sorted, type = \"l\",\n     main = \"K-NN Distance Plot for eps Selection\",\n     xlab = \"Points sorted by distance\",\n     ylab = \"5-NN Distance\")\nabline(h = 1.5, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n# Apply DBSCAN\ndb_housing &lt;- dbscan(scaled_data, eps = 1.5, minPts = 5)\n\n# Analyze results\ndbscan_summary &lt;- tibble(\n  cluster = db_housing$cluster\n) %&gt;%\n  count(cluster) %&gt;%\n  mutate(\n    type = if_else(cluster == 0, \"Noise\", paste(\"Cluster\", cluster))\n  )\n\nknitr::kable(dbscan_summary)\n\n\n\n\ncluster\nn\ntype\n\n\n\n\n0\n18\nNoise\n\n\n1\n482\nCluster 1"
  },
  {
    "objectID": "17-unsupervised-learning.html#dimensionality-reduction",
    "href": "17-unsupervised-learning.html#dimensionality-reduction",
    "title": "Chapter 17: Unsupervised Learning - Discovering Hidden Patterns",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction\n\nPrincipal Component Analysis (PCA)\nPCA finds the directions of maximum variance in high-dimensional data:\n\n# Perform PCA\npca_result &lt;- prcomp(scaled_data, center = FALSE, scale. = FALSE)\n\n# Variance explained\nvariance_explained &lt;- tibble(\n  PC = paste0(\"PC\", 1:length(pca_result$sdev)),\n  var_explained = pca_result$sdev^2 / sum(pca_result$sdev^2),\n  cumulative_var = cumsum(var_explained)\n)\n\nggplot(variance_explained[1:10, ], aes(x = PC, y = var_explained)) +\n  geom_col(fill = \"steelblue\") +\n  geom_line(aes(y = cumulative_var, group = 1), color = \"red\", linewidth = 1) +\n  geom_point(aes(y = cumulative_var), color = \"red\", size = 2) +\n  labs(\n    title = \"PCA: Variance Explained\",\n    subtitle = \"Red line shows cumulative variance\",\n    x = \"Principal Component\",\n    y = \"Proportion of Variance\"\n  ) +\n  scale_y_continuous(sec.axis = sec_axis(~., name = \"Cumulative Variance\"))\n\n\n\n\n\n\n\n# Biplot: variables and observations\nautoplot(pca_result, data = clustered_data, colour = 'cluster',\n         loadings = TRUE, loadings.label = TRUE,\n         loadings.label.size = 3) +\n  labs(title = \"PCA Biplot with Clusters\")\n\n\n\n\n\n\n\n# Interpret principal components\nloadings &lt;- as.data.frame(pca_result$rotation[, 1:3])\nloadings$variable &lt;- rownames(loadings)\n\nloadings_long &lt;- loadings %&gt;%\n  pivot_longer(cols = PC1:PC3, names_to = \"PC\", values_to = \"loading\")\n\nggplot(loadings_long, aes(x = variable, y = loading, fill = variable)) +\n  geom_col() +\n  facet_wrap(~PC) +\n  coord_flip() +\n  labs(\n    title = \"PCA Loadings\",\n    subtitle = \"Variable contributions to each principal component\",\n    x = \"Variable\",\n    y = \"Loading\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n# Use PCA for visualization\npca_data &lt;- as.data.frame(pca_result$x[, 1:3]) %&gt;%\n  mutate(cluster = clustered_data$cluster)\n\n# 2D visualization\nggplot(pca_data, aes(x = PC1, y = PC2, color = cluster)) +\n  geom_point(size = 2, alpha = 0.7) +\n  stat_ellipse(level = 0.95) +\n  labs(\n    title = \"Clusters in PCA Space\",\n    subtitle = \"First two principal components\",\n    x = paste0(\"PC1 (\", round(variance_explained$var_explained[1] * 100, 1), \"%)\"),\n    y = paste0(\"PC2 (\", round(variance_explained$var_explained[2] * 100, 1), \"%)\")\n  )\n\n\n\n\n\n\n\n# 3D visualization with plotly\nplot_ly(pca_data, x = ~PC1, y = ~PC2, z = ~PC3, color = ~cluster,\n        type = \"scatter3d\", mode = \"markers\",\n        marker = list(size = 5)) %&gt;%\n  layout(title = \"3D PCA Visualization\")\n\n\n\n\n\nThe mathematics of PCA: - Objective: Find orthogonal axes maximizing variance - Method: Eigendecomposition of covariance matrix - Result: Linear combinations of original variables - Key insight: Most variance often captured in few components\n\n\nt-SNE and UMAP: Non-linear Dimensionality Reduction\nFor complex, non-linear structures:\n\n# t-SNE (computationally intensive, using sample)\nlibrary(Rtsne)\n\nset.seed(123)\ntsne_result &lt;- Rtsne(scaled_data[1:200, ], dims = 2, perplexity = 30, \n                     verbose = FALSE, max_iter = 500)\n\ntsne_data &lt;- as.data.frame(tsne_result$Y) %&gt;%\n  set_names(c(\"tSNE1\", \"tSNE2\")) %&gt;%\n  mutate(cluster = clustered_data$cluster[1:200])\n\nggplot(tsne_data, aes(x = tSNE1, y = tSNE2, color = cluster)) +\n  geom_point(size = 2, alpha = 0.7) +\n  labs(\n    title = \"t-SNE Visualization\",\n    subtitle = \"Non-linear dimensionality reduction preserves local structure\"\n  )\n\n\n\n\n\n\n\n# Compare PCA vs t-SNE\ncomparison &lt;- bind_rows(\n  pca_data[1:200, c(\"PC1\", \"PC2\", \"cluster\")] %&gt;%\n    mutate(method = \"PCA\") %&gt;%\n    rename(Dim1 = PC1, Dim2 = PC2),\n  tsne_data %&gt;%\n    mutate(method = \"t-SNE\") %&gt;%\n    rename(Dim1 = tSNE1, Dim2 = tSNE2)\n)\n\nggplot(comparison, aes(x = Dim1, y = Dim2, color = cluster)) +\n  geom_point(size = 2, alpha = 0.7) +\n  facet_wrap(~method, scales = \"free\") +\n  labs(\n    title = \"PCA vs t-SNE\",\n    subtitle = \"t-SNE often reveals clearer cluster separation\"\n  )"
  },
  {
    "objectID": "17-unsupervised-learning.html#anomaly-detection",
    "href": "17-unsupervised-learning.html#anomaly-detection",
    "title": "Chapter 17: Unsupervised Learning - Discovering Hidden Patterns",
    "section": "Anomaly Detection",
    "text": "Anomaly Detection\nIdentifying outliers and anomalies is crucial for many applications:\n\n# Method 1: Statistical approach (Z-score)\nz_scores &lt;- abs(scale(scaled_data))\nthreshold &lt;- 3\noutliers_zscore &lt;- which(rowSums(z_scores &gt; threshold) &gt; 0)\n\n# Method 2: Isolation Forest\nlibrary(isotree)\n\niso_forest &lt;- isolation.forest(scaled_data, ntrees = 100, sample_size = 256)\noutlier_scores &lt;- predict(iso_forest, scaled_data)\n\n# Identify outliers (top 5% highest scores)\noutlier_threshold &lt;- quantile(outlier_scores, 0.95)\noutliers_iforest &lt;- which(outlier_scores &gt; outlier_threshold)\n\n# Method 3: Local Outlier Factor (LOF)\nlibrary(dbscan)\nlof_scores &lt;- lof(scaled_data, minPts = 10)\noutliers_lof &lt;- which(lof_scores &gt; 1.5)\n\n# Compare methods\noutlier_comparison &lt;- tibble(\n  index = 1:nrow(scaled_data),\n  z_score = index %in% outliers_zscore,\n  isolation_forest = index %in% outliers_iforest,\n  lof = index %in% outliers_lof\n) %&gt;%\n  mutate(\n    n_methods = z_score + isolation_forest + lof,\n    consensus_outlier = n_methods &gt;= 2\n  )\n\n# Visualize outliers in PCA space\noutlier_pca &lt;- bind_cols(\n  as.data.frame(pca_result$x[, 1:2]),\n  outlier_comparison\n)\n\nggplot(outlier_pca, aes(x = PC1, y = PC2)) +\n  geom_point(aes(color = consensus_outlier), size = 2, alpha = 0.7) +\n  scale_color_manual(values = c(\"FALSE\" = \"gray\", \"TRUE\" = \"red\"),\n                     labels = c(\"Normal\", \"Outlier\")) +\n  labs(\n    title = \"Anomaly Detection Results\",\n    subtitle = \"Points identified as outliers by at least 2 methods\",\n    color = \"Status\"\n  )\n\n\n\n\n\n\n\n# Examine outlier characteristics\noutlier_indices &lt;- which(outlier_comparison$consensus_outlier)\noutlier_analysis &lt;- clustering_data[outlier_indices, ] %&gt;%\n  summarise(across(everything(), list(mean = mean, median = median)))\n\nnormal_analysis &lt;- clustering_data[-outlier_indices, ] %&gt;%\n  summarise(across(everything(), list(mean = mean, median = median)))\n\ncat(\"Outlier characteristics vs normal points:\\n\")\n\nOutlier characteristics vs normal points:\n\nprint(outlier_analysis)\n\n# A tibble: 1 x 10\n  Gr_Liv_Area_mean Gr_Liv_Area_median Lot_Area_mean Lot_Area_median\n             &lt;dbl&gt;              &lt;int&gt;         &lt;dbl&gt;           &lt;int&gt;\n1            2403.               2338        33529.           26400\n# i 6 more variables: Year_Built_mean &lt;dbl&gt;, Year_Built_median &lt;int&gt;,\n#   Total_Bsmt_SF_mean &lt;dbl&gt;, Total_Bsmt_SF_median &lt;dbl&gt;,\n#   Garage_Area_mean &lt;dbl&gt;, Garage_Area_median &lt;dbl&gt;\n\nprint(normal_analysis)\n\n# A tibble: 1 x 10\n  Gr_Liv_Area_mean Gr_Liv_Area_median Lot_Area_mean Lot_Area_median\n             &lt;dbl&gt;              &lt;int&gt;         &lt;dbl&gt;           &lt;int&gt;\n1            1462.               1414         9413.            9260\n# i 6 more variables: Year_Built_mean &lt;dbl&gt;, Year_Built_median &lt;int&gt;,\n#   Total_Bsmt_SF_mean &lt;dbl&gt;, Total_Bsmt_SF_median &lt;dbl&gt;,\n#   Garage_Area_mean &lt;dbl&gt;, Garage_Area_median &lt;dbl&gt;"
  },
  {
    "objectID": "17-unsupervised-learning.html#market-basket-analysis",
    "href": "17-unsupervised-learning.html#market-basket-analysis",
    "title": "Chapter 17: Unsupervised Learning - Discovering Hidden Patterns",
    "section": "Market Basket Analysis",
    "text": "Market Basket Analysis\nDiscover associations in transactional data:\n\n# Create synthetic transaction data\nset.seed(789)\nn_transactions &lt;- 1000\n\n# Housing features that might be purchased together\nfeatures &lt;- c(\"Granite_Counters\", \"Hardwood_Floors\", \"Central_AC\", \n             \"Updated_Kitchen\", \"Finished_Basement\", \"Deck\", \n             \"Pool\", \"Security_System\", \"Smart_Home\", \"Solar_Panels\")\n\n# Generate transactions with realistic patterns\ntransactions &lt;- map(1:n_transactions, function(i) {\n  # Base probability for each item\n  probs &lt;- c(0.3, 0.4, 0.6, 0.35, 0.25, 0.3, 0.1, 0.2, 0.15, 0.05)\n  \n  # Adjust probabilities based on correlations\n  selected &lt;- c()\n  \n  for (j in 1:length(features)) {\n    # Increase probability if related items selected\n    adj_prob &lt;- probs[j]\n    \n    if (\"Granite_Counters\" %in% selected && features[j] == \"Updated_Kitchen\") {\n      adj_prob &lt;- min(1, adj_prob * 3)\n    }\n    if (\"Smart_Home\" %in% selected && features[j] == \"Security_System\") {\n      adj_prob &lt;- min(1, adj_prob * 2.5)\n    }\n    if (\"Pool\" %in% selected && features[j] == \"Deck\") {\n      adj_prob &lt;- min(1, adj_prob * 2)\n    }\n    \n    if (runif(1) &lt; adj_prob) {\n      selected &lt;- c(selected, features[j])\n    }\n  }\n  \n  selected\n})\n\n# Convert to transaction matrix\nlibrary(arules)\n\ntrans_matrix &lt;- as(transactions, \"transactions\")\n\n# Find association rules\nrules &lt;- apriori(trans_matrix, \n                parameter = list(supp = 0.05, conf = 0.5, target = \"rules\"))\n\n# Inspect top rules\ntop_rules &lt;- head(sort(rules, by = \"lift\"), 10)\ninspect(top_rules)\n\n# Visualize rules\nlibrary(arulesViz)\n\nplot(rules, method = \"graph\", \n     control = list(type = \"items\", alpha = 0.7),\n     main = \"Association Rules Network\")\n\n# Create rule quality metrics\nrule_metrics &lt;- as(rules, \"data.frame\") %&gt;%\n  as_tibble() %&gt;%\n  separate(rules, into = c(\"lhs\", \"rhs\"), sep = \" =&gt; \") %&gt;%\n  arrange(desc(lift))\n\nggplot(rule_metrics %&gt;% head(20), \n       aes(x = support, y = confidence, size = lift, color = lift)) +\n  geom_point(alpha = 0.7) +\n  scale_size_continuous(range = c(2, 10)) +\n  scale_color_viridis_c() +\n  labs(\n    title = \"Association Rules Quality\",\n    subtitle = \"Size and color represent lift\",\n    x = \"Support\",\n    y = \"Confidence\"\n  )"
  },
  {
    "objectID": "17-unsupervised-learning.html#evaluating-unsupervised-models",
    "href": "17-unsupervised-learning.html#evaluating-unsupervised-models",
    "title": "Chapter 17: Unsupervised Learning - Discovering Hidden Patterns",
    "section": "Evaluating Unsupervised Models",
    "text": "Evaluating Unsupervised Models\nWithout labels, evaluation is challenging but crucial:\n\n# Internal validation metrics for clustering\nevaluate_clustering &lt;- function(data, clusters) {\n  # Silhouette coefficient\n  sil &lt;- silhouette(clusters, dist(data))\n  avg_silhouette &lt;- mean(sil[, 3])\n  \n  # Calinski-Harabasz Index\n  ch_index &lt;- cluster.stats(dist(data), clusters)$ch\n  \n  # Davies-Bouldin Index (lower is better)\n  db_index &lt;- cluster.stats(dist(data), clusters)$dunn\n  \n  # Within-cluster sum of squares\n  wcss &lt;- sum(cluster.stats(dist(data), clusters)$within.cluster.ss)\n  \n  tibble(\n    avg_silhouette = avg_silhouette,\n    calinski_harabasz = ch_index,\n    davies_bouldin = db_index,\n    wcss = wcss\n  )\n}\n\n# Compare different clustering solutions\nclustering_methods &lt;- list(\n  kmeans_3 = kmeans(scaled_data, 3, nstart = 25)$cluster,\n  kmeans_4 = kmeans(scaled_data, 4, nstart = 25)$cluster,\n  kmeans_5 = kmeans(scaled_data, 5, nstart = 25)$cluster,\n  hierarchical = cutree(hc_ward, k = 4)\n)\n\nevaluation_results &lt;- map_df(clustering_methods, \n                            ~ evaluate_clustering(scaled_data, .),\n                            .id = \"method\")\n\nknitr::kable(evaluation_results, digits = 3)\n\n# Stability analysis: How consistent are clusters?\nstability_analysis &lt;- function(data, k, n_bootstrap = 10) {\n  bootstrap_results &lt;- map(1:n_bootstrap, function(i) {\n    # Bootstrap sample\n    sample_idx &lt;- sample(1:nrow(data), replace = TRUE)\n    boot_data &lt;- data[sample_idx, ]\n    \n    # Cluster\n    kmeans(boot_data, centers = k, nstart = 10)$cluster\n  })\n  \n  # Calculate pairwise agreement\n  agreements &lt;- combn(n_bootstrap, 2, function(pair) {\n    cl1 &lt;- bootstrap_results[[pair[1]]]\n    cl2 &lt;- bootstrap_results[[pair[2]]]\n    \n    # Adjusted Rand Index\n    mclust::adjustedRandIndex(cl1, cl2)\n  })\n  \n  mean(agreements)\n}\n\nstability_scores &lt;- tibble(\n  k = 2:6,\n  stability = map_dbl(2:6, ~ stability_analysis(scaled_data, ., n_bootstrap = 5))\n)\n\nggplot(stability_scores, aes(x = k, y = stability)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Clustering Stability Analysis\",\n    subtitle = \"Higher scores indicate more stable clusters\",\n    x = \"Number of Clusters\",\n    y = \"Average Adjusted Rand Index\"\n  )"
  },
  {
    "objectID": "17-unsupervised-learning.html#real-world-applications",
    "href": "17-unsupervised-learning.html#real-world-applications",
    "title": "Chapter 17: Unsupervised Learning - Discovering Hidden Patterns",
    "section": "Real-World Applications",
    "text": "Real-World Applications\n\nCustomer Segmentation\n\n# Simulate customer data\nset.seed(123)\nn_customers &lt;- 1000\n\ncustomer_data &lt;- tibble(\n  customer_id = 1:n_customers,\n  age = round(rnorm(n_customers, 45, 15)),\n  income = round(rlnorm(n_customers, 11, 0.5)),\n  spending_score = round(runif(n_customers, 1, 100)),\n  purchase_frequency = rpois(n_customers, lambda = 5),\n  avg_transaction = round(rlnorm(n_customers, 4, 0.5)),\n  online_engagement = round(runif(n_customers, 0, 100))\n) %&gt;%\n  mutate(\n    age = pmax(18, pmin(80, age)),\n    income = pmax(20000, pmin(200000, income))\n  )\n\n# Preprocessing\ncustomer_scaled &lt;- customer_data %&gt;%\n  select(-customer_id) %&gt;%\n  scale()\n\n# Segment customers\ncustomer_segments &lt;- kmeans(customer_scaled, centers = 4, nstart = 25)\n\n# Profile segments\ncustomer_profiles &lt;- customer_data %&gt;%\n  mutate(segment = factor(customer_segments$cluster)) %&gt;%\n  group_by(segment) %&gt;%\n  summarise(\n    count = n(),\n    avg_age = mean(age),\n    avg_income = mean(income),\n    avg_spending = mean(spending_score),\n    avg_frequency = mean(purchase_frequency),\n    avg_transaction_value = mean(avg_transaction),\n    avg_engagement = mean(online_engagement)\n  )\n\nknitr::kable(customer_profiles, digits = 0)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsegment\ncount\navg_age\navg_income\navg_spending\navg_frequency\navg_transaction_value\navg_engagement\n\n\n\n\n1\n170\n51\n123792\n52\n5\n55\n42\n\n\n2\n349\n47\n57856\n24\n5\n51\n51\n\n\n3\n131\n45\n63800\n50\n5\n122\n51\n\n\n4\n350\n41\n55388\n74\n5\n52\n50\n\n\n\n\n# Visualize segments\ncustomer_pca &lt;- prcomp(customer_scaled)\n\ncustomer_viz &lt;- as.data.frame(customer_pca$x[, 1:2]) %&gt;%\n  mutate(segment = factor(customer_segments$cluster))\n\nggplot(customer_viz, aes(x = PC1, y = PC2, color = segment)) +\n  geom_point(alpha = 0.5) +\n  stat_ellipse(level = 0.95) +\n  labs(\n    title = \"Customer Segments\",\n    subtitle = \"Four distinct customer groups identified\"\n  )\n\n\n\n\n\n\n\n# Segment characteristics\nsegment_names &lt;- c(\n  \"1\" = \"High-Value Engaged\",\n  \"2\" = \"Young Browsers\",\n  \"3\" = \"Loyal Regulars\",\n  \"4\" = \"Occasional Shoppers\"\n)\n\ncustomer_profiles %&gt;%\n  mutate(segment_name = segment_names[as.character(segment)]) %&gt;%\n  select(segment_name, everything(), -segment) %&gt;%\n  knitr::kable(digits = 0)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsegment_name\ncount\navg_age\navg_income\navg_spending\navg_frequency\navg_transaction_value\navg_engagement\n\n\n\n\nHigh-Value Engaged\n170\n51\n123792\n52\n5\n55\n42\n\n\nYoung Browsers\n349\n47\n57856\n24\n5\n51\n51\n\n\nLoyal Regulars\n131\n45\n63800\n50\n5\n122\n51\n\n\nOccasional Shoppers\n350\n41\n55388\n74\n5\n52\n50"
  },
  {
    "objectID": "17-unsupervised-learning.html#best-practices",
    "href": "17-unsupervised-learning.html#best-practices",
    "title": "Chapter 17: Unsupervised Learning - Discovering Hidden Patterns",
    "section": "Best Practices",
    "text": "Best Practices\n\n1. Data Preprocessing is Critical\n\n# Demonstrate importance of scaling\nunscaled_clusters &lt;- kmeans(clustering_data, centers = 3, nstart = 25)\nscaled_clusters &lt;- kmeans(scaled_data, centers = 3, nstart = 25)\n\n# Compare results\ncomparison_data &lt;- clustering_data %&gt;%\n  mutate(\n    unscaled = factor(unscaled_clusters$cluster),\n    scaled = factor(scaled_clusters$cluster)\n  )\n\np1 &lt;- ggplot(comparison_data, aes(x = Gr_Liv_Area, y = Year_Built, \n                                   color = unscaled)) +\n  geom_point() +\n  labs(title = \"Unscaled Data Clustering\")\n\np2 &lt;- ggplot(comparison_data, aes(x = Gr_Liv_Area, y = Year_Built, \n                                   color = scaled)) +\n  geom_point() +\n  labs(title = \"Scaled Data Clustering\")\n\np1 + p2\n\n\n\n\n\n\n\n\n\n\n2. Choose the Right Algorithm\n\nalgorithm_guide &lt;- tibble(\n  Scenario = c(\n    \"Spherical, similar-sized clusters\",\n    \"Unknown number of clusters\",\n    \"Non-spherical clusters\",\n    \"Outliers present\",\n    \"Hierarchical structure\",\n    \"Large datasets\",\n    \"Mixed data types\"\n  ),\n  `Recommended Algorithm` = c(\n    \"K-means\",\n    \"Hierarchical or DBSCAN\",\n    \"DBSCAN or Spectral\",\n    \"DBSCAN or Robust K-means\",\n    \"Hierarchical clustering\",\n    \"Mini-batch K-means\",\n    \"K-prototypes or Gower distance\"\n  )\n)\n\nknitr::kable(algorithm_guide)\n\n\n\n\nScenario\nRecommended Algorithm\n\n\n\n\nSpherical, similar-sized clusters\nK-means\n\n\nUnknown number of clusters\nHierarchical or DBSCAN\n\n\nNon-spherical clusters\nDBSCAN or Spectral\n\n\nOutliers present\nDBSCAN or Robust K-means\n\n\nHierarchical structure\nHierarchical clustering\n\n\nLarge datasets\nMini-batch K-means\n\n\nMixed data types\nK-prototypes or Gower distance"
  },
  {
    "objectID": "17-unsupervised-learning.html#exercises",
    "href": "17-unsupervised-learning.html#exercises",
    "title": "Chapter 17: Unsupervised Learning - Discovering Hidden Patterns",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Implement Custom Distance Metric\n\n# Your solution\n# Custom distance for mixed data types\ngower_distance &lt;- function(data) {\n  # Gower distance handles mixed types\n  library(cluster)\n  \n  # Create mixed type data\n  mixed_data &lt;- ames %&gt;%\n    select(Gr_Liv_Area, Lot_Area, Neighborhood, Central_Air) %&gt;%\n    slice_sample(n = 100)\n  \n  # Calculate Gower distance\n  gower_dist &lt;- daisy(mixed_data, metric = \"gower\")\n  \n  # Perform clustering\n  gower_clusters &lt;- hclust(gower_dist, method = \"complete\")\n  \n  # Cut tree\n  clusters &lt;- cutree(gower_clusters, k = 3)\n  \n  # Visualize\n  mixed_data %&gt;%\n    mutate(cluster = factor(clusters)) %&gt;%\n    ggplot(aes(x = Gr_Liv_Area, y = Lot_Area, color = cluster)) +\n    geom_point(size = 3) +\n    facet_wrap(~Neighborhood) +\n    labs(title = \"Clustering with Gower Distance\")\n}\n\ngower_distance()\n\n\n\n\n\n\n\n\n\n\nExercise 2: Ensemble Clustering\n\n# Your solution\n# Combine multiple clustering results\nensemble_clustering &lt;- function(data, k = 4) {\n  # Multiple clustering algorithms\n  km &lt;- kmeans(data, centers = k, nstart = 25)$cluster\n  hc &lt;- cutree(hclust(dist(data), method = \"ward.D2\"), k = k)\n  \n  # Create co-association matrix\n  n &lt;- nrow(data)\n  co_assoc &lt;- matrix(0, n, n)\n  \n  # K-means contribution\n  for (i in 1:n) {\n    for (j in 1:n) {\n      if (km[i] == km[j]) co_assoc[i, j] &lt;- co_assoc[i, j] + 1\n    }\n  }\n  \n  # Hierarchical contribution\n  for (i in 1:n) {\n    for (j in 1:n) {\n      if (hc[i] == hc[j]) co_assoc[i, j] &lt;- co_assoc[i, j] + 1\n    }\n  }\n  \n  # Average co-association\n  co_assoc &lt;- co_assoc / 2\n  \n  # Final clustering on co-association matrix\n  final_dist &lt;- as.dist(1 - co_assoc)\n  final_clusters &lt;- cutree(hclust(final_dist, method = \"average\"), k = k)\n  \n  return(final_clusters)\n}\n\n# Apply ensemble clustering\nensemble_result &lt;- ensemble_clustering(scaled_data[1:100, ])\n\n# Visualize\npca_subset &lt;- as.data.frame(pca_result$x[1:100, 1:2]) %&gt;%\n  mutate(ensemble = factor(ensemble_result))\n\nggplot(pca_subset, aes(x = PC1, y = PC2, color = ensemble)) +\n  geom_point(size = 3) +\n  labs(title = \"Ensemble Clustering Result\")\n\n\n\n\n\n\n\n\n\n\nExercise 3: Clustering Interpretation\n\n# Your solution\n# Automated cluster interpretation\ninterpret_clusters &lt;- function(data, clusters) {\n  clustered &lt;- data %&gt;%\n    mutate(cluster = factor(clusters))\n  \n  # Statistical tests for each variable\n  interpretation &lt;- map_df(names(data), function(var) {\n    # ANOVA for continuous variables\n    formula &lt;- as.formula(paste(var, \"~ cluster\"))\n    aov_result &lt;- aov(formula, data = clustered)\n    \n    tibble(\n      variable = var,\n      f_statistic = summary(aov_result)[[1]][[\"F value\"]][1],\n      p_value = summary(aov_result)[[1]][[\"Pr(&gt;F)\"]][1],\n      importance = if_else(p_value &lt; 0.001, \"High\",\n                          if_else(p_value &lt; 0.05, \"Medium\", \"Low\"))\n    )\n  })\n  \n  # Characteristic values per cluster\n  characteristics &lt;- clustered %&gt;%\n    group_by(cluster) %&gt;%\n    summarise(across(where(is.numeric), \n                    list(mean = ~mean(., na.rm = TRUE),\n                         sd = ~sd(., na.rm = TRUE))))\n  \n  list(\n    importance = interpretation %&gt;% arrange(p_value),\n    characteristics = characteristics\n  )\n}\n\n# Apply to our clustering\ninterpretation &lt;- interpret_clusters(clustering_data, km_result$cluster)\n\ncat(\"Variable Importance for Clustering:\\n\")\n\nVariable Importance for Clustering:\n\nprint(interpretation$importance)\n\n# A tibble: 5 x 4\n  variable      f_statistic   p_value importance\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     \n1 Lot_Area             260. 2.25e-101 High      \n2 Year_Built           258. 8.56e-101 High      \n3 Garage_Area          184. 4.44e- 80 High      \n4 Gr_Liv_Area          115. 1.81e- 56 High      \n5 Total_Bsmt_SF        112. 3.43e- 55 High      \n\ncat(\"\\nCluster Characteristics:\\n\")\n\n\nCluster Characteristics:\n\nprint(interpretation$characteristics)\n\n# A tibble: 4 x 11\n  cluster Gr_Liv_Area_mean Gr_Liv_Area_sd Lot_Area_mean Lot_Area_sd\n  &lt;fct&gt;              &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 1                  2513.           925.        56046.      24670.\n2 2                  1296.           409.         8466.       3142.\n3 3                  1397.           307.         9414.       4132.\n4 4                  2147.           466.        13104.       4393.\n# i 6 more variables: Year_Built_mean &lt;dbl&gt;, Year_Built_sd &lt;dbl&gt;,\n#   Total_Bsmt_SF_mean &lt;dbl&gt;, Total_Bsmt_SF_sd &lt;dbl&gt;, Garage_Area_mean &lt;dbl&gt;,\n#   Garage_Area_sd &lt;dbl&gt;"
  },
  {
    "objectID": "17-unsupervised-learning.html#summary",
    "href": "17-unsupervised-learning.html#summary",
    "title": "Chapter 17: Unsupervised Learning - Discovering Hidden Patterns",
    "section": "Summary",
    "text": "Summary\nIn this comprehensive chapter, you’ve mastered:\n✅ Clustering algorithms - K-means for spherical clusters - Hierarchical for nested structure - DBSCAN for arbitrary shapes - Evaluation metrics\n✅ Dimensionality reduction - PCA for linear reduction - t-SNE/UMAP for non-linear - Interpretation of components - Visualization techniques\n✅ Anomaly detection - Statistical methods - Isolation forests - Local outlier factor - Ensemble approaches\n✅ Association analysis - Market basket analysis - Rule mining - Lift and confidence metrics\n✅ Practical applications - Customer segmentation - Data preprocessing importance - Algorithm selection - Evaluation strategies\nKey takeaways: - Unsupervised learning finds structure without labels - Preprocessing (especially scaling) is crucial - No single best algorithm - depends on data structure - Validation is challenging but essential - Combine multiple methods for robustness - Visualization aids interpretation"
  },
  {
    "objectID": "17-unsupervised-learning.html#whats-next",
    "href": "17-unsupervised-learning.html#whats-next",
    "title": "Chapter 17: Unsupervised Learning - Discovering Hidden Patterns",
    "section": "What’s Next?",
    "text": "What’s Next?\nIn Chapter 18, we’ll learn how to deploy models to production."
  },
  {
    "objectID": "17-unsupervised-learning.html#additional-resources",
    "href": "17-unsupervised-learning.html#additional-resources",
    "title": "Chapter 17: Unsupervised Learning - Discovering Hidden Patterns",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nIntroduction to Statistical Learning - Chapter 12\nElements of Statistical Learning - Chapter 14\nCluster Analysis Book\nSurvey of Clustering Algorithms"
  }
]