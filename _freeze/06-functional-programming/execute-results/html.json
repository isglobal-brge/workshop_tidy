{
  "hash": "57ae3d8f4074ba81df7da07a351cd822",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 6: Functional Programming with purrr - Iteration and Mapping Made Elegant\"\nauthor: \"David Sarrat González, Juan R González\"\ndate: today\nformat:\n  html:\n    code-fold: false\n    code-tools: true\n---\n\n## Learning Objectives\n\nBy the end of this chapter, you will master:\n\n- The philosophy of functional programming in R\n- The map family of functions\n- Working with lists and nested data\n- Safe function execution and error handling\n- Parallel iteration with map2 and pmap\n- Functional programming patterns\n- Integration with tidyverse workflows\n- Performance optimization techniques\n\n## Why Functional Programming?\n\nFunctional programming (FP) is a programming paradigm that treats computation as the evaluation of mathematical functions. In data science, FP principles help us write:\n- **Cleaner code**: Less repetition, more abstraction\n- **Safer code**: Fewer side effects, predictable behavior\n- **More maintainable code**: Modular, testable functions\n- **Scalable code**: Easy to parallelize and optimize\n\nThe `purrr` package brings functional programming to the tidyverse, replacing loops with elegant, expressive functions.\n\n### The Problem with Loops\n\nLet's start by understanding why we need functional programming:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.4     v readr     2.1.5\nv forcats   1.0.0     v stringr   1.5.2\nv ggplot2   4.0.0     v tibble    3.3.0\nv lubridate 1.9.4     v tidyr     1.3.1\nv purrr     1.1.0     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(purrr)\nlibrary(broom)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Traditional approach with loops\ndata_list <- list(\n  group_a = rnorm(100, mean = 10, sd = 2),\n  group_b = rnorm(100, mean = 15, sd = 3),\n  group_c = rnorm(100, mean = 12, sd = 2.5)\n)\n\n# Calculate mean with a for loop (the old way)\nmeans_loop <- numeric(length(data_list))\nnames(means_loop) <- names(data_list)\n\nfor (i in seq_along(data_list)) {\n  means_loop[i] <- mean(data_list[[i]])\n}\n\nprint(\"Means calculated with loop:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Means calculated with loop:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(means_loop)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n group_a  group_b  group_c \n10.18081 14.67736 12.30116 \n```\n\n\n:::\n\n```{.r .cell-code}\n# The functional approach with map\nmeans_map <- map_dbl(data_list, mean)\n\nprint(\"Means calculated with map:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Means calculated with map:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(means_map)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n group_a  group_b  group_c \n10.18081 14.67736 12.30116 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Are they the same?\nidentical(means_loop, means_map)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\nWhile both approaches give the same result, the functional approach is:\n- **More concise**: One line instead of four\n- **More readable**: Intent is clear\n- **Less error-prone**: No index management\n- **Vectorized**: Can be easily parallelized\n\n## The map() Family\n\nThe `map()` functions are the workhorses of purrr. They apply a function to each element of a list or vector.\n\n### Basic map() Function\n\nThe basic `map()` function always returns a list:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Apply a function to each element\nnumbers <- list(\n  small = 1:5,\n  medium = 10:15,\n  large = 100:105\n)\n\n# Calculate summary statistics for each group\nsummaries <- map(numbers, summary)\nprint(summaries)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$small\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      1       2       3       3       4       5 \n\n$medium\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10.00   11.25   12.50   12.50   13.75   15.00 \n\n$large\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  100.0   101.2   102.5   102.5   103.8   105.0 \n```\n\n\n:::\n\n```{.r .cell-code}\n# The beauty of map: it preserves names and structure\nstr(summaries)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 3\n $ small : 'summaryDefault' Named num [1:6] 1 2 3 3 4 5\n  ..- attr(*, \"names\")= chr [1:6] \"Min.\" \"1st Qu.\" \"Median\" \"Mean\" ...\n $ medium: 'summaryDefault' Named num [1:6] 10 11.2 12.5 12.5 13.8 ...\n  ..- attr(*, \"names\")= chr [1:6] \"Min.\" \"1st Qu.\" \"Median\" \"Mean\" ...\n $ large : 'summaryDefault' Named num [1:6] 100 101 102 102 104 ...\n  ..- attr(*, \"names\")= chr [1:6] \"Min.\" \"1st Qu.\" \"Median\" \"Mean\" ...\n```\n\n\n:::\n:::\n\n\n### Type-Specific map Variants\n\nOften we want a specific type of output. Purrr provides typed variants:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# map_dbl returns a numeric vector\nmeans <- map_dbl(numbers, mean)\nprint(means)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n small medium  large \n   3.0   12.5  102.5 \n```\n\n\n:::\n\n```{.r .cell-code}\n# map_chr returns a character vector\ndescriptions <- map_chr(numbers, ~ paste(\"Range:\", min(.), \"-\", max(.)))\nprint(descriptions)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             small             medium              large \n    \"Range: 1 - 5\"   \"Range: 10 - 15\" \"Range: 100 - 105\" \n```\n\n\n:::\n\n```{.r .cell-code}\n# map_lgl returns a logical vector\nhas_even_length <- map_lgl(numbers, ~ length(.) %% 2 == 0)\nprint(has_even_length)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n small medium  large \n FALSE   TRUE   TRUE \n```\n\n\n:::\n\n```{.r .cell-code}\n# map_int returns an integer vector\nlengths <- map_int(numbers, length)\nprint(lengths)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n small medium  large \n     5      6      6 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Demonstrate type safety\n# This would error: map_dbl(numbers, class)\n# Because class returns a character, not a double\n```\n:::\n\n\nThe typed variants are important because they:\n- Ensure type consistency\n- Catch errors early\n- Make code more predictable\n- Enable further vectorized operations\n\n### Anonymous Functions and Shortcuts\n\nPurrr provides multiple ways to specify functions, making code more concise:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample data\nvalues <- list(\n  a = 1:10,\n  b = 11:20,\n  c = 21:30\n)\n\n# Method 1: Named function\nresult1 <- map_dbl(values, mean)\n\n# Method 2: Anonymous function (traditional)\nresult2 <- map_dbl(values, function(x) mean(x))\n\n# Method 3: Formula notation (purrr style)\nresult3 <- map_dbl(values, ~ mean(.))\n\n# Method 4: Even more concise for simple extractions\n# Extract the 3rd element from each vector\nthird_elements <- map_dbl(values, 3)  # Same as ~ .[[3]]\n\nprint(\"All methods produce the same result:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"All methods produce the same result:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(all(result1 == result2, result2 == result3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"Third elements:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Third elements:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(third_elements)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n a  b  c \n 3 13 23 \n```\n\n\n:::\n\n```{.r .cell-code}\n# More complex anonymous functions\ncustom_summary <- map(values, ~ {\n  tibble(\n    min = min(.),\n    median = median(.),\n    max = max(.),\n    range = max(.) - min(.),\n    cv = sd(.) / mean(.)  # Coefficient of variation\n  )\n})\n\n# Combine into a single data frame\nbind_rows(custom_summary, .id = \"group\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 x 6\n  group   min median   max range    cv\n  <chr> <int>  <dbl> <int> <int> <dbl>\n1 a         1    5.5    10     9 0.550\n2 b        11   15.5    20     9 0.195\n3 c        21   25.5    30     9 0.119\n```\n\n\n:::\n:::\n\n\nThe `~` notation is particularly elegant:\n- `~` starts an anonymous function\n- `.` represents the current element\n- Can include complex expressions\n\n## Working with Data Frames\n\nOne of purrr's strengths is working with data frames and nested data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a nested data frame\nnested_data <- tibble(\n  group = c(\"A\", \"B\", \"C\"),\n  n = c(50, 75, 100)\n) %>%\n  mutate(\n    # Generate data for each group\n    data = map2(n, group, ~ {\n      tibble(\n        value = rnorm(.x, mean = match(.y, LETTERS) * 10, sd = 2),\n        category = sample(c(\"Type1\", \"Type2\"), .x, replace = TRUE)\n      )\n    })\n  )\n\nprint(\"Nested data structure:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Nested data structure:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(nested_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 x 3\n  group     n data              \n  <chr> <dbl> <list>            \n1 A        50 <tibble [50 x 2]> \n2 B        75 <tibble [75 x 2]> \n3 C       100 <tibble [100 x 2]>\n```\n\n\n:::\n\n```{.r .cell-code}\n# Work with nested data\nanalysis_results <- nested_data %>%\n  mutate(\n    # Calculate statistics for each nested data frame\n    n_obs = map_int(data, nrow),\n    mean_value = map_dbl(data, ~ mean(.$value)),\n    sd_value = map_dbl(data, ~ sd(.$value)),\n    \n    # Fit models to each group\n    model = map(data, ~ lm(value ~ category, data = .)),\n    \n    # Extract model information\n    model_summary = map(model, broom::glance),\n    coefficients = map(model, broom::tidy)\n  )\n\n# View the results\nanalysis_results %>%\n  select(group, n_obs, mean_value, sd_value)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 x 4\n  group n_obs mean_value sd_value\n  <chr> <int>      <dbl>    <dbl>\n1 A        50       10.0     2.06\n2 B        75       19.7     1.98\n3 C       100       29.9     1.78\n```\n\n\n:::\n\n```{.r .cell-code}\n# Unnest model summaries\nanalysis_results %>%\n  select(group, model_summary) %>%\n  unnest(model_summary) %>%\n  select(group, r.squared, p.value, AIC, BIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 x 5\n  group r.squared p.value   AIC   BIC\n  <chr>     <dbl>   <dbl> <dbl> <dbl>\n1 A       0.00109   0.820  219.  225.\n2 B       0.0249    0.176  318.  325.\n3 C       0.00282   0.600  404.  412.\n```\n\n\n:::\n:::\n\n\nThis pattern of nest-map-unnest is incredibly powerful for:\n- Group-wise analysis\n- Multiple model fitting\n- Simulation studies\n- Bootstrap procedures\n\n## map2() and pmap() for Multiple Inputs\n\nSometimes we need to iterate over multiple inputs simultaneously:\n\n### map2() for Two Inputs\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Two vectors to iterate over\nmeans <- c(10, 20, 30)\nsds <- c(1, 2, 3)\nsample_sizes <- c(100, 200, 300)\n\n# Generate samples with different parameters\nsamples <- map2(means, sds, ~ rnorm(100, mean = .x, sd = .y))\n\n# Visualize\nsamples_df <- tibble(\n  group = rep(c(\"A\", \"B\", \"C\"), each = 100),\n  value = unlist(samples)\n)\n\nggplot(samples_df, aes(x = value, fill = group)) +\n  geom_density(alpha = 0.5) +\n  labs(\n    title = \"Distributions Generated with map2()\",\n    subtitle = \"Different means and standard deviations\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](06-functional-programming_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# More complex map2 example\nx_values <- list(1:5, 6:10, 11:15)\ny_values <- list(2:6, 7:11, 12:16)\n\n# Calculate correlations between paired lists\ncorrelations <- map2_dbl(x_values, y_values, cor)\nprint(paste(\"Correlations:\", paste(correlations, collapse = \", \")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Correlations: 1, 1, 1\"\n```\n\n\n:::\n:::\n\n\n### pmap() for Multiple Inputs\n\nWhen you have more than two inputs, use `pmap()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Multiple parameters for simulation\nparams <- tibble(\n  n = c(100, 200, 150),\n  mean = c(10, 15, 12),\n  sd = c(2, 3, 2.5),\n  distribution = c(\"normal\", \"uniform\", \"exponential\")\n)\n\n# Generate samples based on parameters\nsimulated_data <- pmap(params, function(n, mean, sd, distribution) {\n  switch(distribution,\n    normal = rnorm(n, mean, sd),\n    uniform = runif(n, mean - sd, mean + sd),\n    exponential = rexp(n, rate = 1/mean)\n  )\n})\n\n# Add to our data frame - simulated_data is already a list\nparams_with_data <- bind_cols(\n  params,\n  tibble(\n    data = simulated_data,\n    actual_mean = map_dbl(simulated_data, mean),\n    actual_sd = map_dbl(simulated_data, sd)\n  )\n)\n\nparams_with_data %>%\n  select(-data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 x 6\n      n  mean    sd distribution actual_mean actual_sd\n  <dbl> <dbl> <dbl> <chr>              <dbl>     <dbl>\n1   100    10   2   normal              9.79      1.86\n2   200    15   3   uniform            15.0       1.82\n3   150    12   2.5 exponential        11.8      11.9 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize all distributions\nparams_with_data %>%\n  mutate(plot_data = map2(data, distribution, ~ {\n    tibble(value = .x, dist_type = .y)\n  })) %>%\n  select(distribution, plot_data) %>%\n  unnest(plot_data) %>%\n  ggplot(aes(x = value, fill = distribution)) +\n  geom_histogram(bins = 30, alpha = 0.7) +\n  facet_wrap(~distribution, scales = \"free\") +\n  theme_minimal() +\n  labs(title = \"Different Distributions Generated with pmap()\")\n```\n\n::: {.cell-output-display}\n![](06-functional-programming_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nThe power of `pmap()`:\n- Handles any number of inputs\n- Works with data frames naturally\n- Maintains alignment of inputs\n- Enables complex parameterized operations\n\n## Error Handling with safely() and possibly()\n\nReal-world data is messy. Functions fail. Purrr helps us handle errors gracefully:\n\n### safely() - Capture Errors\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create data with potential problems\nmessy_list <- list(\n  good = 1:10,\n  bad = c(1, 2, \"three\", 4),  # Contains a string\n  empty = numeric(0),\n  null = NULL,\n  also_good = 11:20\n)\n\n# This would fail:\n# map_dbl(messy_list, mean)\n\n# Use safely to capture errors\nsafe_mean <- safely(mean)\nresults <- map(messy_list, safe_mean)\n\n# Examine structure\nstr(results[[1]])  # Good result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 2\n $ result: num 5.5\n $ error : NULL\n```\n\n\n:::\n\n```{.r .cell-code}\nstr(results[[2]])  # Error result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 2\n $ result: num NA\n $ error : NULL\n```\n\n\n:::\n\n```{.r .cell-code}\n# Extract results and errors\nextracted_results <- tibble(\n  name = names(messy_list),\n  result = map(results, \"result\"),\n  error = map(results, \"error\")\n) %>%\n  mutate(\n    has_error = map_lgl(error, ~ !is.null(.)),\n    error_message = map_chr(error, ~ ifelse(is.null(.), \"No error\", as.character(.)))\n  )\n\nextracted_results %>%\n  select(name, has_error, error_message)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 3\n  name      has_error error_message\n  <chr>     <lgl>     <chr>        \n1 good      FALSE     No error     \n2 bad       FALSE     No error     \n3 empty     FALSE     No error     \n4 null      FALSE     No error     \n5 also_good FALSE     No error     \n```\n\n\n:::\n:::\n\n\n### possibly() - Provide Default Values\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# possibly() is simpler when you just want a default\npossible_mean <- possibly(mean, otherwise = NA_real_)\n\n# Apply to messy list\nmeans_with_default <- map_dbl(messy_list, possible_mean)\nprint(means_with_default)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     good       bad     empty      null also_good \n      5.5        NA       NaN        NA      15.5 \n```\n\n\n:::\n\n```{.r .cell-code}\n# More complex example with custom function\ncalculate_cv <- function(x) {\n  if (length(x) < 2) stop(\"Need at least 2 values\")\n  sd(x) / mean(x)\n}\n\nsafe_cv <- possibly(calculate_cv, otherwise = NA_real_)\n\n# Test data with edge cases\ntest_data <- list(\n  normal = rnorm(100, 10, 2),\n  single = 5,\n  empty = numeric(0),\n  zeros = rep(0, 10),\n  negative = c(-1, -2, -3)\n)\n\ncv_results <- map_dbl(test_data, safe_cv)\nprint(cv_results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    normal     single      empty      zeros   negative \n 0.2023601         NA         NA        NaN -0.5000000 \n```\n\n\n:::\n:::\n\n\n### quietly() - Capture Warnings and Messages\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function that produces warnings\nnoisy_function <- function(x) {\n  if (any(x < 0)) warning(\"Negative values detected\")\n  if (length(x) > 100) message(\"Large dataset\")\n  mean(x)\n}\n\n# Capture all output\nquiet_function <- quietly(noisy_function)\n\n# Test with various inputs\ntest_inputs <- list(\n  clean = 1:10,\n  negative = c(-5, 0, 5),\n  large = 1:150\n)\n\nquiet_results <- map(test_inputs, quiet_function)\n\n# Extract components\noutput_summary <- tibble(\n  name = names(test_inputs),\n  result = map_dbl(quiet_results, \"result\"),\n  warnings = map(quiet_results, \"warnings\"),\n  messages = map(quiet_results, \"messages\")\n) %>%\n  mutate(\n    has_warnings = map_lgl(warnings, ~ length(.) > 0),\n    has_messages = map_lgl(messages, ~ length(.) > 0)\n  )\n\noutput_summary %>%\n  select(name, result, has_warnings, has_messages)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 x 4\n  name     result has_warnings has_messages\n  <chr>     <dbl> <lgl>        <lgl>       \n1 clean       5.5 FALSE        FALSE       \n2 negative    0   TRUE         FALSE       \n3 large      75.5 FALSE        TRUE        \n```\n\n\n:::\n:::\n\n\n## Advanced Functional Patterns\n\n### Function Factories\n\nCreate functions that return other functions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a function factory for power functions\nmake_power <- function(n) {\n  function(x) x^n\n}\n\n# Create specific power functions\nsquare <- make_power(2)\ncube <- make_power(3)\n\n# Use them\nprint(square(5))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 25\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(cube(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 27\n```\n\n\n:::\n\n```{.r .cell-code}\n# Apply to data\nvalues <- 1:5\npowers <- 2:4\n\n# Create multiple power functions\npower_functions <- map(powers, make_power)\n\n# Apply each to our values\npower_results <- map(power_functions, ~ .(values))\nnames(power_results) <- paste0(\"power_\", powers)\n\n# Convert to data frame\nas_tibble(power_results) %>%\n  mutate(original = values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 4\n  power_2 power_3 power_4 original\n    <dbl>   <dbl>   <dbl>    <int>\n1       1       1       1        1\n2       4       8      16        2\n3       9      27      81        3\n4      16      64     256        4\n5      25     125     625        5\n```\n\n\n:::\n:::\n\n\n### Function Composition\n\nCombine multiple functions into pipelines:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a data processing pipeline\nprocess_data <- compose(\n  ~ mutate(., z_score = (value - mean(value)) / sd(value)),\n  ~ filter(., !is.na(value)),\n  ~ select(., -unwanted_column),\n  .dir = \"forward\"  # Apply left to right\n)\n\n# Test data\ntest_df <- tibble(\n  value = c(1, 2, NA, 4, 5, 100),\n  unwanted_column = \"remove me\"\n)\n\n# This won't work directly with compose, so let's do it manually\npipeline <- . %>%\n  select(-unwanted_column) %>%\n  filter(!is.na(value)) %>%\n  mutate(z_score = (value - mean(value)) / sd(value))\n\nresult <- test_df %>% pipeline\nprint(result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 2\n  value z_score\n  <dbl>   <dbl>\n1     1  -0.493\n2     2  -0.470\n3     4  -0.424\n4     5  -0.401\n5   100   1.79 \n```\n\n\n:::\n\n```{.r .cell-code}\n# More practical: Create reusable statistical transformations\nstandardize <- function(x) (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)\nwinsorize <- function(x, probs = c(0.05, 0.95)) {\n  limits <- quantile(x, probs, na.rm = TRUE)\n  x[x < limits[1]] <- limits[1]\n  x[x > limits[2]] <- limits[2]\n  x\n}\n\n# Apply transformations to multiple columns\ndata_to_transform <- tibble(\n  a = rnorm(100, 10, 5),\n  b = rexp(100, 0.1),\n  c = runif(100, 0, 100)\n)\n\ntransformed_data <- data_to_transform %>%\n  mutate(across(everything(), list(\n    standardized = standardize,\n    winsorized = winsorize\n  )))\n\nglimpse(transformed_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 100\nColumns: 9\n$ a              <dbl> 16.5263077, 14.3804805, 12.3189807, 12.3855712, 7.54297~\n$ b              <dbl> 7.581326695, 11.584588028, 1.362727634, 12.252324372, 2~\n$ c              <dbl> 81.968328, 24.600342, 9.701797, 16.370145, 66.435520, 6~\n$ a_standardized <dbl> 1.30721992, 0.85014788, 0.41103798, 0.42522210, -0.6062~\n$ a_winsorized   <dbl> 16.526308, 14.380481, 12.318981, 12.385571, 7.542973, 3~\n$ b_standardized <dbl> -0.2882992, 0.1875600, -1.0274909, 0.2669324, 1.2023889~\n$ b_winsorized   <dbl> 7.5813267, 11.5845880, 1.3627276, 12.2523244, 20.122039~\n$ c_standardized <dbl> 1.1951049, -0.8248040, -1.3493771, -1.1145866, 0.648199~\n$ c_winsorized   <dbl> 81.968328, 24.600342, 9.701797, 16.370145, 66.435520, 6~\n```\n\n\n:::\n:::\n\n\n### Reduce and Accumulate\n\nCombine elements of a list iteratively:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# reduce() combines all elements into one\nnumbers_list <- list(\n  c(1, 2, 3),\n  c(4, 5, 6),\n  c(7, 8, 9)\n)\n\n# Sum all vectors element-wise\ntotal <- reduce(numbers_list, `+`)\nprint(total)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 12 15 18\n```\n\n\n:::\n\n```{.r .cell-code}\n# Find intersection of multiple sets\nsets <- list(\n  set1 = c(\"a\", \"b\", \"c\", \"d\"),\n  set2 = c(\"b\", \"c\", \"d\", \"e\"),\n  set3 = c(\"c\", \"d\", \"e\", \"f\")\n)\n\ncommon_elements <- reduce(sets, intersect)\nprint(paste(\"Common elements:\", paste(common_elements, collapse = \", \")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Common elements: c, d\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# accumulate() keeps intermediate results\ncumulative_sums <- accumulate(1:5, `+`)\nprint(cumulative_sums)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  1  3  6 10 15\n```\n\n\n:::\n\n```{.r .cell-code}\n# More complex: Merge multiple data frames\ndf_list <- list(\n  tibble(id = 1:3, a = letters[1:3]),\n  tibble(id = 2:4, b = letters[2:4]),\n  tibble(id = 3:5, c = letters[3:5])\n)\n\nmerged_df <- reduce(df_list, full_join, by = \"id\")\nprint(merged_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 4\n     id a     b     c    \n  <int> <chr> <chr> <chr>\n1     1 a     <NA>  <NA> \n2     2 b     b     <NA> \n3     3 c     c     c    \n4     4 <NA>  d     d    \n5     5 <NA>  <NA>  e    \n```\n\n\n:::\n:::\n\n\n## Real-World Applications\n\n### Bootstrap Analysis\n\nUse purrr for bootstrap confidence intervals:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Original data\noriginal_data <- rnorm(100, mean = 50, sd = 10)\n\n# Bootstrap function\nbootstrap_mean <- function(data, n_boot = 1000) {\n  # Generate bootstrap samples\n  boot_samples <- map_dbl(1:n_boot, ~ {\n    sample(data, replace = TRUE) %>% mean()\n  })\n  \n  # Calculate confidence interval\n  ci <- quantile(boot_samples, c(0.025, 0.975))\n  \n  tibble(\n    mean = mean(data),\n    boot_mean = mean(boot_samples),\n    boot_sd = sd(boot_samples),\n    ci_lower = ci[1],\n    ci_upper = ci[2]\n  )\n}\n\n# Apply bootstrap\nboot_result <- bootstrap_mean(original_data)\nprint(boot_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 5\n   mean boot_mean boot_sd ci_lower ci_upper\n  <dbl>     <dbl>   <dbl>    <dbl>    <dbl>\n1  50.8      50.9    1.13     48.7     53.1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize bootstrap distribution\nboot_samples <- map_dbl(1:1000, ~ mean(sample(original_data, replace = TRUE)))\n\nggplot(tibble(x = boot_samples), aes(x = x)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(xintercept = mean(original_data), color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = boot_result$ci_lower, color = \"red\", linetype = \"dashed\") +\n  geom_vline(xintercept = boot_result$ci_upper, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Bootstrap Distribution of Sample Mean\",\n    subtitle = \"Red lines show original mean and 95% CI\",\n    x = \"Bootstrap Mean\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](06-functional-programming_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n### Multiple Model Fitting\n\nFit and compare multiple models efficiently:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate sample data\nset.seed(123)\nmodel_data <- tibble(\n  x1 = rnorm(100),\n  x2 = rnorm(100),\n  x3 = rnorm(100),\n  y = 2 * x1 + 3 * x2 - x3 + rnorm(100, sd = 0.5)\n)\n\n# Define model formulas\nformulas <- list(\n  simple = y ~ x1,\n  additive = y ~ x1 + x2,\n  full = y ~ x1 + x2 + x3,\n  interaction = y ~ x1 * x2 + x3,\n  quadratic = y ~ poly(x1, 2) + x2 + x3\n)\n\n# Fit all models\nmodels <- map(formulas, ~ lm(., data = model_data))\n\n# Extract model metrics\nmodel_comparison <- tibble(\n  model_name = names(formulas),\n  formula = map_chr(formulas, deparse),\n  model = models\n) %>%\n  mutate(\n    glance = map(model, broom::glance),\n    tidy = map(model, broom::tidy),\n    augment = map(model, broom::augment)\n  ) %>%\n  unnest(glance) %>%\n  select(model_name, formula, r.squared, adj.r.squared, AIC, BIC) %>%\n  arrange(desc(adj.r.squared))\n\nprint(model_comparison)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 6\n  model_name  formula                   r.squared adj.r.squared   AIC   BIC\n  <chr>       <chr>                         <dbl>         <dbl> <dbl> <dbl>\n1 full        y ~ x1 + x2 + x3              0.979         0.978  161.  174.\n2 interaction y ~ x1 * x2 + x3              0.979         0.978  163.  178.\n3 quadratic   y ~ poly(x1, 2) + x2 + x3     0.979         0.978  163.  179.\n4 additive    y ~ x1 + x2                   0.906         0.904  310.  320.\n5 simple      y ~ x1                        0.249         0.241  515.  523.\n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize model performance\nmodel_comparison %>%\n  pivot_longer(cols = c(r.squared, adj.r.squared, AIC, BIC),\n               names_to = \"metric\", values_to = \"value\") %>%\n  ggplot(aes(x = model_name, y = value, fill = model_name)) +\n  geom_col() +\n  facet_wrap(~metric, scales = \"free_y\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = \"Model Comparison Metrics\") +\n  guides(fill = \"none\")\n```\n\n::: {.cell-output-display}\n![](06-functional-programming_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n### Simulation Study\n\nUse purrr for Monte Carlo simulations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulation parameters\nn_simulations <- 1000\nsample_sizes <- c(10, 30, 100, 300)\n\n# Function to simulate one t-test\nsimulate_t_test <- function(n, true_diff = 0.5) {\n  group1 <- rnorm(n, mean = 0, sd = 1)\n  group2 <- rnorm(n, mean = true_diff, sd = 1)\n  \n  test_result <- t.test(group1, group2)\n  \n  tibble(\n    p_value = test_result$p.value,\n    significant = test_result$p.value < 0.05,\n    estimate = test_result$estimate[1] - test_result$estimate[2],\n    ci_lower = test_result$conf.int[1],\n    ci_upper = test_result$conf.int[2]\n  )\n}\n\n# Run simulation for different sample sizes\nsimulation_results <- map_df(sample_sizes, function(n) {\n  map_df(1:n_simulations, ~ simulate_t_test(n), .id = \"sim\") %>%\n    mutate(sample_size = n)\n})\n\n# Calculate power for each sample size\npower_analysis <- simulation_results %>%\n  group_by(sample_size) %>%\n  summarise(\n    power = mean(significant),\n    mean_estimate = mean(estimate),\n    sd_estimate = sd(estimate),\n    coverage = mean(ci_lower < -0.5 & ci_upper > -0.5),\n    .groups = \"drop\"\n  )\n\nprint(power_analysis)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 x 5\n  sample_size power mean_estimate sd_estimate coverage\n        <dbl> <dbl>         <dbl>       <dbl>    <dbl>\n1          10 0.18         -0.504      0.451     0.95 \n2          30 0.481        -0.500      0.256     0.949\n3         100 0.942        -0.505      0.140     0.953\n4         300 1            -0.501      0.0840    0.94 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize power curve\nggplot(power_analysis, aes(x = sample_size, y = power)) +\n  geom_line(linewidth = 1.5, color = \"darkblue\") +\n  geom_point(size = 4, color = \"darkblue\") +\n  geom_hline(yintercept = 0.8, linetype = \"dashed\", color = \"red\") +\n  scale_x_log10() +\n  scale_y_continuous(limits = c(0, 1)) +\n  labs(\n    title = \"Statistical Power vs Sample Size\",\n    subtitle = \"True effect size = 0.5, α = 0.05\",\n    x = \"Sample Size (log scale)\",\n    y = \"Statistical Power\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](06-functional-programming_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n## Performance Considerations\n\n### Benchmarking map vs loops\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create test data\ntest_list <- map(1:1000, ~ rnorm(100))\n\n# Benchmark different approaches\nlibrary(microbenchmark)\n\nbenchmark_results <- microbenchmark(\n  for_loop = {\n    results <- numeric(length(test_list))\n    for (i in seq_along(test_list)) {\n      results[i] <- mean(test_list[[i]])\n    }\n  },\n  \n  map_dbl = map_dbl(test_list, mean),\n  \n  vapply = vapply(test_list, mean, numeric(1)),\n  \n  lapply = unlist(lapply(test_list, mean)),\n  \n  times = 100\n)\n\nprint(benchmark_results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnit: milliseconds\n     expr      min       lq     mean   median       uq      max neval\n for_loop 2.324003 2.410226 2.528149 2.488659 2.572894 4.431649   100\n  map_dbl 1.680877 1.717264 1.811503 1.765931 1.818288 3.754165   100\n   vapply 1.499247 1.546192 1.739641 1.579484 1.652833 4.058016   100\n   lapply 1.527701 1.563433 1.775901 1.596663 1.669766 7.912303   100\n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize results\nautoplot(benchmark_results) +\n  labs(title = \"Performance Comparison: Iteration Methods\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](06-functional-programming_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n### Parallel Processing with furrr\n\nFor large-scale operations, use parallel processing:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Note: furrr requires the future package\nlibrary(furrr)\nlibrary(future)\n\n# Set up parallel processing\nplan(multisession, workers = 2)  # Use 2 cores\n\n# Large simulation\nlarge_simulation <- function(n = 10000) {\n  # Simulate expensive computation\n  data <- rnorm(n)\n  tibble(\n    mean = mean(data),\n    sd = sd(data),\n    median = median(data),\n    mad = mad(data)\n  )\n}\n\n# Sequential processing\nsystem.time({\n  sequential_results <- map_df(1:100, ~ large_simulation())\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  0.113   0.006   0.119 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Parallel processing\nsystem.time({\n  parallel_results <- furrr::future_map_dfr(1:100, ~ large_simulation())\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  0.030   0.001   0.328 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Reset to sequential processing\nplan(sequential)\n\nprint(\"Results are identical:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Results are identical:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(identical(sequential_results, parallel_results))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n:::\n\n\n## Exercises\n\n### Exercise 1: Nested Data Analysis\n\nWork with nested data frames to perform group-wise analysis:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Create nested data\nsales_data <- tibble(\n  region = rep(c(\"North\", \"South\", \"East\", \"West\"), each = 50),\n  month = rep(1:12, length.out = 200),\n  sales = c(\n    rnorm(50, 1000, 200),  # North\n    rnorm(50, 1200, 250),  # South\n    rnorm(50, 900, 150),   # East\n    rnorm(50, 1100, 300)   # West\n  )\n)\n\n# Nest and analyze\nsales_analysis <- sales_data %>%\n  nest(data = c(month, sales)) %>%\n  mutate(\n    # Calculate statistics\n    total_sales = map_dbl(data, ~ sum(.$sales)),\n    avg_sales = map_dbl(data, ~ mean(.$sales)),\n    cv = map_dbl(data, ~ sd(.$sales) / mean(.$sales)),\n    \n    # Fit trend model\n    model = map(data, ~ lm(sales ~ month, data = .)),\n    \n    # Extract slope (trend)\n    trend = map_dbl(model, ~ coef(.)[2]),\n    \n    # Model quality\n    r_squared = map_dbl(model, ~ summary(.)$r.squared)\n  )\n\nsales_analysis %>%\n  select(-data, -model) %>%\n  arrange(desc(total_sales))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 x 6\n  region total_sales avg_sales    cv trend r_squared\n  <chr>        <dbl>     <dbl> <dbl> <dbl>     <dbl>\n1 South       60306.     1206. 0.202 -8.47   0.0145 \n2 West        54712.     1094. 0.258 22.5    0.0748 \n3 North       51065.     1021. 0.208 -3.71   0.00386\n4 East        44373.      887. 0.162  2.31   0.00302\n```\n\n\n:::\n:::\n\n\n### Exercise 2: Safe Data Cleaning\n\nCreate a robust data cleaning pipeline that handles errors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Messy data with various problems\nmessy_data <- list(\n  clean = tibble(x = 1:5, y = 2:6),\n  missing_col = tibble(x = 1:5),\n  wrong_type = tibble(x = letters[1:5], y = 2:6),\n  empty = tibble(),\n  null = NULL\n)\n\n# Create safe cleaning function\nsafe_clean <- possibly(function(df) {\n  df %>%\n    mutate(\n      z = x + y,\n      category = if_else(z > 5, \"high\", \"low\")\n    )\n}, otherwise = tibble(error = \"Processing failed\"))\n\n# Apply to all data\ncleaned_results <- map(messy_data, safe_clean)\n\n# Check which ones succeeded\nsuccess_status <- map_lgl(cleaned_results, ~ !(\"error\" %in% names(.)))\nnames(success_status) <- names(messy_data)\nprint(success_status)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      clean missing_col  wrong_type       empty        null \n       TRUE       FALSE       FALSE       FALSE       FALSE \n```\n\n\n:::\n:::\n\n\n### Exercise 3: Bootstrap Confidence Intervals\n\nImplement bootstrap for multiple statistics:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Sample data\nsample_data <- rgamma(100, shape = 2, rate = 0.5)\n\n# Bootstrap function for multiple statistics\nbootstrap_stats <- function(data, n_boot = 1000, conf_level = 0.95) {\n  alpha <- 1 - conf_level\n  \n  # Generate bootstrap samples\n  boot_results <- map_df(1:n_boot, function(i) {\n    boot_sample <- sample(data, replace = TRUE)\n    tibble(\n      mean = mean(boot_sample),\n      median = median(boot_sample),\n      sd = sd(boot_sample),\n      iqr = IQR(boot_sample)\n    )\n  })\n  \n  # Calculate confidence intervals\n  stats_summary <- boot_results %>%\n    pivot_longer(everything(), names_to = \"statistic\", values_to = \"value\") %>%\n    group_by(statistic) %>%\n    summarise(\n      estimate = mean(value),\n      se = sd(value),\n      ci_lower = quantile(value, alpha/2),\n      ci_upper = quantile(value, 1 - alpha/2),\n      .groups = \"drop\"\n    )\n  \n  return(stats_summary)\n}\n\n# Apply bootstrap\nboot_ci <- bootstrap_stats(sample_data)\nprint(boot_ci)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 x 5\n  statistic estimate    se ci_lower ci_upper\n  <chr>        <dbl> <dbl>    <dbl>    <dbl>\n1 iqr           3.25 0.414     2.58     4.19\n2 mean          3.84 0.300     3.28     4.41\n3 median        2.93 0.218     2.56     3.44\n4 sd            2.91 0.321     2.29     3.56\n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize bootstrap distributions\nboot_samples <- map_df(1:1000, function(i) {\n  boot_sample <- sample(sample_data, replace = TRUE)\n  tibble(\n    iteration = i,\n    mean = mean(boot_sample),\n    median = median(boot_sample)\n  )\n})\n\nboot_samples %>%\n  pivot_longer(c(mean, median), names_to = \"statistic\", values_to = \"value\") %>%\n  ggplot(aes(x = value, fill = statistic)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(~statistic, scales = \"free\") +\n  theme_minimal() +\n  labs(title = \"Bootstrap Distributions\")\n```\n\n::: {.cell-output-display}\n![](06-functional-programming_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n### Exercise 4: Simulation Power Analysis\n\nSimulate power for different effect sizes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Simulation parameters\neffect_sizes <- seq(0, 1, by = 0.1)\nn_sims <- 500\nsample_size <- 50\n\n# Power simulation function\ncalculate_power <- function(effect_size, n = sample_size, n_sim = n_sims) {\n  p_values <- map_dbl(1:n_sim, function(i) {\n    control <- rnorm(n, mean = 0, sd = 1)\n    treatment <- rnorm(n, mean = effect_size, sd = 1)\n    t.test(treatment, control)$p.value\n  })\n  \n  mean(p_values < 0.05)\n}\n\n# Calculate power for each effect size\npower_results <- tibble(\n  effect_size = effect_sizes,\n  power = map_dbl(effect_sizes, calculate_power)\n)\n\n# Visualize power curve\nggplot(power_results, aes(x = effect_size, y = power)) +\n  geom_line(linewidth = 1.5) +\n  geom_point(size = 3) +\n  geom_hline(yintercept = 0.8, linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = 0.5, linetype = \"dashed\", color = \"blue\") +\n  scale_y_continuous(limits = c(0, 1)) +\n  labs(\n    title = \"Power Analysis: Effect Size vs Statistical Power\",\n    subtitle = paste(\"n =\", sample_size, \"per group, α = 0.05\"),\n    x = \"Effect Size (Cohen's d)\",\n    y = \"Statistical Power\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](06-functional-programming_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n## Summary\n\nIn this comprehensive chapter, you've mastered:\n\n✅ **Functional programming concepts**\n  - Replacing loops with map functions\n  - Type-safe iterations\n  - Anonymous functions and shortcuts\n\n✅ **Advanced purrr techniques**\n  - map2() and pmap() for multiple inputs\n  - Nested data manipulation\n  - Error handling with safely() and possibly()\n\n✅ **Functional patterns**\n  - Function factories and composition\n  - Reduce and accumulate operations\n  - Parallel processing with furrr\n\n✅ **Real-world applications**\n  - Bootstrap analysis\n  - Multiple model fitting\n  - Simulation studies\n  - Performance optimization\n\nKey takeaways:\n- Functional programming makes code more readable and maintainable\n- purrr integrates seamlessly with tidyverse workflows\n- Error handling is crucial for robust data pipelines\n- Vectorization and parallelization improve performance\n- Think in terms of functions, not loops\n\n## What's Next?\n\nIn [Chapter 7](07-strings-dates.Rmd), we'll explore string manipulation and date/time handling with stringr and lubridate.\n\n## Additional Resources\n\n- [purrr Documentation](https://purrr.tidyverse.org/)\n- [Advanced R - Functionals](https://adv-r.hadley.nz/functionals.html)\n- [R for Data Science - Iteration](https://r4ds.had.co.nz/iteration.html)\n- [Jenny Bryan's purrr Tutorial](https://jennybc.github.io/purrr-tutorial/)\n- [furrr Documentation](https://furrr.futureverse.org/)\n",
    "supporting": [
      "06-functional-programming_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}