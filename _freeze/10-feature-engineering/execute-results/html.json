{
  "hash": "d8229e867b204bcb433a7322682e200e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 10: Feature Engineering with recipes - The Art and Science of Data Preparation\"\nauthor: \"David Sarrat González, Juan R González\"\ndate: today\nformat:\n  html:\n    code-fold: false\n    code-tools: true\n---\n\n## Learning Objectives\n\nBy the end of this chapter, you will master:\n\n- The philosophy and importance of feature engineering\n- Creating and applying recipes in tidymodels\n- Numeric transformations and scaling\n- Handling categorical variables\n- Creating interaction terms and polynomial features\n- Dealing with missing data systematically\n- Feature selection and dimensionality reduction\n- Time-based and text features\n- Best practices and common pitfalls\n\n## What is Feature Engineering?\n\nFeature engineering is the process of transforming raw data into features that better represent the underlying problem to predictive models. It's often said that \"data and features determine the upper limit of machine learning, while models and algorithms only approach this limit.\"\n\n### Why Feature Engineering Matters\n\nThink of feature engineering as translating your data into a language that your model can better understand. Even the most sophisticated algorithm will struggle with poorly prepared data, while a simple model can perform remarkably well with thoughtfully engineered features.\n\nConsider these scenarios:\n- **Raw timestamps** → Extract hour of day, day of week, is_weekend, season\n- **Text addresses** → Extract zip code, city, distance from city center\n- **Numerical ratios** → Price per square foot instead of just price and area\n- **Domain knowledge** → Age of house at sale instead of just year built and sale year\n\nLet's see this in action:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Attaching packages -------------------------------------- tidymodels 1.4.1 --\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nv broom        1.0.10     v recipes      1.3.1 \nv dials        1.4.2      v rsample      1.3.1 \nv dplyr        1.1.4      v tailor       0.1.0 \nv ggplot2      4.0.0      v tidyr        1.3.1 \nv infer        1.0.9      v tune         2.0.0 \nv modeldata    1.5.1      v workflows    1.3.0 \nv parsnip      1.3.3      v workflowsets 1.1.1 \nv purrr        1.1.0      v yardstick    1.3.2 \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx purrr::discard() masks scales::discard()\nx dplyr::filter()  masks stats::filter()\nx dplyr::lag()     masks stats::lag()\nx recipes::step()  masks stats::step()\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv forcats   1.0.0     v stringr   1.5.2\nv lubridate 1.9.4     v tibble    3.3.0\nv readr     2.1.5     \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx readr::col_factor() masks scales::col_factor()\nx purrr::discard()    masks scales::discard()\nx dplyr::filter()     masks stats::filter()\nx stringr::fixed()    masks recipes::fixed()\nx dplyr::lag()        masks stats::lag()\nx readr::spec()       masks yardstick::spec()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(lubridate)\nlibrary(textrecipes)\nlibrary(themis)\nlibrary(corrplot)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\ncorrplot 0.95 loaded\n```\n\n\n:::\n\n```{.r .cell-code}\n# Set theme and seed\ntheme_set(theme_minimal())\nset.seed(123)\n\n# Load example datasets\ndata(ames)\ndata(credit_data)\n\n# Create a simple example to show feature engineering impact\nsimple_data <- tibble(\n  sale_date = seq(as.Date(\"2020-01-01\"), as.Date(\"2022-12-31\"), by = \"day\"),\n  temperature = 50 + 30 * sin(2 * pi * as.numeric(sale_date) / 365) + rnorm(length(sale_date), 0, 5),\n  sales = 1000 + 200 * sin(2 * pi * as.numeric(sale_date) / 365) + \n          100 * (wday(sale_date) %in% c(1, 7)) +  # Weekend boost\n          rnorm(length(sale_date), 0, 50)\n)\n\n# Without feature engineering - just using date as numeric\nbad_model <- lm(sales ~ as.numeric(sale_date), data = simple_data)\n\n# With feature engineering\ngood_data <- simple_data %>%\n  mutate(\n    month = month(sale_date),\n    day_of_week = wday(sale_date, label = TRUE),\n    is_weekend = wday(sale_date) %in% c(1, 7),\n    quarter = quarter(sale_date),\n    days_since_start = as.numeric(sale_date - min(sale_date))\n  )\n\ngood_model <- lm(sales ~ month + is_weekend + temperature + days_since_start, \n                 data = good_data)\n\n# Compare R-squared\ntibble(\n  Model = c(\"Without Feature Engineering\", \"With Feature Engineering\"),\n  `R-squared` = c(summary(bad_model)$r.squared, summary(good_model)$r.squared)\n) %>%\n  knitr::kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|Model                       | R-squared|\n|:---------------------------|---------:|\n|Without Feature Engineering |     0.055|\n|With Feature Engineering    |     0.869|\n\n\n:::\n:::\n\n\nNotice the dramatic improvement! The model with engineered features captures patterns that the raw date couldn't represent.\n\n## The recipes Package Philosophy\n\nThe `recipes` package provides a domain-specific language for feature engineering. Think of it like writing a recipe for a meal:\n\n1. **Ingredients** (raw data): What you start with\n2. **Instructions** (steps): How to transform the ingredients\n3. **Preparation** (prep): Getting everything ready with your training data\n4. **Baking** (bake): Applying the recipe to new data\n\nThis approach ensures:\n- **Reproducibility**: The same transformations applied consistently\n- **Modularity**: Easy to add, remove, or modify steps\n- **Prevention of data leakage**: Transformations learned only from training data\n\n## Creating Your First Recipe\n\nLet's start with a basic recipe and build complexity gradually:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Prepare the Ames housing data\names_train <- ames %>%\n  filter(Sale_Price > 0) %>%\n  sample_frac(0.8)\n\names_test <- ames %>%\n  filter(Sale_Price > 0) %>%\n  anti_join(ames_train)\n\n# Create a basic recipe\nbasic_recipe <- recipe(Sale_Price ~ Lot_Area + Year_Built + Overall_Cond, \n                       data = ames_train)\n\n# View the recipe\nbasic_recipe\n```\n:::\n\n\nAt this point, the recipe is just a specification - it hasn't done anything yet. It's like having a recipe card but not having cooked the meal.\n\n### Adding Steps to the Recipe\n\nNow let's add transformation steps. Each step transforms the data in a specific way:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Enhanced recipe with multiple steps\nenhanced_recipe <- recipe(Sale_Price ~ Lot_Area + Year_Built + Overall_Cond + \n                          Neighborhood + Gr_Liv_Area, \n                          data = ames_train) %>%\n  # Step 1: Log transform the outcome\n  step_log(Sale_Price) %>%\n  # Step 2: Create a new feature\n  step_mutate(House_Age = 2010 - Year_Built) %>%\n  # Step 3: Remove the original Year_Built\n  step_rm(Year_Built) %>%\n  # Step 4: Normalize numeric predictors\n  step_normalize(all_numeric_predictors()) %>%\n  # Step 5: Create dummy variables for categorical predictors\n  step_dummy(all_nominal_predictors())\n\nenhanced_recipe\n```\n:::\n\n\nEach step is performed in order, and the output of one step becomes the input to the next. This is crucial to understand - order matters!\n\n### Preparing and Baking the Recipe\n\nNow we need to \"prepare\" the recipe using the training data, then \"bake\" it to apply the transformations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Prepare the recipe (learn parameters from training data)\nprepped_recipe <- prep(enhanced_recipe, training = ames_train)\n\n# See what was learned\nprepped_recipe\n\n# Apply to training data\nbaked_train <- bake(prepped_recipe, new_data = NULL)  # NULL means use training data\nglimpse(baked_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 2,344\nColumns: 41\n$ Lot_Area                                             <dbl> -0.07145116, -0.1~\n$ Gr_Liv_Area                                          <dbl> -0.88264922, -0.3~\n$ Sale_Price                                           <dbl> 11.86358, 12.2496~\n$ House_Age                                            <dbl> 0.10653670, -1.19~\n$ Overall_Cond_Poor                                    <dbl> 0, 0, 0, 0, 0, 0,~\n$ Overall_Cond_Fair                                    <dbl> 0, 0, 0, 0, 0, 0,~\n$ Overall_Cond_Below_Average                           <dbl> 0, 0, 0, 0, 0, 0,~\n$ Overall_Cond_Average                                 <dbl> 1, 1, 1, 0, 1, 1,~\n$ Overall_Cond_Above_Average                           <dbl> 0, 0, 0, 1, 0, 0,~\n$ Overall_Cond_Good                                    <dbl> 0, 0, 0, 0, 0, 0,~\n$ Overall_Cond_Very_Good                               <dbl> 0, 0, 0, 0, 0, 0,~\n$ Overall_Cond_Excellent                               <dbl> 0, 0, 0, 0, 0, 0,~\n$ Overall_Cond_Very_Excellent                          <dbl> 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_College_Creek                           <dbl> 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Old_Town                                <dbl> 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Edwards                                 <dbl> 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Somerset                                <dbl> 0, 1, 1, 0, 0, 0,~\n$ Neighborhood_Northridge_Heights                      <dbl> 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Gilbert                                 <dbl> 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Sawyer                                  <dbl> 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Northwest_Ames                          <dbl> 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Sawyer_West                             <dbl> 0, 0, 0, 0, 1, 0,~\n$ Neighborhood_Mitchell                                <dbl> 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Brookside                               <dbl> 0, 0, 0, 1, 0, 0,~\n$ Neighborhood_Crawford                                <dbl> 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Iowa_DOT_and_Rail_Road                  <dbl> 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Timberland                              <dbl> 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Northridge                              <dbl> 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Stone_Brook                             <dbl> 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_South_and_West_of_Iowa_State_University <dbl> 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Clear_Creek                             <dbl> 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Meadow_Village                          <dbl> 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Briardale                               <dbl> 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Bloomington_Heights                     <dbl> 0, 0, 0, 0, 0, 1,~\n$ Neighborhood_Veenker                                 <dbl> 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Northpark_Villa                         <dbl> 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Blueste                                 <dbl> 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Greens                                  <dbl> 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Green_Hills                             <dbl> 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Landmark                                <dbl> 0, 0, 0, 0, 0, 0,~\n$ Neighborhood_Hayden_Lake                             <dbl> 0, 0, 0, 0, 0, 0,~\n```\n\n\n:::\n\n```{.r .cell-code}\n# Apply to test data\nbaked_test <- bake(prepped_recipe, new_data = ames_test)\n\n# Check that dimensions match (except for rows)\ntibble(\n  Dataset = c(\"Training\", \"Test\"),\n  Rows = c(nrow(baked_train), nrow(baked_test)),\n  Columns = c(ncol(baked_train), ncol(baked_test))\n) %>%\n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|Dataset  | Rows| Columns|\n|:--------|----:|-------:|\n|Training | 2344|      41|\n|Test     |  586|      41|\n\n\n:::\n:::\n\n\nThe key insight: `prep()` learns any necessary parameters (like mean and SD for normalization) from the training data, and `bake()` applies these learned transformations to any dataset.\n\n## Numeric Transformations\n\nNumeric features often need transformation to work well with models. Let's explore the most important transformations:\n\n### Scaling and Normalization\n\nDifferent scaling methods serve different purposes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create example data with different scales\nscaling_demo <- tibble(\n  feature_A = rnorm(1000, mean = 100, sd = 15),      # Normal, mean=100\n  feature_B = rexp(1000, rate = 0.01),               # Exponential, right-skewed\n  feature_C = runif(1000, min = 0, max = 1),         # Uniform, 0-1 range\n  feature_D = rlnorm(1000, meanlog = 10, sdlog = 2)  # Log-normal, very large\n)\n\n# Different scaling recipes\nscaling_recipes <- list(\n  original = recipe(~ ., data = scaling_demo),\n  \n  normalized = recipe(~ ., data = scaling_demo) %>%\n    step_normalize(all_predictors()),\n  \n  range = recipe(~ ., data = scaling_demo) %>%\n    step_range(all_predictors(), min = 0, max = 1),\n  \n  robust = recipe(~ ., data = scaling_demo) %>%\n    step_center(all_predictors()) %>%  # Center using median\n    step_scale(all_predictors())       # Scale using standard deviation\n)\n\n# Apply each recipe\nscaled_data <- map_df(names(scaling_recipes), function(name) {\n  scaling_recipes[[name]] %>%\n    prep() %>%\n    bake(new_data = NULL) %>%\n    mutate(scaling_method = name) %>%\n    pivot_longer(cols = -scaling_method, \n                 names_to = \"feature\", \n                 values_to = \"value\")\n})\n\n# Visualize the effects\nggplot(scaled_data, aes(x = value, fill = scaling_method)) +\n  geom_histogram(bins = 30, alpha = 0.6, position = \"identity\") +\n  facet_grid(scaling_method ~ feature, scales = \"free\") +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"Effects of Different Scaling Methods\",\n    subtitle = \"Each method has different properties and use cases\",\n    x = \"Scaled Value\",\n    y = \"Count\"\n  ) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](10-feature-engineering_files/figure-html/unnamed-chunk-5-1.png){width=1344}\n:::\n:::\n\n\nKey insights about scaling:\n- **Normalization** (z-score): Centers at 0, scales by standard deviation. Good for normally distributed features.\n- **Range scaling**: Forces values between min and max. Preserves shape but sensitive to outliers.\n- **Robust scaling**: Uses median and MAD, resistant to outliers.\n\n### Transformations for Skewed Data\n\nMany real-world variables are skewed. Let's handle them properly:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create skewed data\nskewed_data <- tibble(\n  mild_skew = rgamma(1000, shape = 2, rate = 0.5),\n  moderate_skew = rlnorm(1000, meanlog = 0, sdlog = 1),\n  severe_skew = rexp(1000, rate = 0.1),\n  outcome = rnorm(1000)\n)\n\n# Different transformation recipes\ntransform_recipes <- list(\n  original = recipe(outcome ~ ., data = skewed_data),\n  \n  log = recipe(outcome ~ ., data = skewed_data) %>%\n    step_log(all_predictors(), offset = 1),  # offset prevents log(0)\n  \n  sqrt = recipe(outcome ~ ., data = skewed_data) %>%\n    step_sqrt(all_predictors()),\n  \n  yeo_johnson = recipe(outcome ~ ., data = skewed_data) %>%\n    step_YeoJohnson(all_predictors()),  # Automatic optimal transformation\n  \n  box_cox = recipe(outcome ~ ., data = skewed_data) %>%\n    step_BoxCox(all_predictors())  # Requires positive values\n)\n\n# Apply transformations and calculate skewness\nskewness_comparison <- map_df(names(transform_recipes), function(name) {\n  transformed <- transform_recipes[[name]] %>%\n    prep() %>%\n    bake(new_data = NULL)\n  \n  tibble(\n    method = name,\n    mild_skew = moments::skewness(transformed$mild_skew),\n    moderate_skew = moments::skewness(transformed$moderate_skew),\n    severe_skew = moments::skewness(transformed$severe_skew)\n  )\n})\n\n# Display results\nskewness_comparison %>%\n  pivot_longer(cols = -method, names_to = \"feature\", values_to = \"skewness\") %>%\n  ggplot(aes(x = method, y = abs(skewness), fill = method)) +\n  geom_col() +\n  facet_wrap(~feature) +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"Effectiveness of Different Transformations on Skewed Data\",\n    subtitle = \"Lower absolute skewness is better (closer to normal distribution)\",\n    x = \"Transformation Method\",\n    y = \"Absolute Skewness\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](10-feature-engineering_files/figure-html/unnamed-chunk-6-1.png){width=1152}\n:::\n:::\n\n\nThe Yeo-Johnson transformation is particularly useful because it:\n- Automatically finds the optimal transformation parameter\n- Handles both positive and negative values\n- Often achieves near-normal distributions\n\n## Handling Categorical Variables\n\nCategorical variables require special treatment. The approach depends on the model type and the nature of the categories.\n\n### Dummy Variables (One-Hot Encoding)\n\nThis is the most common approach for linear models:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example with different types of categorical variables\ncat_data <- tibble(\n  color = factor(c(\"red\", \"blue\", \"green\", \"red\", \"blue\")),\n  size = factor(c(\"S\", \"M\", \"L\", \"XL\", \"M\"), \n                levels = c(\"S\", \"M\", \"L\", \"XL\"), ordered = TRUE),\n  quality = factor(c(\"good\", \"bad\", \"excellent\", \"good\", \"bad\")),\n  outcome = c(10, 15, 20, 12, 14)\n)\n\n# Basic dummy encoding\ndummy_recipe <- recipe(outcome ~ ., data = cat_data) %>%\n  step_dummy(all_nominal_predictors())\n\ndummy_result <- dummy_recipe %>%\n  prep() %>%\n  bake(new_data = NULL)\n\ndummy_result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 8\n  outcome color_green color_red size_1 size_2 size_3 quality_excellent\n    <dbl>       <dbl>     <dbl>  <dbl>  <dbl>  <dbl>             <dbl>\n1      10           0         1 -0.671    0.5 -0.224                 0\n2      15           0         0 -0.224   -0.5  0.671                 0\n3      20           1         0  0.224   -0.5 -0.671                 1\n4      12           0         1  0.671    0.5  0.224                 0\n5      14           0         0 -0.224   -0.5  0.671                 0\n# i 1 more variable: quality_good <dbl>\n```\n\n\n:::\n:::\n\n\nNotice how each category becomes its own binary column, except one category is dropped (reference level) to avoid perfect multicollinearity.\n\n### Advanced Categorical Encoding\n\nFor high-cardinality categorical variables (many unique values), simple dummy encoding can create too many features:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create high-cardinality example\nhigh_card_data <- ames_train %>%\n  select(Sale_Price, Neighborhood, MS_SubClass) %>%\n  mutate(\n    Neighborhood_Freq = n(),\n    .by = Neighborhood\n  )\n\n# Different encoding strategies\nencoding_recipes <- list(\n  # Standard dummy encoding\n  dummy = recipe(Sale_Price ~ Neighborhood, data = high_card_data) %>%\n    step_dummy(Neighborhood),\n  \n  # Frequency encoding\n  frequency = recipe(Sale_Price ~ Neighborhood, data = high_card_data) %>%\n    step_mutate(Neighborhood_Freq = n(), .by = Neighborhood) %>%\n    step_rm(Neighborhood),\n  \n  # Target encoding (mean of target for each category)\n  target = recipe(Sale_Price ~ Neighborhood, data = high_card_data) %>%\n    step_mutate(\n      Neighborhood_Mean = mean(Sale_Price, na.rm = TRUE),\n      .by = Neighborhood\n    ) %>%\n    step_rm(Neighborhood),\n  \n  # Lumping rare categories\n  lumped = recipe(Sale_Price ~ Neighborhood, data = high_card_data) %>%\n    step_other(Neighborhood, threshold = 0.05) %>%  # Combine rare levels\n    step_dummy(Neighborhood)\n)\n\n# Compare number of features created\nfeature_counts <- map_df(names(encoding_recipes), function(name) {\n  n_features <- encoding_recipes[[name]] %>%\n    prep() %>%\n    bake(new_data = NULL) %>%\n    select(-Sale_Price) %>%\n    ncol()\n  \n  tibble(\n    method = name,\n    n_features = n_features\n  )\n})\n\nfeature_counts %>%\n  ggplot(aes(x = method, y = n_features, fill = method)) +\n  geom_col() +\n  geom_text(aes(label = n_features), vjust = -0.5) +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"Feature Count with Different Encoding Methods\",\n    subtitle = \"High-cardinality categorical variables can create many features\",\n    x = \"Encoding Method\",\n    y = \"Number of Features\"\n  ) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](10-feature-engineering_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nEach method has trade-offs:\n- **Dummy encoding**: Simple but creates many features\n- **Frequency encoding**: Single feature but loses category identity\n- **Target encoding**: Powerful but risks overfitting\n- **Lumping**: Reduces features while preserving main categories\n\n## Creating Interaction Terms\n\nInteractions capture relationships between features that aren't additive:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate data with interaction effect\nset.seed(123)\ninteraction_data <- tibble(\n  x1 = runif(500, 0, 10),\n  x2 = runif(500, 0, 10),\n  # True relationship includes interaction\n  y = 10 + 2*x1 + 3*x2 + 0.5*x1*x2 + rnorm(500, 0, 2)\n)\n\n# Models with and without interaction\nno_interaction_recipe <- recipe(y ~ x1 + x2, data = interaction_data)\n\nwith_interaction_recipe <- recipe(y ~ x1 + x2, data = interaction_data) %>%\n  step_interact(terms = ~ x1:x2)\n\n# Fit both models\nno_int_fit <- workflow() %>%\n  add_recipe(no_interaction_recipe) %>%\n  add_model(linear_reg()) %>%\n  fit(interaction_data)\n\nwith_int_fit <- workflow() %>%\n  add_recipe(with_interaction_recipe) %>%\n  add_model(linear_reg()) %>%\n  fit(interaction_data)\n\n# Create prediction surface\ngrid <- expand_grid(\n  x1 = seq(0, 10, length.out = 50),\n  x2 = seq(0, 10, length.out = 50)\n)\n\ngrid_no_int <- grid %>%\n  mutate(\n    prediction = predict(no_int_fit, grid)$.pred,\n    model = \"Without Interaction\"\n  )\n\ngrid_with_int <- grid %>%\n  mutate(\n    prediction = predict(with_int_fit, grid)$.pred,\n    model = \"With Interaction\"\n  )\n\n# Visualize the difference\nbind_rows(grid_no_int, grid_with_int) %>%\n  ggplot(aes(x = x1, y = x2, fill = prediction)) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  facet_wrap(~model) +\n  labs(\n    title = \"Effect of Including Interaction Terms\",\n    subtitle = \"Interaction allows the effect of x1 to depend on x2\",\n    x = \"Feature 1\",\n    y = \"Feature 2\",\n    fill = \"Predicted\\nValue\"\n  )\n```\n\n::: {.cell-output-display}\n![](10-feature-engineering_files/figure-html/unnamed-chunk-9-1.png){width=1344}\n:::\n\n```{.r .cell-code}\n# Compare model performance\ntibble(\n  Model = c(\"Without Interaction\", \"With Interaction\"),\n  RMSE = c(\n    sqrt(mean((interaction_data$y - predict(no_int_fit, interaction_data)$.pred)^2)),\n    sqrt(mean((interaction_data$y - predict(with_int_fit, interaction_data)$.pred)^2))\n  )\n) %>%\n  knitr::kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|Model               |  RMSE|\n|:-------------------|-----:|\n|Without Interaction | 4.511|\n|With Interaction    | 2.018|\n\n\n:::\n:::\n\n\nThe interaction term allows the model to capture how the effect of one variable depends on another. This is crucial in many real-world scenarios:\n- Price elasticity depending on income level\n- Drug effectiveness depending on patient age\n- Marketing response depending on customer segment\n\n## Handling Missing Data\n\nMissing data is ubiquitous in real-world datasets. The strategy depends on why data is missing:\n\n### Types of Missingness\n\n1. **Missing Completely at Random (MCAR)**: Missingness is independent of all variables\n2. **Missing at Random (MAR)**: Missingness depends on observed variables\n3. **Missing Not at Random (MNAR)**: Missingness depends on the missing value itself\n\nLet's explore different imputation strategies:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create data with different missing patterns\nset.seed(123)\nmissing_data <- tibble(\n  x1 = rnorm(1000),\n  x2 = rnorm(1000),\n  x3 = x1 + x2 + rnorm(1000, 0, 0.5),\n  y = 2*x1 + 3*x2 + x3 + rnorm(1000)\n)\n\n# Introduce different missing patterns\nmissing_data <- missing_data %>%\n  mutate(\n    # MCAR: Random 20% missing\n    x1_mcar = ifelse(runif(n()) < 0.2, NA, x1),\n    # MAR: Missing depends on x2\n    x2_mar = ifelse(x2 < quantile(x2, 0.2), NA, x2),\n    # MNAR: Large values more likely missing\n    x3_mnar = ifelse(x3 > quantile(x3, 0.8) & runif(n()) < 0.5, NA, x3)\n  )\n\n# Different imputation strategies\nimputation_recipes <- list(\n  # Remove rows with missing data\n  complete_case = recipe(y ~ x1_mcar + x2_mar + x3_mnar, data = missing_data) %>%\n    step_naomit(all_predictors()),\n  \n  # Mean imputation\n  mean_imp = recipe(y ~ x1_mcar + x2_mar + x3_mnar, data = missing_data) %>%\n    step_impute_mean(all_predictors()),\n  \n  # Median imputation (robust to outliers)\n  median_imp = recipe(y ~ x1_mcar + x2_mar + x3_mnar, data = missing_data) %>%\n    step_impute_median(all_predictors()),\n  \n  # K-nearest neighbors imputation\n  knn_imp = recipe(y ~ x1_mcar + x2_mar + x3_mnar, data = missing_data) %>%\n    step_impute_knn(all_predictors(), neighbors = 5),\n  \n  # Linear imputation (using other variables)\n  linear_imp = recipe(y ~ x1_mcar + x2_mar + x3_mnar, data = missing_data) %>%\n    step_impute_linear(x1_mcar, impute_with = imp_vars(x2_mar, x3_mnar)) %>%\n    step_impute_linear(x2_mar, impute_with = imp_vars(x1_mcar, x3_mnar)) %>%\n    step_impute_linear(x3_mnar, impute_with = imp_vars(x1_mcar, x2_mar))\n)\n\n# Apply imputation and evaluate\nimputation_results <- map_df(names(imputation_recipes), function(name) {\n  imputed <- imputation_recipes[[name]] %>%\n    prep() %>%\n    bake(new_data = NULL)\n  \n  # Calculate statistics\n  tibble(\n    method = name,\n    n_rows = nrow(imputed),\n    x1_mean_error = mean(imputed$x1_mcar - missing_data$x1[!is.na(imputed$x1_mcar)], \n                         na.rm = TRUE),\n    x2_mean_error = mean(imputed$x2_mar - missing_data$x2[!is.na(imputed$x2_mar)], \n                         na.rm = TRUE),\n    x3_mean_error = mean(imputed$x3_mnar - missing_data$x3[!is.na(imputed$x3_mnar)], \n                         na.rm = TRUE)\n  )\n})\n\n# Visualize imputation quality\nimputation_results %>%\n  pivot_longer(cols = contains(\"error\"), \n               names_to = \"variable\", \n               values_to = \"error\") %>%\n  ggplot(aes(x = method, y = abs(error), fill = variable)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"Imputation Error by Method\",\n    subtitle = \"Lower is better - comparing imputed values to true values\",\n    x = \"Imputation Method\",\n    y = \"Absolute Mean Error\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](10-feature-engineering_files/figure-html/unnamed-chunk-10-1.png){width=1344}\n:::\n\n```{.r .cell-code}\n# Show data loss with complete case\ntibble(\n  Method = c(\"Complete Case\", \"Imputation Methods\"),\n  `Rows Retained` = c(\n    imputation_results %>% filter(method == \"complete_case\") %>% pull(n_rows),\n    1000\n  ),\n  `Percentage` = c(\n    imputation_results %>% filter(method == \"complete_case\") %>% pull(n_rows) / 10,\n    100\n  )\n) %>%\n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|Method             | Rows Retained| Percentage|\n|:------------------|-------------:|----------:|\n|Complete Case      |           551|       55.1|\n|Imputation Methods |          1000|      100.0|\n\n\n:::\n:::\n\n\nKey insights about imputation:\n- **Complete case analysis** loses a lot of data\n- **Mean/median imputation** is simple but ignores relationships\n- **KNN imputation** uses similar observations\n- **Linear imputation** preserves linear relationships\n- Choice depends on missing mechanism and data structure\n\n## Feature Selection and Dimensionality Reduction\n\nToo many features can lead to overfitting and computational issues. Let's explore methods to reduce dimensionality:\n\n### Filter Methods\n\nFilter methods select features based on statistical tests:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create data with relevant and irrelevant features\nset.seed(123)\nfeature_data <- tibble(\n  # Relevant features\n  relevant_1 = rnorm(500),\n  relevant_2 = rnorm(500),\n  relevant_3 = rnorm(500),\n  # Irrelevant features\n  noise_1 = rnorm(500),\n  noise_2 = rnorm(500),\n  noise_3 = rnorm(500),\n  noise_4 = rnorm(500),\n  # Target depends only on relevant features\n  target = 2*relevant_1 + 3*relevant_2 - relevant_3 + rnorm(500, 0, 0.5)\n)\n\n# Calculate correlations with target\ncorrelations <- feature_data %>%\n  select(-target) %>%\n  map_dbl(~ cor(., feature_data$target, use = \"complete.obs\")) %>%\n  enframe(name = \"feature\", value = \"correlation\") %>%\n  mutate(\n    abs_correlation = abs(correlation),\n    feature_type = ifelse(str_detect(feature, \"relevant\"), \"Relevant\", \"Noise\")\n  )\n\n# Visualize correlations\nggplot(correlations, aes(x = reorder(feature, abs_correlation), \n                         y = abs_correlation, \n                         fill = feature_type)) +\n  geom_col() +\n  coord_flip() +\n  scale_fill_manual(values = c(\"Relevant\" = \"darkgreen\", \"Noise\" = \"gray50\")) +\n  geom_hline(yintercept = 0.1, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Feature Selection Using Correlation Filter\",\n    subtitle = \"Red line shows potential threshold for feature selection\",\n    x = \"Feature\",\n    y = \"Absolute Correlation with Target\",\n    fill = \"True Feature Type\"\n  )\n```\n\n::: {.cell-output-display}\n![](10-feature-engineering_files/figure-html/unnamed-chunk-11-1.png){width=1152}\n:::\n\n```{.r .cell-code}\n# Recipe with correlation filter\nfiltered_recipe <- recipe(target ~ ., data = feature_data) %>%\n  step_corr(all_predictors(), threshold = 0.9) %>%  # Remove highly correlated features\n  step_rm(all_predictors(), -all_outcomes(), \n          skip = FALSE,\n          threshold = 0.1)  # This would remove low correlation features\n\n# Near-zero variance filter\nnzv_recipe <- recipe(target ~ ., data = feature_data) %>%\n  step_nzv(all_predictors())  # Remove features with near-zero variance\n```\n:::\n\n\n### Principal Component Analysis (PCA)\n\nPCA creates new features that are linear combinations of original features:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create correlated features for PCA demonstration\npca_data <- tibble(\n  x1 = rnorm(500),\n  x2 = x1 + rnorm(500, 0, 0.5),  # Correlated with x1\n  x3 = rnorm(500),\n  x4 = x3 + rnorm(500, 0, 0.5),  # Correlated with x3\n  x5 = rnorm(500),\n  y = x1 + x3 + rnorm(500, 0, 0.5)\n)\n\n# PCA recipe\npca_recipe <- recipe(y ~ ., data = pca_data) %>%\n  step_normalize(all_predictors()) %>%  # Important: normalize before PCA\n  step_pca(all_predictors(), num_comp = 3)  # Keep 3 components\n\n# Prepare and examine\npca_prep <- prep(pca_recipe)\npca_result <- bake(pca_prep, new_data = NULL)\n\n# Extract loadings\npca_loadings <- tidy(pca_prep, 2) %>%  # 2nd step is PCA\n  filter(component %in% paste0(\"PC\", 1:3)) %>%\n  mutate(\n    component = factor(component, levels = paste0(\"PC\", 1:3))\n  )\n\n# Visualize loadings\nggplot(pca_loadings, aes(x = terms, y = value, fill = value > 0)) +\n  geom_col() +\n  facet_wrap(~component) +\n  coord_flip() +\n  scale_fill_manual(values = c(\"FALSE\" = \"red\", \"TRUE\" = \"blue\")) +\n  labs(\n    title = \"PCA Loadings\",\n    subtitle = \"How original features contribute to each principal component\",\n    x = \"Original Feature\",\n    y = \"Loading\",\n    fill = \"Sign\"\n  ) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](10-feature-engineering_files/figure-html/unnamed-chunk-12-1.png){width=1344}\n:::\n\n```{.r .cell-code}\n# Variance explained\npca_variance <- pca_prep$steps[[2]]$res$sdev^2\nvariance_explained <- tibble(\n  PC = paste0(\"PC\", 1:length(pca_variance)),\n  Variance = pca_variance,\n  `Proportion Explained` = Variance / sum(Variance),\n  `Cumulative Proportion` = cumsum(`Proportion Explained`)\n)\n\n# Scree plot\nggplot(variance_explained %>% head(5), \n       aes(x = PC, y = `Proportion Explained`)) +\n  geom_col(fill = \"steelblue\") +\n  geom_line(aes(group = 1), color = \"red\", linewidth = 1) +\n  geom_point(size = 3, color = \"red\") +\n  geom_text(aes(label = round(`Cumulative Proportion`, 2)), \n            vjust = -1, size = 3) +\n  labs(\n    title = \"PCA Scree Plot\",\n    subtitle = \"Shows variance explained by each component\",\n    x = \"Principal Component\",\n    y = \"Proportion of Variance Explained\"\n  )\n```\n\n::: {.cell-output-display}\n![](10-feature-engineering_files/figure-html/unnamed-chunk-12-2.png){width=1344}\n:::\n:::\n\n\nPCA is powerful for:\n- Reducing dimensionality while preserving variance\n- Removing multicollinearity\n- Visualization (first 2-3 components)\n- Noise reduction\n\n## Time-Based Features\n\nTime series data requires special feature engineering:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create time series data\ntime_data <- tibble(\n  date = seq(as.Date(\"2020-01-01\"), as.Date(\"2022-12-31\"), by = \"day\"),\n  base_value = 1000,\n  trend = seq(0, 200, length.out = length(date)),\n  seasonal = 100 * sin(2 * pi * as.numeric(date) / 365),\n  weekly = 50 * sin(2 * pi * wday(date) / 7),\n  noise = rnorm(length(date), 0, 30),\n  sales = base_value + trend + seasonal + weekly + noise\n)\n\n# Time-based feature engineering\ntime_features <- time_data %>%\n  mutate(\n    # Basic time components\n    year = year(date),\n    month = month(date),\n    day = day(date),\n    day_of_week = wday(date, label = TRUE),\n    day_of_year = yday(date),\n    week_of_year = week(date),\n    quarter = quarter(date),\n    \n    # Cyclical encoding (preserves continuity)\n    month_sin = sin(2 * pi * month / 12),\n    month_cos = cos(2 * pi * month / 12),\n    day_sin = sin(2 * pi * day / 31),\n    day_cos = cos(2 * pi * day / 31),\n    \n    # Binary indicators\n    is_weekend = wday(date) %in% c(1, 7),\n    is_month_start = day <= 7,\n    is_month_end = day >= day(ceiling_date(date, \"month\") - days(7)),\n    \n    # Lag features\n    sales_lag_1 = lag(sales, 1),\n    sales_lag_7 = lag(sales, 7),\n    sales_lag_30 = lag(sales, 30),\n    \n    # Rolling statistics\n    sales_ma_7 = zoo::rollmean(sales, 7, fill = NA, align = \"right\"),\n    sales_ma_30 = zoo::rollmean(sales, 30, fill = NA, align = \"right\"),\n    sales_std_7 = zoo::rollapply(sales, 7, sd, fill = NA, align = \"right\")\n  )\n\n# Visualize some engineered features\nfeature_importance <- time_features %>%\n  drop_na() %>%\n  select(sales, month_sin, month_cos, is_weekend, sales_lag_7, sales_ma_30) %>%\n  cor() %>%\n  as.data.frame() %>%\n  rownames_to_column(\"feature\") %>%\n  filter(feature != \"sales\") %>%\n  select(feature, correlation = sales) %>%\n  arrange(desc(abs(correlation)))\n\nggplot(feature_importance, aes(x = reorder(feature, abs(correlation)), \n                               y = abs(correlation))) +\n  geom_col(fill = \"darkblue\") +\n  coord_flip() +\n  labs(\n    title = \"Importance of Time-Based Features\",\n    subtitle = \"Correlation with sales\",\n    x = \"Feature\",\n    y = \"Absolute Correlation\"\n  )\n```\n\n::: {.cell-output-display}\n![](10-feature-engineering_files/figure-html/unnamed-chunk-13-1.png){width=1344}\n:::\n\n```{.r .cell-code}\n# Show cyclical encoding\ncyclical_demo <- time_features %>%\n  select(month, month_sin, month_cos) %>%\n  distinct() %>%\n  arrange(month)\n\nggplot(cyclical_demo, aes(x = month_cos, y = month_sin)) +\n  geom_path(color = \"blue\", linewidth = 1) +\n  geom_point(aes(color = factor(month)), size = 3) +\n  geom_text(aes(label = month), vjust = -1) +\n  coord_equal() +\n  scale_color_viridis_d() +\n  labs(\n    title = \"Cyclical Encoding of Months\",\n    subtitle = \"Preserves continuity: December is close to January\",\n    x = \"Cosine Component\",\n    y = \"Sine Component\"\n  ) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](10-feature-engineering_files/figure-html/unnamed-chunk-13-2.png){width=1344}\n:::\n:::\n\n\nTime features are crucial for:\n- Capturing seasonality and trends\n- Accounting for day-of-week effects\n- Incorporating historical information (lags)\n- Smoothing noisy signals (moving averages)\n\n## Text Features (Brief Introduction)\n\nText data requires specialized preprocessing:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple text feature engineering example\ntext_data <- tibble(\n  id = 1:5,\n  review = c(\n    \"This product is absolutely amazing! Best purchase ever!\",\n    \"Terrible quality. Very disappointed. Would not recommend.\",\n    \"Good value for money. Satisfied with the purchase.\",\n    \"Excellent service and fast delivery. Five stars!\",\n    \"Product broke after one day. Complete waste of money.\"\n  ),\n  rating = c(5, 1, 4, 5, 1)\n)\n\n# Text feature recipe using textrecipes\ntext_recipe <- recipe(rating ~ review, data = text_data) %>%\n  # Tokenize text\n  step_tokenize(review) %>%\n  # Remove stop words\n  step_stopwords(review) %>%\n  # Create n-grams\n  step_ngram(review, num_tokens = 2) %>%\n  # Convert to term frequency\n  step_tf(review, weight_scheme = \"binary\") %>%\n  # Optionally: TF-IDF weighting\n  # step_tfidf(review) %>%\n  # Keep only most common terms\n  step_tokenfilter(review, max_tokens = 20)\n\n# Note: This is just a demonstration - real text processing needs more data\n```\n:::\n\n\nText features can include:\n- Word counts and frequencies\n- N-grams (sequences of words)\n- TF-IDF weights\n- Sentiment scores\n- Word embeddings\n\n## Best Practices and Common Pitfalls\n\n### Best Practices\n\n1. **Always split before feature engineering**\n   - Prevents data leakage\n   - Ensures fair evaluation\n\n2. **Order of operations matters**\n   - Impute missing values before normalization\n   - Create interactions after dummy encoding\n   - Normalize after all transformations\n\n3. **Keep it simple initially**\n   - Start with basic features\n   - Add complexity gradually\n   - Validate improvements\n\n4. **Document your choices**\n   - Why each transformation?\n   - What domain knowledge informed decisions?\n\n### Common Pitfalls to Avoid\n\nLet's demonstrate some common mistakes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create split if not already done\nif (!exists(\"ames_split\")) {\n  set.seed(123)\n  ames_split <- initial_split(ames, prop = 0.75, strata = Sale_Price)\n  ames_train <- training(ames_split)\n  ames_test <- testing(ames_split)\n}\n\n# WRONG: Normalizing before splitting\n# This leaks information from test set into training\nwrong_approach <- ames %>%\n  mutate(Gr_Liv_Area_scaled = scale(Gr_Liv_Area)[,1]) %>%  # Uses ALL data!\n  initial_split()\n\n# RIGHT: Normalize within recipe\nright_recipe <- recipe(Sale_Price ~ Gr_Liv_Area, data = training(ames_split)) %>%\n  step_normalize(Gr_Liv_Area)  # Will use only training data statistics\n\n# WRONG: Creating too many features\noverengineered_recipe <- recipe(Sale_Price ~ ., data = ames_train) %>%\n  step_poly(all_numeric_predictors(), degree = 5) %>%  # Too many polynomial terms\n  step_interact(terms = ~ all_numeric_predictors()^2)  # All 2-way interactions\n\n# RIGHT: Thoughtful feature engineering\nthoughtful_recipe <- recipe(Sale_Price ~ ., data = ames_train) %>%\n  step_log(Sale_Price) %>%\n  step_poly(Gr_Liv_Area, degree = 2) %>%  # Only where needed\n  step_interact(terms = ~ Gr_Liv_Area:Overall_Cond)  # Specific, meaningful interaction\n```\n:::\n\n\n## Complete Example: Putting It All Together\n\nLet's create a comprehensive feature engineering pipeline:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Use credit data for a complete example\ncredit_split <- initial_split(credit_data, prop = 0.75, strata = Status)\ncredit_train <- training(credit_split)\ncredit_test <- testing(credit_split)\n\n# Comprehensive recipe\ncomprehensive_recipe <- recipe(Status ~ ., data = credit_train) %>%\n  # 1. Handle missing values\n  step_impute_median(all_numeric_predictors()) %>%\n  step_impute_mode(all_nominal_predictors()) %>%\n  \n  # 2. Feature creation\n  step_mutate(\n    debt_to_income = Expenses / Income,\n    savings_rate = (Income - Expenses) / Income,\n    has_records = !is.na(Records)\n  ) %>%\n  \n  # 3. Handle skewness\n  step_YeoJohnson(Income, Amount) %>%\n  \n  # 4. Create dummy variables\n  step_dummy(all_nominal_predictors(), -has_records) %>%\n  \n  # 5. Remove near-zero variance\n  step_nzv(all_predictors()) %>%\n  \n  # 6. Normalize\n  step_normalize(all_numeric_predictors()) %>%\n  \n  # 7. Remove highly correlated features\n  step_corr(all_numeric_predictors(), threshold = 0.9) %>%\n  \n  # 8. PCA for dimensionality reduction (optional)\n  # step_pca(all_numeric_predictors(), threshold = 0.95) %>%\n  \n  # 9. Balance classes (for classification)\n  step_smote(Status)  # Synthetic minority oversampling\n\n# Examine the recipe\ncomprehensive_recipe\n\n# Prepare and check results\ncomprehensive_prep <- prep(comprehensive_recipe)\ncomprehensive_baked <- bake(comprehensive_prep, new_data = NULL)\n\n# Summary of transformations\ntibble(\n  Stage = c(\"Original\", \"After Engineering\"),\n  `N Features` = c(ncol(credit_train) - 1, ncol(comprehensive_baked) - 1),\n  `N Observations` = c(nrow(credit_train), nrow(comprehensive_baked)),\n  `Class Balance` = c(\n    sum(credit_train$Status == \"good\") / nrow(credit_train),\n    sum(comprehensive_baked$Status == \"good\") / nrow(comprehensive_baked)\n  )\n) %>%\n  knitr::kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|Stage             | N Features| N Observations| Class Balance|\n|:-----------------|----------:|--------------:|-------------:|\n|Original          |         13|           3340|         0.719|\n|After Engineering |         19|           4800|         0.500|\n\n\n:::\n\n```{.r .cell-code}\n# Fit a model with the engineered features\nrf_spec <- rand_forest(trees = 100) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"classification\")\n\nworkflow() %>%\n  add_recipe(comprehensive_recipe) %>%\n  add_model(rf_spec) %>%\n  fit(credit_train) %>%\n  predict(credit_test) %>%\n  bind_cols(credit_test) %>%\n  accuracy(truth = Status, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.777\n```\n\n\n:::\n:::\n\n\n## Exercises\n\n### Exercise 1: Engineer Features for House Prices\n\nCreate a comprehensive feature engineering pipeline for the Ames housing data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\nexercise_recipe <- recipe(Sale_Price ~ ., data = ames_train) %>%\n  # Transform outcome\n  step_log(Sale_Price) %>%\n  \n  # Create meaningful features\n  step_mutate(\n    House_Age = 2010 - Year_Built,\n    Remod_Age = 2010 - Year_Remod_Add,\n    Has_Garage = !is.na(Garage_Type),\n    Total_Bathrooms = Full_Bath + Half_Bath * 0.5,\n    Total_SF = Gr_Liv_Area + Total_Bsmt_SF,\n    Quality_x_Condition = Overall_Cond * Overall_Cond\n  ) %>%\n  \n  # Remove original year variables\n  step_rm(Year_Built, Year_Remod_Add, Mo_Sold, Year_Sold) %>%\n  \n  # Handle missing values\n  step_impute_median(all_numeric_predictors()) %>%\n  step_impute_mode(all_nominal_predictors()) %>%\n  \n  # Rare categories\n  step_other(all_nominal_predictors(), threshold = 0.05) %>%\n  \n  # Transform skewed variables (before creating dummies)\n  step_YeoJohnson(Lot_Area, Gr_Liv_Area, Total_SF) %>%\n  \n  # Interactions (before creating dummies)\n  step_interact(terms = ~ Total_SF:Overall_Cond) %>%\n  \n  # Create dummies\n  step_dummy(all_nominal_predictors()) %>%\n  \n  # Scale\n  step_normalize(all_numeric_predictors()) %>%\n  \n  # Remove zero variance\n  step_nzv(all_predictors())\n\n# Test the recipe\nexercise_prep <- prep(exercise_recipe)\nexercise_baked <- bake(exercise_prep, new_data = NULL)\n\nprint(paste(\"Created\", ncol(exercise_baked) - 1, \"features from\", \n            ncol(ames_train) - 1, \"original features\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Created 103 features from 73 original features\"\n```\n\n\n:::\n:::\n\n\n### Exercise 2: Handle High-Cardinality Categorical\n\nWork with a high-cardinality categorical variable:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Create synthetic high-cardinality data\nhigh_card_ex <- tibble(\n  category = sample(paste0(\"Cat_\", 1:100), 1000, replace = TRUE),\n  value = rnorm(1000),\n  target = rnorm(1000)\n) %>%\n  mutate(\n    # Make target depend somewhat on category\n    target = target + as.numeric(factor(category)) / 20\n  )\n\n# Try different encoding strategies\nstrategies <- list(\n  # Frequency encoding\n  frequency = recipe(target ~ ., data = high_card_ex) %>%\n    step_mutate(cat_freq = n(), .by = category) %>%\n    step_rm(category),\n  \n  # Target encoding with smoothing\n  target_enc = recipe(target ~ ., data = high_card_ex) %>%\n    step_mutate(\n      cat_mean = mean(target),\n      cat_count = n(),\n      .by = category\n    ) %>%\n    step_mutate(\n      # Smooth with global mean for rare categories\n      cat_smooth = (cat_mean * cat_count + mean(target) * 10) / (cat_count + 10)\n    ) %>%\n    step_rm(category, cat_mean, cat_count),\n  \n  # Embedding-like (PCA on dummies)\n  embedding = recipe(target ~ ., data = high_card_ex) %>%\n    step_dummy(category) %>%\n    step_pca(starts_with(\"category_\"), num_comp = 10)\n)\n\n# Compare approaches\ncomparison <- map_df(names(strategies), function(name) {\n  prepped <- prep(strategies[[name]])\n  baked <- bake(prepped, new_data = NULL)\n  \n  tibble(\n    method = name,\n    n_features = ncol(baked) - 1\n  )\n})\n\nprint(comparison)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 x 2\n  method     n_features\n  <chr>           <dbl>\n1 frequency           3\n2 target_enc          3\n3 embedding          11\n```\n\n\n:::\n:::\n\n\n### Exercise 3: Time Series Feature Engineering\n\nCreate features for time series prediction:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Generate time series data\nts_exercise <- tibble(\n  date = seq(as.Date(\"2021-01-01\"), as.Date(\"2023-12-31\"), by = \"day\")\n) %>%\n  mutate(\n    trend = row_number() / n() * 100,\n    seasonal = 50 * sin(2 * pi * yday(date) / 365),\n    weekly = 20 * sin(2 * pi * wday(date) / 7),\n    noise = rnorm(n(), 0, 10),\n    sales = 1000 + trend + seasonal + weekly + noise\n  )\n\n# Create time features\nts_features <- ts_exercise %>%\n  mutate(\n    # Calendar features\n    year = year(date),\n    month = month(date),\n    week = week(date),\n    day_of_week = wday(date),\n    day_of_month = day(date),\n    day_of_year = yday(date),\n    quarter = quarter(date),\n    \n    # Cyclical encoding\n    month_sin = sin(2 * pi * month / 12),\n    month_cos = cos(2 * pi * month / 12),\n    dow_sin = sin(2 * pi * day_of_week / 7),\n    dow_cos = cos(2 * pi * day_of_week / 7),\n    \n    # Indicators\n    is_weekend = day_of_week %in% c(1, 7),\n    is_month_start = day_of_month <= 3,\n    is_month_end = day_of_month >= 28,\n    \n    # Lag features\n    sales_lag_1 = lag(sales, 1),\n    sales_lag_7 = lag(sales, 7),\n    sales_lag_30 = lag(sales, 30),\n    sales_lag_365 = lag(sales, 365),\n    \n    # Rolling statistics\n    sales_ma_7 = zoo::rollmean(sales, 7, fill = NA, align = \"right\"),\n    sales_ma_30 = zoo::rollmean(sales, 30, fill = NA, align = \"right\"),\n    sales_std_7 = zoo::rollapply(sales, 7, sd, fill = NA, align = \"right\"),\n    sales_std_30 = zoo::rollapply(sales, 30, sd, fill = NA, align = \"right\"),\n    \n    # Differences\n    sales_diff_1 = sales - lag(sales, 1),\n    sales_diff_7 = sales - lag(sales, 7)\n  ) %>%\n  drop_na()\n\n# Evaluate feature importance\nfeature_cors <- ts_features %>%\n  select(-date, -trend, -seasonal, -weekly, -noise) %>%\n  select(-sales) %>%\n  map_dbl(~ cor(., ts_features$sales)) %>%\n  enframe(name = \"feature\", value = \"correlation\") %>%\n  arrange(desc(abs(correlation))) %>%\n  head(15)\n\nggplot(feature_cors, aes(x = reorder(feature, abs(correlation)), \n                         y = abs(correlation))) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Top Time Series Features\",\n    subtitle = \"Absolute correlation with sales\",\n    x = \"Feature\",\n    y = \"|Correlation|\"\n  )\n```\n\n::: {.cell-output-display}\n![](10-feature-engineering_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n## Summary\n\nFeature engineering is both an art and a science. You've learned:\n\n✅ **Core concepts**: Why feature engineering matters and how it works  \n✅ **Numeric transformations**: Scaling, normalization, handling skewness  \n✅ **Categorical encoding**: Dummies, target encoding, handling high cardinality  \n✅ **Interaction terms**: Capturing non-additive relationships  \n✅ **Missing data strategies**: Various imputation methods and when to use them  \n✅ **Dimensionality reduction**: PCA and feature selection  \n✅ **Time features**: Extracting temporal patterns  \n✅ **Best practices**: Avoiding leakage, proper ordering, validation  \n\nRemember:\n- Feature engineering often has more impact than model selection\n- Domain knowledge is invaluable for creating meaningful features\n- Always validate that engineered features improve performance\n- Keep transformations in recipes for reproducibility\n- Start simple and add complexity gradually\n\n## What's Next?\n\nIn [Chapter 11](11-model-specification.Rmd), we'll explore parsnip for unified model specification across different engines.\n\n## Additional Resources\n\n- [Feature Engineering and Selection](http://www.feat.engineering/)\n- [Recipes Documentation](https://recipes.tidymodels.org/)\n- [Feature Engineering for Machine Learning](https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/)\n- [Tidy Modeling with R - Recipes Chapter](https://www.tmwr.org/recipes.html)\n",
    "supporting": [
      "10-feature-engineering_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}