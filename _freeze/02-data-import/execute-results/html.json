{
  "hash": "e559ed1e7ff01121a83c12887be2e993",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 2: Data Import and Export with readr and Beyond\"\nauthor: \"David Sarrat González, Juan R González\"\ndate: today\nformat:\n  html:\n    code-fold: false\n    code-tools: true\n---\n\n## Learning Objectives\n\nBy the end of this chapter, you will:\n\n- Import CSV, TSV, and delimited files with readr\n- Handle Excel files with readxl\n- Work with JSON and XML data\n- Connect to databases\n- Export data in various formats\n- Handle common import problems (encoding, data types, missing values)\n- Work with large files efficiently\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)  # Includes readr\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.4     v readr     2.1.5\nv forcats   1.0.0     v stringr   1.5.2\nv ggplot2   4.0.0     v tibble    3.3.0\nv lubridate 1.9.4     v tidyr     1.3.1\nv purrr     1.1.0     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(readxl)     # For Excel files\nlibrary(jsonlite)   # For JSON files\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAdjuntando el paquete: 'jsonlite'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(DBI)        # For databases\nlibrary(RSQLite)    # For SQLite\nlibrary(haven)      # For SPSS, Stata, SAS files\nlibrary(palmerpenguins)  # For penguins dataset\n\n# Create a temporary directory for our examples\ntemp_dir <- tempdir()\n\n# Load the penguins dataset\ndata(penguins)\n```\n:::\n\n\n## Reading CSV Files with readr\n\n### Basic CSV Import\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a sample CSV file\nsample_data <- tibble(\n  id = 1:5,\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\"),\n  age = c(25, 30, 35, 28, 33),\n  salary = c(50000, 65000, 80000, 55000, 70000),\n  hire_date = c(\"2020-01-15\", \"2019-03-22\", \"2018-07-01\", \"2021-02-10\", \"2019-11-30\")\n)\n\n# Write to CSV\ncsv_file <- file.path(temp_dir, \"employees.csv\")\nwrite_csv(sample_data, csv_file)\n\n# Read it back\nemployees <- read_csv(csv_file)\nemployees\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 5\n     id name      age salary hire_date \n  <dbl> <chr>   <dbl>  <dbl> <date>    \n1     1 Alice      25  50000 2020-01-15\n2     2 Bob        30  65000 2019-03-22\n3     3 Charlie    35  80000 2018-07-01\n4     4 Diana      28  55000 2021-02-10\n5     5 Eve        33  70000 2019-11-30\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare with base R read.csv\nemployees_base <- read.csv(csv_file)\nstr(employees)      # readr version - note the data types\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nspc_tbl_ [5 x 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ id       : num [1:5] 1 2 3 4 5\n $ name     : chr [1:5] \"Alice\" \"Bob\" \"Charlie\" \"Diana\" ...\n $ age      : num [1:5] 25 30 35 28 33\n $ salary   : num [1:5] 50000 65000 80000 55000 70000\n $ hire_date: Date[1:5], format: \"2020-01-15\" \"2019-03-22\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   id = col_double(),\n  ..   name = col_character(),\n  ..   age = col_double(),\n  ..   salary = col_double(),\n  ..   hire_date = col_date(format = \"\")\n  .. )\n - attr(*, \"problems\")=<externalptr> \n```\n\n\n:::\n\n```{.r .cell-code}\nstr(employees_base) # base R version - different types!\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t5 obs. of  5 variables:\n $ id       : int  1 2 3 4 5\n $ name     : chr  \"Alice\" \"Bob\" \"Charlie\" \"Diana\" ...\n $ age      : int  25 30 35 28 33\n $ salary   : int  50000 65000 80000 55000 70000\n $ hire_date: chr  \"2020-01-15\" \"2019-03-22\" \"2018-07-01\" \"2021-02-10\" ...\n```\n\n\n:::\n:::\n\n\n### Specifying Column Types\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Explicit column types\nemployees_typed <- read_csv(\n  csv_file,\n  col_types = cols(\n    id = col_integer(),\n    name = col_character(),\n    age = col_double(),\n    salary = col_number(),\n    hire_date = col_date(format = \"%Y-%m-%d\")\n  )\n)\n\nemployees_typed\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 5\n     id name      age salary hire_date \n  <int> <chr>   <dbl>  <dbl> <date>    \n1     1 Alice      25  50000 2020-01-15\n2     2 Bob        30  65000 2019-03-22\n3     3 Charlie    35  80000 2018-07-01\n4     4 Diana      28  55000 2021-02-10\n5     5 Eve        33  70000 2019-11-30\n```\n\n\n:::\n:::\n\n\n### Handling Messy Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a messy CSV\nmessy_csv <- \"\nName,Age,Income,Start Date\nAlice,25,$50,000,15/01/2020\nBob,30,$65,000,22/03/2019\nCharlie,35,$80,000,01/07/2018\n#Diana,28,$55,000,10/02/2021\nEve,NA,$70,000,30/11/2019\n\"\n\n# Write to file\nmessy_file <- file.path(temp_dir, \"messy_data.csv\")\nwriteLines(messy_csv, messy_file)\n\n# Read with various options\nclean_data <- read_csv(\n  messy_file,\n  skip = 1,  # Skip the first line\n  comment = \"#\",  # Treat lines starting with # as comments\n  na = c(\"\", \"NA\", \"N/A\", \"missing\"),  # Define NA values\n  col_names = c(\"name\", \"age\", \"income\", \"start_date\"),\n  col_types = cols(\n    name = col_character(),\n    age = col_double(),\n    income = col_character(),  # Keep as character for now\n    start_date = col_character()\n  )\n)\n\n# Clean the income column\nclean_data <- clean_data %>%\n  mutate(\n    income = parse_number(income),  # Remove $ and , from numbers\n    start_date = parse_date(start_date, format = \"%d/%m/%Y\")\n  )\n\nclean_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 4\n  name      age income start_date\n  <chr>   <dbl>  <dbl> <date>    \n1 Name       NA     NA NA        \n2 Alice      25     50 NA        \n3 Bob        30     65 NA        \n4 Charlie    35     80 NA        \n5 Eve        NA     70 NA        \n```\n\n\n:::\n:::\n\n\n## Reading Other Delimited Files\n\n### TSV and Custom Delimiters\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tab-separated values\ntsv_data <- \"name\\tage\\tcity\nAlice\\t25\\tNew York\nBob\\t30\\tLos Angeles\nCharlie\\t35\\tChicago\"\n\ntsv_file <- file.path(temp_dir, \"data.tsv\")\nwriteLines(tsv_data, tsv_file)\n\n# Read TSV\nread_tsv(tsv_file)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 x 3\n  name      age city       \n  <chr>   <dbl> <chr>      \n1 Alice      25 New York   \n2 Bob        30 Los Angeles\n3 Charlie    35 Chicago    \n```\n\n\n:::\n\n```{.r .cell-code}\n# Custom delimiter (pipe-separated)\npipe_data <- \"name|age|city\nAlice|25|New York\nBob|30|Los Angeles\"\n\npipe_file <- file.path(temp_dir, \"data.txt\")\nwriteLines(pipe_data, pipe_file)\n\nread_delim(pipe_file, delim = \"|\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 3\n  name    age city       \n  <chr> <dbl> <chr>      \n1 Alice    25 New York   \n2 Bob      30 Los Angeles\n```\n\n\n:::\n:::\n\n\n### Fixed Width Files\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fixed width format\nfwf_data <- \"Alice    25   50000\nBob      30   65000\nCharlie  35   80000\"\n\nfwf_file <- file.path(temp_dir, \"fixed_width.txt\")\nwriteLines(fwf_data, fwf_file)\n\n# Read with column positions\nread_fwf(\n  fwf_file,\n  col_positions = fwf_widths(\n    widths = c(9, 5, 7),\n    col_names = c(\"name\", \"age\", \"salary\")\n  )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 x 3\n  name      age salary\n  <chr>   <dbl>  <dbl>\n1 Alice      25  50000\n2 Bob        30  65000\n3 Charlie    35  80000\n```\n\n\n:::\n:::\n\n\n## Excel Files with readxl\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create an Excel file with multiple sheets\nexcel_file <- file.path(temp_dir, \"company_data.xlsx\")\n\n# Create sample data\nsales_data <- tibble(\n  month = month.name[1:6],\n  revenue = c(100000, 120000, 115000, 130000, 125000, 140000),\n  costs = c(80000, 85000, 82000, 90000, 88000, 95000)\n)\n\nemployee_data <- tibble(\n  department = c(\"Sales\", \"Marketing\", \"IT\", \"HR\"),\n  headcount = c(25, 15, 20, 10),\n  avg_salary = c(60000, 55000, 75000, 50000)\n)\n\n# Write to Excel (requires writexl package)\nif (require(writexl, quietly = TRUE)) {\n  write_xlsx(\n    list(\n      Sales = sales_data,\n      Employees = employee_data\n    ),\n    excel_file\n  )\n  \n  # Read from Excel\n  # List all sheets\n  excel_sheets(excel_file)\n  \n  # Read specific sheet\n  sales <- read_excel(excel_file, sheet = \"Sales\")\n  sales\n  \n  # Read with specific range\n  partial_data <- read_excel(\n    excel_file, \n    sheet = \"Sales\",\n    range = \"A1:B4\"  # Read only first 3 rows and 2 columns\n  )\n  partial_data\n  \n  # Read all sheets at once\n  all_sheets <- excel_sheets(excel_file) %>%\n    set_names() %>%\n    map(~ read_excel(excel_file, sheet = .))\n  \n  all_sheets\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$Sales\n# A tibble: 6 x 3\n  month    revenue costs\n  <chr>      <dbl> <dbl>\n1 January   100000 80000\n2 February  120000 85000\n3 March     115000 82000\n4 April     130000 90000\n5 May       125000 88000\n6 June      140000 95000\n\n$Employees\n# A tibble: 4 x 3\n  department headcount avg_salary\n  <chr>          <dbl>      <dbl>\n1 Sales             25      60000\n2 Marketing         15      55000\n3 IT                20      75000\n4 HR                10      50000\n```\n\n\n:::\n:::\n\n\n## JSON Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create JSON data\njson_data <- list(\n  employees = list(\n    list(\n      id = 1,\n      name = \"Alice\",\n      skills = c(\"R\", \"Python\", \"SQL\"),\n      projects = list(\n        list(name = \"Project A\", status = \"completed\"),\n        list(name = \"Project B\", status = \"in progress\")\n      )\n    ),\n    list(\n      id = 2,\n      name = \"Bob\",\n      skills = c(\"JavaScript\", \"HTML\", \"CSS\"),\n      projects = list(\n        list(name = \"Project C\", status = \"completed\")\n      )\n    )\n  )\n)\n\n# Convert to JSON string\njson_string <- toJSON(json_data, pretty = TRUE, auto_unbox = TRUE)\ncat(json_string)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n{\n  \"employees\": [\n    {\n      \"id\": 1,\n      \"name\": \"Alice\",\n      \"skills\": [\"R\", \"Python\", \"SQL\"],\n      \"projects\": [\n        {\n          \"name\": \"Project A\",\n          \"status\": \"completed\"\n        },\n        {\n          \"name\": \"Project B\",\n          \"status\": \"in progress\"\n        }\n      ]\n    },\n    {\n      \"id\": 2,\n      \"name\": \"Bob\",\n      \"skills\": [\"JavaScript\", \"HTML\", \"CSS\"],\n      \"projects\": [\n        {\n          \"name\": \"Project C\",\n          \"status\": \"completed\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n\n:::\n\n```{.r .cell-code}\n# Write to file\njson_file <- file.path(temp_dir, \"employees.json\")\nwrite(json_string, json_file)\n\n# Read JSON\njson_imported <- fromJSON(json_file)\nstr(json_imported)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 1\n $ employees:'data.frame':\t2 obs. of  4 variables:\n  ..$ id      : int [1:2] 1 2\n  ..$ name    : chr [1:2] \"Alice\" \"Bob\"\n  ..$ skills  :List of 2\n  .. ..$ : chr [1:3] \"R\" \"Python\" \"SQL\"\n  .. ..$ : chr [1:3] \"JavaScript\" \"HTML\" \"CSS\"\n  ..$ projects:List of 2\n  .. ..$ :'data.frame':\t2 obs. of  2 variables:\n  .. .. ..$ name  : chr [1:2] \"Project A\" \"Project B\"\n  .. .. ..$ status: chr [1:2] \"completed\" \"in progress\"\n  .. ..$ :'data.frame':\t1 obs. of  2 variables:\n  .. .. ..$ name  : chr \"Project C\"\n  .. .. ..$ status: chr \"completed\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Flatten nested structure\nemployees_flat <- json_imported$employees %>%\n  as_tibble() %>%\n  mutate(\n    skills = map_chr(skills, ~ paste(., collapse = \", \")),\n    num_projects = map_int(projects, length)\n  ) %>%\n  select(-projects)  # Remove nested column for simplicity\n\nemployees_flat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 4\n     id name  skills                num_projects\n  <int> <chr> <chr>                        <int>\n1     1 Alice R, Python, SQL                   2\n2     2 Bob   JavaScript, HTML, CSS            2\n```\n\n\n:::\n:::\n\n\n## Database Connections\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a SQLite database\ndb_file <- file.path(temp_dir, \"company.db\")\ncon <- dbConnect(RSQLite::SQLite(), db_file)\n\n# Write data to database\ndbWriteTable(con, \"employees\", sample_data, overwrite = TRUE)\ndbWriteTable(con, \"sales\", sales_data, overwrite = TRUE)\n\n# List tables\ndbListTables(con)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"employees\" \"sales\"    \n```\n\n\n:::\n\n```{.r .cell-code}\n# Query data using SQL\nresult <- dbGetQuery(\n  con,\n  \"SELECT * FROM employees WHERE salary > 60000\"\n)\nresult\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  id    name age salary  hire_date\n1  2     Bob  30  65000 2019-03-22\n2  3 Charlie  35  80000 2018-07-01\n3  5     Eve  33  70000 2019-11-30\n```\n\n\n:::\n\n```{.r .cell-code}\n# Using dplyr with databases\nemployees_db <- tbl(con, \"employees\")\n\n# dplyr operations are translated to SQL\nemployees_db %>%\n  filter(salary > 60000) %>%\n  select(name, salary) %>%\n  arrange(desc(salary)) %>%\n  show_query()  # Show the SQL query\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<SQL>\nSELECT `name`, `salary`\nFROM `employees`\nWHERE (`salary` > 60000.0)\nORDER BY `salary` DESC\n```\n\n\n:::\n\n```{.r .cell-code}\n# Collect results\nhigh_earners <- employees_db %>%\n  filter(salary > 60000) %>%\n  collect()  # Bring data into R\n\nhigh_earners\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 x 5\n     id name      age salary hire_date \n  <int> <chr>   <dbl>  <dbl> <chr>     \n1     2 Bob        30  65000 2019-03-22\n2     3 Charlie    35  80000 2018-07-01\n3     5 Eve        33  70000 2019-11-30\n```\n\n\n:::\n\n```{.r .cell-code}\n# Close connection\ndbDisconnect(con)\n```\n:::\n\n\n## Working with APIs and Web Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example: Reading data from a URL\n# Using a public dataset\nurl <- \"https://raw.githubusercontent.com/tidyverse/ggplot2/main/data-raw/diamonds.csv\"\n\n# Direct read from URL\ndiamonds_web <- read_csv(url, n_max = 1000)  # Read first 1000 rows\nglimpse(diamonds_web)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 1,000\nColumns: 10\n$ carat   <dbl> 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.~\n$ cut     <chr> \"Ideal\", \"Premium\", \"Good\", \"Premium\", \"Good\", \"Very Good\", \"V~\n$ color   <chr> \"E\", \"E\", \"E\", \"I\", \"J\", \"J\", \"I\", \"H\", \"E\", \"H\", \"J\", \"J\", \"F~\n$ clarity <chr> \"SI2\", \"SI1\", \"VS1\", \"VS2\", \"SI2\", \"VVS2\", \"VVS1\", \"SI1\", \"VS2~\n$ depth   <dbl> 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64~\n$ table   <dbl> 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58~\n$ price   <dbl> 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34~\n$ x       <dbl> 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.~\n$ y       <dbl> 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.~\n$ z       <dbl> 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.~\n```\n\n\n:::\n\n```{.r .cell-code}\n# Download first, then read (useful for large files)\nlocal_file <- file.path(temp_dir, \"diamonds.csv\")\ndownload.file(url, local_file, quiet = TRUE)\ndiamonds_local <- read_csv(local_file, n_max = 1000)\n```\n:::\n\n\n## Statistical Software Files (SPSS, Stata, SAS)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create example data\nsurvey_data <- tibble(\n  respondent_id = 1:10,\n  age_group = factor(c(1, 2, 1, 3, 2, 2, 1, 3, 2, 1),\n                     labels = c(\"18-30\", \"31-50\", \"51+\")),\n  satisfaction = c(4, 5, 3, 4, 5, 2, 3, 4, 5, 4),\n  comments = c(rep(\"Good service\", 5), rep(\"Needs improvement\", 5))\n)\n\n# Save as different formats\nsav_file <- file.path(temp_dir, \"survey.sav\")\ndta_file <- file.path(temp_dir, \"survey.dta\")\n\nwrite_sav(survey_data, sav_file)\nwrite_dta(survey_data, dta_file)\n\n# Read them back\nspss_data <- read_sav(sav_file)\nstata_data <- read_dta(dta_file)\n\nglimpse(spss_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 10\nColumns: 4\n$ respondent_id <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n$ age_group     <dbl+lbl> 1, 2, 1, 3, 2, 2, 1, 3, 2, 1\n$ satisfaction  <dbl> 4, 5, 3, 4, 5, 2, 3, 4, 5, 4\n$ comments      <chr> \"Good service\", \"Good service\", \"Good service\", \"Good se~\n```\n\n\n:::\n:::\n\n\n## Handling Large Files\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a large CSV file\nlarge_data <- tibble(\n  id = 1:100000,\n  value1 = rnorm(100000),\n  value2 = runif(100000),\n  category = sample(LETTERS[1:5], 100000, replace = TRUE)\n)\n\nlarge_file <- file.path(temp_dir, \"large_data.csv\")\nwrite_csv(large_data, large_file)\n\n# Read only first few rows to check structure\npreview <- read_csv(large_file, n_max = 5)\npreview\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 4\n     id  value1 value2 category\n  <dbl>   <dbl>  <dbl> <chr>   \n1     1 -0.843   0.544 A       \n2     2 -0.679   0.345 E       \n3     3  0.505   0.466 C       \n4     4  0.706   0.841 C       \n5     5  0.0569  0.546 A       \n```\n\n\n:::\n\n```{.r .cell-code}\n# Read specific columns\nsubset_data <- read_csv(\n  large_file,\n  col_select = c(id, category, value1)\n)\nglimpse(subset_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 100,000\nColumns: 3\n$ id       <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18~\n$ category <chr> \"A\", \"E\", \"C\", \"C\", \"A\", \"A\", \"E\", \"A\", \"D\", \"B\", \"C\", \"A\", \"~\n$ value1   <dbl> -0.84294707, -0.67948015, 0.50494394, 0.70600843, 0.05687878,~\n```\n\n\n:::\n\n```{.r .cell-code}\n# Read in chunks\nprocess_chunk <- function(data, pos) {\n  # Process each chunk\n  summarise(data,\n    chunk = pos,\n    mean_value1 = mean(value1),\n    n = n()\n  )\n}\n\n# Read and process in chunks\nchunked_results <- read_csv_chunked(\n  large_file,\n  callback = DataFrameCallback$new(process_chunk),\n  chunk_size = 10000\n)\n\nchunked_results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 x 3\n   chunk mean_value1     n\n   <int>       <dbl> <int>\n 1     1     0.00300 10000\n 2 10001     0.0153  10000\n 3 20001    -0.0135  10000\n 4 30001    -0.00530 10000\n 5 40001    -0.0180  10000\n 6 50001    -0.00333 10000\n 7 60001    -0.00729 10000\n 8 70001    -0.00633 10000\n 9 80001    -0.00363 10000\n10 90001     0.00922 10000\n```\n\n\n:::\n:::\n\n\n## Data Export\n\n### Writing CSV and TSV\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Prepare data for export\nexport_data <- penguins %>%\n  drop_na() %>%\n  group_by(species, island) %>%\n  summarise(\n    count = n(),\n    avg_mass = mean(body_mass_g),\n    .groups = \"drop\"\n  )\n\n# Write CSV\nwrite_csv(export_data, file.path(temp_dir, \"penguin_summary.csv\"))\n\n# Write TSV\nwrite_tsv(export_data, file.path(temp_dir, \"penguin_summary.tsv\"))\n\n# Write with custom settings\nwrite_csv2(export_data, file.path(temp_dir, \"penguin_summary_eu.csv\"))  # European format (semicolon separator)\n\n# Write Excel\nif (require(writexl, quietly = TRUE)) {\n  write_xlsx(export_data, file.path(temp_dir, \"penguin_summary.xlsx\"))\n}\n```\n:::\n\n\n### Advanced Export Options\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a complex dataset\ncomplex_data <- tibble(\n  id = 1:5,\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\"),\n  scores = list(\n    c(85, 90, 88),\n    c(92, 88, 95),\n    c(78, 85, 82),\n    c(90, 92, 94),\n    c(88, 86, 90)\n  ),\n  metadata = c(\n    '{\"department\": \"Sales\", \"level\": 2}',\n    '{\"department\": \"IT\", \"level\": 3}',\n    '{\"department\": \"Marketing\", \"level\": 1}',\n    '{\"department\": \"Sales\", \"level\": 3}',\n    '{\"department\": \"IT\", \"level\": 2}'\n  )\n)\n\n# Export to JSON\njson_export <- toJSON(complex_data, pretty = TRUE)\nwrite(json_export, file.path(temp_dir, \"complex_data.json\"))\n\n# Export to RDS (R's native format - preserves all data types)\nsaveRDS(complex_data, file.path(temp_dir, \"complex_data.rds\"))\n\n# Read it back\nrestored_data <- readRDS(file.path(temp_dir, \"complex_data.rds\"))\nidentical(complex_data, restored_data)  # TRUE - perfect preservation\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n## Common Import Problems and Solutions\n\n### Problem 1: Encoding Issues\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create file with special characters\nspecial_text <- \"Café,Zürich,São Paulo,北京\\n25,30,35,40\"\nspecial_file <- file.path(temp_dir, \"special_chars.csv\")\n\n# Write with UTF-8 encoding\nwriteLines(special_text, special_file, useBytes = TRUE)\n\n# Read with encoding specification\nread_csv(special_file, locale = locale(encoding = \"UTF-8\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 4\n  `Caf<U+00E9>` `Z<U+00FC>rich` `S<U+00E3>o Paulo` `<U+5317><U+4EAC>`\n          <dbl>           <dbl>              <dbl>              <dbl>\n1            25              30                 35                 40\n```\n\n\n:::\n:::\n\n\n### Problem 2: Inconsistent Data Types\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Mixed types in a column\nmixed_csv <- \"id,value\n1,100\n2,200\n3,NA\n4,missing\n5,300\"\n\nmixed_file <- file.path(temp_dir, \"mixed.csv\")\nwriteLines(mixed_csv, mixed_file)\n\n# Read with proper NA handling\nread_csv(\n  mixed_file,\n  na = c(\"NA\", \"missing\", \"\"),\n  col_types = cols(\n    id = col_integer(),\n    value = col_double()\n  )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 2\n     id value\n  <int> <dbl>\n1     1   100\n2     2   200\n3     3    NA\n4     4    NA\n5     5   300\n```\n\n\n:::\n:::\n\n\n### Problem 3: Date/Time Formats\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Various date formats\ndates_csv <- \"event,date\nMeeting,2023-01-15\nConference,15/01/2023\nWorkshop,Jan 15 2023\nSeminar,15-Jan-23\"\n\ndates_file <- file.path(temp_dir, \"dates.csv\")\nwriteLines(dates_csv, dates_file)\n\n# Read and parse dates\ndates_data <- read_csv(dates_file, col_types = cols(date = col_character()))\n\n# Parse different formats\ndates_parsed <- dates_data %>%\n  mutate(\n    parsed_date = case_when(\n      str_detect(date, \"^\\\\d{4}-\\\\d{2}-\\\\d{2}$\") ~ parse_date(date, \"%Y-%m-%d\"),\n      str_detect(date, \"^\\\\d{2}/\\\\d{2}/\\\\d{4}$\") ~ parse_date(date, \"%d/%m/%Y\"),\n      str_detect(date, \"^[A-Za-z]+ \\\\d+ \\\\d{4}$\") ~ parse_date(date, \"%b %d %Y\"),\n      str_detect(date, \"^\\\\d{2}-[A-Za-z]+-\\\\d{2}$\") ~ parse_date(date, \"%d-%b-%y\"),\n      TRUE ~ NA_Date_\n    )\n  )\n\ndates_parsed\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 x 3\n  event      date        parsed_date\n  <chr>      <chr>       <date>     \n1 Meeting    2023-01-15  2023-01-15 \n2 Conference 15/01/2023  2023-01-15 \n3 Workshop   Jan 15 2023 2023-01-15 \n4 Seminar    15-Jan-23   2023-01-15 \n```\n\n\n:::\n:::\n\n\n## Exercises\n\n### Exercise 1: Multi-format Import\n\nCreate a function that can read data from CSV, Excel, or JSON based on file extension:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nread_any <- function(file_path) {\n  ext <- tools::file_ext(file_path)\n  \n  data <- switch(\n    tolower(ext),\n    \"csv\" = read_csv(file_path, show_col_types = FALSE),\n    \"tsv\" = read_tsv(file_path, show_col_types = FALSE),\n    \"xlsx\" = read_excel(file_path),\n    \"xls\" = read_excel(file_path),\n    \"json\" = fromJSON(file_path) %>% as_tibble(),\n    stop(paste(\"Unsupported file type:\", ext))\n  )\n  \n  return(data)\n}\n\n# Test the function\ntest_files <- c(\n  file.path(temp_dir, \"employees.csv\"),\n  file.path(temp_dir, \"data.tsv\")\n)\n\nfor (file in test_files) {\n  if (file.exists(file)) {\n    cat(\"Reading:\", basename(file), \"\\n\")\n    print(read_any(file))\n    cat(\"\\n\")\n  }\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nReading: employees.csv \n# A tibble: 5 x 5\n     id name      age salary hire_date \n  <dbl> <chr>   <dbl>  <dbl> <date>    \n1     1 Alice      25  50000 2020-01-15\n2     2 Bob        30  65000 2019-03-22\n3     3 Charlie    35  80000 2018-07-01\n4     4 Diana      28  55000 2021-02-10\n5     5 Eve        33  70000 2019-11-30\n\nReading: data.tsv \n# A tibble: 3 x 3\n  name      age city       \n  <chr>   <dbl> <chr>      \n1 Alice      25 New York   \n2 Bob        30 Los Angeles\n3 Charlie    35 Chicago    \n```\n\n\n:::\n:::\n\n\n### Exercise 2: Data Cleaning Pipeline\n\nClean and import this messy dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmessy_sales <- \"\nDate,Product,,,Price,Quantity\n2023-01-15,Widget A,,,29.99,10\n2023/01/16,Widget B,,,39.99,missing\n17-01-2023,Widget C,,,49.99,5\n2023-01-18,Widget A,,,29.99,\n,Widget B,,,39.99,8\n\"\n\n# Write to file\nsales_file <- file.path(temp_dir, \"messy_sales.csv\")\nwriteLines(messy_sales, sales_file)\n\n# Your cleaning pipeline here\ncleaned_sales <- read_csv(\n  sales_file,\n  skip_empty_rows = TRUE,\n  na = c(\"\", \"missing\", \"NA\")\n) %>%\n  select(Date, Product, Price, Quantity) %>%\n  filter(!is.na(Date) & !is.na(Product)) %>%\n  mutate(\n    Date = case_when(\n      str_detect(Date, \"^\\\\d{4}-\\\\d{2}-\\\\d{2}$\") ~ parse_date(Date, \"%Y-%m-%d\"),\n      str_detect(Date, \"^\\\\d{4}/\\\\d{2}/\\\\d{2}$\") ~ parse_date(Date, \"%Y/%m/%d\"),\n      str_detect(Date, \"^\\\\d{2}-\\\\d{2}-\\\\d{4}$\") ~ parse_date(Date, \"%d-%m-%Y\"),\n      TRUE ~ NA_Date_\n    ),\n    Quantity = replace_na(Quantity, 0),\n    Total = Price * Quantity\n  )\n\ncleaned_sales\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 x 5\n  Date       Product  Price Quantity Total\n  <date>     <chr>    <dbl>    <dbl> <dbl>\n1 2023-01-15 Widget A  30.0       10  300.\n2 2023-01-16 Widget B  40.0        0    0 \n3 2023-01-17 Widget C  50.0        5  250.\n4 2023-01-18 Widget A  30.0        0    0 \n```\n\n\n:::\n:::\n\n\n### Exercise 3: Database Operations\n\nCreate a mini database with related tables and perform joins:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create database\ndb <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\n# Create tables\ncustomers <- tibble(\n  customer_id = 1:5,\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\"),\n  city = c(\"NYC\", \"LA\", \"Chicago\", \"Houston\", \"Phoenix\")\n)\n\norders <- tibble(\n  order_id = 1:8,\n  customer_id = c(1, 2, 1, 3, 4, 2, 5, 1),\n  amount = c(100, 200, 150, 300, 250, 175, 400, 225),\n  order_date = seq(as.Date(\"2023-01-01\"), by = \"week\", length.out = 8)\n)\n\n# Write to database\ndbWriteTable(db, \"customers\", customers)\ndbWriteTable(db, \"orders\", orders)\n\n# Query with join\nresult <- dbGetQuery(db, \"\n  SELECT c.name, c.city, COUNT(o.order_id) as num_orders, SUM(o.amount) as total_spent\n  FROM customers c\n  LEFT JOIN orders o ON c.customer_id = o.customer_id\n  GROUP BY c.customer_id, c.name, c.city\n  ORDER BY total_spent DESC\n\")\n\nresult\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     name    city num_orders total_spent\n1   Alice     NYC          3         475\n2     Eve Phoenix          1         400\n3     Bob      LA          2         375\n4 Charlie Chicago          1         300\n5   Diana Houston          1         250\n```\n\n\n:::\n\n```{.r .cell-code}\n# Using dplyr\ncustomers_tbl <- tbl(db, \"customers\")\norders_tbl <- tbl(db, \"orders\")\n\nsummary_tbl <- customers_tbl %>%\n  left_join(orders_tbl, by = \"customer_id\") %>%\n  group_by(customer_id, name, city) %>%\n  summarise(\n    num_orders = n(),\n    total_spent = sum(amount, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  arrange(desc(total_spent)) %>%\n  collect()\n\nsummary_tbl\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 5\n  customer_id name    city    num_orders total_spent\n        <int> <chr>   <chr>        <int>       <dbl>\n1           1 Alice   NYC              3         475\n2           5 Eve     Phoenix          1         400\n3           2 Bob     LA               2         375\n4           3 Charlie Chicago          1         300\n5           4 Diana   Houston          1         250\n```\n\n\n:::\n\n```{.r .cell-code}\ndbDisconnect(db)\n```\n:::\n\n\n### Exercise 4: Batch Processing\n\nProcess multiple files in a directory:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create multiple CSV files\nfor (i in 1:3) {\n  data <- tibble(\n    file_id = i,\n    values = rnorm(100, mean = i * 10, sd = 2),\n    category = sample(c(\"A\", \"B\", \"C\"), 100, replace = TRUE)\n  )\n  write_csv(data, file.path(temp_dir, paste0(\"batch_\", i, \".csv\")))\n}\n\n# Read and combine all files\nbatch_files <- list.files(temp_dir, pattern = \"^batch_.*\\\\.csv$\", full.names = TRUE)\n\ncombined_data <- batch_files %>%\n  map_dfr(~ read_csv(., show_col_types = FALSE)) %>%\n  group_by(file_id, category) %>%\n  summarise(\n    mean_value = mean(values),\n    sd_value = sd(values),\n    n = n(),\n    .groups = \"drop\"\n  )\n\ncombined_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 x 5\n  file_id category mean_value sd_value     n\n    <dbl> <chr>         <dbl>    <dbl> <int>\n1       1 A              9.80     1.63    29\n2       1 B             10.5      1.80    37\n3       1 C              9.89     2.06    34\n4       2 A             19.6      2.02    35\n5       2 B             20.5      2.26    36\n6       2 C             19.4      2.75    29\n7       3 A             30.3      2.19    35\n8       3 B             29.8      1.83    38\n9       3 C             30.1      2.15    27\n```\n\n\n:::\n:::\n\n\n## Summary\n\nIn this chapter, you learned:\n\n✅ Reading various file formats (CSV, Excel, JSON, databases)  \n✅ Handling messy and inconsistent data  \n✅ Working with large files efficiently  \n✅ Exporting data in multiple formats  \n✅ Solving common import problems  \n✅ Connecting to and querying databases  \n\n## What's Next?\n\nIn [Chapter 3](03-data-wrangling.Rmd), we'll dive deep into data manipulation with dplyr, learning to filter, select, mutate, and summarize data efficiently.\n\n## Additional Resources\n\n- [readr Documentation](https://readr.tidyverse.org/)\n- [readxl Documentation](https://readxl.tidyverse.org/)\n- [DBI Documentation](https://dbi.r-dbi.org/)\n- [Data Import Cheat Sheet](https://github.com/rstudio/cheatsheets/blob/main/data-import.pdf)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}