{
  "hash": "8db67fc39baef05066cea03ae93b99f0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 11: Model Specification with parsnip - A Unified Interface for Models\"\ndate: today\nformat:\n  html:\n    code-fold: false\n    code-tools: true\n---\n\n## Learning Objectives\n\nBy the end of this chapter, you will master:\n\n- The parsnip philosophy and design principles\n- Specifying models across different engines\n- Setting modes and engines appropriately\n- Model arguments and hyperparameters\n- Translating between different modeling packages\n- Creating custom model specifications\n- Understanding computational engines\n- Best practices for model specification\n\n## The Problem parsnip Solves\n\nOne of the most frustrating aspects of machine learning in R is the inconsistency across modeling packages. Each package has its own syntax, arguments, and output format. Let's see this problem in action:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Attaching packages -------------------------------------- tidymodels 1.4.1 --\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nv broom        1.0.10     v recipes      1.3.1 \nv dials        1.4.2      v rsample      1.3.1 \nv dplyr        1.1.4      v tailor       0.1.0 \nv ggplot2      4.0.0      v tidyr        1.3.1 \nv infer        1.0.9      v tune         2.0.0 \nv modeldata    1.5.1      v workflows    1.3.0 \nv parsnip      1.3.3      v workflowsets 1.1.1 \nv purrr        1.1.0      v yardstick    1.3.2 \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx purrr::discard() masks scales::discard()\nx dplyr::filter()  masks stats::filter()\nx dplyr::lag()     masks stats::lag()\nx recipes::step()  masks stats::step()\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv forcats   1.0.0     v stringr   1.5.2\nv lubridate 1.9.4     v tibble    3.3.0\nv readr     2.1.5     \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx readr::col_factor() masks scales::col_factor()\nx purrr::discard()    masks scales::discard()\nx dplyr::filter()     masks stats::filter()\nx stringr::fixed()    masks recipes::fixed()\nx dplyr::lag()        masks stats::lag()\nx readr::spec()       masks yardstick::spec()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(modeldata)\nlibrary(ranger)     # For random forests\nlibrary(glmnet)     # For regularized regression\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCargando paquete requerido: Matrix\n\nAdjuntando el paquete: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoaded glmnet 4.1-10\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(kknn)       # For k-nearest neighbors\nlibrary(kernlab)    # For support vector machines\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAdjuntando el paquete: 'kernlab'\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    alpha\n\nThe following object is masked from 'package:dials':\n\n    buffer\n\nThe following object is masked from 'package:scales':\n\n    alpha\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(xgboost)    # For gradient boosting\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAdjuntando el paquete: 'xgboost'\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n```\n\n\n:::\n\n```{.r .cell-code}\n# Set theme and seed\ntheme_set(theme_minimal())\nset.seed(123)\n\n# Load data\ndata(ames)\names_small <- ames %>%\n  select(Sale_Price, Gr_Liv_Area, Year_Built, Overall_Cond, Neighborhood) %>%\n  slice_sample(n = 500)  # Smaller sample for demonstration\n\n# The chaos of different interfaces\n# Linear regression with lm()\nlm_fit <- lm(Sale_Price ~ ., data = ames_small)\n\n# Random forest with ranger()\nrf_fit <- ranger(Sale_Price ~ ., data = ames_small, num.trees = 100)\n\n# Elastic net with glmnet() - requires matrix input!\nx_matrix <- model.matrix(Sale_Price ~ . - 1, data = ames_small)\ny_vector <- ames_small$Sale_Price\nglmnet_fit <- glmnet(x_matrix, y_vector, alpha = 0.5)\n\n# Different prediction methods\nlm_pred <- predict(lm_fit, ames_small)  # Returns vector\nrf_pred <- predict(rf_fit, ames_small)$predictions  # Returns list with $predictions\nglmnet_pred <- predict(glmnet_fit, x_matrix, s = 0.01)  # Requires matrix and lambda\n\n# The outputs are all different!\nstr(lm_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Named num [1:500] 222479 219821 228298 259642 129456 ...\n - attr(*, \"names\")= chr [1:500] \"1\" \"2\" \"3\" \"4\" ...\n```\n\n\n:::\n\n```{.r .cell-code}\nstr(rf_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n num [1:500] 234231 188176 197626 263308 124126 ...\n```\n\n\n:::\n\n```{.r .cell-code}\nstr(glmnet_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n num [1:500, 1] 222179 219528 228247 259319 128907 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:500] \"1\" \"2\" \"3\" \"4\" ...\n  ..$ : chr \"s=0.01\"\n```\n\n\n:::\n:::\n\n\nNotice the problems:\n- Different function names and arguments\n- Different data format requirements (data frame vs matrix)\n- Different prediction interfaces\n- Different output structures\n\nThis inconsistency makes it hard to:\n- Switch between models\n- Compare different approaches\n- Build reproducible workflows\n- Write general-purpose code\n\n## Enter parsnip: One Interface to Rule Them All\n\nParsnip provides a unified interface for model specification. The philosophy is simple but powerful:\n1. **Separate model specification from model fitting**\n2. **Use consistent naming across all models**\n3. **Provide a common interface for predictions**\n4. **Allow engine flexibility while maintaining consistency**\n\n### The parsnip Workflow\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Step 1: Specify the model type\nlinear_spec <- linear_reg()\nprint(linear_spec)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n\n\n:::\n\n```{.r .cell-code}\n# Step 2: Set the engine (implementation)\nlinear_spec <- linear_spec %>%\n  set_engine(\"lm\")\nprint(linear_spec)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n\n\n:::\n\n```{.r .cell-code}\n# Step 3: Set the mode (if needed - regression vs classification)\n# For linear_reg, mode is always regression, so this is automatic\n\n# Step 4: Fit the model\nlinear_fit <- linear_spec %>%\n  fit(Sale_Price ~ ., data = ames_small)\n\n# Consistent prediction interface\nlinear_pred <- predict(linear_fit, ames_small)\nstr(linear_pred)  # Always returns a tibble with .pred column\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntibble [500 x 1] (S3: tbl_df/tbl/data.frame)\n $ .pred: num [1:500] 222479 219821 228298 259642 129456 ...\n```\n\n\n:::\n:::\n\n\nThe beauty of this approach:\n- **Model specification** is separate from fitting\n- **Consistent interface** across all models\n- **Predictable output** format\n- **Easy to swap** different implementations\n\n## Model Types in parsnip\n\nParsnip supports a wide variety of model types. Let's explore the main categories:\n\n### Regression Models\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Linear regression\nlinear_reg_spec <- linear_reg() %>%\n  set_engine(\"lm\")\n\n# Decision tree\ntree_reg_spec <- decision_tree() %>%\n  set_engine(\"rpart\") %>%\n  set_mode(\"regression\")\n\n# Random forest\nrf_reg_spec <- rand_forest() %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"regression\")\n\n# Support vector machine\nsvm_reg_spec <- svm_rbf() %>%\n  set_engine(\"kernlab\") %>%\n  set_mode(\"regression\")\n\n# Neural network\nnn_reg_spec <- mlp() %>%\n  set_engine(\"nnet\") %>%\n  set_mode(\"regression\")\n\n# Show info for linear regression\nparsnip::show_model_info(\"linear_reg\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nInformation for `linear_reg`\n modes: unknown, regression, quantile regression \n\n engines: \n   quantile regression: quantreg1\n   regression:          brulee, glm1, glmnet1, keras, lm1, spark1, stan1\n\n1The model can use case weights.\n\n arguments: \n   glmnet: \n      penalty --> lambda\n      mixture --> alpha\n   spark:  \n      penalty --> reg_param\n      mixture --> elastic_net_param\n   keras:  \n      penalty --> penalty\n   brulee: \n      penalty --> penalty\n      mixture --> mixture\n\n fit modules:\n     engine                mode\n         lm          regression\n        glm          regression\n     glmnet          regression\n       stan          regression\n      spark          regression\n      keras          regression\n     brulee          regression\n   quantreg quantile regression\n\n prediction modules:\n                  mode   engine                          methods\n   quantile regression quantreg                         quantile\n            regression   brulee                          numeric\n            regression      glm           conf_int, numeric, raw\n            regression   glmnet                     numeric, raw\n            regression    keras                          numeric\n            regression       lm conf_int, numeric, pred_int, raw\n            regression    spark                          numeric\n            regression     stan conf_int, numeric, pred_int, raw\n```\n\n\n:::\n:::\n\n\n### Classification Models\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create classification data\names_class <- ames %>%\n  mutate(expensive = factor(if_else(Sale_Price > median(Sale_Price), \n                                    \"yes\", \"no\"))) %>%\n  select(expensive, Gr_Liv_Area, Year_Built, Overall_Cond) %>%\n  slice_sample(n = 500)\n\n# Logistic regression\nlogistic_spec <- logistic_reg() %>%\n  set_engine(\"glm\")\n\n# Random forest for classification\nrf_class_spec <- rand_forest() %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"classification\")\n\n# Support vector machine for classification\nsvm_class_spec <- svm_rbf() %>%\n  set_engine(\"kernlab\") %>%\n  set_mode(\"classification\")\n\n# K-nearest neighbors\nknn_spec <- nearest_neighbor() %>%\n  set_engine(\"kknn\") %>%\n  set_mode(\"classification\")\n\n# Naive Bayes\nnb_spec <- naive_Bayes() %>%\n  set_engine(\"naivebayes\") %>%\n  set_mode(\"classification\")\n```\n:::\n\n\n## Understanding Engines\n\nEach model type can have multiple engines (implementations). The choice of engine affects:\n- Available hyperparameters\n- Computational efficiency\n- Additional features\n- Required packages\n\n### Exploring Available Engines\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# See all engines for a model type\nshow_engines(\"rand_forest\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 x 2\n  engine       mode          \n  <chr>        <chr>         \n1 ranger       classification\n2 ranger       regression    \n3 randomForest classification\n4 randomForest regression    \n5 spark        classification\n6 spark        regression    \n```\n\n\n:::\n\n```{.r .cell-code}\n# Different engines for linear regression\nlinear_engines <- show_engines(\"linear_reg\")\nprint(linear_engines)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8 x 2\n  engine   mode               \n  <chr>    <chr>              \n1 lm       regression         \n2 glm      regression         \n3 glmnet   regression         \n4 stan     regression         \n5 spark    regression         \n6 keras    regression         \n7 brulee   regression         \n8 quantreg quantile regression\n```\n\n\n:::\n\n```{.r .cell-code}\n# Let's compare different engines for the same model\nengines_to_compare <- c(\"lm\", \"glm\", \"glmnet\")\n\nlinear_comparisons <- map(engines_to_compare, function(eng) {\n  # Special handling for glmnet\n  if (eng == \"glmnet\") {\n    spec <- linear_reg(penalty = 0) %>% set_engine(eng)  # No regularization\n  } else {\n    spec <- linear_reg() %>% set_engine(eng)\n  }\n  \n  # Fit the model\n  fit <- spec %>% fit(Sale_Price ~ Gr_Liv_Area + Overall_Cond, data = ames_small)\n  \n  # Get predictions\n  preds <- predict(fit, ames_small)\n  \n  # Return summary\n  tibble(\n    engine = eng,\n    rmse = sqrt(mean((ames_small$Sale_Price - preds$.pred)^2))\n  )\n})\n\nbind_rows(linear_comparisons) %>%\n  knitr::kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n|engine |     rmse|\n|:------|--------:|\n|lm     | 53749.53|\n|glm    | 53749.53|\n|glmnet | 53751.01|\n\n\n:::\n:::\n\n\n### Engine-Specific Arguments\n\nDifferent engines support different arguments:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Random forest with ranger engine\nrf_ranger <- rand_forest(trees = 500) %>%\n  set_engine(\"ranger\",\n             importance = \"impurity\",  # ranger-specific\n             num.threads = 2)          # ranger-specific\n\n# Random forest with randomForest engine\nrf_randomForest <- rand_forest(trees = 500) %>%\n  set_engine(\"randomForest\",\n             nodesize = 5,             # randomForest-specific\n             maxnodes = 100)           # randomForest-specific\n\n# The model specification is the same, but engine arguments differ\nprint(rf_ranger)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRandom Forest Model Specification (unknown mode)\n\nMain Arguments:\n  trees = 500\n\nEngine-Specific Arguments:\n  importance = impurity\n  num.threads = 2\n\nComputational engine: ranger \n```\n\n\n:::\n\n```{.r .cell-code}\nprint(rf_randomForest)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRandom Forest Model Specification (unknown mode)\n\nMain Arguments:\n  trees = 500\n\nEngine-Specific Arguments:\n  nodesize = 5\n  maxnodes = 100\n\nComputational engine: randomForest \n```\n\n\n:::\n:::\n\n\n## Model Arguments and Hyperparameters\n\nParsnip distinguishes between:\n- **Main arguments**: Common across engines (e.g., `trees` for random forests)\n- **Engine arguments**: Specific to an implementation\n\n### Main Arguments\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Main arguments are specified in the model function\nrf_with_args <- rand_forest(\n  trees = 1000,      # Number of trees\n  mtry = 3,          # Variables per split\n  min_n = 10         # Minimum node size\n) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"regression\")\n\nprint(rf_with_args)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 3\n  trees = 1000\n  min_n = 10\n\nComputational engine: ranger \n```\n\n\n:::\n\n```{.r .cell-code}\n# These translate to engine-specific names\ntranslate(rf_with_args)  # See the actual ranger call\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 3\n  trees = 1000\n  min_n = 10\n\nComputational engine: ranger \n\nModel fit template:\nranger::ranger(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    mtry = min_cols(~3, x), num.trees = 1000, min.node.size = min_rows(~10, \n        x), num.threads = 1, verbose = FALSE, seed = sample.int(10^5, \n        1))\n```\n\n\n:::\n:::\n\n\n### Updating Model Specifications\n\nModel specifications can be updated dynamically:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Start with a basic specification\nbase_rf <- rand_forest() %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"regression\")\n\n# Update with new values\nupdated_rf <- base_rf %>%\n  set_args(trees = 2000, mtry = 5)\n\nprint(updated_rf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 5\n  trees = 2000\n\nComputational engine: ranger \n```\n\n\n:::\n\n```{.r .cell-code}\n# This is useful for tuning\ntunable_rf <- rand_forest(\n  trees = tune(),    # Mark for tuning\n  mtry = tune(),\n  min_n = tune()\n) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"regression\")\n\nprint(tunable_rf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  trees = tune()\n  min_n = tune()\n\nComputational engine: ranger \n```\n\n\n:::\n:::\n\n\n## Consistent Prediction Interface\n\nOne of parsnip's greatest strengths is consistent predictions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit different models\nmodels <- list(\n  linear = linear_reg() %>% \n    set_engine(\"lm\") %>%\n    fit(Sale_Price ~ Gr_Liv_Area + Overall_Cond, data = ames_small),\n  \n  tree = decision_tree() %>%\n    set_engine(\"rpart\") %>%\n    set_mode(\"regression\") %>%\n    fit(Sale_Price ~ Gr_Liv_Area + Overall_Cond, data = ames_small),\n  \n  knn = nearest_neighbor(neighbors = 5) %>%\n    set_engine(\"kknn\") %>%\n    set_mode(\"regression\") %>%\n    fit(Sale_Price ~ Gr_Liv_Area + Overall_Cond, data = ames_small)\n)\n\n# All predictions have the same format\npredictions <- map(models, ~ predict(., ames_small))\n\n# Check structure - all identical!\nmap(predictions, str)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntibble [500 x 1] (S3: tbl_df/tbl/data.frame)\n $ .pred: num [1:500] 190481 194350 211597 241778 121628 ...\ntibble [500 x 1] (S3: tbl_df/tbl/data.frame)\n $ .pred: num [1:500] 212497 212497 212497 212497 118782 ...\ntibble [500 x 1] (S3: tbl_df/tbl/data.frame)\n $ .pred: num [1:500] 232004 183235 186429 243400 129300 ...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$linear\nNULL\n\n$tree\nNULL\n\n$knn\nNULL\n```\n\n\n:::\n\n```{.r .cell-code}\n# For classification, we can get probabilities consistently\nclass_model <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  fit(expensive ~ Gr_Liv_Area + Overall_Cond, data = ames_class)\n\n# Class predictions\nclass_preds <- predict(class_model, ames_class)\nhead(class_preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 x 1\n  .pred_class\n  <fct>      \n1 no         \n2 yes        \n3 yes        \n4 yes        \n5 no         \n6 yes        \n```\n\n\n:::\n\n```{.r .cell-code}\n# Probability predictions\nprob_preds <- predict(class_model, ames_class, type = \"prob\")\nhead(prob_preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 x 2\n  .pred_no .pred_yes\n     <dbl>     <dbl>\n1   0.734      0.266\n2   0.465      0.535\n3   0.0965     0.904\n4   0.0685     0.932\n5   0.859      0.141\n6   0.118      0.882\n```\n\n\n:::\n:::\n\n\n## Advanced Model Specifications\n\n### Regularized Regression\n\nRegularized models require special handling for the penalty parameter:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Ridge regression\nridge_spec <- linear_reg(penalty = 0.1, mixture = 0) %>%\n  set_engine(\"glmnet\")\n\n# Lasso regression  \nlasso_spec <- linear_reg(penalty = 0.1, mixture = 1) %>%\n  set_engine(\"glmnet\")\n\n# Elastic net\nelastic_spec <- linear_reg(penalty = 0.1, mixture = 0.5) %>%\n  set_engine(\"glmnet\")\n\n# Fit and compare\nregularized_models <- list(\n  ridge = ridge_spec,\n  lasso = lasso_spec,\n  elastic = elastic_spec\n) %>%\n  map(~ fit(., Sale_Price ~ ., data = ames_small))\n\n# Extract coefficients\ncoef_comparison <- map_df(names(regularized_models), function(model_name) {\n  coefs <- regularized_models[[model_name]] %>%\n    tidy() %>%\n    filter(term != \"(Intercept)\") %>%\n    mutate(model = model_name)\n})\n\n# Visualize coefficient shrinkage\nggplot(coef_comparison, aes(x = term, y = estimate, fill = model)) +\n  geom_col(position = \"dodge\") +\n  coord_flip() +\n  labs(\n    title = \"Coefficient Comparison: Ridge vs Lasso vs Elastic Net\",\n    subtitle = \"Notice how Lasso sets some coefficients to exactly zero\",\n    x = \"Variable\", y = \"Coefficient\"\n  ) +\n  theme(axis.text.y = element_text(size = 8))\n```\n\n::: {.cell-output-display}\n![](11-model-specification_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n### Boosted Trees\n\nGradient boosting has many hyperparameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# XGBoost specification\nxgb_spec <- boost_tree(\n  trees = 1000,           # Number of trees\n  tree_depth = 6,         # Maximum tree depth\n  min_n = 10,             # Minimum node size\n  loss_reduction = 0.01,  # Minimum loss reduction\n  sample_size = 0.8,      # Subsample ratio\n  learn_rate = 0.01       # Learning rate\n) %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"regression\")\n\nprint(xgb_spec)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n  min_n = 10\n  tree_depth = 6\n  learn_rate = 0.01\n  loss_reduction = 0.01\n  sample_size = 0.8\n\nComputational engine: xgboost \n```\n\n\n:::\n\n```{.r .cell-code}\n# Fit the model\nxgb_fit <- xgb_spec %>%\n  fit(Sale_Price ~ ., data = ames_small)\n\n# Feature importance\nxgb_importance <- xgb_fit %>%\n  extract_fit_engine() %>%\n  xgboost::xgb.importance(model = .) %>%\n  as_tibble()\n\nggplot(xgb_importance %>% head(10), \n       aes(x = reorder(Feature, Gain), y = Gain)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"XGBoost Feature Importance\",\n    subtitle = \"Top 10 most important features\",\n    x = \"Feature\", y = \"Importance (Gain)\"\n  )\n```\n\n::: {.cell-output-display}\n![](11-model-specification_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n## Model Comparison Framework\n\nParsnip makes it easy to compare different models systematically:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define multiple model specifications\nmodel_specs <- list(\n  linear = linear_reg() %>% \n    set_engine(\"lm\"),\n  \n  ridge = linear_reg(penalty = 0.1, mixture = 0) %>%\n    set_engine(\"glmnet\"),\n  \n  tree = decision_tree(tree_depth = 10) %>%\n    set_engine(\"rpart\") %>%\n    set_mode(\"regression\"),\n  \n  rf = rand_forest(trees = 100) %>%\n    set_engine(\"ranger\") %>%\n    set_mode(\"regression\"),\n  \n  knn = nearest_neighbor(neighbors = 10) %>%\n    set_engine(\"kknn\") %>%\n    set_mode(\"regression\")\n)\n\n# Fit all models\nfitted_models <- map(model_specs, ~ fit(., Sale_Price ~ ., data = ames_small))\n\n# Evaluate all models\nmodel_evaluation <- map_df(names(fitted_models), function(model_name) {\n  model <- fitted_models[[model_name]]\n  \n  # Get predictions\n  preds <- predict(model, ames_small)$.pred\n  \n  # Calculate metrics\n  tibble(\n    model = model_name,\n    rmse = sqrt(mean((ames_small$Sale_Price - preds)^2)),\n    mae = mean(abs(ames_small$Sale_Price - preds)),\n    r_squared = cor(ames_small$Sale_Price, preds)^2\n  )\n})\n\n# Visualize comparison\nmodel_evaluation %>%\n  pivot_longer(cols = c(rmse, mae, r_squared), \n               names_to = \"metric\", values_to = \"value\") %>%\n  ggplot(aes(x = model, y = value, fill = model)) +\n  geom_col() +\n  facet_wrap(~metric, scales = \"free_y\") +\n  labs(\n    title = \"Model Performance Comparison\",\n    subtitle = \"Different metrics across model types\",\n    x = \"Model\", y = \"Value\"\n  ) +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](11-model-specification_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n## Creating Model Specifications Programmatically\n\nSometimes we need to create model specifications dynamically:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to create model specs with different hyperparameters\ncreate_rf_spec <- function(n_trees, mtry_prop, min_node) {\n  rand_forest(\n    trees = n_trees,\n    mtry = floor(mtry_prop * ncol(ames_small) - 1),  # Convert to integer\n    min_n = min_node\n  ) %>%\n    set_engine(\"ranger\") %>%\n    set_mode(\"regression\")\n}\n\n# Create a grid of specifications\nrf_grid <- expand_grid(\n  n_trees = c(100, 500, 1000),\n  mtry_prop = c(0.3, 0.5, 0.7),\n  min_node = c(5, 10, 20)\n)\n\n# Create specifications\nrf_specs <- pmap(rf_grid, create_rf_spec)\n\n# Fit a subset and compare\nsubset_specs <- rf_specs[c(1, 14, 27)]  # Low, medium, high complexity\nsubset_fits <- map(subset_specs, ~ fit(., Sale_Price ~ ., data = ames_small))\n\n# Evaluate\nsubset_evaluation <- map_df(1:3, function(i) {\n  preds <- predict(subset_fits[[i]], ames_small)$.pred\n  tibble(\n    config = c(\"Low\", \"Medium\", \"High\")[i],\n    trees = rf_grid$n_trees[c(1, 14, 27)][i],\n    mtry_prop = rf_grid$mtry_prop[c(1, 14, 27)][i],\n    min_node = rf_grid$min_node[c(1, 14, 27)][i],\n    rmse = sqrt(mean((ames_small$Sale_Price - preds)^2))\n  )\n})\n\nknitr::kable(subset_evaluation, digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n|config | trees| mtry_prop| min_node|     rmse|\n|:------|-----:|---------:|--------:|--------:|\n|Low    |   100|       0.3|        5| 20494.26|\n|Medium |   500|       0.5|       10| 33261.85|\n|High   |  1000|       0.7|       20| 30141.50|\n\n\n:::\n:::\n\n\n## Understanding Model Translations\n\nParsnip translates your specifications to the underlying engine calls:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# See how parsnip translates to different engines\nrf_spec <- rand_forest(trees = 500, mtry = 5, min_n = 10) %>%\n  set_mode(\"regression\")\n\n# Translation for ranger\nrf_spec %>%\n  set_engine(\"ranger\") %>%\n  translate()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 5\n  trees = 500\n  min_n = 10\n\nComputational engine: ranger \n\nModel fit template:\nranger::ranger(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n    mtry = min_cols(~5, x), num.trees = 500, min.node.size = min_rows(~10, \n        x), num.threads = 1, verbose = FALSE, seed = sample.int(10^5, \n        1))\n```\n\n\n:::\n\n```{.r .cell-code}\n# Translation for randomForest\nrf_spec %>%\n  set_engine(\"randomForest\") %>%\n  translate()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 5\n  trees = 500\n  min_n = 10\n\nComputational engine: randomForest \n\nModel fit template:\nrandomForest::randomForest(x = missing_arg(), y = missing_arg(), \n    mtry = min_cols(~5, x), ntree = 500, nodesize = min_rows(~10, \n        x))\n```\n\n\n:::\n\n```{.r .cell-code}\n# The arguments are mapped appropriately!\n# trees -> num.trees (ranger) or ntree (randomForest)\n# mtry -> mtry (both)\n# min_n -> min.node.size (ranger) or nodesize (randomForest)\n```\n:::\n\n\n## Model Specification Best Practices\n\n### 1. Start Simple, Add Complexity\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Start with default values\nsimple_rf <- rand_forest() %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"regression\")\n\n# Add complexity as needed\ncomplex_rf <- rand_forest(\n  trees = 1000,\n  mtry = tune(),\n  min_n = tune()\n) %>%\n  set_engine(\"ranger\",\n             importance = \"impurity\",\n             num.threads = parallel::detectCores() - 1) %>%\n  set_mode(\"regression\")\n```\n:::\n\n\n### 2. Use Consistent Naming\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a naming convention for your specifications\nmodels <- list(\n  # Baseline models\n  baseline_mean = null_model() %>% \n    set_engine(\"parsnip\") %>%\n    set_mode(\"regression\"),\n  \n  baseline_linear = linear_reg() %>%\n    set_engine(\"lm\"),\n  \n  # Regularized models\n  reg_ridge = linear_reg(penalty = 0.1, mixture = 0) %>%\n    set_engine(\"glmnet\"),\n  \n  reg_lasso = linear_reg(penalty = 0.1, mixture = 1) %>%\n    set_engine(\"glmnet\"),\n  \n  # Tree models\n  tree_single = decision_tree() %>%\n    set_engine(\"rpart\") %>%\n    set_mode(\"regression\"),\n  \n  tree_rf = rand_forest() %>%\n    set_engine(\"ranger\") %>%\n    set_mode(\"regression\"),\n  \n  tree_boost = boost_tree() %>%\n    set_engine(\"xgboost\") %>%\n    set_mode(\"regression\")\n)\n```\n:::\n\n\n### 3. Document Your Choices\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Document why you chose specific values\nproduction_rf <- rand_forest(\n  trees = 500,        # Balanced accuracy vs training time\n  mtry = 5,          # sqrt(p) rule for regression\n  min_n = 20         # Prevent overfitting on small samples\n) %>%\n  set_engine(\"ranger\",\n             importance = \"impurity\",  # Need feature importance\n             seed = 123,               # Reproducibility\n             num.threads = 4) %>%      # Server has 8 cores, use half\n  set_mode(\"regression\")\n```\n:::\n\n\n## Troubleshooting Common Issues\n\n### Issue 1: Missing Required Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check if required package is installed\ncheck_model_package <- function(model_spec) {\n  required_pkg <- model_spec$engine\n  \n  if (!requireNamespace(required_pkg, quietly = TRUE)) {\n    cat(\"Package\", required_pkg, \"is not installed.\\n\")\n    cat(\"Install with: install.packages('\", required_pkg, \"')\\n\", sep = \"\")\n    return(FALSE)\n  }\n  \n  cat(\"Package\", required_pkg, \"is available.\\n\")\n  return(TRUE)\n}\n\n# Test\nsvm_spec <- svm_rbf() %>% set_engine(\"kernlab\")\ncheck_model_package(svm_spec)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPackage kernlab is available.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n### Issue 2: Incompatible Mode/Engine Combinations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Not all combinations work\ntryCatch({\n  bad_spec <- linear_reg() %>%\n    set_engine(\"rpart\")  # Decision tree engine for linear regression?\n}, error = function(e) {\n  cat(\"Error:\", e$message, \"\\n\")\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nError: Engine \"rpart\" is not supported for `linear_reg()` \n```\n\n\n:::\n\n```{.r .cell-code}\n# Check valid combinations\nshow_engines(\"linear_reg\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8 x 2\n  engine   mode               \n  <chr>    <chr>              \n1 lm       regression         \n2 glm      regression         \n3 glmnet   regression         \n4 stan     regression         \n5 spark    regression         \n6 keras    regression         \n7 brulee   regression         \n8 quantreg quantile regression\n```\n\n\n:::\n:::\n\n\n### Issue 3: Missing Arguments\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Some engines require specific arguments\ntryCatch({\n  bad_glmnet <- linear_reg() %>%\n    set_engine(\"glmnet\")  # Missing penalty!\n  \n  fit(bad_glmnet, Sale_Price ~ ., data = ames_small)\n}, error = function(e) {\n  cat(\"Error: glmnet requires penalty argument\\n\")\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nError: glmnet requires penalty argument\n```\n\n\n:::\n\n```{.r .cell-code}\n# Correct specification\ngood_glmnet <- linear_reg(penalty = 0.1) %>%\n  set_engine(\"glmnet\")\n```\n:::\n\n\n## Exercises\n\n### Exercise 1: Multi-Engine Comparison\n\nCompare the same model type across different engines:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Compare random forest implementations\nengines <- c(\"ranger\", \"randomForest\")\n\nrf_comparison <- map_df(engines, function(eng) {\n  # Skip if package not available\n  if (!requireNamespace(eng, quietly = TRUE)) {\n    return(tibble(engine = eng, status = \"Package not installed\"))\n  }\n  \n  # Create specification\n  spec <- rand_forest(trees = 100) %>%\n    set_engine(eng) %>%\n    set_mode(\"regression\")\n  \n  # Time the fitting\n  start_time <- Sys.time()\n  fit <- spec %>% fit(Sale_Price ~ ., data = ames_small)\n  fit_time <- as.numeric(Sys.time() - start_time, units = \"secs\")\n  \n  # Get predictions\n  preds <- predict(fit, ames_small)$.pred\n  \n  # Return metrics\n  tibble(\n    engine = eng,\n    fit_time = fit_time,\n    rmse = sqrt(mean((ames_small$Sale_Price - preds)^2)),\n    status = \"Success\"\n  )\n})\n\nknitr::kable(rf_comparison, digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n|engine       | fit_time|     rmse|status                |\n|:------------|--------:|--------:|:---------------------|\n|ranger       |     0.02| 20461.06|Success               |\n|randomForest |       NA|       NA|Package not installed |\n\n\n:::\n:::\n\n\n### Exercise 2: Custom Model Grid\n\nCreate a grid of model specifications for comparison:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Create diverse model specifications\nmodel_grid <- tribble(\n  ~name, ~type, ~engine, ~hyperparams,\n  \"linear_basic\", \"linear_reg\", \"lm\", list(),\n  \"linear_ridge\", \"linear_reg\", \"glmnet\", list(penalty = 0.01, mixture = 0),\n  \"tree_shallow\", \"decision_tree\", \"rpart\", list(tree_depth = 5),\n  \"tree_deep\", \"decision_tree\", \"rpart\", list(tree_depth = 15),\n  \"rf_small\", \"rand_forest\", \"ranger\", list(trees = 50),\n  \"rf_large\", \"rand_forest\", \"ranger\", list(trees = 500),\n  \"knn_few\", \"nearest_neighbor\", \"kknn\", list(neighbors = 3),\n  \"knn_many\", \"nearest_neighbor\", \"kknn\", list(neighbors = 20)\n)\n\n# Function to create spec from grid row\ncreate_spec <- function(type, engine, hyperparams) {\n  # Get base specification\n  spec <- switch(type,\n    linear_reg = linear_reg(),\n    decision_tree = decision_tree(),\n    rand_forest = rand_forest(),\n    nearest_neighbor = nearest_neighbor()\n  )\n  \n  # Add hyperparameters\n  if (length(hyperparams) > 0) {\n    spec <- do.call(set_args, c(list(spec), hyperparams))\n  }\n  \n  # Set engine and mode\n  spec %>%\n    set_engine(engine) %>%\n    set_mode(\"regression\")\n}\n\n# Create all specifications\nall_specs <- pmap(model_grid %>% select(-name), create_spec)\nnames(all_specs) <- model_grid$name\n\n# Fit and evaluate a few\nsample_models <- all_specs[c(\"linear_basic\", \"tree_deep\", \"rf_large\")]\nsample_fits <- map(sample_models, ~ fit(., Sale_Price ~ ., data = ames_small))\n\n# Quick evaluation\nmap_dbl(sample_fits, function(fit) {\n  preds <- predict(fit, ames_small)$.pred\n  sqrt(mean((ames_small$Sale_Price - preds)^2))\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nlinear_basic    tree_deep     rf_large \n    37041.70     40841.91     20588.47 \n```\n\n\n:::\n:::\n\n\n### Exercise 3: Engine-Specific Features\n\nExplore engine-specific features for the same model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Random forest with different engine features\n\n# Ranger with importance\nrf_ranger_imp <- rand_forest(trees = 100) %>%\n  set_engine(\"ranger\", \n             importance = \"permutation\",\n             keep.inbag = TRUE) %>%  # For prediction intervals\n  set_mode(\"regression\") %>%\n  fit(Sale_Price ~ ., data = ames_small)\n\n# Extract ranger-specific features\nranger_importance <- rf_ranger_imp %>%\n  extract_fit_engine() %>%\n  .$variable.importance %>%\n  sort(decreasing = TRUE) %>%\n  head(10)\n\nprint(\"Ranger Variable Importance:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Ranger Variable Importance:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(ranger_importance)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Gr_Liv_Area   Year_Built Neighborhood Overall_Cond \n  4515460893   3479724041    689627617    215089951 \n```\n\n\n:::\n\n```{.r .cell-code}\n# RandomForest with proximity\nif (requireNamespace(\"randomForest\", quietly = TRUE)) {\n  rf_rf_prox <- rand_forest(trees = 100) %>%\n    set_engine(\"randomForest\",\n               proximity = TRUE,  # Calculate proximity matrix\n               keep.forest = TRUE) %>%\n    set_mode(\"regression\") %>%\n    fit(Sale_Price ~ ., data = ames_small)\n  \n  # The proximity matrix shows similarity between observations\n  # This is useful for clustering and outlier detection\n  cat(\"\\nrandomForest can provide proximity matrix for clustering\\n\")\n}\n```\n:::\n\n\n## Summary\n\nIn this comprehensive chapter, you've mastered:\n\n✅ **Core parsnip concepts**\n  - Unified model interface philosophy\n  - Separation of specification and fitting\n  - Consistent prediction interface\n\n✅ **Model specifications**\n  - Different model types\n  - Setting engines and modes\n  - Main vs engine arguments\n\n✅ **Advanced techniques**\n  - Multi-engine comparisons\n  - Programmatic model creation\n  - Model translations\n\n✅ **Best practices**\n  - Starting simple and adding complexity\n  - Consistent naming conventions\n  - Proper documentation\n\nKey takeaways:\n- Parsnip provides consistency across diverse models\n- Same interface whether using lm or xgboost\n- Easy to swap and compare different approaches\n- Engine flexibility with common interface\n- Predictable, tidy outputs\n\n## What's Next?\n\nIn [Chapter 12](12-workflows-evaluation.Rmd), we'll learn about workflows that combine models with preprocessing and evaluation metrics.\n\n## Additional Resources\n\n- [parsnip Documentation](https://parsnip.tidymodels.org/)\n- [List of Available Models](https://www.tidymodels.org/find/parsnip/)\n- [Adding New Models to parsnip](https://www.tidymodels.org/learn/develop/models/)\n- [Tidy Modeling with R - Models Chapter](https://www.tmwr.org/models.html)\n",
    "supporting": [
      "11-model-specification_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}