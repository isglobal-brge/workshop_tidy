{
  "hash": "fe902b738cb444455e9dc93beae1ee7a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 16: Ensemble Methods - The Power of Many\"\ndate: today\nformat:\n  html:\n    code-fold: false\n    code-tools: true\n---\n\n## Learning Objectives\n\nBy the end of this chapter, you will master:\n\n- The theory behind ensemble methods\n- Bagging and random forests\n- Boosting algorithms (AdaBoost, GBM, XGBoost)\n- Stacking and blending models\n- Voting classifiers\n- Model diversity and ensemble selection\n- Practical implementation with tidymodels\n- When and why ensembles work\n\n## The Wisdom of Crowds in Machine Learning\n\nImagine you're trying to guess the number of jellybeans in a jar. One person might overestimate, another might underestimate, but the average of many guesses often comes remarkably close to the truth. This is the fundamental principle behind ensemble methods: combining multiple models can produce predictions that are more accurate than any individual model.\n\nThe mathematics behind this are elegant. If we have multiple independent models with uncorrelated errors, combining them reduces the overall error. This is why ensemble methods consistently win machine learning competitions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Attaching packages -------------------------------------- tidymodels 1.4.1 --\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nv broom        1.0.10     v recipes      1.3.1 \nv dials        1.4.2      v rsample      1.3.1 \nv dplyr        1.1.4      v tailor       0.1.0 \nv ggplot2      4.0.0      v tidyr        1.3.1 \nv infer        1.0.9      v tune         2.0.0 \nv modeldata    1.5.1      v workflows    1.3.0 \nv parsnip      1.3.3      v workflowsets 1.1.1 \nv purrr        1.1.0      v yardstick    1.3.2 \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx purrr::discard() masks scales::discard()\nx dplyr::filter()  masks stats::filter()\nx dplyr::lag()     masks stats::lag()\nx recipes::step()  masks stats::step()\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv forcats   1.0.0     v stringr   1.5.2\nv lubridate 1.9.4     v tibble    3.3.0\nv readr     2.1.5     \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx readr::col_factor() masks scales::col_factor()\nx purrr::discard()    masks scales::discard()\nx dplyr::filter()     masks stats::filter()\nx stringr::fixed()    masks recipes::fixed()\nx dplyr::lag()        masks stats::lag()\nx readr::spec()       masks yardstick::spec()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(modeldata)\nlibrary(vip)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAdjuntando el paquete: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(patchwork)\nlibrary(stacks)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRegistered S3 method overwritten by 'butcher':\n  method                 from    \n  as.character.dev_topic generics\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(baguette)\nlibrary(rules)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAdjuntando el paquete: 'rules'\n\nThe following object is masked from 'package:dials':\n\n    max_rules\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(ranger)\nlibrary(xgboost)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAdjuntando el paquete: 'xgboost'\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n```\n\n\n:::\n\n```{.r .cell-code}\n# Set theme and seed\ntheme_set(theme_minimal())\nset.seed(123)\n\n# Load and prepare data\ndata(ames)\names_split <- initial_split(ames, prop = 0.75, strata = Sale_Price)\names_train <- training(ames_split)\names_test <- testing(ames_split)\n\n# Create resamples for evaluation\names_folds <- vfold_cv(ames_train, v = 5, strata = Sale_Price)\n\n# Prepare a simpler dataset for visualization\names_simple <- ames_train %>%\n  select(Sale_Price, Gr_Liv_Area, Overall_Cond, Year_Built, Neighborhood) %>%\n  slice_sample(n = 500)\n```\n:::\n\n\n## Why Ensembles Work: The Mathematical Foundation\n\nLet's understand why combining models reduces error:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate the power of ensembles\nset.seed(456)\nn_models <- 50\nn_points <- 100\n\n# True function\nx <- seq(0, 10, length.out = n_points)\ny_true <- sin(x) + 0.5 * x\ny_observed <- y_true + rnorm(n_points, sd = 0.5)\n\n# Create multiple \"weak\" models (with random variations)\nweak_predictions <- map(1:n_models, function(i) {\n  # Each model sees slightly different data (bootstrap)\n  sample_idx <- sample(1:n_points, replace = TRUE)\n  \n  # Simple polynomial model with random degree\n  degree <- sample(2:5, 1)\n  model <- lm(y_observed[sample_idx] ~ poly(x[sample_idx], degree))\n  \n  # Predict on all points\n  predict(model, newdata = data.frame(x = x))\n})\n\n# Combine predictions\nensemble_pred <- reduce(weak_predictions, `+`) / n_models\n\n# Calculate errors\nindividual_errors <- map_dbl(weak_predictions, ~ mean((. - y_true)^2))\nensemble_error <- mean((ensemble_pred - y_true)^2)\n\n# Visualize\nresults_df <- tibble(\n  x = x,\n  y_true = y_true,\n  y_observed = y_observed,\n  ensemble = ensemble_pred\n) %>%\n  bind_cols(\n    as_tibble(weak_predictions, .name_repair = \"minimal\") %>%\n      set_names(paste0(\"model_\", 1:n_models))\n  )\n\n# Plot individual models vs ensemble\np1 <- ggplot(results_df, aes(x = x)) +\n  geom_point(aes(y = y_observed), alpha = 0.3) +\n  geom_line(aes(y = y_true), color = \"black\", linewidth = 1.5) +\n  geom_line(aes(y = model_1), color = \"blue\", alpha = 0.5) +\n  geom_line(aes(y = model_2), color = \"blue\", alpha = 0.5) +\n  geom_line(aes(y = model_3), color = \"blue\", alpha = 0.5) +\n  labs(\n    title = \"Individual Weak Models\",\n    subtitle = \"Each model overfits in different ways\",\n    y = \"Value\"\n  )\n\np2 <- ggplot(results_df, aes(x = x)) +\n  geom_point(aes(y = y_observed), alpha = 0.3) +\n  geom_line(aes(y = y_true), color = \"black\", linewidth = 1.5) +\n  geom_line(aes(y = ensemble), color = \"red\", linewidth = 1.5) +\n  labs(\n    title = \"Ensemble Prediction\",\n    subtitle = paste(\"Ensemble MSE:\", round(ensemble_error, 3), \n                    \"vs Avg Individual:\", round(mean(individual_errors), 3)),\n    y = \"Value\"\n  )\n\np1 + p2\n```\n\n::: {.cell-output-display}\n![](16-ensemble-methods_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Error reduction\ncat(\"Average individual model MSE:\", mean(individual_errors), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAverage individual model MSE: 4.698849 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Ensemble MSE:\", ensemble_error, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEnsemble MSE: 2.540965 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Error reduction:\", round((1 - ensemble_error/mean(individual_errors)) * 100, 1), \"%\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nError reduction: 45.9 %\n```\n\n\n:::\n:::\n\n\nThe key insight: errors cancel out when models make different mistakes!\n\n## Bagging: Bootstrap Aggregating\n\nBagging creates diverse models by training on different bootstrap samples:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Implement bagging for decision trees\nbagging_spec <- bag_tree(\n  cost_complexity = 0,\n  tree_depth = 10,\n  min_n = 2\n) %>%\n  set_engine(\"rpart\", times = 25) %>%  # 25 bootstrap samples\n  set_mode(\"regression\")\n\n# Create a simple recipe\nsimple_recipe <- recipe(Sale_Price ~ ., data = ames_train) %>%\n  step_rm(Street, Utilities, Condition_2, Roof_Matl, Heating, Pool_QC,\n          Misc_Feature, Pool_Area, Longitude, Latitude) %>%\n  step_impute_median(all_numeric_predictors()) %>%\n  step_impute_mode(all_nominal_predictors()) %>%\n  step_dummy(all_nominal_predictors())\n\n# Create workflow\nbagging_workflow <- workflow() %>%\n  add_recipe(simple_recipe) %>%\n  add_model(bagging_spec)\n\n# Fit and evaluate\nbagging_fit <- bagging_workflow %>%\n  fit(ames_train)\n\n# Compare with single tree\nsingle_tree_spec <- decision_tree(\n  cost_complexity = 0,\n  tree_depth = 10,\n  min_n = 2\n) %>%\n  set_engine(\"rpart\") %>%\n  set_mode(\"regression\")\n\nsingle_tree_workflow <- workflow() %>%\n  add_recipe(simple_recipe) %>%\n  add_model(single_tree_spec)\n\nsingle_tree_fit <- single_tree_workflow %>%\n  fit(ames_train)\n\n# Evaluate both\ntest_predictions <- tibble(\n  actual = ames_test$Sale_Price,\n  single_tree = predict(single_tree_fit, ames_test)$.pred,\n  bagging = predict(bagging_fit, ames_test)$.pred\n)\n\ncomparison_metrics <- test_predictions %>%\n  summarise(\n    single_tree_rmse = sqrt(mean((actual - single_tree)^2)),\n    bagging_rmse = sqrt(mean((actual - bagging)^2)),\n    improvement = (single_tree_rmse - bagging_rmse) / single_tree_rmse * 100\n  )\n\nknitr::kable(comparison_metrics, digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n| single_tree_rmse| bagging_rmse| improvement|\n|----------------:|------------:|-----------:|\n|         35793.08|     26948.51|       24.71|\n\n\n:::\n\n```{.r .cell-code}\n# Visualize predictions\ntest_predictions %>%\n  pivot_longer(cols = c(single_tree, bagging), \n               names_to = \"model\", values_to = \"prediction\") %>%\n  ggplot(aes(x = actual, y = prediction)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +\n  facet_wrap(~model) +\n  labs(\n    title = \"Single Tree vs Bagged Trees\",\n    subtitle = \"Bagging reduces overfitting and improves predictions\",\n    x = \"Actual Sale Price\",\n    y = \"Predicted Sale Price\"\n  ) +\n  coord_equal()\n```\n\n::: {.cell-output-display}\n![](16-ensemble-methods_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nBagging characteristics:\n- **Reduces variance** without increasing bias\n- **Works best** with high-variance, low-bias models (like deep trees)\n- **Parallel training** possible (each bootstrap independent)\n- **Out-of-bag (OOB)** error provides free validation\n\n## Random Forests: Bagging with a Twist\n\nRandom forests add feature randomness to bagging:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare different mtry values\nrf_comparison <- tibble(\n  mtry_prop = c(0.1, 0.33, 0.5, 0.75, 1.0),\n  mtry_desc = c(\"10% features\", \"33% features (sqrt)\", \"50% features\", \n                \"75% features\", \"100% features (bagging)\")\n) %>%\n  mutate(\n    model = map(mtry_prop, function(prop) {\n      rand_forest(\n        trees = 100,\n        mtry = floor(prop * (ncol(ames_train) - 1)),\n        min_n = 5\n      ) %>%\n        set_engine(\"ranger\", importance = \"impurity\") %>%\n        set_mode(\"regression\")\n    })\n  )\n\n# Fit all models\nrf_workflows <- rf_comparison %>%\n  mutate(\n    workflow = map(model, function(m) {\n      workflow() %>%\n        add_recipe(simple_recipe) %>%\n        add_model(m)\n    })\n  )\n\n# Evaluate with cross-validation (simplified for speed)\nrf_results <- rf_workflows %>%\n  mutate(\n    cv_results = map(workflow, ~ fit_resamples(\n      .,\n      resamples = vfold_cv(ames_train, v = 3),  # Reduced folds for speed\n      metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq)\n    ))\n  )\n\n# Extract metrics\nrf_metrics <- rf_results %>%\n  mutate(\n    metrics = map(cv_results, collect_metrics)\n  ) %>%\n  unnest(metrics) %>%\n  select(mtry_desc, .metric, mean, std_err)\n\n# Visualize mtry effect\nrf_metrics %>%\n  filter(.metric == \"rmse\") %>%\n  ggplot(aes(x = mtry_desc, y = mean)) +\n  geom_col(fill = \"steelblue\") +\n  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), width = 0.2) +\n  labs(\n    title = \"Effect of Feature Randomness (mtry)\",\n    subtitle = \"Moderate mtry often performs best\",\n    x = \"Features Considered per Split\",\n    y = \"RMSE\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](16-ensemble-methods_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Feature importance from best model\nbest_rf <- rand_forest(\n  trees = 500,\n  mtry = floor(sqrt(ncol(ames_train) - 1)),\n  min_n = 5\n) %>%\n  set_engine(\"ranger\", importance = \"permutation\") %>%\n  set_mode(\"regression\")\n\nbest_rf_fit <- workflow() %>%\n  add_recipe(simple_recipe) %>%\n  add_model(best_rf) %>%\n  fit(ames_train)\n\n# Extract and plot importance\nbest_rf_fit %>%\n  extract_fit_parsnip() %>%\n  vip(num_features = 15) +\n  labs(title = \"Random Forest Feature Importance\")\n```\n\n::: {.cell-output-display}\n![](16-ensemble-methods_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n:::\n\n\nRandom forest advantages:\n- **Further reduces overfitting** compared to bagging\n- **Decorrelates trees** through feature sampling\n- **Provides feature importance** naturally\n- **Robust to hyperparameters** (often works well out-of-box)\n\n## Boosting: Learning from Mistakes\n\nBoosting sequentially builds models, each learning from previous errors:\n\n### AdaBoost: The Original Boosting Algorithm\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Demonstrate boosting concept with simple example\n# Create a difficult classification dataset\nset.seed(789)\nspiral_data <- tibble(\n  angle = runif(300, 0, 4 * pi),\n  radius = runif(300, 0.5, 2)\n) %>%\n  mutate(\n    x = radius * cos(angle) + rnorm(300, sd = 0.1),\n    y = radius * sin(angle) + rnorm(300, sd = 0.1),\n    class = factor(if_else(angle %% (2 * pi) < pi, \"A\", \"B\"))\n  )\n\n# Visualize the problem\nggplot(spiral_data, aes(x = x, y = y, color = class)) +\n  geom_point(size = 2) +\n  labs(\n    title = \"Spiral Classification Problem\",\n    subtitle = \"Difficult for single linear boundary\"\n  ) +\n  coord_equal()\n```\n\n::: {.cell-output-display}\n![](16-ensemble-methods_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Boosting builds sequential models\n# Each focuses on misclassified points from previous models\n```\n:::\n\n\n### Gradient Boosting Machines (GBM)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# XGBoost - state-of-the-art gradient boosting\nxgb_spec <- boost_tree(\n  trees = 100,\n  tree_depth = 4,\n  min_n = 10,\n  loss_reduction = 0.01,\n  sample_size = 0.8,\n  learn_rate = 0.1\n) %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"regression\")\n\nxgb_workflow <- workflow() %>%\n  add_recipe(simple_recipe) %>%\n  add_model(xgb_spec)\n\n# Fit the model\nxgb_fit <- xgb_workflow %>%\n  fit(ames_train)\n\n# Compare learning rates\nlearning_rates <- c(0.01, 0.05, 0.1, 0.3)\n\nlr_comparison <- map_df(learning_rates, function(lr) {\n  spec <- boost_tree(\n    trees = 100,\n    tree_depth = 4,\n    learn_rate = lr\n  ) %>%\n    set_engine(\"xgboost\") %>%\n    set_mode(\"regression\")\n  \n  wf <- workflow() %>%\n    add_recipe(simple_recipe) %>%\n    add_model(spec)\n  \n  # Fit and evaluate\n  fit <- wf %>% fit(ames_train)\n  \n  # Get training history (if available)\n  train_pred <- predict(fit, ames_train)\n  test_pred <- predict(fit, ames_test)\n  \n  tibble(\n    learn_rate = lr,\n    train_rmse = sqrt(mean((ames_train$Sale_Price - train_pred$.pred)^2)),\n    test_rmse = sqrt(mean((ames_test$Sale_Price - test_pred$.pred)^2))\n  )\n})\n\n# Visualize learning rate effect\nlr_comparison %>%\n  pivot_longer(cols = c(train_rmse, test_rmse), \n               names_to = \"set\", values_to = \"rmse\") %>%\n  ggplot(aes(x = factor(learn_rate), y = rmse, fill = set)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Learning Rate Effect in Gradient Boosting\",\n    subtitle = \"Lower rates often generalize better but need more trees\",\n    x = \"Learning Rate\",\n    y = \"RMSE\"\n  )\n```\n\n::: {.cell-output-display}\n![](16-ensemble-methods_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nBoosting characteristics:\n- **Sequential training** (can't parallelize easily)\n- **Focuses on difficult cases** progressively\n- **Can overfit** if not regularized properly\n- **Often achieves** best single-model performance\n\n## Model Stacking: The Meta-Learning Approach\n\nStacking uses a meta-model to combine predictions from base models:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple stacking example with manual blending\n# Create base models\nlm_spec <- linear_reg() %>%\n  set_engine(\"lm\")\n\nrf_spec <- rand_forest(trees = 200) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"regression\")\n\nxgb_spec <- boost_tree(trees = 100, tree_depth = 4) %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"regression\")\n\n# Fit models\nlm_fit <- workflow() %>%\n  add_recipe(simple_recipe) %>%\n  add_model(lm_spec) %>%\n  fit(ames_train)\n\nrf_fit <- workflow() %>%\n  add_recipe(simple_recipe) %>%\n  add_model(rf_spec) %>%\n  fit(ames_train)\n\nxgb_fit <- workflow() %>%\n  add_recipe(simple_recipe) %>%\n  add_model(xgb_spec) %>%\n  fit(ames_train)\n\n# Get predictions from each model\nlm_pred <- predict(lm_fit, ames_test)$.pred\nrf_pred <- predict(rf_fit, ames_test)$.pred\nxgb_pred <- predict(xgb_fit, ames_test)$.pred\n\n# Simple average ensemble\navg_pred <- (lm_pred + rf_pred + xgb_pred) / 3\n\n# Weighted average (weights could be tuned)\nweighted_pred <- 0.2 * lm_pred + 0.4 * rf_pred + 0.4 * xgb_pred\n\n# Calculate RMSE for each approach\nrmse_results <- tibble(\n  Model = c(\"Linear\", \"Random Forest\", \"XGBoost\", \"Simple Average\", \"Weighted Average\"),\n  RMSE = c(\n    sqrt(mean((ames_test$Sale_Price - lm_pred)^2)),\n    sqrt(mean((ames_test$Sale_Price - rf_pred)^2)),\n    sqrt(mean((ames_test$Sale_Price - xgb_pred)^2)),\n    sqrt(mean((ames_test$Sale_Price - avg_pred)^2)),\n    sqrt(mean((ames_test$Sale_Price - weighted_pred)^2))\n  )\n)\n\n# Display results\nknitr::kable(rmse_results, digits = 0)\n```\n\n::: {.cell-output-display}\n\n\n|Model            |  RMSE|\n|:----------------|-----:|\n|Linear           | 28724|\n|Random Forest    | 27934|\n|XGBoost          | 24228|\n|Simple Average   | 24186|\n|Weighted Average | 24076|\n\n\n:::\n\n```{.r .cell-code}\n# Visualize ensemble effect\nggplot(rmse_results, aes(x = reorder(Model, RMSE), y = RMSE, fill = Model)) +\n  geom_col() +\n  coord_flip() +\n  labs(\n    title = \"Model Stacking Performance\",\n    subtitle = \"Ensemble methods typically outperform individual models\",\n    x = NULL\n  ) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](16-ensemble-methods_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n## Voting Ensembles\n\nFor classification, voting combines predictions through majority vote or averaging:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create classification problem\names_class <- ames_train %>%\n  mutate(expensive = factor(if_else(Sale_Price > median(Sale_Price), \n                                    \"yes\", \"no\"))) %>%\n  select(-Sale_Price)\n\nclass_split <- initial_split(ames_class, strata = expensive)\nclass_train <- training(class_split)\nclass_test <- testing(class_split)\n\n# Create diverse classifiers\nlogistic_spec <- logistic_reg(penalty = 0.01, mixture = 0.5) %>%\n  set_engine(\"glmnet\")\n\ntree_spec <- decision_tree(tree_depth = 10) %>%\n  set_engine(\"rpart\") %>%\n  set_mode(\"classification\")\n\nrf_class_spec <- rand_forest(trees = 100) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"classification\")\n\n# Simple recipe for classification\nclass_recipe <- recipe(expensive ~ Gr_Liv_Area + Total_Bsmt_SF + Year_Built, \n                      data = class_train) %>%\n  step_normalize(all_numeric_predictors())\n\n# Fit individual models\nmodels <- list(\n  logistic = workflow() %>%\n    add_recipe(class_recipe) %>%\n    add_model(logistic_spec) %>%\n    fit(class_train),\n  \n  tree = workflow() %>%\n    add_recipe(class_recipe) %>%\n    add_model(tree_spec) %>%\n    fit(class_train),\n  \n  rf = workflow() %>%\n    add_recipe(class_recipe) %>%\n    add_model(rf_class_spec) %>%\n    fit(class_train)\n)\n\n# Get predictions from each model\npredictions <- map_dfc(models, function(model) {\n  predict(model, class_test, type = \"prob\") %>%\n    select(.pred_yes) %>%\n    pull()\n}) %>%\n  set_names(paste0(names(models), \"_prob_yes\"))\n\n# Hard voting (majority vote)\nhard_vote <- predictions %>%\n  mutate(\n    vote_yes = rowSums(. > 0.5),\n    prediction = factor(if_else(vote_yes >= 2, \"yes\", \"no\"))\n  )\n\n# Soft voting (average probabilities)\nsoft_vote <- predictions %>%\n  mutate(\n    avg_prob = rowMeans(.),\n    prediction = factor(if_else(avg_prob > 0.5, \"yes\", \"no\"))\n  )\n\n# Evaluate voting methods\nvoting_results <- tibble(\n  method = c(\"Hard Voting\", \"Soft Voting\"),\n  accuracy = c(\n    mean(hard_vote$prediction == class_test$expensive),\n    mean(soft_vote$prediction == class_test$expensive)\n  )\n)\n\n# Add individual model accuracies\nindividual_acc <- map_dbl(models, function(model) {\n  pred <- predict(model, class_test)\n  mean(pred$.pred_class == class_test$expensive)\n})\n\nall_results <- bind_rows(\n  tibble(method = names(individual_acc), accuracy = individual_acc),\n  voting_results\n)\n\n# Visualize\nggplot(all_results, aes(x = reorder(method, accuracy), y = accuracy)) +\n  geom_col(fill = c(rep(\"steelblue\", 3), rep(\"coral\", 2))) +\n  geom_text(aes(label = round(accuracy, 3)), vjust = -0.5) +\n  labs(\n    title = \"Voting Ensemble Performance\",\n    subtitle = \"Voting often outperforms individual models\",\n    x = \"Method\",\n    y = \"Accuracy\"\n  ) +\n  ylim(0, 1)\n```\n\n::: {.cell-output-display}\n![](16-ensemble-methods_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n## Diversity in Ensembles\n\nEnsemble success depends on model diversity:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Measure diversity through correlation\n# Get predictions from base models\nbase_predictions <- map_dfc(models, ~ predict(., class_test, type = \"prob\")$.pred_yes) %>%\n  set_names(names(models))\n\n# Calculate correlation matrix\ncor_matrix <- cor(base_predictions)\n\n# Visualize\ncorrplot::corrplot(cor_matrix, method = \"circle\", type = \"upper\",\n                   title = \"Model Prediction Correlations\")\n```\n\n::: {.cell-output-display}\n![](16-ensemble-methods_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Diversity metrics\ndiversity_metrics <- tibble(\n  metric = c(\"Average Pairwise Correlation\", \"Disagreement Rate\"),\n  value = c(\n    mean(cor_matrix[upper.tri(cor_matrix)]),\n    mean(apply(base_predictions > 0.5, 1, function(x) length(unique(x))) > 1)\n  )\n)\n\nknitr::kable(diversity_metrics, digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|metric                       | value|\n|:----------------------------|-----:|\n|Average Pairwise Correlation | 0.909|\n|Disagreement Rate            | 0.133|\n\n\n:::\n\n```{.r .cell-code}\n# Show how diversity affects ensemble performance\n# Create models with varying diversity\ndiverse_models <- list(\n  # Similar models (low diversity)\n  rf1 = rand_forest(trees = 100, mtry = 5) %>%\n    set_engine(\"ranger\") %>%\n    set_mode(\"classification\"),\n  \n  rf2 = rand_forest(trees = 100, mtry = 6) %>%\n    set_engine(\"ranger\") %>%\n    set_mode(\"classification\"),\n  \n  # Different model types (high diversity)\n  linear = logistic_reg() %>%\n    set_engine(\"glm\"),\n  \n  tree = decision_tree(tree_depth = 5) %>%\n    set_engine(\"rpart\") %>%\n    set_mode(\"classification\")\n)\n\n# Fit and evaluate\ndiverse_fits <- map(diverse_models, function(model) {\n  workflow() %>%\n    add_recipe(class_recipe) %>%\n    add_model(model) %>%\n    fit(class_train)\n})\n\n# Compare ensemble of similar vs diverse models\nsimilar_ensemble <- map_dfc(diverse_fits[1:2], \n                           ~ predict(., class_test, type = \"prob\")$.pred_yes) %>%\n  rowMeans()\n\ndiverse_ensemble <- map_dfc(diverse_fits[3:4], \n                           ~ predict(., class_test, type = \"prob\")$.pred_yes) %>%\n  rowMeans()\n\nensemble_comparison <- tibble(\n  ensemble_type = c(\"Similar Models\", \"Diverse Models\"),\n  accuracy = c(\n    mean((similar_ensemble > 0.5) == (class_test$expensive == \"yes\")),\n    mean((diverse_ensemble > 0.5) == (class_test$expensive == \"yes\"))\n  )\n)\n\nknitr::kable(ensemble_comparison, digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|ensemble_type  | accuracy|\n|:--------------|--------:|\n|Similar Models |    0.876|\n|Diverse Models |    0.858|\n\n\n:::\n:::\n\n\n## Advanced Ensemble Techniques\n\n### Dynamic Ensemble Selection\n\nChoose different models for different regions of the feature space:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Demonstrate region-based ensemble\n# Create a 2D problem for visualization\nset.seed(123)\nregion_data <- tibble(\n  x1 = runif(500, -2, 2),\n  x2 = runif(500, -2, 2)\n) %>%\n  mutate(\n    region = case_when(\n      x1 < 0 & x2 < 0 ~ \"A\",\n      x1 >= 0 & x2 < 0 ~ \"B\",\n      x1 < 0 & x2 >= 0 ~ \"C\",\n      TRUE ~ \"D\"\n    ),\n    y = case_when(\n      region == \"A\" ~ 2 * x1 + x2,        # Linear in region A\n      region == \"B\" ~ x1^2 + x2,          # Quadratic in region B\n      region == \"C\" ~ sin(2 * x1) + x2,   # Sinusoidal in region C\n      TRUE ~ exp(0.5 * x1) + x2           # Exponential in region D\n    ) + rnorm(500, sd = 0.3)\n  )\n\n# Visualize regions\nggplot(region_data, aes(x = x1, y = x2, color = y)) +\n  geom_point(size = 2) +\n  scale_color_viridis_c() +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(\n    title = \"Different Patterns in Different Regions\",\n    subtitle = \"Dynamic selection can use best model for each region\"\n  ) +\n  coord_equal()\n```\n\n::: {.cell-output-display}\n![](16-ensemble-methods_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n### Cascade Ensembles\n\nUse simple models for easy cases, complex models for hard cases:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Demonstrate cascade concept\n# First tier: Simple, fast model\nsimple_model <- linear_reg() %>%\n  set_engine(\"lm\")\n\nsimple_fit <- workflow() %>%\n  add_recipe(simple_recipe) %>%\n  add_model(simple_model) %>%\n  fit(ames_train)\n\n# Get predictions and residuals\nsimple_pred <- predict(simple_fit, ames_train)\ntrain_with_residuals <- ames_train %>%\n  mutate(\n    simple_pred = simple_pred$.pred,\n    residual = Sale_Price - simple_pred,\n    is_difficult = abs(residual) > quantile(abs(residual), 0.75)\n  )\n\n# Second tier: Complex model for difficult cases\ncomplex_model <- boost_tree(trees = 200, tree_depth = 6) %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"regression\")\n\n# Train on difficult cases\ndifficult_cases <- train_with_residuals %>%\n  filter(is_difficult)\n\ncomplex_recipe <- recipe(residual ~ Gr_Liv_Area + Year_Built + Total_Bsmt_SF, \n                        data = difficult_cases)\n\ncomplex_fit <- workflow() %>%\n  add_recipe(complex_recipe) %>%\n  add_model(complex_model) %>%\n  fit(difficult_cases)\n\ncat(\"Cascade ensemble:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCascade ensemble:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"- Simple model handles\", sum(!train_with_residuals$is_difficult), \"cases\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n- Simple model handles 1648 cases\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"- Complex model handles\", sum(train_with_residuals$is_difficult), \"difficult cases\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n- Complex model handles 549 difficult cases\n```\n\n\n:::\n:::\n\n\n## Best Practices for Ensembles\n\n### 1. Ensure Model Diversity\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Strategies for diversity\ndiversity_strategies <- tibble(\n  Strategy = c(\n    \"Different algorithms\",\n    \"Different hyperparameters\",\n    \"Different features\",\n    \"Different training samples\",\n    \"Different target transformations\"\n  ),\n  Example = c(\n    \"Linear + Tree + Neural Network\",\n    \"Shallow trees + Deep trees\",\n    \"Subset 1 features + Subset 2 features\",\n    \"Bootstrap + Stratified + Random\",\n    \"Log(y) + sqrt(y) + y\"\n  ),\n  Benefit = c(\n    \"Captures different patterns\",\n    \"Varies complexity\",\n    \"Different perspectives\",\n    \"Reduces correlation\",\n    \"Different error distributions\"\n  )\n)\n\nknitr::kable(diversity_strategies)\n```\n\n::: {.cell-output-display}\n\n\n|Strategy                         |Example                               |Benefit                       |\n|:--------------------------------|:-------------------------------------|:-----------------------------|\n|Different algorithms             |Linear + Tree + Neural Network        |Captures different patterns   |\n|Different hyperparameters        |Shallow trees + Deep trees            |Varies complexity             |\n|Different features               |Subset 1 features + Subset 2 features |Different perspectives        |\n|Different training samples       |Bootstrap + Stratified + Random       |Reduces correlation           |\n|Different target transformations |Log(y) + sqrt(y) + y                  |Different error distributions |\n\n\n:::\n:::\n\n\n### 2. Choose Appropriate Ensemble Method\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Decision guide\nensemble_guide <- tibble(\n  Scenario = c(\n    \"High-variance base models\",\n    \"Need interpretability\",\n    \"Limited computational budget\",\n    \"Maximizing performance\",\n    \"Imbalanced classes\",\n    \"Different expertise regions\"\n  ),\n  `Recommended Method` = c(\n    \"Bagging or Random Forest\",\n    \"Simple voting or linear stacking\",\n    \"Voting ensemble\",\n    \"Gradient boosting or stacking\",\n    \"Balanced bagging or cost-sensitive boosting\",\n    \"Dynamic selection\"\n  )\n)\n\nknitr::kable(ensemble_guide)\n```\n\n::: {.cell-output-display}\n\n\n|Scenario                     |Recommended Method                          |\n|:----------------------------|:-------------------------------------------|\n|High-variance base models    |Bagging or Random Forest                    |\n|Need interpretability        |Simple voting or linear stacking            |\n|Limited computational budget |Voting ensemble                             |\n|Maximizing performance       |Gradient boosting or stacking               |\n|Imbalanced classes           |Balanced bagging or cost-sensitive boosting |\n|Different expertise regions  |Dynamic selection                           |\n\n\n:::\n:::\n\n\n## Exercises\n\n### Exercise 1: Build a Custom Ensemble\n\nCreate your own ensemble combining different approaches:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Create a custom ensemble for Ames housing\ncustom_recipe <- recipe(Sale_Price ~ ., data = ames_train) %>%\n  step_rm(Street, Utilities, Condition_2, Roof_Matl, Heating, Pool_QC,\n          Misc_Feature, Pool_Area, Longitude, Latitude) %>%\n  step_impute_median(all_numeric_predictors()) %>%\n  step_normalize(all_numeric_predictors()) %>%\n  step_dummy(all_nominal_predictors())\n\n# Base models\nbase_models <- list(\n  # Regularized regression\n  elastic = linear_reg(penalty = 0.01, mixture = 0.5) %>%\n    set_engine(\"glmnet\"),\n  \n  # Tree-based\n  rf = rand_forest(trees = 200, mtry = 10, min_n = 5) %>%\n    set_engine(\"ranger\") %>%\n    set_mode(\"regression\"),\n  \n  # Boosting\n  xgb = boost_tree(trees = 100, tree_depth = 5, learn_rate = 0.1) %>%\n    set_engine(\"xgboost\") %>%\n    set_mode(\"regression\"),\n  \n  # Local model\n  knn = nearest_neighbor(neighbors = 15) %>%\n    set_engine(\"kknn\") %>%\n    set_mode(\"regression\")\n)\n\n# Fit all base models\nbase_fits <- map(base_models, function(model) {\n  workflow() %>%\n    add_recipe(custom_recipe) %>%\n    add_model(model) %>%\n    fit(ames_train)\n})\n\n# Get out-of-sample predictions using cross-validation\n# (In practice, use proper validation set)\nval_split <- initial_split(ames_train, prop = 0.8)\nval_train <- training(val_split)\nval_test <- testing(val_split)\n\n# Refit on validation training\nval_fits <- map(base_models, function(model) {\n  workflow() %>%\n    add_recipe(custom_recipe) %>%\n    add_model(model) %>%\n    fit(val_train)\n})\n\n# Get validation predictions for stacking\nval_predictions <- map_dfc(val_fits, ~ predict(., val_test)$.pred) %>%\n  set_names(names(base_models))\n\n# Train meta-learner\nmeta_data <- val_predictions %>%\n  mutate(target = val_test$Sale_Price)\n\nmeta_model <- lm(target ~ ., data = meta_data)\n\n# Function to make ensemble predictions\nensemble_predict <- function(new_data) {\n  # Get base predictions\n  base_preds <- map_dfc(base_fits, ~ predict(., new_data)$.pred) %>%\n    set_names(names(base_models))\n  \n  # Apply meta-model\n  predict(meta_model, base_preds)\n}\n\n# Evaluate custom ensemble\nensemble_pred <- ensemble_predict(ames_test)\nensemble_rmse <- sqrt(mean((ames_test$Sale_Price - ensemble_pred)^2))\n\n# Compare with individual models\nindividual_rmse <- map_dbl(base_fits, function(fit) {\n  pred <- predict(fit, ames_test)$.pred\n  sqrt(mean((ames_test$Sale_Price - pred)^2))\n})\n\nresults <- c(individual_rmse, ensemble = ensemble_rmse)\nresults <- sort(results)\n\nbarplot(results, main = \"Custom Ensemble Performance\",\n        ylab = \"RMSE\", col = c(rep(\"steelblue\", length(results)-1), \"coral\"))\n```\n\n::: {.cell-output-display}\n![](16-ensemble-methods_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n### Exercise 2: Optimize Ensemble Weights\n\nFind optimal weights for combining models:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Get predictions from each model\ntest_preds <- map_dfc(base_fits, ~ predict(., ames_test)$.pred) %>%\n  set_names(names(base_models))\n\n# Optimization function\noptimize_weights <- function(weights, predictions, target) {\n  # Normalize weights\n  weights <- weights / sum(weights)\n  \n  # Weighted average\n  ensemble_pred <- as.matrix(predictions) %*% weights\n  \n  # Return RMSE\n  sqrt(mean((target - ensemble_pred)^2))\n}\n\n# Initial equal weights\nn_models <- length(base_models)\ninitial_weights <- rep(1/n_models, n_models)\n\n# Optimize\nopt_result <- optim(\n  par = initial_weights,\n  fn = optimize_weights,\n  predictions = test_preds,\n  target = ames_test$Sale_Price,\n  method = \"L-BFGS-B\",\n  lower = rep(0, n_models),\n  upper = rep(1, n_models)\n)\n\n# Optimal weights\noptimal_weights <- opt_result$par / sum(opt_result$par)\nnames(optimal_weights) <- names(base_models)\n\n# Compare equal vs optimal weights\nequal_pred <- rowMeans(test_preds)\noptimal_pred <- as.matrix(test_preds) %*% optimal_weights\n\nweight_comparison <- tibble(\n  Method = c(\"Equal Weights\", \"Optimal Weights\"),\n  RMSE = c(\n    sqrt(mean((ames_test$Sale_Price - equal_pred)^2)),\n    sqrt(mean((ames_test$Sale_Price - optimal_pred)^2))\n  )\n)\n\nknitr::kable(weight_comparison, digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|Method          |     RMSE|\n|:---------------|--------:|\n|Equal Weights   | 26566.04|\n|Optimal Weights | 23026.81|\n\n\n:::\n\n```{.r .cell-code}\n# Show optimal weights\nbarplot(optimal_weights, main = \"Optimal Ensemble Weights\",\n        ylab = \"Weight\", col = \"steelblue\")\n```\n\n::: {.cell-output-display}\n![](16-ensemble-methods_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n### Exercise 3: Implement Boosting from Scratch\n\nUnderstand boosting by implementing a simple version:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Simple boosting implementation\nsimple_boosting <- function(x, y, n_iterations = 10, learning_rate = 0.1) {\n  n <- length(y)\n  \n  # Initialize with mean\n  predictions <- rep(mean(y), n)\n  models <- list()\n  \n  for (i in 1:n_iterations) {\n    # Calculate residuals\n    residuals <- y - predictions\n    \n    # Fit a simple model to residuals (using a decision stump)\n    # For simplicity, we'll use a linear model here\n    model_data <- data.frame(x = x, residual = residuals)\n    weak_model <- lm(residual ~ x, data = model_data)\n    \n    # Store model\n    models[[i]] <- weak_model\n    \n    # Update predictions\n    update <- predict(weak_model, model_data)\n    predictions <- predictions + learning_rate * update\n    \n    # Calculate current error\n    current_rmse <- sqrt(mean((y - predictions)^2))\n    \n    cat(\"Iteration\", i, \"- RMSE:\", current_rmse, \"\\n\")\n  }\n  \n  return(list(models = models, final_predictions = predictions,\n              learning_rate = learning_rate))\n}\n\n# Test on simple data\ntest_x <- ames_train$Gr_Liv_Area[1:100]\ntest_y <- ames_train$Sale_Price[1:100]\n\nboost_result <- simple_boosting(test_x, test_y, n_iterations = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIteration 1 - RMSE: 22607.51 \nIteration 2 - RMSE: 22598.13 \nIteration 3 - RMSE: 22590.53 \nIteration 4 - RMSE: 22584.37 \nIteration 5 - RMSE: 22579.38 \nIteration 6 - RMSE: 22575.34 \nIteration 7 - RMSE: 22572.07 \nIteration 8 - RMSE: 22569.41 \nIteration 9 - RMSE: 22567.27 \nIteration 10 - RMSE: 22565.52 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize boosting progress\nplot(test_x, test_y, main = \"Simple Boosting Results\",\n     xlab = \"Gr_Liv_Area\", ylab = \"Sale_Price\")\npoints(test_x, boost_result$final_predictions, col = \"red\", pch = 16)\nlegend(\"topleft\", legend = c(\"Actual\", \"Predicted\"), \n       col = c(\"black\", \"red\"), pch = c(1, 16))\n```\n\n::: {.cell-output-display}\n![](16-ensemble-methods_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n## Summary\n\nIn this comprehensive chapter, you've mastered:\n\n✅ **Ensemble fundamentals**\n  - Why ensembles work mathematically\n  - Bias-variance decomposition\n  - The importance of diversity\n\n✅ **Bagging methods**\n  - Bootstrap aggregating\n  - Random forests\n  - Out-of-bag error\n\n✅ **Boosting algorithms**\n  - Sequential learning\n  - AdaBoost and gradient boosting\n  - XGBoost implementation\n\n✅ **Stacking and blending**\n  - Meta-learning approaches\n  - Cross-validation for stacking\n  - Optimal weight finding\n\n✅ **Advanced techniques**\n  - Dynamic selection\n  - Cascade ensembles\n  - Custom ensemble design\n\nKey takeaways:\n- Ensembles almost always outperform single models\n- Diversity is crucial for ensemble success\n- Different methods suit different problems\n- Boosting for accuracy, bagging for stability\n- Stacking combines strengths of different approaches\n- Computational cost vs performance trade-off\n\n## What's Next?\n\nIn [Chapter 17](17-unsupervised-learning.Rmd), we'll explore unsupervised learning techniques for discovering patterns without labels.\n\n## Additional Resources\n\n- [Introduction to Statistical Learning - Chapter 8](https://www.statlearning.com/)\n- [Elements of Statistical Learning - Chapters 10, 15, 16](https://hastie.su.domains/ElemStatLearn/)\n- [XGBoost Documentation](https://xgboost.readthedocs.io/)\n- [Random Forests Original Paper](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)\n- [Ensemble Methods: Foundations and Algorithms](https://www.amazon.com/Ensemble-Methods-Foundations-Algorithms-Learning/dp/1439830037)\n",
    "supporting": [
      "16-ensemble-methods_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}