{
  "hash": "55c32eb7b6f467418e8c8d2f44379fdc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 14: Classification Models - Theory and Implementation\"\ndate: today\nformat:\n  html:\n    code-fold: false\n    code-tools: true\n---\n\n## Learning Objectives\n\nBy the end of this chapter, you will understand:\n\n- Classification theory and concepts\n- Binary vs multiclass classification\n- Logistic regression mathematics\n- Decision trees and random forests theory\n- Support vector machines (SVM) concepts\n- Evaluation metrics for classification\n- Class imbalance handling\n- Practical implementation with tidymodels\n\n## Classification Theory\n\n### What is Classification?\n\nClassification is a supervised learning task where we predict discrete class labels. Unlike regression (continuous outputs), classification assigns observations to categories.\n\n**Mathematical Framework:**\n\nGiven features $X = (x_1, x_2, ..., x_p)$ and classes $Y \\in \\{C_1, C_2, ..., C_k\\}$, we seek:\n\n$$P(Y = C_k | X)$$\n\nThe predicted class is:\n$$\\hat{y} = \\arg\\max_{k} P(Y = C_k | X)$$\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Attaching packages -------------------------------------- tidymodels 1.4.1 --\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nv broom        1.0.10     v recipes      1.3.1 \nv dials        1.4.2      v rsample      1.3.1 \nv dplyr        1.1.4      v tailor       0.1.0 \nv ggplot2      4.0.0      v tidyr        1.3.1 \nv infer        1.0.9      v tune         2.0.0 \nv modeldata    1.5.1      v workflows    1.3.0 \nv parsnip      1.3.3      v workflowsets 1.1.1 \nv purrr        1.1.0      v yardstick    1.3.2 \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx purrr::discard() masks scales::discard()\nx dplyr::filter()  masks stats::filter()\nx dplyr::lag()     masks stats::lag()\nx recipes::step()  masks stats::step()\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv forcats   1.0.0     v stringr   1.5.2\nv lubridate 1.9.4     v tibble    3.3.0\nv readr     2.1.5     \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx readr::col_factor() masks scales::col_factor()\nx purrr::discard()    masks scales::discard()\nx dplyr::filter()     masks stats::filter()\nx stringr::fixed()    masks recipes::fixed()\nx dplyr::lag()        masks stats::lag()\nx readr::spec()       masks yardstick::spec()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(modeldata)\nlibrary(vip)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAdjuntando el paquete: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(probably)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAdjuntando el paquete: 'probably'\n\nThe following objects are masked from 'package:base':\n\n    as.factor, as.ordered\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(discrim)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAdjuntando el paquete: 'discrim'\n\nThe following object is masked from 'package:dials':\n\n    smoothness\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(corrplot)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\ncorrplot 0.95 loaded\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(patchwork)\n\n# For reproducibility\nset.seed(123)\ntheme_set(theme_minimal())\n\n# Load datasets\ndata(credit_data)  # Credit default dataset\ndata(cells)        # Cell segmentation dataset\n```\n:::\n\n\n## Logistic Regression Theory\n\n### Binary Classification\n\nFor binary classification with classes 0 and 1, logistic regression models:\n\n$$P(Y = 1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + ... + \\beta_px_p)}}$$\n\nThis is the **logistic (sigmoid) function**:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize sigmoid function\nx <- seq(-10, 10, 0.1)\nsigmoid <- function(x) 1 / (1 + exp(-x))\n\nsigmoid_plot <- tibble(\n  x = x,\n  probability = sigmoid(x)\n) %>%\n  ggplot(aes(x = x, y = probability)) +\n  geom_line(linewidth = 1.5, color = \"darkblue\") +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  labs(\n    title = \"Logistic (Sigmoid) Function\",\n    subtitle = \"Maps linear combinations to probabilities [0,1]\",\n    x = \"Linear Combination (β₀ + β₁x₁ + ... + βₚxₚ)\",\n    y = \"P(Y = 1)\"\n  ) +\n  annotate(\"text\", x = 5, y = 0.25, label = \"Class 0 likely\", size = 5, color = \"blue\") +\n  annotate(\"text\", x = 5, y = 0.75, label = \"Class 1 likely\", size = 5, color = \"blue\")\n\n# Decision boundary illustration\nset.seed(123)\nbinary_data <- tibble(\n  x1 = c(rnorm(50, -1, 1), rnorm(50, 1, 1)),\n  x2 = c(rnorm(50, -1, 1), rnorm(50, 1, 1)),\n  class = factor(rep(c(\"A\", \"B\"), each = 50))\n)\n\nboundary_plot <- ggplot(binary_data, aes(x = x1, y = x2, color = class)) +\n  geom_point(size = 3, alpha = 0.7) +\n  stat_smooth(method = \"glm\", method.args = list(family = \"binomial\"), \n              se = FALSE, color = \"black\", linewidth = 1) +\n  scale_color_manual(values = c(\"A\" = \"red\", \"B\" = \"blue\")) +\n  labs(\n    title = \"Logistic Regression Decision Boundary\",\n    subtitle = \"Linear boundary in feature space\",\n    x = \"Feature 1\",\n    y = \"Feature 2\"\n  )\n\nsigmoid_plot + boundary_plot\n```\n\n::: {.cell-output-display}\n![](14-classification_files/figure-html/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n### Odds and Log-Odds\n\nThe **odds** of an event:\n$$\\text{Odds} = \\frac{P(Y = 1)}{P(Y = 0)} = \\frac{p}{1-p}$$\n\nThe **log-odds (logit)**:\n$$\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1x_1 + ... + \\beta_px_p$$\n\n### Maximum Likelihood Estimation\n\nParameters are estimated by maximizing the likelihood:\n\n$$L(\\beta) = \\prod_{i=1}^{n} p_i^{y_i}(1-p_i)^{1-y_i}$$\n\nWhere $p_i = P(Y_i = 1 | X_i)$\n\n## Implementing Logistic Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Prepare credit data\ncredit_clean <- credit_data %>%\n  drop_na() %>%\n  mutate(Status = factor(Status))\n\n# Split data\ncredit_split <- initial_split(credit_clean, prop = 0.75, strata = Status)\ncredit_train <- training(credit_split)\ncredit_test <- testing(credit_split)\n\n# Create recipe\ncredit_recipe <- recipe(Status ~ ., data = credit_train) %>%\n  step_normalize(all_numeric_predictors()) %>%\n  step_dummy(all_nominal_predictors())\n\n# Logistic regression specification\nlogistic_spec <- logistic_reg() %>%\n  set_engine(\"glm\") %>%\n  set_mode(\"classification\")\n\n# Workflow\nlogistic_wf <- workflow() %>%\n  add_recipe(credit_recipe) %>%\n  add_model(logistic_spec)\n\n# Fit model\nlogistic_fit <- logistic_wf %>%\n  fit(credit_train)\n\n# Extract coefficients\nlogistic_coefs <- logistic_fit %>%\n  extract_fit_parsnip() %>%\n  tidy() %>%\n  filter(term != \"(Intercept)\") %>%\n  arrange(desc(abs(estimate)))\n\n# Visualize coefficients\nggplot(logistic_coefs %>% head(10), \n       aes(x = reorder(term, estimate), y = estimate)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Logistic Regression Coefficients\",\n    subtitle = \"Top 10 most influential features\",\n    x = \"Feature\",\n    y = \"Coefficient (log-odds)\"\n  )\n```\n\n::: {.cell-output-display}\n![](14-classification_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n## Decision Trees Theory\n\n### How Decision Trees Work\n\nDecision trees recursively partition the feature space using binary splits.\n\n**Splitting Criteria:**\n\nFor classification, common criteria include:\n\n1. **Gini Impurity:**\n$$G = \\sum_{k=1}^{K} p_k(1 - p_k)$$\n\n2. **Entropy (Information Gain):**\n$$H = -\\sum_{k=1}^{K} p_k \\log_2(p_k)$$\n\nWhere $p_k$ is the proportion of samples in class $k$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize decision tree concepts\n# Create sample data for visualization\ntree_data <- tibble(\n  x1 = runif(200, 0, 10),\n  x2 = runif(200, 0, 10),\n  class = factor(case_when(\n    x1 < 3 & x2 < 5 ~ \"A\",\n    x1 < 3 & x2 >= 5 ~ \"B\",\n    x1 >= 3 & x1 < 7 ~ \"C\",\n    TRUE ~ \"D\"\n  ))\n)\n\n# Decision tree boundaries\ntree_boundary_plot <- ggplot(tree_data, aes(x = x1, y = x2, color = class)) +\n  geom_point(size = 2, alpha = 0.6) +\n  geom_vline(xintercept = c(3, 7), linetype = \"dashed\", linewidth = 1) +\n  geom_hline(yintercept = 5, linetype = \"dashed\", linewidth = 1) +\n  scale_color_viridis_d() +\n  labs(\n    title = \"Decision Tree Partitioning\",\n    subtitle = \"Recursive binary splits create rectangular regions\",\n    x = \"Feature 1\",\n    y = \"Feature 2\"\n  ) +\n  annotate(\"text\", x = 1.5, y = 2.5, label = \"Region A\", size = 4) +\n  annotate(\"text\", x = 1.5, y = 7.5, label = \"Region B\", size = 4) +\n  annotate(\"text\", x = 5, y = 5, label = \"Region C\", size = 4) +\n  annotate(\"text\", x = 8.5, y = 5, label = \"Region D\", size = 4)\n\n# Gini vs Entropy\np_range <- seq(0.01, 0.99, 0.01)\nimpurity_data <- tibble(\n  p = p_range,\n  Gini = 2 * p * (1 - p),\n  Entropy = -p * log2(p) - (1 - p) * log2(1 - p)\n) %>%\n  pivot_longer(cols = c(Gini, Entropy), names_to = \"Measure\", values_to = \"Impurity\")\n\nimpurity_plot <- ggplot(impurity_data, aes(x = p, y = Impurity, color = Measure)) +\n  geom_line(linewidth = 1.5) +\n  scale_color_manual(values = c(\"Gini\" = \"blue\", \"Entropy\" = \"red\")) +\n  labs(\n    title = \"Impurity Measures for Binary Classification\",\n    subtitle = \"Both measures peak at p = 0.5 (maximum uncertainty)\",\n    x = \"Proportion of Class 1\",\n    y = \"Impurity\"\n  )\n\ntree_boundary_plot + impurity_plot\n```\n\n::: {.cell-output-display}\n![](14-classification_files/figure-html/unnamed-chunk-4-1.png){width=1152}\n:::\n:::\n\n\n### Implementing Decision Trees\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Decision tree specification\ntree_spec <- decision_tree(\n  cost_complexity = 0.01,\n  tree_depth = 10,\n  min_n = 20\n) %>%\n  set_engine(\"rpart\") %>%\n  set_mode(\"classification\")\n\n# Workflow\ntree_wf <- workflow() %>%\n  add_recipe(credit_recipe) %>%\n  add_model(tree_spec)\n\n# Fit model\ntree_fit <- tree_wf %>%\n  fit(credit_train)\n\n# Visualize tree (if rpart.plot is available)\nif (require(rpart.plot, quietly = TRUE)) {\n  tree_fit %>%\n    extract_fit_engine() %>%\n    rpart.plot(roundint = FALSE, type = 4, extra = 101)\n}\n\n# Feature importance\ntree_imp <- tree_fit %>%\n  extract_fit_parsnip() %>%\n  vip(num_features = 10)\n\ntree_imp + labs(title = \"Decision Tree Feature Importance\")\n```\n\n::: {.cell-output-display}\n![](14-classification_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## Random Forests Theory\n\n### Ensemble Learning\n\nRandom Forests combine multiple decision trees through:\n\n1. **Bootstrap Aggregating (Bagging):** Train each tree on a bootstrap sample\n2. **Feature Randomness:** Consider random subset of features at each split\n3. **Voting:** Aggregate predictions (majority vote for classification)\n\n**Why it works:**\n- Reduces overfitting through averaging\n- Decorrelates trees through randomness\n- Maintains low bias while reducing variance\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Random Forest implementation\nrf_spec <- rand_forest(\n  trees = 500,\n  mtry = 3,\n  min_n = 10\n) %>%\n  set_engine(\"ranger\", importance = \"impurity\") %>%\n  set_mode(\"classification\")\n\nrf_wf <- workflow() %>%\n  add_recipe(credit_recipe) %>%\n  add_model(rf_spec)\n\n# Fit with cross-validation\nset.seed(123)\ncredit_folds <- vfold_cv(credit_train, v = 5, strata = Status)\n\nrf_cv <- rf_wf %>%\n  fit_resamples(\n    resamples = credit_folds,\n    metrics = metric_set(roc_auc, accuracy, precision, recall),\n    control = control_resamples(save_pred = TRUE)\n  )\n\n# Performance metrics\ncollect_metrics(rf_cv) %>%\n  knitr::kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|.metric   |.estimator |  mean|  n| std_err|.config         |\n|:---------|:----------|-----:|--:|-------:|:---------------|\n|accuracy  |binary     | 0.796|  5|   0.004|pre0_mod0_post0 |\n|precision |binary     | 0.693|  5|   0.018|pre0_mod0_post0 |\n|recall    |binary     | 0.354|  5|   0.019|pre0_mod0_post0 |\n|roc_auc   |binary     | 0.824|  5|   0.009|pre0_mod0_post0 |\n\n\n:::\n\n```{.r .cell-code}\n# ROC curve\nrf_roc <- rf_cv %>%\n  collect_predictions() %>%\n  roc_curve(Status, .pred_bad)\n\nautoplot(rf_roc) +\n  labs(title = \"Random Forest ROC Curve\",\n       subtitle = \"5-fold cross-validation results\")\n```\n\n::: {.cell-output-display}\n![](14-classification_files/figure-html/unnamed-chunk-6-1.png){width=1152}\n:::\n:::\n\n\n## Support Vector Machines Theory\n\n### Maximum Margin Classifier\n\nSVMs find the hyperplane that maximizes the margin between classes.\n\n**Linear SVM Optimization:**\n$$\\min_{w,b} \\frac{1}{2}||w||^2$$\nSubject to: $y_i(w^Tx_i + b) \\geq 1$ for all $i$\n\n### The Kernel Trick\n\nFor non-linear boundaries, SVMs use kernel functions to map data to higher dimensions:\n\n**Common Kernels:**\n1. **Linear:** $K(x_i, x_j) = x_i^T x_j$\n2. **Polynomial:** $K(x_i, x_j) = (x_i^T x_j + c)^d$\n3. **RBF (Gaussian):** $K(x_i, x_j) = \\exp(-\\gamma||x_i - x_j||^2)$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Demonstrate SVM with different kernels\n# Create non-linear data\nset.seed(123)\nspiral_data <- tibble(\n  angle = runif(200, 0, 4 * pi),\n  radius = runif(200, 0.5, 2),\n  class = factor(rep(c(\"A\", \"B\"), each = 100))\n) %>%\n  mutate(\n    x1 = radius * cos(angle) + ifelse(class == \"A\", 0, 0.5) + rnorm(200, 0, 0.2),\n    x2 = radius * sin(angle) + ifelse(class == \"A\", 0, 0.5) + rnorm(200, 0, 0.2)\n  )\n\n# Linear SVM\nsvm_linear_spec <- svm_linear(cost = 1) %>%\n  set_engine(\"kernlab\") %>%\n  set_mode(\"classification\")\n\n# RBF SVM\nsvm_rbf_spec <- svm_rbf(cost = 1, rbf_sigma = 0.1) %>%\n  set_engine(\"kernlab\") %>%\n  set_mode(\"classification\")\n\n# Fit both models\nsvm_linear_fit <- svm_linear_spec %>%\n  fit(class ~ x1 + x2, data = spiral_data)\n\nsvm_rbf_fit <- svm_rbf_spec %>%\n  fit(class ~ x1 + x2, data = spiral_data)\n\n# Create prediction grid\ngrid <- expand_grid(\n  x1 = seq(min(spiral_data$x1), max(spiral_data$x1), length.out = 100),\n  x2 = seq(min(spiral_data$x2), max(spiral_data$x2), length.out = 100)\n)\n\n# Get predictions\ngrid_linear <- grid %>%\n  bind_cols(predict(svm_linear_fit, grid, type = \"prob\"))\n\ngrid_rbf <- grid %>%\n  bind_cols(predict(svm_rbf_fit, grid, type = \"prob\"))\n\n# Visualize\np_linear <- ggplot() +\n  geom_tile(data = grid_linear, aes(x = x1, y = x2, fill = .pred_A), alpha = 0.3) +\n  geom_point(data = spiral_data, aes(x = x1, y = x2, color = class), size = 2) +\n  scale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\", midpoint = 0.5) +\n  scale_color_manual(values = c(\"A\" = \"red\", \"B\" = \"blue\")) +\n  labs(title = \"Linear SVM\", subtitle = \"Linear decision boundary\") +\n  theme(legend.position = \"none\")\n\np_rbf <- ggplot() +\n  geom_tile(data = grid_rbf, aes(x = x1, y = x2, fill = .pred_A), alpha = 0.3) +\n  geom_point(data = spiral_data, aes(x = x1, y = x2, color = class), size = 2) +\n  scale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\", midpoint = 0.5) +\n  scale_color_manual(values = c(\"A\" = \"red\", \"B\" = \"blue\")) +\n  labs(title = \"RBF SVM\", subtitle = \"Non-linear decision boundary\") +\n  theme(legend.position = \"none\")\n\np_linear + p_rbf\n```\n\n::: {.cell-output-display}\n![](14-classification_files/figure-html/unnamed-chunk-7-1.png){width=1152}\n:::\n:::\n\n\n## Classification Metrics\n\n### Understanding Different Metrics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit final model\nfinal_rf <- rf_wf %>%\n  fit(credit_train)\n\n# Get predictions\ncredit_pred <- final_rf %>%\n  predict(credit_test) %>%\n  bind_cols(\n    final_rf %>% predict(credit_test, type = \"prob\")\n  ) %>%\n  bind_cols(credit_test %>% select(Status))\n\n# Confusion matrix\nconf_mat <- credit_pred %>%\n  conf_mat(truth = Status, estimate = .pred_class)\n\nautoplot(conf_mat, type = \"heatmap\") +\n  scale_fill_gradient(low = \"white\", high = \"darkblue\") +\n  labs(title = \"Confusion Matrix\")\n```\n\n::: {.cell-output-display}\n![](14-classification_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Calculate various metrics\nmetrics_summary <- credit_pred %>%\n  metrics(truth = Status, estimate = .pred_class, .pred_bad) %>%\n  bind_rows(\n    credit_pred %>% roc_auc(truth = Status, .pred_bad),\n    credit_pred %>% pr_auc(truth = Status, .pred_bad)\n  )\n\nmetrics_summary %>%\n  knitr::kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|.metric     |.estimator | .estimate|\n|:-----------|:----------|---------:|\n|accuracy    |binary     |     0.817|\n|kap         |binary     |     0.441|\n|mn_log_loss |binary     |     0.416|\n|roc_auc     |binary     |     0.853|\n|roc_auc     |binary     |     0.853|\n|pr_auc      |binary     |     0.663|\n\n\n:::\n:::\n\n\n### Metrics Explained\n\n**Confusion Matrix Terms:**\n- **True Positives (TP):** Correctly predicted positive\n- **True Negatives (TN):** Correctly predicted negative\n- **False Positives (FP):** Type I error\n- **False Negatives (FN):** Type II error\n\n**Key Metrics:**\n\n$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n\n$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n\n$$\\text{Recall (Sensitivity)} = \\frac{TP}{TP + FN}$$\n\n$$\\text{Specificity} = \\frac{TN}{TN + FP}$$\n\n$$\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n\n## Handling Class Imbalance\n\n### The Problem\n\nClass imbalance occurs when one class is much more frequent than others.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create imbalanced dataset\nset.seed(123)\nimbalanced_data <- tibble(\n  x1 = c(rnorm(950, 0, 1), rnorm(50, 3, 1)),\n  x2 = c(rnorm(950, 0, 1), rnorm(50, 3, 1)),\n  class = factor(c(rep(\"Majority\", 950), rep(\"Minority\", 50)))\n)\n\n# Show class distribution\nclass_dist <- imbalanced_data %>%\n  count(class) %>%\n  mutate(prop = n / sum(n))\n\nggplot(class_dist, aes(x = class, y = n, fill = class)) +\n  geom_col() +\n  geom_text(aes(label = paste0(n, \"\\n(\", round(prop * 100, 1), \"%)\")), \n            vjust = -0.5, size = 5) +\n  scale_fill_manual(values = c(\"Majority\" = \"steelblue\", \"Minority\" = \"coral\")) +\n  labs(\n    title = \"Class Imbalance Example\",\n    subtitle = \"95% Majority vs 5% Minority\",\n    y = \"Count\"\n  ) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](14-classification_files/figure-html/unnamed-chunk-9-1.png){width=1152}\n:::\n:::\n\n\n### Solutions for Class Imbalance\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. Class weights\nweighted_spec <- logistic_reg(penalty = 0.01) %>%\n  set_engine(\"glmnet\", \n             weights = ifelse(imbalanced_data$class == \"Minority\", 19, 1)) %>%\n  set_mode(\"classification\")\n\n# 2. SMOTE (Synthetic Minority Over-sampling)\nlibrary(themis)  # For sampling methods\n\nimbalanced_recipe <- recipe(class ~ ., data = imbalanced_data) %>%\n  step_smote(class, over_ratio = 0.5) %>%  # Create synthetic minority samples\n  step_normalize(all_predictors())\n\n# 3. Downsampling\ndownsample_recipe <- recipe(class ~ ., data = imbalanced_data) %>%\n  step_downsample(class, under_ratio = 1) %>%\n  step_normalize(all_predictors())\n\n# Compare approaches\nsampling_comparison <- tibble(\n  Method = c(\"Original\", \"SMOTE\", \"Downsampling\"),\n  Recipe = list(\n    recipe(class ~ ., data = imbalanced_data),\n    imbalanced_recipe,\n    downsample_recipe\n  )\n) %>%\n  mutate(\n    prepped = map(Recipe, ~ prep(., training = imbalanced_data)),\n    baked = map(prepped, ~ bake(., new_data = NULL)),\n    class_counts = map(baked, ~ count(., class))\n  )\n\n# Show results\nsampling_comparison %>%\n  select(Method, class_counts) %>%\n  unnest(class_counts) %>%\n  ggplot(aes(x = Method, y = n, fill = class)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Majority\" = \"steelblue\", \"Minority\" = \"coral\")) +\n  labs(\n    title = \"Effect of Different Sampling Strategies\",\n    y = \"Count\"\n  )\n```\n\n::: {.cell-output-display}\n![](14-classification_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n## Multiclass Classification\n\n### One-vs-Rest and One-vs-One\n\nFor K classes:\n- **One-vs-Rest:** Train K binary classifiers\n- **One-vs-One:** Train K(K-1)/2 binary classifiers\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Multiclass example with penguins\npenguins_clean <- palmerpenguins::penguins %>%\n  drop_na()\n\n# Split data\npenguin_split <- initial_split(penguins_clean, prop = 0.75, strata = species)\npenguin_train <- training(penguin_split)\npenguin_test <- testing(penguin_split)\n\n# Recipe\npenguin_recipe <- recipe(species ~ ., data = penguin_train) %>%\n  step_rm(year) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_normalize(all_numeric_predictors())\n\n# Multinomial regression\nmultinom_spec <- multinom_reg() %>%\n  set_engine(\"nnet\") %>%\n  set_mode(\"classification\")\n\n# Workflow\nmultinom_wf <- workflow() %>%\n  add_recipe(penguin_recipe) %>%\n  add_model(multinom_spec)\n\n# Fit model\nmultinom_fit <- multinom_wf %>%\n  fit(penguin_train)\n\n# Predictions\npenguin_pred <- multinom_fit %>%\n  predict(penguin_test) %>%\n  bind_cols(\n    multinom_fit %>% predict(penguin_test, type = \"prob\")\n  ) %>%\n  bind_cols(penguin_test %>% select(species))\n\n# Multiclass confusion matrix\nmulticlass_conf <- penguin_pred %>%\n  conf_mat(truth = species, estimate = .pred_class)\n\nautoplot(multiclass_conf, type = \"heatmap\") +\n  scale_fill_gradient(low = \"white\", high = \"darkgreen\") +\n  labs(title = \"Multiclass Confusion Matrix\")\n```\n\n::: {.cell-output-display}\n![](14-classification_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Per-class metrics\npenguin_pred %>%\n  accuracy(truth = species, estimate = .pred_class) %>%\n  bind_rows(\n    penguin_pred %>% \n      roc_auc(truth = species, .pred_Adelie:.pred_Gentoo)\n  ) %>%\n  knitr::kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|.metric  |.estimator | .estimate|\n|:--------|:----------|---------:|\n|accuracy |multiclass |         1|\n|roc_auc  |hand_till  |         1|\n\n\n:::\n:::\n\n\n## Calibration and Probability Thresholds\n\n### Probability Calibration\n\nWell-calibrated models produce probabilities that match actual frequencies.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calibration plot\ncalibration_data <- credit_pred %>%\n  mutate(\n    prob_bin = cut(.pred_bad, breaks = seq(0, 1, 0.1), include.lowest = TRUE)\n  ) %>%\n  group_by(prob_bin) %>%\n  summarise(\n    mean_predicted = mean(.pred_bad),\n    fraction_positive = mean(Status == \"bad\"),\n    n = n()\n  ) %>%\n  drop_na()\n\ncalib_plot <- ggplot(calibration_data, aes(x = mean_predicted, y = fraction_positive)) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_point(aes(size = n), color = \"darkblue\") +\n  geom_line(color = \"darkblue\") +\n  scale_size_continuous(range = c(2, 8)) +\n  labs(\n    title = \"Calibration Plot\",\n    subtitle = \"Well-calibrated models follow the diagonal\",\n    x = \"Mean Predicted Probability\",\n    y = \"Fraction of Positives\",\n    size = \"Count\"\n  ) +\n  coord_equal()\n\n# Threshold optimization\nthresholds <- seq(0.1, 0.9, 0.05)\nthreshold_metrics <- map_df(thresholds, function(thresh) {\n  credit_pred %>%\n    mutate(.pred_class_adj = factor(ifelse(.pred_bad > thresh, \"bad\", \"good\"),\n                                    levels = c(\"bad\", \"good\"))) %>%\n    metrics(truth = Status, estimate = .pred_class_adj) %>%\n    mutate(threshold = thresh)\n})\n\nthresh_plot <- threshold_metrics %>%\n  filter(.metric %in% c(\"accuracy\", \"precision\", \"recall\")) %>%\n  ggplot(aes(x = threshold, y = .estimate, color = .metric)) +\n  geom_line(linewidth = 1.2) +\n  geom_point(size = 2) +\n  scale_color_viridis_d() +\n  labs(\n    title = \"Metrics vs Classification Threshold\",\n    subtitle = \"Trade-off between different metrics\",\n    x = \"Probability Threshold\",\n    y = \"Metric Value\",\n    color = \"Metric\"\n  )\n\ncalib_plot + thresh_plot\n```\n\n::: {.cell-output-display}\n![](14-classification_files/figure-html/unnamed-chunk-12-1.png){width=1152}\n:::\n:::\n\n\n## Model Comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare multiple models\nmodels <- list(\n  \"Logistic Regression\" = logistic_spec,\n  \"Decision Tree\" = tree_spec,\n  \"Random Forest\" = rf_spec,\n  \"SVM (RBF)\" = svm_rbf(cost = 1, rbf_sigma = 0.01) %>%\n    set_engine(\"kernlab\") %>%\n    set_mode(\"classification\")\n)\n\n# Fit all models\nmodel_fits <- map(models, function(spec) {\n  workflow() %>%\n    add_recipe(credit_recipe) %>%\n    add_model(spec) %>%\n    fit_resamples(\n      resamples = credit_folds,\n      metrics = metric_set(accuracy, roc_auc, precision, recall),\n      control = control_resamples(save_pred = TRUE)\n    )\n})\n\n# Collect metrics\nmodel_metrics <- map2_df(model_fits, names(model_fits), function(fit, name) {\n  collect_metrics(fit) %>%\n    mutate(model = name)\n})\n\n# Visualize comparison\nggplot(model_metrics, aes(x = model, y = mean, fill = model)) +\n  geom_col() +\n  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), width = 0.2) +\n  facet_wrap(~.metric, scales = \"free_y\") +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"Model Performance Comparison\",\n    subtitle = \"5-fold cross-validation results\",\n    y = \"Score\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](14-classification_files/figure-html/unnamed-chunk-13-1.png){width=1152}\n:::\n\n```{.r .cell-code}\n# ROC curves comparison\nroc_data <- map2_df(model_fits, names(model_fits), function(fit, name) {\n  collect_predictions(fit) %>%\n    roc_curve(Status, .pred_bad) %>%\n    mutate(model = name)\n})\n\nggplot(roc_data, aes(x = 1 - specificity, y = sensitivity, color = model)) +\n  geom_line(linewidth = 1.2) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  scale_color_viridis_d() +\n  labs(\n    title = \"ROC Curves Comparison\",\n    subtitle = \"All models performance\",\n    x = \"False Positive Rate (1 - Specificity)\",\n    y = \"True Positive Rate (Sensitivity)\"\n  ) +\n  coord_equal()\n```\n\n::: {.cell-output-display}\n![](14-classification_files/figure-html/unnamed-chunk-13-2.png){width=1152}\n:::\n:::\n\n\n## Exercises\n\n### Exercise 1: Implement Different Classifiers\n\nBuild and compare three different classifiers on the cells dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Prepare data\ncells_clean <- cells %>%\n  select(-case) %>%\n  drop_na()\n\ncells_split <- initial_split(cells_clean, prop = 0.75, strata = class)\ncells_train <- training(cells_split)\ncells_test <- testing(cells_split)\n\n# Recipe\ncells_recipe <- recipe(class ~ ., data = cells_train) %>%\n  step_normalize(all_numeric_predictors())\n\n# Models\nmodels_ex1 <- list(\n  knn = nearest_neighbor(neighbors = 5) %>%\n    set_engine(\"kknn\") %>%\n    set_mode(\"classification\"),\n  \n  lda = discrim_linear() %>%\n    set_engine(\"MASS\") %>%\n    set_mode(\"classification\"),\n  \n  nb = naive_Bayes() %>%\n    set_engine(\"naivebayes\") %>%\n    set_mode(\"classification\")\n)\n\n# Fit and evaluate\ncells_folds <- vfold_cv(cells_train, v = 5, strata = class)\n\nresults_ex1 <- map_df(names(models_ex1), function(model_name) {\n  wf <- workflow() %>%\n    add_recipe(cells_recipe) %>%\n    add_model(models_ex1[[model_name]])\n  \n  fit_resamples(\n    wf,\n    resamples = cells_folds,\n    metrics = metric_set(accuracy, roc_auc)\n  ) %>%\n    collect_metrics() %>%\n    mutate(model = model_name)\n})\n\n# Visualize results\nggplot(results_ex1, aes(x = model, y = mean, fill = .metric)) +\n  geom_col(position = \"dodge\") +\n  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err),\n                position = position_dodge(0.9), width = 0.2) +\n  scale_fill_viridis_d() +\n  labs(title = \"Model Comparison on Cells Dataset\",\n       y = \"Score\")\n```\n\n::: {.cell-output-display}\n![](14-classification_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n### Exercise 2: Handle Class Imbalance\n\nWork with an imbalanced dataset and apply different techniques:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Create severely imbalanced data\nset.seed(456)\nsevere_imbalanced <- tibble(\n  x1 = c(rnorm(980, 0, 1), rnorm(20, 2, 0.5)),\n  x2 = c(rnorm(980, 0, 1), rnorm(20, 2, 0.5)),\n  x3 = c(rnorm(980, 0, 1), rnorm(20, 2, 0.5)),\n  class = factor(c(rep(\"common\", 980), rep(\"rare\", 20)))\n)\n\n# Split data\nimb_split <- initial_split(severe_imbalanced, prop = 0.75, strata = class)\nimb_train <- training(imb_split)\nimb_test <- testing(imb_split)\n\n# Different recipes\nrecipe_baseline <- recipe(class ~ ., data = imb_train) %>%\n  step_normalize(all_predictors())\n\nrecipe_upsample <- recipe(class ~ ., data = imb_train) %>%\n  step_upsample(class, over_ratio = 1) %>%\n  step_normalize(all_predictors())\n\nrecipe_rose <- recipe(class ~ ., data = imb_train) %>%\n  step_rose(class) %>%\n  step_normalize(all_predictors())\n\n# Compare approaches\nrecipes_list <- list(\n  baseline = recipe_baseline,\n  upsample = recipe_upsample,\n  rose = recipe_rose\n)\n\n# Use same model for all\nrf_imb_spec <- rand_forest(trees = 100) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"classification\")\n\n# Evaluate each approach\nimb_results <- map_df(names(recipes_list), function(recipe_name) {\n  wf <- workflow() %>%\n    add_recipe(recipes_list[[recipe_name]]) %>%\n    add_model(rf_imb_spec)\n  \n  fit <- wf %>% fit(imb_train)\n  \n  pred <- fit %>%\n    predict(imb_test) %>%\n    bind_cols(imb_test %>% select(class))\n  \n  pred %>%\n    metrics(truth = class, estimate = .pred_class) %>%\n    mutate(method = recipe_name)\n})\n\n# Plot results\nggplot(imb_results, aes(x = method, y = .estimate, fill = .metric)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_viridis_d() +\n  labs(title = \"Handling Class Imbalance: Method Comparison\",\n       y = \"Score\") +\n  facet_wrap(~.metric, scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![](14-classification_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n### Exercise 3: Optimize Classification Threshold\n\nFind the optimal threshold for a specific business objective:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Business scenario: False negatives cost 5x more than false positives\ncost_fn <- 5  # Cost of false negative\ncost_fp <- 1  # Cost of false positive\n\n# Get probabilities\nfinal_rf_refit <- rf_wf %>% fit(credit_train)\nprob_preds <- final_rf_refit %>%\n  predict(credit_test, type = \"prob\") %>%\n  bind_cols(credit_test %>% select(Status))\n\n# Calculate cost for different thresholds\nthreshold_costs <- map_df(seq(0.1, 0.9, 0.01), function(thresh) {\n  preds <- prob_preds %>%\n    mutate(\n      pred_class = factor(ifelse(.pred_bad > thresh, \"bad\", \"good\"),\n                         levels = c(\"bad\", \"good\"))\n    )\n  \n  cm <- preds %>%\n    conf_mat(truth = Status, estimate = pred_class)\n  \n  # Extract values from confusion matrix\n  tn <- cm$table[1,1]\n  fp <- cm$table[1,2]\n  fn <- cm$table[2,1]\n  tp <- cm$table[2,2]\n  \n  tibble(\n    threshold = thresh,\n    total_cost = fp * cost_fp + fn * cost_fn,\n    accuracy = (tp + tn) / (tp + tn + fp + fn),\n    precision = tp / (tp + fp),\n    recall = tp / (tp + fn)\n  )\n})\n\n# Find optimal threshold\noptimal_thresh <- threshold_costs %>%\n  arrange(total_cost) %>%\n  slice(1)\n\n# Visualize\nggplot(threshold_costs, aes(x = threshold)) +\n  geom_line(aes(y = total_cost), color = \"red\", linewidth = 1.2) +\n  geom_line(aes(y = accuracy * max(total_cost)), color = \"blue\", linewidth = 1.2) +\n  geom_vline(xintercept = optimal_thresh$threshold, \n             linetype = \"dashed\", color = \"green\", linewidth = 1) +\n  scale_y_continuous(\n    name = \"Total Cost\",\n    sec.axis = sec_axis(~./max(threshold_costs$total_cost), name = \"Accuracy\")\n  ) +\n  labs(\n    title = \"Cost-Sensitive Threshold Optimization\",\n    subtitle = paste(\"Optimal threshold:\", round(optimal_thresh$threshold, 3)),\n    x = \"Classification Threshold\"\n  ) +\n  theme(axis.title.y.left = element_text(color = \"red\"),\n        axis.title.y.right = element_text(color = \"blue\"))\n```\n\n::: {.cell-output-display}\n![](14-classification_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n\n```{.r .cell-code}\nprint(paste(\"Optimal threshold:\", optimal_thresh$threshold))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Optimal threshold: 0.23\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Total cost at optimal threshold:\", optimal_thresh$total_cost))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Total cost at optimal threshold: 399\"\n```\n\n\n:::\n:::\n\n\n## Summary\n\nYou've mastered classification theory and practice:\n\n✅ Logistic regression mathematics and implementation  \n✅ Decision trees and splitting criteria  \n✅ Random forests and ensemble methods  \n✅ Support vector machines and kernel trick  \n✅ Classification metrics and their interpretation  \n✅ Handling class imbalance  \n✅ Multiclass classification strategies  \n✅ Probability calibration and threshold optimization  \n\n## What's Next?\n\nIn [Chapter 15](15-regression.Rmd), we'll explore regression models with similar depth in theory and practice.\n\n## Additional Resources\n\n- [The Elements of Statistical Learning](https://hastie.su.domains/ElemStatLearn/)\n- [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)\n- [Applied Predictive Modeling](http://appliedpredictivemodeling.com/)\n- [Classification and Regression Trees](https://www.taylorfrancis.com/books/mono/10.1201/9781315139470/classification-regression-trees-leo-breiman)\n",
    "supporting": [
      "14-classification_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}