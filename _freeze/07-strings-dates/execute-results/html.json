{
  "hash": "1538a426c94dabdab365b0c082493239",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 7: Working with Strings and Dates - Text and Time in the Tidyverse\"\ndate: today\nformat:\n  html:\n    code-fold: false\n    code-tools: true\n---\n\n## Learning Objectives\n\nBy the end of this chapter, you will master:\n\n- String manipulation with stringr\n- Pattern matching with regular expressions\n- Text cleaning and extraction\n- Date and time handling with lubridate\n- Time zone management\n- Date arithmetic and intervals\n- Combining text and temporal data in analyses\n- Real-world applications and best practices\n\n## Why Strings and Dates Matter\n\nIn real-world data analysis, you'll rarely work with perfectly clean numeric data. Most datasets contain:\n- **Text data**: Names, addresses, descriptions, categories, IDs\n- **Temporal data**: Timestamps, dates, durations, time zones\n\nThese data types are notoriously tricky to work with because:\n- Text can be messy, inconsistent, and encoded differently\n- Dates come in countless formats\n- Time zones add complexity\n- Both require special handling for analysis\n\nThe tidyverse provides powerful, consistent tools to tackle these challenges. Let's master them!\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.4     v readr     2.1.5\nv forcats   1.0.0     v stringr   1.5.2\nv ggplot2   4.0.0     v tibble    3.3.0\nv lubridate 1.9.4     v tidyr     1.3.1\nv purrr     1.1.0     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(lubridate)  # For dates and times\nlibrary(stringr)    # For string manipulation (loaded with tidyverse)\nlibrary(hms)        # For time-of-day values\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAdjuntando el paquete: 'hms'\n\nThe following object is masked from 'package:lubridate':\n\n    hms\n```\n\n\n:::\n\n```{.r .cell-code}\n# Set seed for reproducibility\nset.seed(123)\n\n# We'll create various example datasets throughout\ntheme_set(theme_minimal())\n```\n:::\n\n\n## Part 1: String Manipulation with stringr\n\n### Understanding Strings in R\n\nStrings (text data) are fundamental to data analysis, but they're often the messiest part of our data. The `stringr` package provides a consistent, intuitive interface for string manipulation.\n\nAll stringr functions:\n- Start with `str_` for easy autocomplete\n- Take the string as the first argument (pipe-friendly)\n- Use consistent naming conventions\n- Handle NA values gracefully\n\nLet's start with basic operations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create example strings\nmessy_names <- c(\n  \"  John Smith  \",\n  \"mary jones\",\n  \"ROBERT BROWN\",\n  \"Sarah O'Connor\",\n  \"José García\",\n  NA,\n  \"anne-marie wilson\"\n)\n\n# Basic string information\nstr_length(messy_names)  # Length of each string\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 14 10 12 14 25 NA 17\n```\n\n\n:::\n\n```{.r .cell-code}\nstr_count(messy_names, pattern = \" \")  # Count spaces\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  5  1  1  1  1 NA  1\n```\n\n\n:::\n\n```{.r .cell-code}\n# The power of vectorization - operations work on entire vectors\ndata.frame(\n  original = messy_names,\n  length = str_length(messy_names),\n  n_spaces = str_count(messy_names, \" \"),\n  n_vowels = str_count(messy_names, \"[aeiouAEIOU]\")\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   original length n_spaces n_vowels\n1              John Smith       14        5        2\n2                mary jones     10        1        3\n3              ROBERT BROWN     12        1        3\n4            Sarah O'Connor     14        1        5\n5 Jos<U+00E9> Garc<U+00ED>a     25        1        7\n6                      <NA>     NA       NA       NA\n7         anne-marie wilson     17        1        7\n```\n\n\n:::\n:::\n\n\nNotice how stringr handles NA values gracefully - operations on NA return NA rather than erroring out. This is crucial for real-world data processing.\n\n### String Transformation\n\nLet's clean up our messy names systematically:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Step-by-step cleaning\ncleaned_names <- messy_names %>%\n  str_trim() %>%                    # Remove leading/trailing whitespace\n  str_squish() %>%                  # Remove extra internal whitespace\n  str_to_title() %>%                # Proper case (Title Case)\n  str_replace_all(\"'\", \"'\")         # Standardize apostrophes\n\n# Compare before and after\ntibble(\n  original = messy_names,\n  cleaned = cleaned_names\n) %>%\n  print(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 x 2\n  original                    cleaned                  \n  <chr>                       <chr>                    \n1 \"  John Smith  \"            John Smith               \n2 \"mary jones\"                Mary Jones               \n3 \"ROBERT BROWN\"              Robert Brown             \n4 \"Sarah O'Connor\"            Sarah O'connor           \n5 \"Jos<U+00E9> Garc<U+00ED>a\" Jos<U+00e9> Garc<U+00ed>A\n6  <NA>                       <NA>                     \n7 \"anne-marie wilson\"         Anne-Marie Wilson        \n```\n\n\n:::\n\n```{.r .cell-code}\n# Other case transformations\ncase_examples <- \"The Quick BROWN Fox\"\ntibble(\n  original = case_examples,\n  lower = str_to_lower(case_examples),\n  upper = str_to_upper(case_examples),\n  title = str_to_title(case_examples),\n  sentence = str_to_sentence(case_examples)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 5\n  original            lower               upper               title     sentence\n  <chr>               <chr>               <chr>               <chr>     <chr>   \n1 The Quick BROWN Fox the quick brown fox THE QUICK BROWN FOX The Quic~ The qui~\n```\n\n\n:::\n:::\n\n\nEach transformation serves a specific purpose:\n- `str_trim()`: Removes accidental spaces from data entry\n- `str_squish()`: Fixes multiple spaces between words\n- `str_to_title()`: Standardizes capitalization for names\n- `str_replace_all()`: Fixes encoding issues with special characters\n\n### Pattern Matching and Regular Expressions\n\nRegular expressions (regex) are powerful pattern-matching tools. While they look cryptic at first, they're invaluable for text processing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample text data\nemails <- c(\n  \"john.doe@company.com\",\n  \"alice@university.edu\",\n  \"bob.smith@email.co.uk\",\n  \"invalid-email\",\n  \"another@domain.org\",\n  \"not an email at all\",\n  NA\n)\n\n# Detect patterns\nhas_at <- str_detect(emails, \"@\")  # Simple pattern\nhas_dot_com <- str_detect(emails, \"\\\\.com$\")  # .com at end ($ = end of string)\n\n# Extract patterns\ndomains <- str_extract(emails, \"@[^.]+\\\\.[a-z]+\")  # Extract domain\nusernames <- str_extract(emails, \"^[^@]+\")  # Everything before @\n\n# Create a summary\nemail_analysis <- tibble(\n  email = emails,\n  is_valid = str_detect(emails, \"^[^@]+@[^@]+\\\\.[^@]+$\"),\n  username = usernames,\n  domain = str_remove(domains, \"@\"),\n  tld = str_extract(emails, \"\\\\.[a-z]+$\")\n)\n\nprint(email_analysis)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 x 5\n  email                 is_valid username            domain         tld  \n  <chr>                 <lgl>    <chr>               <chr>          <chr>\n1 john.doe@company.com  TRUE     john.doe            company.com    .com \n2 alice@university.edu  TRUE     alice               university.edu .edu \n3 bob.smith@email.co.uk TRUE     bob.smith           email.co       .uk  \n4 invalid-email         FALSE    invalid-email       <NA>           <NA> \n5 another@domain.org    TRUE     another             domain.org     .org \n6 not an email at all   FALSE    not an email at all <NA>           <NA> \n7 <NA>                  NA       <NA>                <NA>           <NA> \n```\n\n\n:::\n:::\n\n\nLet's break down these regex patterns:\n- `@` - Literal @ symbol\n- `\\\\.` - Literal period (. alone means \"any character\")\n- `^` - Start of string\n- `$` - End of string\n- `[^@]+` - One or more characters that are NOT @\n- `[a-z]+` - One or more lowercase letters\n\n### Advanced Pattern Matching\n\nLet's work with more complex patterns:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Phone numbers in various formats\nphone_numbers <- c(\n  \"(555) 123-4567\",\n  \"555-123-4567\",\n  \"5551234567\",\n  \"555.123.4567\",\n  \"+1 555 123 4567\",\n  \"Call me at 555-1234\",\n  \"invalid\",\n  NA\n)\n\n# Extract digits only\ndigits_only <- str_remove_all(phone_numbers, \"[^0-9]\")\nprint(digits_only)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"5551234567\"  \"5551234567\"  \"5551234567\"  \"5551234567\"  \"15551234567\"\n[6] \"5551234\"     \"\"            NA           \n```\n\n\n:::\n\n```{.r .cell-code}\n# Standardize format\nstandardized <- phone_numbers %>%\n  str_remove_all(\"[^0-9]\") %>%  # Remove non-digits\n  str_replace(\"^1\", \"\") %>%      # Remove leading 1\n  str_replace(\"(\\\\d{3})(\\\\d{3})(\\\\d{4})\", \"(\\\\1) \\\\2-\\\\3\")  # Format\n\ntibble(\n  original = phone_numbers,\n  standardized = standardized\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8 x 2\n  original            standardized    \n  <chr>               <chr>           \n1 (555) 123-4567      \"(555) 123-4567\"\n2 555-123-4567        \"(555) 123-4567\"\n3 5551234567          \"(555) 123-4567\"\n4 555.123.4567        \"(555) 123-4567\"\n5 +1 555 123 4567     \"(555) 123-4567\"\n6 Call me at 555-1234 \"5551234\"       \n7 invalid             \"\"              \n8 <NA>                 <NA>           \n```\n\n\n:::\n\n```{.r .cell-code}\n# Extract structured information\naddresses <- c(\n  \"123 Main St, New York, NY 10001\",\n  \"456 Oak Avenue, Los Angeles, CA 90028\",\n  \"789 Elm Rd, Chicago, IL 60601\",\n  \"321 Pine Street, Houston, TX 77002\"\n)\n\n# Extract components using regex groups\naddress_parts <- str_match(addresses, \n  \"(\\\\d+) ([^,]+), ([^,]+), ([A-Z]{2}) (\\\\d{5})\")\n\ncolnames(address_parts) <- c(\"full\", \"number\", \"street\", \"city\", \"state\", \"zip\")\nas_tibble(address_parts) %>%\n  select(-full)  # Remove the full match column\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 x 5\n  number street      city        state zip  \n  <chr>  <chr>       <chr>       <chr> <chr>\n1 123    Main St     New York    NY    10001\n2 456    Oak Avenue  Los Angeles CA    90028\n3 789    Elm Rd      Chicago     IL    60601\n4 321    Pine Street Houston     TX    77002\n```\n\n\n:::\n:::\n\n\nThe power of regex groups (parentheses) is that they let us extract multiple pieces of information in one operation. Each group becomes a separate column in the output.\n\n### String Splitting and Combining\n\nReal data often requires splitting combined fields or joining separate ones:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Full names that need splitting\nfull_names <- c(\n  \"Smith, John\",\n  \"Jones, Mary Ann\",\n  \"Brown Jr., Robert\",\n  \"O'Connor, Sarah\",\n  \"García-López, José María\"\n)\n\n# Split on comma and clean up\nname_parts <- str_split_fixed(full_names, \", \", n = 2)\ncolnames(name_parts) <- c(\"last_name\", \"first_name\")\n\nname_df <- as_tibble(name_parts) %>%\n  mutate(\n    # Clean up any extra spaces\n    first_name = str_trim(first_name),\n    last_name = str_trim(last_name),\n    # Create display name\n    display_name = str_c(first_name, \" \", last_name),\n    # Create email-friendly version\n    email_name = str_c(\n      str_to_lower(str_extract(first_name, \"\\\\w+\")),  # First word only\n      \".\",\n      str_to_lower(str_replace_all(last_name, \"[^a-zA-Z]\", \"\")),  # Letters only\n      \"@company.com\"\n    )\n  )\n\nprint(name_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 4\n  last_name                  first_name               display_name    email_name\n  <chr>                      <chr>                    <chr>           <chr>     \n1 Smith                      John                     John Smith      john.smit~\n2 Jones                      Mary Ann                 Mary Ann Jones  mary.jone~\n3 Brown Jr.                  Robert                   Robert Brown J~ robert.br~\n4 O'Connor                   Sarah                    Sarah O'Connor  sarah.oco~\n5 Garc<U+00ED>a-L<U+00F3>pez Jos<U+00E9> Mar<U+00ED>a Jos<U+00E9> Ma~ jos.garcu~\n```\n\n\n:::\n\n```{.r .cell-code}\n# Combining multiple fields\nproduct_data <- tibble(\n  brand = c(\"Apple\", \"Samsung\", \"Google\"),\n  model = c(\"iPhone\", \"Galaxy\", \"Pixel\"),\n  version = c(\"14 Pro\", \"S23\", \"7a\"),\n  year = c(2022, 2023, 2023)\n)\n\nproduct_data %>%\n  mutate(\n    # Various ways to combine\n    full_name = str_c(brand, model, version, sep = \" \"),\n    sku = str_c(\n      str_to_upper(str_sub(brand, 1, 3)),  # First 3 letters of brand\n      str_extract(version, \"\\\\d+\"),         # Version number\n      year,\n      sep = \"-\"\n    ),\n    marketing_name = str_glue(\"{brand} {model} {version} ({year})\")\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 x 7\n  brand   model  version  year full_name           sku         marketing_name   \n  <chr>   <chr>  <chr>   <dbl> <chr>               <chr>       <glue>           \n1 Apple   iPhone 14 Pro   2022 Apple iPhone 14 Pro APP-14-2022 Apple iPhone 14 ~\n2 Samsung Galaxy S23      2023 Samsung Galaxy S23  SAM-23-2023 Samsung Galaxy S~\n3 Google  Pixel  7a       2023 Google Pixel 7a     GOO-7-2023  Google Pixel 7a ~\n```\n\n\n:::\n:::\n\n\nNotice the different joining functions:\n- `str_c()`: Basic concatenation with separator\n- `str_glue()`: Template-based joining (like Python f-strings)\n- `paste()` and `paste0()`: Base R alternatives (less consistent)\n\n## Part 2: Date and Time with lubridate\n\n### Understanding Temporal Data\n\nDates and times are deceptively complex:\n- Different formats (MM/DD/YYYY vs DD/MM/YYYY)\n- Time zones\n- Daylight saving time\n- Leap years and leap seconds\n- Different calendar systems\n\nLubridate makes working with dates intuitive and reliable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Today's date and current time\ntoday()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"2025-09-27\"\n```\n\n\n:::\n\n```{.r .cell-code}\nnow()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"2025-09-27 11:07:19 CEST\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Parse dates from strings - lubridate is smart!\ndate_strings <- c(\n  \"2023-01-15\",\n  \"01/15/2023\",\n  \"15-Jan-2023\",\n  \"January 15, 2023\",\n  \"20230115\"\n)\n\n# Different parsing functions for different formats\nparsed_dates <- tibble(\n  original = date_strings,\n  ymd = ymd(\"2023-01-15\"),\n  mdy = mdy(\"01/15/2023\"),\n  dmy = dmy(\"15-01-2023\"),\n  smart_parse = parse_date_time(date_strings, \n    orders = c(\"ymd\", \"mdy\", \"dmy\", \"Bdy\", \"ymd\"))\n)\n\nprint(parsed_dates)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 5\n  original         ymd        mdy        dmy        smart_parse        \n  <chr>            <date>     <date>     <date>     <dttm>             \n1 2023-01-15       2023-01-15 2023-01-15 2023-01-15 2023-01-15 00:00:00\n2 01/15/2023       2023-01-15 2023-01-15 2023-01-15 2023-01-15 00:00:00\n3 15-Jan-2023      2023-01-15 2023-01-15 2023-01-15 2023-01-15 00:00:00\n4 January 15, 2023 2023-01-15 2023-01-15 2023-01-15 2023-01-15 00:00:00\n5 20230115         2023-01-15 2023-01-15 2023-01-15 2023-01-15 00:00:00\n```\n\n\n:::\n:::\n\n\nThe genius of lubridate is the intuitive function names:\n- `ymd()`: Year-Month-Day\n- `mdy()`: Month-Day-Year\n- `dmy()`: Day-Month-Year\n- `ymd_hms()`: With time included\n\n### Date Components and Arithmetic\n\nOnce we have dates, we can extract components and do arithmetic:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a date range\ndates <- seq(ymd(\"2023-01-01\"), ymd(\"2023-12-31\"), by = \"month\")\n\ndate_analysis <- tibble(\n  date = dates,\n  year = year(date),\n  month = month(date),\n  month_name = month(date, label = TRUE, abbr = FALSE),\n  day = day(date),\n  weekday = wday(date, label = TRUE, abbr = FALSE),\n  quarter = quarter(date),\n  week_of_year = week(date),\n  day_of_year = yday(date),\n  is_weekend = wday(date) %in% c(1, 7),  # Sunday = 1, Saturday = 7\n  days_in_month = days_in_month(date)\n)\n\nprint(date_analysis, n = 12)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 12 x 11\n   date        year month month_name   day weekday   quarter week_of_year\n   <date>     <dbl> <dbl> <ord>      <int> <ord>       <int>        <dbl>\n 1 2023-01-01  2023     1 January        1 Sunday          1            1\n 2 2023-02-01  2023     2 February       1 Wednesday       1            5\n 3 2023-03-01  2023     3 March          1 Wednesday       1            9\n 4 2023-04-01  2023     4 April          1 Saturday        2           13\n 5 2023-05-01  2023     5 May            1 Monday          2           18\n 6 2023-06-01  2023     6 June           1 Thursday        2           22\n 7 2023-07-01  2023     7 July           1 Saturday        3           26\n 8 2023-08-01  2023     8 August         1 Tuesday         3           31\n 9 2023-09-01  2023     9 September      1 Friday          3           35\n10 2023-10-01  2023    10 October        1 Sunday          4           40\n11 2023-11-01  2023    11 November       1 Wednesday       4           44\n12 2023-12-01  2023    12 December       1 Friday          4           48\n# i 3 more variables: day_of_year <dbl>, is_weekend <lgl>, days_in_month <int>\n```\n\n\n:::\n\n```{.r .cell-code}\n# Date arithmetic is intuitive\nreference_date <- ymd(\"2023-06-15\")\n\ntibble(\n  description = c(\n    \"Original date\",\n    \"Plus 30 days\",\n    \"Plus 2 months\",\n    \"Plus 1 year\",\n    \"Next Monday\",\n    \"End of month\",\n    \"Beginning of year\"\n  ),\n  date = c(\n    reference_date,\n    reference_date + days(30),\n    reference_date + months(2),\n    reference_date + years(1),\n    ceiling_date(reference_date, \"week\", week_start = 1),\n    ceiling_date(reference_date, \"month\") - days(1),\n    floor_date(reference_date, \"year\")\n  )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 x 2\n  description       date      \n  <chr>             <date>    \n1 Original date     2023-06-15\n2 Plus 30 days      2023-07-15\n3 Plus 2 months     2023-08-15\n4 Plus 1 year       2024-06-15\n5 Next Monday       2023-06-19\n6 End of month      2023-06-30\n7 Beginning of year 2023-01-01\n```\n\n\n:::\n:::\n\n\nKey insights about date arithmetic:\n- Adding days is straightforward\n- Adding months/years handles edge cases (e.g., Jan 31 + 1 month = Feb 28/29)\n- `floor_date()` and `ceiling_date()` are perfect for grouping\n\n### Working with Time\n\nTime adds another layer of complexity:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Different time representations\ntime_examples <- tibble(\n  datetime_string = c(\n    \"2023-06-15 14:30:00\",\n    \"2023-06-15 2:30:00 PM\",\n    \"15/06/2023 14:30\",\n    \"June 15, 2023 2:30 PM\"\n  )\n)\n\n# Parse with different formats\nparsed_times <- time_examples %>%\n  mutate(\n    parsed = parse_date_time(datetime_string, \n      orders = c(\"ymd HMS\", \"ymd IMS p\", \"dmy HM\", \"Bdy IMS p\")),\n    date_only = as_date(parsed),\n    time_only = format(parsed, \"%H:%M:%S\"),\n    hour = hour(parsed),\n    minute = minute(parsed),\n    am_pm = if_else(hour(parsed) < 12, \"AM\", \"PM\")\n  )\n\nprint(parsed_times)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 x 7\n  datetime_string    parsed              date_only  time_only  hour minute am_pm\n  <chr>              <dttm>              <date>     <chr>     <int>  <int> <chr>\n1 2023-06-15 14:30:~ 2023-06-15 14:30:00 2023-06-15 14:30:00     14     30 PM   \n2 2023-06-15 2:30:0~ 2023-06-15 14:30:00 2023-06-15 14:30:00     14     30 PM   \n3 15/06/2023 14:30   2015-06-20 23:14:30 2015-06-20 23:14:30     23     14 PM   \n4 June 15, 2023 2:3~ NA                  NA         <NA>         NA     NA <NA> \n```\n\n\n:::\n\n```{.r .cell-code}\n# Time differences and durations\nstart_time <- ymd_hms(\"2023-06-15 09:00:00\")\nend_time <- ymd_hms(\"2023-06-15 17:30:00\")\n\n# Different ways to express time differences\ntime_diff <- end_time - start_time\nprint(time_diff)  # In seconds by default\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTime difference of 8.5 hours\n```\n\n\n:::\n\n```{.r .cell-code}\n# Convert to different units\ntibble(\n  unit = c(\"seconds\", \"minutes\", \"hours\", \"days\"),\n  value = c(\n    as.numeric(time_diff, units = \"secs\"),\n    as.numeric(time_diff, units = \"mins\"),\n    as.numeric(time_diff, units = \"hours\"),\n    as.numeric(time_diff, units = \"days\")\n  )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 x 2\n  unit        value\n  <chr>       <dbl>\n1 seconds 30600    \n2 minutes   510    \n3 hours       8.5  \n4 days        0.354\n```\n\n\n:::\n:::\n\n\n### Time Zones - The Hidden Complexity\n\nTime zones are often the source of subtle bugs in data analysis:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Working with time zones\nutc_time <- ymd_hms(\"2023-06-15 12:00:00\", tz = \"UTC\")\n\n# Convert to different time zones\ntime_zones <- tibble(\n  timezone = c(\"UTC\", \"America/New_York\", \"Europe/London\", \n               \"Asia/Tokyo\", \"Australia/Sydney\"),\n  local_time = c(\n    utc_time,\n    with_tz(utc_time, \"America/New_York\"),\n    with_tz(utc_time, \"Europe/London\"),\n    with_tz(utc_time, \"Asia/Tokyo\"),\n    with_tz(utc_time, \"Australia/Sydney\")\n  ),\n  offset_hours = c(0, -4, 1, 9, 10)  # Offset from UTC (varies with DST!)\n)\n\nprint(time_zones)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 3\n  timezone         local_time          offset_hours\n  <chr>            <dttm>                     <dbl>\n1 UTC              2023-06-15 12:00:00            0\n2 America/New_York 2023-06-15 12:00:00           -4\n3 Europe/London    2023-06-15 12:00:00            1\n4 Asia/Tokyo       2023-06-15 12:00:00            9\n5 Australia/Sydney 2023-06-15 12:00:00           10\n```\n\n\n:::\n\n```{.r .cell-code}\n# Force_tz vs with_tz\nambiguous_time <- ymd_hms(\"2023-06-15 12:00:00\")  # No timezone specified\n\ntibble(\n  description = c(\n    \"Original (no tz)\",\n    \"with_tz - converts time\",\n    \"force_tz - keeps time, changes zone\"\n  ),\n  result = c(\n    as.character(ambiguous_time),\n    as.character(with_tz(ambiguous_time, \"America/New_York\")),\n    as.character(force_tz(ambiguous_time, \"America/New_York\"))\n  )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 x 2\n  description                         result             \n  <chr>                               <chr>              \n1 Original (no tz)                    2023-06-15 12:00:00\n2 with_tz - converts time             2023-06-15 08:00:00\n3 force_tz - keeps time, changes zone 2023-06-15 12:00:00\n```\n\n\n:::\n:::\n\n\nThe difference between `with_tz()` and `force_tz()`:\n- `with_tz()`: Changes the display timezone (same moment in time)\n- `force_tz()`: Changes the timezone interpretation (different moment)\n\n### Intervals, Durations, and Periods\n\nLubridate distinguishes between three concepts:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Intervals: specific start and end times\ninterval_1 <- interval(ymd(\"2023-01-01\"), ymd(\"2023-12-31\"))\ninterval_2 <- interval(ymd(\"2023-06-01\"), ymd(\"2023-08-31\"))\n\n# Check overlap\nint_overlaps(interval_1, interval_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\n# Durations: exact time spans (in seconds)\nduration_1 <- ddays(7) + dhours(3) + dminutes(30)\nprint(duration_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"617400s (~1.02 weeks)\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Periods: human-friendly time spans\nperiod_1 <- days(7) + hours(3) + minutes(30)\nprint(period_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"7d 3H 30M 0S\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# The difference matters!\n# Consider daylight saving time\ndst_start <- ymd_hms(\"2023-03-11 00:00:00\", tz = \"America/New_York\")\n\ntibble(\n  description = c(\n    \"Original time\",\n    \"Plus duration (24 hours)\",\n    \"Plus period (1 day)\"\n  ),\n  result = c(\n    dst_start,\n    dst_start + ddays(1),  # Exactly 24 hours later\n    dst_start + days(1)    # Next calendar day at same time\n  )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 x 2\n  description              result             \n  <chr>                    <dttm>             \n1 Original time            2023-03-11 00:00:00\n2 Plus duration (24 hours) 2023-03-12 00:00:00\n3 Plus period (1 day)      2023-03-12 00:00:00\n```\n\n\n:::\n:::\n\n\nUse:\n- **Intervals**: When you need specific start/end times\n- **Durations**: For exact time calculations\n- **Periods**: For human-friendly calculations\n\n## Combining Strings and Dates in Real Analysis\n\nLet's work with a realistic example combining both:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a realistic dataset - customer support tickets\nset.seed(123)\nn_tickets <- 500\n\nsupport_tickets <- tibble(\n  ticket_id = sprintf(\"TICK-%06d\", 1:n_tickets),\n  created_at = ymd_hms(\"2023-01-01 00:00:00\") + \n    seconds(runif(n_tickets, 0, 365*24*60*60)),\n  customer_email = str_c(\n    sample(c(\"john\", \"mary\", \"bob\", \"alice\", \"charlie\"), n_tickets, replace = TRUE),\n    sample(1:100, n_tickets, replace = TRUE),\n    \"@\",\n    sample(c(\"gmail.com\", \"yahoo.com\", \"company.com\"), n_tickets, replace = TRUE)\n  ),\n  subject = sample(c(\n    \"Login issue - can't access account\",\n    \"Payment failed - ERROR 402\",\n    \"Question about pricing\",\n    \"Feature request: dark mode\",\n    \"Bug report - app crashes on startup\",\n    \"Refund request #12345\"\n  ), n_tickets, replace = TRUE),\n  priority = sample(c(\"Low\", \"Medium\", \"High\", \"Critical\"), n_tickets, \n                   replace = TRUE, prob = c(0.4, 0.3, 0.2, 0.1)),\n  resolved_at = created_at + hours(round(rexp(n_tickets, rate = 1/24)))  # Exponential resolution time\n) %>%\n  mutate(\n    resolved_at = if_else(runif(n_tickets) < 0.1, NA_POSIXct_, resolved_at)  # 10% unresolved\n  )\n\n# Analyze the tickets\nticket_analysis <- support_tickets %>%\n  mutate(\n    # Extract information from strings\n    issue_type = case_when(\n      str_detect(subject, \"Login|access|password\") ~ \"Authentication\",\n      str_detect(subject, \"Payment|refund|billing\") ~ \"Billing\",\n      str_detect(subject, \"Bug|crash|error\") ~ \"Bug\",\n      str_detect(subject, \"Feature|request\") ~ \"Feature Request\",\n      TRUE ~ \"Other\"\n    ),\n    has_error_code = str_detect(subject, \"ERROR \\\\d+\"),\n    error_code = str_extract(subject, \"ERROR \\\\d+\"),\n    \n    # Extract from email\n    email_domain = str_extract(customer_email, \"@(.+)$\") %>% str_remove(\"@\"),\n    is_corporate = email_domain == \"company.com\",\n    \n    # Date/time analysis\n    created_date = as_date(created_at),\n    created_hour = hour(created_at),\n    created_weekday = wday(created_at, label = TRUE),\n    created_month = month(created_at, label = TRUE),\n    is_business_hours = between(created_hour, 9, 17),\n    is_weekend = wday(created_at) %in% c(1, 7),\n    \n    # Resolution time\n    resolution_time = as.numeric(resolved_at - created_at, units = \"hours\"),\n    is_resolved = !is.na(resolved_at),\n    resolution_category = case_when(\n      is.na(resolution_time) ~ \"Unresolved\",\n      resolution_time < 1 ~ \"< 1 hour\",\n      resolution_time < 4 ~ \"1-4 hours\",\n      resolution_time < 24 ~ \"4-24 hours\",\n      resolution_time < 72 ~ \"1-3 days\",\n      TRUE ~ \"> 3 days\"\n    )\n  )\n\n# Summary statistics\nsummary_stats <- ticket_analysis %>%\n  group_by(issue_type, priority) %>%\n  summarise(\n    count = n(),\n    resolved_pct = mean(is_resolved) * 100,\n    avg_resolution_hours = mean(resolution_time, na.rm = TRUE),\n    median_resolution_hours = median(resolution_time, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  arrange(issue_type, priority)\n\nprint(summary_stats, n = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 20 x 6\n   issue_type      priority count resolved_pct avg_resolution_hours\n   <chr>           <chr>    <int>        <dbl>                <dbl>\n 1 Authentication  Critical    12         91.7                 20.8\n 2 Authentication  High        11         90.9                 20.3\n 3 Authentication  Low         28         78.6                 23.9\n 4 Authentication  Medium      29         93.1                 20.4\n 5 Billing         Critical     4         75                   25.7\n 6 Billing         High        18         83.3                 11.6\n 7 Billing         Low         29         96.6                 30.7\n 8 Billing         Medium      22         86.4                 29.2\n 9 Bug             Critical    10        100                   32  \n10 Bug             High        18         83.3                 25.6\n11 Bug             Low         45         97.8                 39.7\n12 Bug             Medium      25         88                   18.6\n13 Feature Request Critical    23        100                   17.7\n14 Feature Request High        27         92.6                 16.6\n15 Feature Request Low         61         88.5                 29.9\n16 Feature Request Medium      50         84                   21.5\n17 Other           Critical    11         72.7                 20.6\n18 Other           High        14         92.9                 26.8\n19 Other           Low         31         96.8                 23.2\n20 Other           Medium      32         84.4                 19.4\n# i 1 more variable: median_resolution_hours <dbl>\n```\n\n\n:::\n:::\n\n\nThis analysis demonstrates:\n- Extracting categories from free text\n- Finding patterns in strings\n- Working with timestamps\n- Calculating time differences\n- Combining string and date operations\n\n### Visualization of String and Date Patterns\n\nLet's visualize our findings:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Ticket volume over time\np1 <- ticket_analysis %>%\n  count(created_date) %>%\n  ggplot(aes(x = created_date, y = n)) +\n  geom_line(color = \"steelblue\", linewidth = 1) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"red\", linewidth = 0.8) +\n  labs(\n    title = \"Daily Ticket Volume\",\n    subtitle = \"With smoothed trend line\",\n    x = \"Date\",\n    y = \"Number of Tickets\"\n  )\n\n# Hour of day pattern\np2 <- ticket_analysis %>%\n  count(created_hour, is_weekend) %>%\n  ggplot(aes(x = created_hour, y = n, fill = is_weekend)) +\n  geom_col(position = \"dodge\", alpha = 0.8) +\n  scale_fill_manual(values = c(\"FALSE\" = \"steelblue\", \"TRUE\" = \"coral\")) +\n  labs(\n    title = \"Tickets by Hour of Day\",\n    subtitle = \"Weekday vs Weekend patterns\",\n    x = \"Hour of Day\",\n    y = \"Number of Tickets\",\n    fill = \"Weekend\"\n  ) +\n  scale_x_continuous(breaks = seq(0, 23, 3))\n\n# Issue types by priority\np3 <- ticket_analysis %>%\n  count(issue_type, priority) %>%\n  mutate(priority = factor(priority, \n    levels = c(\"Low\", \"Medium\", \"High\", \"Critical\"))) %>%\n  ggplot(aes(x = issue_type, y = n, fill = priority)) +\n  geom_col(position = \"fill\") +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"Issue Types by Priority\",\n    subtitle = \"Proportion of priority levels\",\n    x = \"Issue Type\",\n    y = \"Proportion\",\n    fill = \"Priority\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Resolution time distribution\np4 <- ticket_analysis %>%\n  filter(is_resolved) %>%\n  ggplot(aes(x = resolution_time, fill = priority)) +\n  geom_histogram(bins = 30, alpha = 0.7) +\n  scale_x_log10() +\n  scale_fill_viridis_d() +\n  facet_wrap(~priority) +\n  labs(\n    title = \"Resolution Time Distribution\",\n    subtitle = \"Log scale, by priority\",\n    x = \"Resolution Time (hours, log scale)\",\n    y = \"Count\"\n  ) +\n  theme(legend.position = \"none\")\n\n# Combine plots\nlibrary(patchwork)\n(p1 + p2) / (p3 + p4) +\n  plot_annotation(\n    title = \"Support Ticket Analysis\",\n    subtitle = \"Combining string extraction and temporal patterns\"\n  )\n```\n\n::: {.cell-output-display}\n![](07-strings-dates_files/figure-html/unnamed-chunk-13-1.png){width=1344}\n:::\n:::\n\n\n## Advanced String Patterns\n\n### Working with Structured Text\n\nMany datasets contain semi-structured text that needs parsing:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Log file entries\nlog_entries <- c(\n  \"[2023-06-15 10:30:45] INFO: User john_doe logged in from 192.168.1.100\",\n  \"[2023-06-15 10:31:02] ERROR: Database connection failed - timeout after 30s\",\n  \"[2023-06-15 10:31:15] WARNING: High memory usage detected (85%)\",\n  \"[2023-06-15 10:32:00] INFO: Backup completed successfully\",\n  \"[2023-06-15 10:33:21] ERROR: File not found: /data/config.json\"\n)\n\n# Parse log entries with regex\nparsed_logs <- tibble(log = log_entries) %>%\n  mutate(\n    # Extract timestamp\n    timestamp = str_extract(log, \"\\\\[([^\\\\]]+)\\\\]\") %>% \n      str_remove_all(\"\\\\[|\\\\]\") %>%\n      ymd_hms(),\n    \n    # Extract log level\n    level = str_extract(log, \"INFO|ERROR|WARNING|DEBUG\"),\n    \n    # Extract message\n    message = str_extract(log, \"(?<=: ).*$\"),  # Everything after \": \"\n    \n    # Extract specific patterns\n    username = str_extract(message, \"User (\\\\w+)\") %>% str_remove(\"User \"),\n    ip_address = str_extract(message, \"\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+\"),\n    file_path = str_extract(message, \"/[\\\\w/\\\\.]+\"),\n    duration = str_extract(message, \"\\\\d+s\") %>% str_remove(\"s\") %>% as.numeric()\n  )\n\nparsed_logs %>%\n  select(-log) %>%  # Remove original for display\n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 7\n  timestamp           level   message     username ip_address file_path duration\n  <dttm>              <chr>   <chr>       <chr>    <chr>      <chr>        <dbl>\n1 2023-06-15 10:30:45 INFO    User john_~ john_doe 192.168.1~ <NA>            NA\n2 2023-06-15 10:31:02 ERROR   Database c~ <NA>     <NA>       <NA>            30\n3 2023-06-15 10:31:15 WARNING High memor~ <NA>     <NA>       <NA>            NA\n4 2023-06-15 10:32:00 INFO    Backup com~ <NA>     <NA>       <NA>            NA\n5 2023-06-15 10:33:21 ERROR   File not f~ <NA>     <NA>       /data/co~       NA\n```\n\n\n:::\n:::\n\n\n### Text Normalization and Cleaning\n\nReal text data needs extensive cleaning:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Messy product descriptions\nmessy_products <- c(\n  \"Apple iPhone 14 Pro Max - 256GB - Space Black!!!\",\n  \"samsung galaxy s23 ultra 512gb phantom black\",\n  \"Google Pixel 7 Pro (128 GB) -- Obsidian\",\n  \"OnePlus 11 5G | 16GB RAM | 256GB Storage\",\n  \"  Xiaomi 13 Pro    256GB/12GB    Ceramic Black  \"\n)\n\n# Comprehensive cleaning pipeline\nclean_products <- tibble(original = messy_products) %>%\n  mutate(\n    # Step 1: Basic cleaning\n    cleaned = str_trim(original) %>%\n      str_squish() %>%\n      str_remove_all(\"!!!|\\\\||--\") %>%\n      str_replace_all(\"\\\\s+\", \" \"),\n    \n    # Step 2: Extract components\n    brand = str_extract(cleaned, \"^\\\\w+\") %>% str_to_title(),\n    \n    storage = str_extract(cleaned, \"\\\\d+\\\\s*GB\") %>%\n      str_remove_all(\"\\\\s\") %>%\n      str_to_upper(),\n    \n    ram = str_extract(cleaned, \"\\\\d+GB\\\\s*RAM|RAM\\\\s*\\\\d+GB\") %>%\n      str_extract(\"\\\\d+\") %>%\n      paste0(\"GB\"),\n    \n    color = str_extract(cleaned, \n      \"Space Black|Phantom Black|Obsidian|Ceramic Black\") %>%\n      str_to_title(),\n    \n    # Step 3: Standardized format\n    standardized = str_glue(\"{brand} - {storage} Storage - {color}\")\n  )\n\nclean_products %>%\n  select(original, standardized)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 2\n  original                                           standardized               \n  <chr>                                              <glue>                     \n1 \"Apple iPhone 14 Pro Max - 256GB - Space Black!!!\" Apple - 256GB Storage - Sp~\n2 \"samsung galaxy s23 ultra 512gb phantom black\"     Samsung - NA Storage - NA  \n3 \"Google Pixel 7 Pro (128 GB) -- Obsidian\"          Google - 128GB Storage - O~\n4 \"OnePlus 11 5G | 16GB RAM | 256GB Storage\"         Oneplus - 16GB Storage - NA\n5 \"  Xiaomi 13 Pro    256GB/12GB    Ceramic Black  \" Xiaomi - 256GB Storage - C~\n```\n\n\n:::\n:::\n\n\n## Working with Date Ranges and Business Logic\n\nReal-world applications often involve complex date logic:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Business calendar functions\nis_business_day <- function(date) {\n  !wday(date) %in% c(1, 7)  # Not weekend\n}\n\nnext_business_day <- function(date) {\n  next_day <- date + days(1)\n  while (!is_business_day(next_day)) {\n    next_day <- next_day + days(1)\n  }\n  return(next_day)\n}\n\nadd_business_days <- function(date, n) {\n  result <- date\n  for (i in 1:n) {\n    result <- next_business_day(result)\n  }\n  return(result)\n}\n\n# Example: Project timeline\nproject_tasks <- tibble(\n  task = c(\"Planning\", \"Development\", \"Testing\", \"Deployment\", \"Review\"),\n  duration_days = c(5, 15, 10, 2, 3),\n  start_date = ymd(\"2023-07-03\")  # Starting on a Monday\n)\n\n# Calculate end dates considering only business days\nproject_timeline <- project_tasks %>%\n  mutate(\n    # Simple calendar days\n    end_date_calendar = start_date + days(duration_days - 1),\n    \n    # Business days only\n    end_date_business = map2(start_date, duration_days, \n                            ~add_business_days(.x, .y - 1)),\n    end_date_business = as_date(unlist(end_date_business)),\n    \n    # Actual duration including weekends\n    actual_calendar_days = as.numeric(end_date_business - start_date + 1),\n    \n    # Next task start\n    next_start = lead(end_date_business) %>% \n      map_if(~!is.na(.), ~next_business_day(.)) %>%\n      unlist() %>%\n      as_date()\n  )\n\nproject_timeline %>%\n  select(task, duration_days, start_date, end_date_business, actual_calendar_days)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 5\n  task        duration_days start_date end_date_business actual_calendar_days\n  <chr>               <dbl> <date>     <date>                           <dbl>\n1 Planning                5 2023-07-03 2023-07-07                           5\n2 Development            15 2023-07-03 2023-07-21                          19\n3 Testing                10 2023-07-03 2023-07-14                          12\n4 Deployment              2 2023-07-03 2023-07-04                           2\n5 Review                  3 2023-07-03 2023-07-05                           3\n```\n\n\n:::\n:::\n\n\n## Exercises\n\n### Exercise 1: Email Parsing and Validation\n\nParse and validate a set of email addresses:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\nemail_data <- c(\n  \"john.doe@company.com\",\n  \"alice_smith@university.edu\",\n  \"bob@sub.domain.co.uk\",\n  \"invalid.email\",\n  \"no-at-sign.com\",\n  \"@missing-user.com\",\n  \"user@\",\n  \"first.last+tag@gmail.com\"\n)\n\nemail_validation <- tibble(email = email_data) %>%\n  mutate(\n    # Check basic structure\n    has_at = str_detect(email, \"@\"),\n    has_dot_after_at = str_detect(email, \"@.*\\\\.\"),\n    \n    # Extract parts\n    username = str_extract(email, \"^[^@]+\"),\n    domain = str_extract(email, \"@(.+)$\") %>% str_remove(\"@\"),\n    tld = str_extract(email, \"\\\\.[a-z]+$\"),\n    \n    # Validate\n    valid_username = str_detect(username, \"^[a-zA-Z0-9._+-]+$\"),\n    valid_domain = str_detect(domain, \"^[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\"),\n    \n    # Overall validity\n    is_valid = has_at & has_dot_after_at & \n               !is.na(username) & !is.na(domain) &\n               valid_username & valid_domain\n  )\n\nemail_validation %>%\n  select(email, username, domain, is_valid)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8 x 4\n  email                      username       domain           is_valid\n  <chr>                      <chr>          <chr>            <lgl>   \n1 john.doe@company.com       john.doe       company.com      TRUE    \n2 alice_smith@university.edu alice_smith    university.edu   TRUE    \n3 bob@sub.domain.co.uk       bob            sub.domain.co.uk TRUE    \n4 invalid.email              invalid.email  <NA>             FALSE   \n5 no-at-sign.com             no-at-sign.com <NA>             FALSE   \n6 @missing-user.com          <NA>           missing-user.com FALSE   \n7 user@                      user           <NA>             FALSE   \n8 first.last+tag@gmail.com   first.last+tag gmail.com        TRUE    \n```\n\n\n:::\n:::\n\n\n### Exercise 2: Date Range Calculations\n\nCalculate various date ranges and intervals:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Calculate age, next birthday, days until birthday\npeople <- tibble(\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"Diana\"),\n  birth_date = ymd(c(\"1990-03-15\", \"1985-07-22\", \"1995-12-01\", \"2000-01-30\"))\n)\n\ntoday_date <- today()\n\npeople_ages <- people %>%\n  mutate(\n    # Calculate age\n    age_years = interval(birth_date, today_date) / years(1),\n    age_exact = floor(age_years),\n    \n    # Next birthday\n    birthday_this_year = `year<-`(birth_date, year(today_date)),\n    next_birthday = if_else(\n      birthday_this_year < today_date,\n      `year<-`(birth_date, year(today_date) + 1),\n      birthday_this_year\n    ),\n    \n    # Days until birthday\n    days_to_birthday = as.numeric(next_birthday - today_date),\n    \n    # Day of week for next birthday\n    birthday_weekday = wday(next_birthday, label = TRUE, abbr = FALSE)\n  )\n\npeople_ages %>%\n  select(name, age_exact, next_birthday, days_to_birthday, birthday_weekday)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 x 5\n  name    age_exact next_birthday days_to_birthday birthday_weekday\n  <chr>       <dbl> <date>                   <dbl> <ord>           \n1 Alice          35 2026-03-15                 169 Sunday          \n2 Bob            40 2026-07-22                 298 Wednesday       \n3 Charlie        29 2025-12-01                  65 Monday          \n4 Diana          25 2026-01-30                 125 Friday          \n```\n\n\n:::\n:::\n\n\n### Exercise 3: Log File Analysis\n\nAnalyze server log patterns:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Generate sample log data\nset.seed(456)\nn_logs <- 1000\n\nlog_data <- tibble(\n  timestamp = ymd_hms(\"2023-06-01 00:00:00\") + seconds(sort(runif(n_logs, 0, 30*24*60*60))),\n  level = sample(c(\"INFO\", \"WARNING\", \"ERROR\", \"DEBUG\"), n_logs, \n                replace = TRUE, prob = c(0.5, 0.3, 0.15, 0.05)),\n  service = sample(c(\"auth\", \"api\", \"database\", \"cache\"), n_logs, replace = TRUE),\n  message = sample(c(\n    \"Request processed successfully\",\n    \"Connection timeout after 30s\",\n    \"Invalid authentication token\",\n    \"Cache miss for key: user_123\",\n    \"Database query took 1250ms\",\n    \"Rate limit exceeded for IP 192.168.1.1\"\n  ), n_logs, replace = TRUE)\n) %>%\n  mutate(\n    log_entry = str_glue(\"[{timestamp}] {level} [{service}]: {message}\")\n  )\n\n# Analyze the logs\nlog_analysis <- log_data %>%\n  mutate(\n    date = as_date(timestamp),\n    hour = hour(timestamp),\n    \n    # Extract metrics from messages\n    has_timeout = str_detect(message, \"timeout\"),\n    timeout_duration = str_extract(message, \"\\\\d+s\") %>% \n      str_remove(\"s\") %>% as.numeric(),\n    \n    has_ip = str_detect(message, \"\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+\"),\n    ip_address = str_extract(message, \"\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+\"),\n    \n    query_time = str_extract(message, \"\\\\d+ms\") %>% \n      str_remove(\"ms\") %>% as.numeric(),\n    \n    # Categorize issues\n    issue_type = case_when(\n      str_detect(message, \"timeout|slow|took \\\\d+ms\") ~ \"Performance\",\n      str_detect(message, \"Invalid|error|failed\") ~ \"Error\",\n      str_detect(message, \"exceeded|limit\") ~ \"Rate Limiting\",\n      TRUE ~ \"Normal\"\n    )\n  )\n\n# Summary by service and level\nlog_summary <- log_analysis %>%\n  group_by(service, level) %>%\n  summarise(\n    count = n(),\n    pct_errors = mean(issue_type == \"Error\") * 100,\n    avg_query_time = mean(query_time, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  arrange(service, level)\n\nprint(log_summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 16 x 5\n   service  level   count pct_errors avg_query_time\n   <chr>    <chr>   <int>      <dbl>          <dbl>\n 1 api      DEBUG       8      25               NaN\n 2 api      ERROR      48      25              1250\n 3 api      INFO      122      16.4            1250\n 4 api      WARNING    80      15              1250\n 5 auth     DEBUG      11      18.2            1250\n 6 auth     ERROR      34      14.7            1250\n 7 auth     INFO      137      13.9            1250\n 8 auth     WARNING    65      16.9            1250\n 9 cache    DEBUG      13       0              1250\n10 cache    ERROR      33      27.3            1250\n11 cache    INFO      116      20.7            1250\n12 cache    WARNING    81      18.5            1250\n13 database DEBUG      15       6.67           1250\n14 database ERROR      42       9.52           1250\n15 database INFO      127      15.0            1250\n16 database WARNING    68      23.5            1250\n```\n\n\n:::\n\n```{.r .cell-code}\n# Time pattern analysis\nhourly_pattern <- log_analysis %>%\n  group_by(hour, level) %>%\n  summarise(count = n(), .groups = \"drop\") %>%\n  ggplot(aes(x = hour, y = count, color = level)) +\n  geom_line(linewidth = 1) +\n  scale_color_manual(values = c(\n    \"INFO\" = \"green\",\n    \"WARNING\" = \"orange\", \n    \"ERROR\" = \"red\",\n    \"DEBUG\" = \"blue\"\n  )) +\n  labs(\n    title = \"Log Patterns by Hour\",\n    x = \"Hour of Day\",\n    y = \"Log Count\"\n  )\n\nprint(hourly_pattern)\n```\n\n::: {.cell-output-display}\n![](07-strings-dates_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n### Exercise 4: Text Mining Product Reviews\n\nAnalyze product review text and dates:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Generate sample review data\nset.seed(789)\nn_reviews <- 200\n\nreviews <- tibble(\n  review_id = 1:n_reviews,\n  review_date = ymd(\"2023-01-01\") + days(sample(0:180, n_reviews, replace = TRUE)),\n  rating = sample(1:5, n_reviews, replace = TRUE, prob = c(0.05, 0.1, 0.2, 0.35, 0.3)),\n  review_text = sample(c(\n    \"Great product! Highly recommend. Fast shipping too.\",\n    \"Terrible quality. Broke after one day. Very disappointed.\",\n    \"Good value for money. Works as expected.\",\n    \"Amazing! Best purchase ever! 10/10 would buy again!\",\n    \"Not bad, but not great either. Average product.\",\n    \"Excellent customer service. Product is okay.\",\n    \"Waste of money. Do not buy!\",\n    \"Pretty good, some minor issues but overall satisfied.\",\n    \"Perfect! Exactly what I needed. Five stars!\",\n    \"Meh. Expected better for the price.\"\n  ), n_reviews, replace = TRUE)\n)\n\n# Analyze reviews\nreview_analysis <- reviews %>%\n  mutate(\n    # Text length and complexity\n    text_length = str_length(review_text),\n    word_count = str_count(review_text, \"\\\\w+\"),\n    sentence_count = str_count(review_text, \"[.!?]\"),\n    avg_word_length = text_length / word_count,\n    \n    # Sentiment indicators\n    has_positive = str_detect(review_text, \n      \"(?i)great|excellent|amazing|perfect|love|best\"),\n    has_negative = str_detect(review_text, \n      \"(?i)terrible|bad|worst|hate|disappoint|waste\"),\n    \n    exclamation_count = str_count(review_text, \"!\"),\n    \n    # Time-based features\n    review_month = month(review_date, label = TRUE),\n    review_weekday = wday(review_date, label = TRUE),\n    days_since_launch = as.numeric(review_date - min(review_date)),\n    \n    # Categorize sentiment\n    sentiment = case_when(\n      has_positive & !has_negative ~ \"Positive\",\n      has_negative & !has_positive ~ \"Negative\",\n      has_positive & has_negative ~ \"Mixed\",\n      TRUE ~ \"Neutral\"\n    )\n  )\n\n# Correlation between text features and rating\nfeature_correlation <- review_analysis %>%\n  summarise(\n    cor_length_rating = cor(text_length, rating),\n    cor_exclamation_rating = cor(exclamation_count, rating),\n    cor_positive_rating = cor(as.numeric(has_positive), rating),\n    cor_negative_rating = cor(as.numeric(has_negative), rating)\n  )\n\nprint(\"Feature correlations with rating:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Feature correlations with rating:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(feature_correlation)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 4\n  cor_length_rating cor_exclamation_rating cor_positive_rating\n              <dbl>                  <dbl>               <dbl>\n1           -0.0412                -0.0605             -0.0880\n# i 1 more variable: cor_negative_rating <dbl>\n```\n\n\n:::\n\n```{.r .cell-code}\n# Sentiment vs actual rating\nsentiment_accuracy <- review_analysis %>%\n  group_by(sentiment) %>%\n  summarise(\n    avg_rating = mean(rating),\n    count = n(),\n    .groups = \"drop\"\n  )\n\nprint(\"Sentiment analysis accuracy:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Sentiment analysis accuracy:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(sentiment_accuracy)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 x 3\n  sentiment avg_rating count\n  <chr>          <dbl> <int>\n1 Mixed           3.64    22\n2 Negative        3.47    32\n3 Neutral         3.77    64\n4 Positive        3.41    82\n```\n\n\n:::\n:::\n\n\n## Summary\n\nIn this comprehensive chapter, you've mastered:\n\n✅ **String manipulation with stringr**\n  - Pattern matching and regular expressions\n  - Text cleaning and standardization\n  - String splitting and combining\n  - Advanced text extraction\n\n✅ **Date and time handling with lubridate**\n  - Parsing various date formats\n  - Date arithmetic and components\n  - Time zones and daylight saving\n  - Intervals, durations, and periods\n\n✅ **Combined analysis**\n  - Real-world text and temporal data\n  - Log file parsing\n  - Business date calculations\n  - Pattern extraction and validation\n\nKey takeaways:\n- Always validate and clean text data before analysis\n- Be explicit about time zones to avoid bugs\n- Regular expressions are powerful but need practice\n- Combine string and date operations for rich insights\n- Document your cleaning and parsing decisions\n\n## What's Next?\n\nIn [Chapter 8](08-tidymodels-intro.Rmd), we transition to machine learning with tidymodels, building on our data manipulation skills.\n\n## Additional Resources\n\n- [stringr Documentation](https://stringr.tidyverse.org/)\n- [lubridate Documentation](https://lubridate.tidyverse.org/)\n- [Regular Expression Testing](https://regex101.com/)\n- [R for Data Science - Strings](https://r4ds.had.co.nz/strings.html)\n- [R for Data Science - Dates and Times](https://r4ds.had.co.nz/dates-and-times.html)\n- [Time Zone Database](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones)\n",
    "supporting": [
      "07-strings-dates_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}