{
  "hash": "d196a1bb294e0917bd2a72a3df9c62d9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 15: Regression Models - Theory, Implementation, and Best Practices\"\nauthor: \"David Sarrat GonzÃ¡lez, Juan R GonzÃ¡lez\"\ndate: today\nformat:\n  html:\n    code-fold: false\n    code-tools: true\n---\n\n## Learning Objectives\n\nBy the end of this chapter, you will understand:\n\n- Linear regression theory and assumptions\n- Polynomial and non-linear regression\n- Regularization techniques (Ridge, Lasso, Elastic Net)\n- Regression trees and ensemble methods\n- Model diagnostics and validation\n- Advanced regression techniques\n- Practical implementation with tidymodels\n\n::: {.callout-tip}\n## Download R Script\nYou can download the complete R code for this chapter:\n[ðŸ“¥ Download 15-regression.R](R_scripts/15-regression.R){.btn .btn-primary download=\"15-regression.R\"}\n:::\n\n## What is Regression?\n\nRegression analysis is a fundamental statistical technique for modeling the relationship between a dependent variable (target) and one or more independent variables (features). Unlike classification, which predicts discrete categories, regression predicts continuous numerical values.\n\n### The Fundamental Question\n\nIn regression, we're essentially asking: \"Given these input features, what numerical value should we predict?\" This could be:\n- Predicting house prices based on size, location, and amenities\n- Estimating customer lifetime value from behavior patterns\n- Forecasting sales based on historical data and market conditions\n- Determining optimal drug dosage based on patient characteristics\n\n## Setup and Data Preparation\n\nLet's begin by loading the necessary libraries and preparing our datasets. We'll use multiple datasets throughout this chapter to illustrate different concepts and techniques.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(modeldata)\nlibrary(vip)\nlibrary(glmnet)\nlibrary(corrplot)\nlibrary(patchwork)\nlibrary(broom)\nlibrary(performance)\n\n# Set theme and seed for reproducibility\ntheme_set(theme_minimal())\nset.seed(123)\n\n# Load datasets\ndata(ames)        # House prices dataset\ndata(concrete)    # Concrete strength dataset\ndata(Chicago)     # Chicago ridership data\n\n# Quick overview of our main dataset\nglimpse(ames)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 2,930\nColumns: 74\n$ MS_SubClass        <fct> One_Story_1946_and_Newer_All_Styles, One_Story_1946â€¦\n$ MS_Zoning          <fct> Residential_Low_Density, Residential_High_Density, â€¦\n$ Lot_Frontage       <dbl> 141, 80, 81, 93, 74, 78, 41, 43, 39, 60, 75, 0, 63,â€¦\n$ Lot_Area           <int> 31770, 11622, 14267, 11160, 13830, 9978, 4920, 5005â€¦\n$ Street             <fct> Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pavâ€¦\n$ Alley              <fct> No_Alley_Access, No_Alley_Access, No_Alley_Access, â€¦\n$ Lot_Shape          <fct> Slightly_Irregular, Regular, Slightly_Irregular, Reâ€¦\n$ Land_Contour       <fct> Lvl, Lvl, Lvl, Lvl, Lvl, Lvl, Lvl, HLS, Lvl, Lvl, Lâ€¦\n$ Utilities          <fct> AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, Allâ€¦\n$ Lot_Config         <fct> Corner, Inside, Corner, Corner, Inside, Inside, Insâ€¦\n$ Land_Slope         <fct> Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gâ€¦\n$ Neighborhood       <fct> North_Ames, North_Ames, North_Ames, North_Ames, Gilâ€¦\n$ Condition_1        <fct> Norm, Feedr, Norm, Norm, Norm, Norm, Norm, Norm, Noâ€¦\n$ Condition_2        <fct> Norm, Norm, Norm, Norm, Norm, Norm, Norm, Norm, Norâ€¦\n$ Bldg_Type          <fct> OneFam, OneFam, OneFam, OneFam, OneFam, OneFam, Twnâ€¦\n$ House_Style        <fct> One_Story, One_Story, One_Story, One_Story, Two_Stoâ€¦\n$ Overall_Cond       <fct> Average, Above_Average, Above_Average, Average, Aveâ€¦\n$ Year_Built         <int> 1960, 1961, 1958, 1968, 1997, 1998, 2001, 1992, 199â€¦\n$ Year_Remod_Add     <int> 1960, 1961, 1958, 1968, 1998, 1998, 2001, 1992, 199â€¦\n$ Roof_Style         <fct> Hip, Gable, Hip, Hip, Gable, Gable, Gable, Gable, Gâ€¦\n$ Roof_Matl          <fct> CompShg, CompShg, CompShg, CompShg, CompShg, CompShâ€¦\n$ Exterior_1st       <fct> BrkFace, VinylSd, Wd Sdng, BrkFace, VinylSd, VinylSâ€¦\n$ Exterior_2nd       <fct> Plywood, VinylSd, Wd Sdng, BrkFace, VinylSd, VinylSâ€¦\n$ Mas_Vnr_Type       <fct> Stone, None, BrkFace, None, None, BrkFace, None, Noâ€¦\n$ Mas_Vnr_Area       <dbl> 112, 0, 108, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6â€¦\n$ Exter_Cond         <fct> Typical, Typical, Typical, Typical, Typical, Typicaâ€¦\n$ Foundation         <fct> CBlock, CBlock, CBlock, CBlock, PConc, PConc, PConcâ€¦\n$ Bsmt_Cond          <fct> Good, Typical, Typical, Typical, Typical, Typical, â€¦\n$ Bsmt_Exposure      <fct> Gd, No, No, No, No, No, Mn, No, No, No, No, No, No,â€¦\n$ BsmtFin_Type_1     <fct> BLQ, Rec, ALQ, ALQ, GLQ, GLQ, GLQ, ALQ, GLQ, Unf, Uâ€¦\n$ BsmtFin_SF_1       <dbl> 2, 6, 1, 1, 3, 3, 3, 1, 3, 7, 7, 1, 7, 3, 3, 1, 3, â€¦\n$ BsmtFin_Type_2     <fct> Unf, LwQ, Unf, Unf, Unf, Unf, Unf, Unf, Unf, Unf, Uâ€¦\n$ BsmtFin_SF_2       <dbl> 0, 144, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1120, 0â€¦\n$ Bsmt_Unf_SF        <dbl> 441, 270, 406, 1045, 137, 324, 722, 1017, 415, 994,â€¦\n$ Total_Bsmt_SF      <dbl> 1080, 882, 1329, 2110, 928, 926, 1338, 1280, 1595, â€¦\n$ Heating            <fct> GasA, GasA, GasA, GasA, GasA, GasA, GasA, GasA, Gasâ€¦\n$ Heating_QC         <fct> Fair, Typical, Typical, Excellent, Good, Excellent,â€¦\n$ Central_Air        <fct> Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, â€¦\n$ Electrical         <fct> SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SBâ€¦\n$ First_Flr_SF       <int> 1656, 896, 1329, 2110, 928, 926, 1338, 1280, 1616, â€¦\n$ Second_Flr_SF      <int> 0, 0, 0, 0, 701, 678, 0, 0, 0, 776, 892, 0, 676, 0,â€¦\n$ Gr_Liv_Area        <int> 1656, 896, 1329, 2110, 1629, 1604, 1338, 1280, 1616â€¦\n$ Bsmt_Full_Bath     <dbl> 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, â€¦\n$ Bsmt_Half_Bath     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n$ Full_Bath          <int> 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 3, 2, â€¦\n$ Half_Bath          <int> 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, â€¦\n$ Bedroom_AbvGr      <int> 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 1, 4, 4, â€¦\n$ Kitchen_AbvGr      <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, â€¦\n$ TotRms_AbvGrd      <int> 7, 5, 6, 8, 6, 7, 6, 5, 5, 7, 7, 6, 7, 5, 4, 12, 8,â€¦\n$ Functional         <fct> Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, Tâ€¦\n$ Fireplaces         <int> 2, 0, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, â€¦\n$ Garage_Type        <fct> Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, Attâ€¦\n$ Garage_Finish      <fct> Fin, Unf, Unf, Fin, Fin, Fin, Fin, RFn, RFn, Fin, Fâ€¦\n$ Garage_Cars        <dbl> 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, â€¦\n$ Garage_Area        <dbl> 528, 730, 312, 522, 482, 470, 582, 506, 608, 442, 4â€¦\n$ Garage_Cond        <fct> Typical, Typical, Typical, Typical, Typical, Typicaâ€¦\n$ Paved_Drive        <fct> Partial_Pavement, Paved, Paved, Paved, Paved, Pavedâ€¦\n$ Wood_Deck_SF       <int> 210, 140, 393, 0, 212, 360, 0, 0, 237, 140, 157, 48â€¦\n$ Open_Porch_SF      <int> 62, 0, 36, 0, 34, 36, 0, 82, 152, 60, 84, 21, 75, 0â€¦\n$ Enclosed_Porch     <int> 0, 0, 0, 0, 0, 0, 170, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0â€¦\n$ Three_season_porch <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n$ Screen_Porch       <int> 0, 120, 0, 0, 0, 0, 0, 144, 0, 0, 0, 0, 0, 0, 140, â€¦\n$ Pool_Area          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n$ Pool_QC            <fct> No_Pool, No_Pool, No_Pool, No_Pool, No_Pool, No_Pooâ€¦\n$ Fence              <fct> No_Fence, Minimum_Privacy, No_Fence, No_Fence, Miniâ€¦\n$ Misc_Feature       <fct> None, None, Gar2, None, None, None, None, None, Nonâ€¦\n$ Misc_Val           <int> 0, 0, 12500, 0, 0, 0, 0, 0, 0, 0, 0, 500, 0, 0, 0, â€¦\n$ Mo_Sold            <int> 5, 6, 6, 4, 3, 6, 4, 1, 3, 6, 4, 3, 5, 2, 6, 6, 6, â€¦\n$ Year_Sold          <int> 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 201â€¦\n$ Sale_Type          <fct> WD , WD , WD , WD , WD , WD , WD , WD , WD , WD , Wâ€¦\n$ Sale_Condition     <fct> Normal, Normal, Normal, Normal, Normal, Normal, Norâ€¦\n$ Sale_Price         <int> 215000, 105000, 172000, 244000, 189900, 195500, 213â€¦\n$ Longitude          <dbl> -93.61975, -93.61976, -93.61939, -93.61732, -93.638â€¦\n$ Latitude           <dbl> 42.05403, 42.05301, 42.05266, 42.05125, 42.06090, 4â€¦\n```\n\n\n:::\n:::\n\n\nThe Ames housing dataset contains information about residential homes sold in Ames, Iowa. It's perfect for demonstrating regression techniques because:\n- It has a clear continuous target (Sale_Price)\n- It contains both numeric and categorical predictors\n- It has enough complexity to showcase advanced techniques\n- It's real-world data with typical challenges (missing values, outliers, etc.)\n\n## Linear Regression: The Foundation\n\n### Mathematical Framework\n\nLinear regression assumes a linear relationship between predictors and the response. The model can be expressed as:\n\n$$Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p + \\epsilon$$\n\nWhere:\n- $Y$ is the response variable\n- $X_1, ..., X_p$ are the predictor variables\n- $\\beta_0$ is the intercept (the expected value of Y when all X = 0)\n- $\\beta_1, ..., \\beta_p$ are the coefficients (slopes)\n- $\\epsilon$ is the error term (assumed to be normally distributed with mean 0)\n\n### The Ordinary Least Squares (OLS) Method\n\nThe most common method for estimating the coefficients is Ordinary Least Squares, which minimizes the sum of squared residuals:\n\n$$\\text{RSS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij})^2$$\n\nLet's visualize this concept:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create simple example data\nset.seed(123)\nsimple_data <- tibble(\n  x = seq(1, 10, length.out = 50),\n  y_true = 2 + 3 * x,  # True relationship\n  y = y_true + rnorm(50, sd = 3)  # Add noise\n)\n\n# Fit simple linear regression\nsimple_lm <- lm(y ~ x, data = simple_data)\n\n# Add predictions and residuals\nsimple_data <- simple_data %>%\n  mutate(\n    y_pred = predict(simple_lm),\n    residual = y - y_pred\n  )\n\n# Visualize OLS concept\np1 <- ggplot(simple_data, aes(x = x)) +\n  geom_segment(aes(xend = x, y = y, yend = y_pred), \n               color = \"red\", alpha = 0.5) +\n  geom_point(aes(y = y), size = 2) +\n  geom_line(aes(y = y_pred), color = \"blue\", linewidth = 1.2) +\n  geom_line(aes(y = y_true), color = \"green\", linetype = \"dashed\") +\n  labs(\n    title = \"Ordinary Least Squares Visualization\",\n    subtitle = \"Red lines show residuals (errors) being minimized\",\n    x = \"Predictor (X)\",\n    y = \"Response (Y)\"\n  ) +\n  annotate(\"text\", x = 8, y = 15, label = \"True relationship\", \n           color = \"green\", size = 4) +\n  annotate(\"text\", x = 8, y = 25, label = \"Fitted line\", \n           color = \"blue\", size = 4)\n\n# Residual distribution\np2 <- ggplot(simple_data, aes(x = residual)) +\n  geom_histogram(bins = 20, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Distribution of Residuals\",\n    subtitle = \"Should be centered at zero and approximately normal\",\n    x = \"Residual\",\n    y = \"Count\"\n  )\n\np1 + p2\n```\n\n::: {.cell-output-display}\n![](15-regression_files/figure-html/unnamed-chunk-2-1.png){width=1152}\n:::\n:::\n\n\n### Key Assumptions of Linear Regression\n\nUnderstanding the assumptions is crucial for proper model interpretation and validation:\n\n1. **Linearity**: The relationship between X and Y is linear\n2. **Independence**: Observations are independent of each other\n3. **Homoscedasticity**: Constant variance of residuals\n4. **Normality**: Residuals are normally distributed\n5. **No multicollinearity**: Predictors are not highly correlated\n\nLet's check these assumptions with real data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Prepare Ames data for simple example\names_simple <- ames %>%\n  select(Sale_Price, Gr_Liv_Area, Year_Built, Overall_Cond) %>%\n  drop_na()\n\n# Fit model\names_lm <- lm(Sale_Price ~ Gr_Liv_Area + Year_Built + Overall_Cond, \n              data = ames_simple)\n\n# Create diagnostic plots\npar(mfrow = c(2, 2))\nplot(ames_lm)\n```\n\n::: {.cell-output-display}\n![](15-regression_files/figure-html/unnamed-chunk-3-1.png){width=1152}\n:::\n:::\n\n\nEach diagnostic plot tells us something important:\n- **Residuals vs Fitted**: Checks linearity and homoscedasticity. We want no clear pattern.\n- **Q-Q Plot**: Checks normality of residuals. Points should follow the diagonal line.\n- **Scale-Location**: Another check for homoscedasticity. We want a horizontal line.\n- **Residuals vs Leverage**: Identifies influential points that might unduly affect the model.\n\n## Implementing Linear Regression with Tidymodels\n\nNow let's implement a proper regression workflow using tidymodels. We'll predict house prices using multiple features.\n\n### Data Splitting and Preprocessing\n\nFirst, we need to properly split our data and create a preprocessing recipe. This is crucial to avoid data leakage and ensure fair model evaluation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Clean and prepare the Ames data\names_clean <- ames %>%\n  mutate(Sale_Price = log10(Sale_Price)) %>%  # Log transform for better distribution\n  select(-contains(\"_Condition\"))  # Remove some problematic variables\n\n# Split the data\names_split <- initial_split(ames_clean, prop = 0.75, strata = Sale_Price)\names_train <- training(ames_split)\names_test <- testing(ames_split)\n\n# Create preprocessing recipe\names_recipe <- recipe(Sale_Price ~ ., data = ames_train) %>%\n  # Remove variables with near-zero variance\n  step_nzv(all_predictors()) %>%\n  # Impute missing values\n  step_impute_median(all_numeric_predictors()) %>%\n  step_impute_mode(all_nominal_predictors()) %>%\n  # Create dummy variables for categorical predictors\n  step_dummy(all_nominal_predictors()) %>%\n  # Normalize numeric predictors\n  step_normalize(all_numeric_predictors())\n\n# Check the recipe\nprep(ames_recipe) %>%\n  bake(new_data = NULL) %>%\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 2,197\nColumns: 210\n$ Lot_Frontage                                          <dbl> 0.38099161, -1.0â€¦\n$ Lot_Area                                              <dbl> -0.23232512, -1.â€¦\n$ Year_Built                                            <dbl> -0.036951183, -0â€¦\n$ Year_Remod_Add                                        <dbl> -0.671110551, -0â€¦\n$ Mas_Vnr_Area                                          <dbl> -0.56439715, 2.2â€¦\n$ BsmtFin_SF_1                                          <dbl> -1.4325147, 0.80â€¦\n$ BsmtFin_SF_2                                          <dbl> 0.1623553, -0.29â€¦\n$ Bsmt_Unf_SF                                           <dbl> -1.26807359, -0.â€¦\n$ Total_Bsmt_SF                                         <dbl> -0.369147073, -1â€¦\n$ First_Flr_SF                                          <dbl> -0.705259283, -1â€¦\n$ Second_Flr_SF                                         <dbl> -0.78932267, 0.3â€¦\n$ Gr_Liv_Area                                           <dbl> -1.2416271, -1.0â€¦\n$ Bsmt_Full_Bath                                        <dbl> 1.0937422, -0.81â€¦\n$ Bsmt_Half_Bath                                        <dbl> -0.2447953, -0.2â€¦\n$ Full_Bath                                             <dbl> -1.0378355, -1.0â€¦\n$ Half_Bath                                             <dbl> -0.7549524, 1.22â€¦\n$ Bedroom_AbvGr                                         <dbl> -1.0622200, -1.0â€¦\n$ TotRms_AbvGrd                                         <dbl> -1.5633313, -0.9â€¦\n$ Fireplaces                                            <dbl> -0.9316181, -0.9â€¦\n$ Garage_Cars                                           <dbl> 0.3179223, -1.00â€¦\n$ Garage_Area                                           <dbl> 0.2574653, -0.96â€¦\n$ Wood_Deck_SF                                          <dbl> 1.22164555, 1.50â€¦\n$ Open_Porch_SF                                         <dbl> -0.69696285, -0.â€¦\n$ Mo_Sold                                               <dbl> -0.81175416, -1.â€¦\n$ Year_Sold                                             <dbl> 1.67607, 1.67607â€¦\n$ Longitude                                             <dbl> 0.6146441, 0.608â€¦\n$ Latitude                                              <dbl> 1.03189989, 0.94â€¦\n$ Sale_Price                                            <dbl> 5.100371, 4.9822â€¦\n$ MS_SubClass_One_Story_1945_and_Older                  <dbl> -0.2194655, -0.2â€¦\n$ MS_SubClass_One_Story_with_Finished_Attic_All_Ages    <dbl> -0.04269842, -0.â€¦\n$ MS_SubClass_One_and_Half_Story_Unfinished_All_Ages    <dbl> -0.08006419, -0.â€¦\n$ MS_SubClass_One_and_Half_Story_Finished_All_Ages      <dbl> -0.3368694, -0.3â€¦\n$ MS_SubClass_Two_Story_1946_and_Newer                  <dbl> -0.4924801, -0.4â€¦\n$ MS_SubClass_Two_Story_1945_and_Older                  <dbl> -0.2148708, -0.2â€¦\n$ MS_SubClass_Two_and_Half_Story_All_Ages               <dbl> -0.09086754, -0.â€¦\n$ MS_SubClass_Split_or_Multilevel                       <dbl> -0.2066285, -0.2â€¦\n$ MS_SubClass_Split_Foyer                               <dbl> -0.1290402, -0.1â€¦\n$ MS_SubClass_Duplex_All_Styles_and_Ages                <dbl> -0.1968579, -0.1â€¦\n$ MS_SubClass_One_Story_PUD_1946_and_Newer              <dbl> -0.2628085, -0.2â€¦\n$ MS_SubClass_One_and_Half_Story_PUD_All_Ages           <dbl> -0.02133462, -0.â€¦\n$ MS_SubClass_Two_Story_PUD_1946_and_Newer              <dbl> -0.2171781, 4.60â€¦\n$ MS_SubClass_PUD_Multilevel_Split_Level_Foyer          <dbl> -0.08289329, -0.â€¦\n$ MS_SubClass_Two_Family_conversion_All_Styles_and_Ages <dbl> -0.1412578, -0.1â€¦\n$ MS_Zoning_Residential_High_Density                    <dbl> -0.08828712, -0.â€¦\n$ MS_Zoning_Residential_Low_Density                     <dbl> 0.5433843, -1.83â€¦\n$ MS_Zoning_Residential_Medium_Density                  <dbl> -0.4352129, 2.29â€¦\n$ MS_Zoning_A_agr                                       <dbl> -0.03017859, -0.â€¦\n$ MS_Zoning_C_all                                       <dbl> -0.1005502, -0.1â€¦\n$ MS_Zoning_I_all                                       <dbl> -0.02133462, -0.â€¦\n$ Lot_Shape_Slightly_Irregular                          <dbl> -0.7154097, -0.7â€¦\n$ Lot_Shape_Moderately_Irregular                        <dbl> -0.1660823, -0.1â€¦\n$ Lot_Shape_Irregular                                   <dbl> -0.06760468, -0.â€¦\n$ Lot_Config_CulDSac                                    <dbl> -0.2548026, -0.2â€¦\n$ Lot_Config_FR2                                        <dbl> -0.1786766, -0.1â€¦\n$ Lot_Config_FR3                                        <dbl> -0.07713411, -0.â€¦\n$ Lot_Config_Inside                                     <dbl> -1.649997, 0.605â€¦\n$ Neighborhood_College_Creek                            <dbl> -0.318997, -0.31â€¦\n$ Neighborhood_Old_Town                                 <dbl> -0.2950355, -0.2â€¦\n$ Neighborhood_Edwards                                  <dbl> -0.266744, -0.26â€¦\n$ Neighborhood_Somerset                                 <dbl> -0.2618178, -0.2â€¦\n$ Neighborhood_Northridge_Heights                       <dbl> -0.2392499, -0.2â€¦\n$ Neighborhood_Gilbert                                  <dbl> -0.2434722, -0.2â€¦\n$ Neighborhood_Sawyer                                   <dbl> -0.2217336, -0.2â€¦\n$ Neighborhood_Northwest_Ames                           <dbl> -0.2183242, -0.2â€¦\n$ Neighborhood_Sawyer_West                              <dbl> -0.2171781, -0.2â€¦\n$ Neighborhood_Mitchell                                 <dbl> -0.1981015, -0.1â€¦\n$ Neighborhood_Brookside                                <dbl> -0.1981015, -0.1â€¦\n$ Neighborhood_Crawford                                 <dbl> -0.1968579, -0.1â€¦\n$ Neighborhood_Iowa_DOT_and_Rail_Road                   <dbl> -0.1918152, -0.1â€¦\n$ Neighborhood_Timberland                               <dbl> -0.1587037, -0.1â€¦\n$ Neighborhood_Northridge                               <dbl> -0.1571906, -0.1â€¦\n$ Neighborhood_Stone_Brook                              <dbl> -0.1378697, -0.1â€¦\n$ Neighborhood_South_and_West_of_Iowa_State_University  <dbl> -0.1196059, -0.1â€¦\n$ Neighborhood_Clear_Creek                              <dbl> -0.1215478, -0.1â€¦\n$ Neighborhood_Meadow_Village                           <dbl> -0.1135927, -0.1â€¦\n$ Neighborhood_Briardale                                <dbl> -0.09582679, 10.â€¦\n$ Neighborhood_Bloomington_Heights                      <dbl> -0.09582679, -0.â€¦\n$ Neighborhood_Veenker                                  <dbl> -0.09582679, -0.â€¦\n$ Neighborhood_Northpark_Villa                          <dbl> -0.09086754, -0.â€¦\n$ Neighborhood_Blueste                                  <dbl> -0.06043983, -0.â€¦\n$ Neighborhood_Greens                                   <dbl> -0.04269842, -0.â€¦\n$ Neighborhood_Green_Hills                              <dbl> 0, 0, 0, 0, 0, 0â€¦\n$ Neighborhood_Landmark                                 <dbl> -0.02133462, -0.â€¦\n$ Neighborhood_Hayden_Lake                              <dbl> 0, 0, 0, 0, 0, 0â€¦\n$ Condition_1_Feedr                                     <dbl> -0.2497013, -0.2â€¦\n$ Condition_1_Norm                                      <dbl> 0.4059855, 0.405â€¦\n$ Condition_1_PosA                                      <dbl> -0.08828712, -0.â€¦\n$ Condition_1_PosN                                      <dbl> -0.1094102, -0.1â€¦\n$ Condition_1_RRAe                                      <dbl> -0.09582679, -0.â€¦\n$ Condition_1_RRAn                                      <dbl> -0.1308504, -0.1â€¦\n$ Condition_1_RRNe                                      <dbl> -0.04269842, -0.â€¦\n$ Condition_1_RRNn                                      <dbl> -0.05231854, -0.â€¦\n$ Bldg_Type_TwoFmCon                                    <dbl> -0.1429241, -0.1â€¦\n$ Bldg_Type_Duplex                                      <dbl> -0.1968579, -0.1â€¦\n$ Bldg_Type_Twnhs                                       <dbl> -0.1892507, 5.28â€¦\n$ Bldg_Type_TwnhsE                                      <dbl> -0.2950355, -0.2â€¦\n$ House_Style_One_and_Half_Unf                          <dbl> -0.08289329, -0.â€¦\n$ House_Style_One_Story                                 <dbl> 0.996592, -1.002â€¦\n$ House_Style_SFoyer                                    <dbl> -0.1631668, -0.1â€¦\n$ House_Style_SLvl                                      <dbl> -0.2171781, -0.2â€¦\n$ House_Style_Two_and_Half_Fin                          <dbl> -0.05231854, -0.â€¦\n$ House_Style_Two_and_Half_Unf                          <dbl> -0.0982158, -0.0â€¦\n$ House_Style_Two_Story                                 <dbl> -0.6508892, 1.53â€¦\n$ Overall_Cond_Poor                                     <dbl> -0.05652338, -0.â€¦\n$ Overall_Cond_Fair                                     <dbl> -0.1326376, -0.1â€¦\n$ Overall_Cond_Below_Average                            <dbl> -0.1813704, -0.1â€¦\n$ Overall_Cond_Average                                  <dbl> 0.8734386, 0.873â€¦\n$ Overall_Cond_Above_Average                            <dbl> -0.4688046, -0.4â€¦\n$ Overall_Cond_Good                                     <dbl> -0.3859932, -0.3â€¦\n$ Overall_Cond_Very_Good                                <dbl> -0.2317166, -0.2â€¦\n$ Overall_Cond_Excellent                                <dbl> -0.1308504, -0.1â€¦\n$ Overall_Cond_Very_Excellent                           <dbl> 0, 0, 0, 0, 0, 0â€¦\n$ Roof_Style_Gable                                      <dbl> 0.5109551, 0.510â€¦\n$ Roof_Style_Gambrel                                    <dbl> -0.09337895, -0.â€¦\n$ Roof_Style_Hip                                        <dbl> -0.4788845, -0.4â€¦\n$ Roof_Style_Mansard                                    <dbl> -0.06043983, -0.â€¦\n$ Roof_Style_Shed                                       <dbl> -0.03696949, -0.â€¦\n$ Exterior_1st_AsphShn                                  <dbl> -0.03017859, -0.â€¦\n$ Exterior_1st_BrkComm                                  <dbl> -0.03696949, -0.â€¦\n$ Exterior_1st_BrkFace                                  <dbl> -0.1800279, -0.1â€¦\n$ Exterior_1st_CBlock                                   <dbl> -0.02133462, -0.â€¦\n$ Exterior_1st_CemntBd                                  <dbl> -0.2171781, -0.2â€¦\n$ Exterior_1st_HdBoard                                  <dbl> -0.4165718, 2.39â€¦\n$ Exterior_1st_ImStucc                                  <dbl> -0.02133462, -0.â€¦\n$ Exterior_1st_MetalSd                                  <dbl> -0.4337328, -0.4â€¦\n$ Exterior_1st_Plywood                                  <dbl> 3.497052, -0.285â€¦\n$ Exterior_1st_PreCast                                  <dbl> 0, 0, 0, 0, 0, 0â€¦\n$ Exterior_1st_Stone                                    <dbl> -0.03017859, -0.â€¦\n$ Exterior_1st_Stucco                                   <dbl> -0.1272059, -0.1â€¦\n$ Exterior_1st_VinylSd                                  <dbl> -0.7307357, -0.7â€¦\n$ Exterior_1st_Wd.Sdng                                  <dbl> -0.4014121, -0.4â€¦\n$ Exterior_1st_WdShing                                  <dbl> -0.1429241, -0.1â€¦\n$ Exterior_2nd_AsphShn                                  <dbl> -0.04269842, -0.â€¦\n$ Exterior_2nd_Brk.Cmn                                  <dbl> -0.08563145, -0.â€¦\n$ Exterior_2nd_BrkFace                                  <dbl> -0.1290402, -0.1â€¦\n$ Exterior_2nd_CBlock                                   <dbl> -0.02133462, -0.â€¦\n$ Exterior_2nd_CmentBd                                  <dbl> -0.2171781, -0.2â€¦\n$ Exterior_2nd_HdBoard                                  <dbl> -0.3937376, 2.53â€¦\n$ Exterior_2nd_ImStucc                                  <dbl> -0.07713411, -0.â€¦\n$ Exterior_2nd_MetalSd                                  <dbl> -0.4300246, -0.4â€¦\n$ Exterior_2nd_Other                                    <dbl> 0, 0, 0, 0, 0, 0â€¦\n$ Exterior_2nd_Plywood                                  <dbl> 3.1249258, -0.31â€¦\n$ Exterior_2nd_PreCast                                  <dbl> 0, 0, 0, 0, 0, 0â€¦\n$ Exterior_2nd_Stone                                    <dbl> -0.04774917, -0.â€¦\n$ Exterior_2nd_Stucco                                   <dbl> -0.1308504, -0.1â€¦\n$ Exterior_2nd_VinylSd                                  <dbl> -0.7263444, -0.7â€¦\n$ Exterior_2nd_Wd.Sdng                                  <dbl> -0.3898745, -0.3â€¦\n$ Exterior_2nd_Wd.Shng                                  <dbl> -0.1786766, -0.1â€¦\n$ Mas_Vnr_Type_BrkFace                                  <dbl> -0.6558526, 1.52â€¦\n$ Mas_Vnr_Type_CBlock                                   <dbl> -0.02133462, -0.â€¦\n$ Mas_Vnr_Type_None                                     <dbl> 0.8095158, -1.23â€¦\n$ Mas_Vnr_Type_Stone                                    <dbl> -0.3049467, -0.3â€¦\n$ Exter_Cond_Fair                                       <dbl> -0.1541244, -0.1â€¦\n$ Exter_Cond_Good                                       <dbl> -0.3460075, -0.3â€¦\n$ Exter_Cond_Poor                                       <dbl> -0.03017859, -0.â€¦\n$ Exter_Cond_Typical                                    <dbl> 0.3975833, 0.397â€¦\n$ Foundation_CBlock                                     <dbl> 1.1615155, 1.161â€¦\n$ Foundation_PConc                                      <dbl> -0.8872845, -0.8â€¦\n$ Foundation_Slab                                       <dbl> -0.1395732, -0.1â€¦\n$ Foundation_Stone                                      <dbl> -0.06043983, -0.â€¦\n$ Foundation_Wood                                       <dbl> -0.04774917, -0.â€¦\n$ Bsmt_Exposure_Gd                                      <dbl> -0.3137757, -0.3â€¦\n$ Bsmt_Exposure_Mn                                      <dbl> -0.3013679, -0.3â€¦\n$ Bsmt_Exposure_No                                      <dbl> 0.7358721, 0.735â€¦\n$ Bsmt_Exposure_No_Basement                             <dbl> -0.1773163, -0.1â€¦\n$ BsmtFin_Type_1_BLQ                                    <dbl> -0.312900, -0.31â€¦\n$ BsmtFin_Type_1_GLQ                                    <dbl> -0.6423969, -0.6â€¦\n$ BsmtFin_Type_1_LwQ                                    <dbl> -0.2317166, -0.2â€¦\n$ BsmtFin_Type_1_No_Basement                            <dbl> -0.1745678, -0.1â€¦\n$ BsmtFin_Type_1_Rec                                    <dbl> -0.3351923, 2.98â€¦\n$ BsmtFin_Type_1_Unf                                    <dbl> -0.6416901, -0.6â€¦\n$ Heating_QC_Fair                                       <dbl> -0.1918152, -0.1â€¦\n$ Heating_QC_Good                                       <dbl> -0.4374295, -0.4â€¦\n$ Heating_QC_Poor                                       <dbl> -0.03696949, -0.â€¦\n$ Heating_QC_Typical                                    <dbl> 1.5645654, 1.564â€¦\n$ Central_Air_Y                                         <dbl> 0.2725689, 0.272â€¦\n$ Electrical_FuseF                                      <dbl> -0.1412578, -0.1â€¦\n$ Electrical_FuseP                                      <dbl> -0.05652338, -0.â€¦\n$ Electrical_Mix                                        <dbl> 0, 0, 0, 0, 0, 0â€¦\n$ Electrical_SBrkr                                      <dbl> 0.312900, 0.3129â€¦\n$ Electrical_Unknown                                    <dbl> -0.02133462, -0.â€¦\n$ Garage_Type_Basment                                   <dbl> -0.1196059, -0.1â€¦\n$ Garage_Type_BuiltIn                                   <dbl> -0.2548026, -0.2â€¦\n$ Garage_Type_CarPort                                   <dbl> -0.0709206, -0.0â€¦\n$ Garage_Type_Detchd                                    <dbl> -0.6085939, 1.64â€¦\n$ Garage_Type_More_Than_Two_Types                       <dbl> -0.0709206, -0.0â€¦\n$ Garage_Type_No_Garage                                 <dbl> -0.2434722, -0.2â€¦\n$ Garage_Finish_No_Garage                               <dbl> -0.2445188, -0.2â€¦\n$ Garage_Finish_RFn                                     <dbl> -0.623355, -0.62â€¦\n$ Garage_Finish_Unf                                     <dbl> -0.8478001, 1.17â€¦\n$ Garage_Cond_Fair                                      <dbl> -0.1602038, -0.1â€¦\n$ Garage_Cond_Good                                      <dbl> -0.06760468, -0.â€¦\n$ Garage_Cond_No_Garage                                 <dbl> -0.2445188, -0.2â€¦\n$ Garage_Cond_Poor                                      <dbl> -0.0740911, -0.0â€¦\n$ Garage_Cond_Typical                                   <dbl> 0.3198619, 0.319â€¦\n$ Paved_Drive_Partial_Pavement                          <dbl> -0.1541244, -0.1â€¦\n$ Paved_Drive_Paved                                     <dbl> 0.3275821, 0.327â€¦\n$ Fence_Good_Wood                                       <dbl> -0.2042228, -0.2â€¦\n$ Fence_Minimum_Privacy                                 <dbl> 2.7713349, -0.36â€¦\n$ Fence_Minimum_Wood_Wire                               <dbl> -0.06412077, -0.â€¦\n$ Fence_No_Fence                                        <dbl> -1.9865551, 0.50â€¦\n$ Sale_Type_Con                                         <dbl> -0.04269842, -0.â€¦\n$ Sale_Type_ConLD                                       <dbl> -0.08289329, -0.â€¦\n$ Sale_Type_ConLI                                       <dbl> -0.06043983, -0.â€¦\n$ Sale_Type_ConLw                                       <dbl> -0.05231854, -0.â€¦\n$ Sale_Type_CWD                                         <dbl> -0.0709206, -0.0â€¦\n$ Sale_Type_New                                         <dbl> -0.2886094, -0.2â€¦\n$ Sale_Type_Oth                                         <dbl> -0.04269842, -0.â€¦\n$ Sale_Type_VWD                                         <dbl> -0.02133462, -0.â€¦\n$ Sale_Type_WD.                                         <dbl> 0.3906485, -2.55â€¦\n```\n\n\n:::\n:::\n\n\nNotice how we're taking several important steps:\n1. **Log transformation**: Sale prices often have a right-skewed distribution. Log transformation makes the distribution more normal.\n2. **Stratified splitting**: Ensures both training and test sets have similar distributions of the target variable.\n3. **Systematic preprocessing**: Handles missing values, creates dummy variables, and normalizes features in a reproducible way.\n\n### Model Specification and Training\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Specify linear regression model\nlm_spec <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  set_mode(\"regression\")\n\n# Create workflow\nlm_workflow <- workflow() %>%\n  add_recipe(ames_recipe) %>%\n  add_model(lm_spec)\n\n# Fit the model\nlm_fit <- lm_workflow %>%\n  fit(ames_train)\n\n# Extract and examine coefficients\nlm_coefs <- lm_fit %>%\n  extract_fit_parsnip() %>%\n  tidy() %>%\n  filter(term != \"(Intercept)\") %>%\n  arrange(desc(abs(estimate))) %>%\n  head(20)\n\n# Visualize top coefficients\nggplot(lm_coefs, aes(x = reorder(term, estimate), y = estimate)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Top 20 Most Influential Features in Linear Regression\",\n    subtitle = \"Coefficients represent change in log10(Sale_Price) per unit change in feature\",\n    x = \"Feature\",\n    y = \"Coefficient\"\n  )\n```\n\n::: {.cell-output-display}\n![](15-regression_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nThe coefficients tell us the relationship between each feature and the sale price. Since we normalized the predictors, we can compare coefficient magnitudes directly. Positive coefficients increase price, negative ones decrease it.\n\n## Polynomial and Non-linear Regression\n\nSometimes relationships aren't linear. Polynomial regression can capture curved relationships by including polynomial terms.\n\n### Understanding Polynomial Regression\n\nPolynomial regression extends linear regression by adding polynomial terms:\n\n$$Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + ... + \\beta_dX^d + \\epsilon$$\n\nLet's visualize different polynomial degrees:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate non-linear data\nset.seed(123)\nnonlinear_data <- tibble(\n  x = seq(0, 10, length.out = 100),\n  y_true = 10 + 5*x - 0.5*x^2 + 0.02*x^3,\n  y = y_true + rnorm(100, sd = 5)\n)\n\n# Fit models with different polynomial degrees\ndegrees <- c(1, 2, 3, 5, 10)\npoly_models <- map(degrees, function(d) {\n  lm(y ~ poly(x, degree = d, raw = TRUE), data = nonlinear_data)\n})\n\n# Generate predictions\npoly_predictions <- map2_df(poly_models, degrees, function(model, d) {\n  nonlinear_data %>%\n    mutate(\n      y_pred = predict(model),\n      degree = paste(\"Degree\", d)\n    )\n})\n\n# Visualize\nggplot(poly_predictions, aes(x = x)) +\n  geom_point(aes(y = y), alpha = 0.3, data = nonlinear_data) +\n  geom_line(aes(y = y_pred, color = degree), linewidth = 1.2) +\n  geom_line(aes(y = y_true), color = \"black\", linetype = \"dashed\", \n            data = nonlinear_data, linewidth = 1) +\n  facet_wrap(~degree, ncol = 3) +\n  scale_color_viridis_d() +\n  labs(\n    title = \"Polynomial Regression with Different Degrees\",\n    subtitle = \"Black dashed line shows true relationship. Higher degrees can lead to overfitting.\",\n    x = \"X\",\n    y = \"Y\"\n  ) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](15-regression_files/figure-html/unnamed-chunk-6-1.png){width=1152}\n:::\n:::\n\n\nNotice how:\n- **Degree 1** (linear) underfits - it can't capture the curve\n- **Degree 2-3** capture the main pattern well\n- **Degree 10** overfits - it follows noise in the data\n\n### Implementing Polynomial Features\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create recipe with polynomial features\npoly_recipe <- recipe(Sale_Price ~ Gr_Liv_Area + Lot_Area, \n                      data = ames_train) %>%\n  step_poly(Gr_Liv_Area, degree = 2) %>%\n  step_poly(Lot_Area, degree = 2) %>%\n  step_interact(terms = ~ Gr_Liv_Area_poly_1:Lot_Area_poly_1)\n\n# Fit model with polynomial features\npoly_workflow <- workflow() %>%\n  add_recipe(poly_recipe) %>%\n  add_model(lm_spec)\n\npoly_fit <- poly_workflow %>%\n  fit(ames_train)\n\n# Compare with simple linear model\nsimple_recipe <- recipe(Sale_Price ~ Gr_Liv_Area + Lot_Area, \n                       data = ames_train)\n\nsimple_workflow <- workflow() %>%\n  add_recipe(simple_recipe) %>%\n  add_model(lm_spec)\n\nsimple_fit <- simple_workflow %>%\n  fit(ames_train)\n\n# Evaluate both models\nmodels_comparison <- tibble(\n  model = c(\"Linear\", \"Polynomial\"),\n  workflow = list(simple_workflow, poly_workflow)\n) %>%\n  mutate(\n    fit = map(workflow, ~ fit(., ames_train)),\n    train_pred = map(fit, ~ predict(., ames_train)),\n    test_pred = map(fit, ~ predict(., ames_test))\n  )\n\n# Calculate metrics\ncomparison_metrics <- models_comparison %>%\n  mutate(\n    train_metrics = map2(train_pred, list(ames_train), ~ {\n      bind_cols(.x, .y) %>%\n        metrics(truth = Sale_Price, estimate = .pred)\n    }),\n    test_metrics = map2(test_pred, list(ames_test), ~ {\n      bind_cols(.x, .y) %>%\n        metrics(truth = Sale_Price, estimate = .pred)\n    })\n  ) %>%\n  select(model, train_metrics, test_metrics) %>%\n  pivot_longer(c(train_metrics, test_metrics), \n               names_to = \"dataset\", values_to = \"metrics\") %>%\n  unnest(metrics)\n\n# Visualize comparison\nggplot(comparison_metrics, \n       aes(x = model, y = .estimate, fill = dataset)) +\n  geom_col(position = \"dodge\") +\n  facet_wrap(~.metric, scales = \"free_y\") +\n  scale_fill_manual(values = c(\"train_metrics\" = \"lightblue\", \n                               \"test_metrics\" = \"darkblue\")) +\n  labs(\n    title = \"Linear vs Polynomial Model Comparison\",\n    subtitle = \"Watch for overfitting: train performance much better than test\",\n    y = \"Metric Value\"\n  )\n```\n\n::: {.cell-output-display}\n![](15-regression_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n## Regularization: Ridge, Lasso, and Elastic Net\n\nRegularization adds a penalty term to the loss function to prevent overfitting and handle multicollinearity.\n\n### Understanding Regularization Mathematics\n\nThe three main types of regularization are:\n\n**Ridge Regression (L2 penalty):**\n$$\\text{Loss} = \\text{RSS} + \\lambda \\sum_{j=1}^{p} \\beta_j^2$$\n\nRidge regression shrinks coefficients toward zero but never exactly to zero. It's good when you have many predictors with small effects.\n\n**Lasso Regression (L1 penalty):**\n$$\\text{Loss} = \\text{RSS} + \\lambda \\sum_{j=1}^{p} |\\beta_j|$$\n\nLasso can shrink coefficients exactly to zero, performing automatic feature selection. It's good when you believe only a subset of predictors are important.\n\n**Elastic Net (Combination):**\n$$\\text{Loss} = \\text{RSS} + \\lambda \\left[ \\alpha \\sum_{j=1}^{p} |\\beta_j| + (1-\\alpha) \\sum_{j=1}^{p} \\beta_j^2 \\right]$$\n\nElastic Net combines Ridge and Lasso, controlled by the mixing parameter Î±.\n\nLet's visualize how regularization affects coefficients:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Prepare data for regularization\names_reg <- ames_train %>%\n  select(Sale_Price, Gr_Liv_Area, Garage_Area, Year_Built, \n         Lot_Area, Total_Bsmt_SF, First_Flr_SF) %>%\n  drop_na()\n\n# Create model matrix\nX <- model.matrix(Sale_Price ~ . - 1, data = ames_reg)\ny <- ames_reg$Sale_Price\n\n# Fit Ridge regression\nridge_fit <- glmnet(X, y, alpha = 0)  # alpha = 0 for Ridge\n\n# Fit Lasso regression\nlasso_fit <- glmnet(X, y, alpha = 1)  # alpha = 1 for Lasso\n\n# Fit Elastic Net\nelastic_fit <- glmnet(X, y, alpha = 0.5)  # alpha = 0.5 for 50/50 mix\n\n# Create coefficient path plots\npar(mfrow = c(1, 3))\nplot(ridge_fit, xvar = \"lambda\", main = \"Ridge Regression Path\")\nplot(lasso_fit, xvar = \"lambda\", main = \"Lasso Regression Path\")\nplot(elastic_fit, xvar = \"lambda\", main = \"Elastic Net Path\")\n```\n\n::: {.cell-output-display}\n![](15-regression_files/figure-html/unnamed-chunk-8-1.png){width=1344}\n:::\n:::\n\n\nThese plots show how coefficients change as the penalty parameter Î» increases:\n- **Ridge**: All coefficients shrink smoothly toward zero but never reach it\n- **Lasso**: Coefficients can hit zero abruptly (feature selection)\n- **Elastic Net**: Combination of both behaviors\n\n### Implementing Regularized Regression\n\nNow let's implement these methods properly with tidymodels:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a more complete recipe\nreg_recipe <- recipe(Sale_Price ~ ., data = ames_train) %>%\n  step_nzv(all_predictors()) %>%\n  step_impute_median(all_numeric_predictors()) %>%\n  step_impute_mode(all_nominal_predictors()) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_normalize(all_numeric_predictors())\n\n# Ridge regression\nridge_spec <- linear_reg(penalty = 0.01, mixture = 0) %>%\n  set_engine(\"glmnet\") %>%\n  set_mode(\"regression\")\n\n# Lasso regression\nlasso_spec <- linear_reg(penalty = 0.01, mixture = 1) %>%\n  set_engine(\"glmnet\") %>%\n  set_mode(\"regression\")\n\n# Elastic Net\nelastic_spec <- linear_reg(penalty = 0.01, mixture = 0.5) %>%\n  set_engine(\"glmnet\") %>%\n  set_mode(\"regression\")\n\n# Create workflows\nridge_wf <- workflow() %>%\n  add_recipe(reg_recipe) %>%\n  add_model(ridge_spec)\n\nlasso_wf <- workflow() %>%\n  add_recipe(reg_recipe) %>%\n  add_model(lasso_spec)\n\nelastic_wf <- workflow() %>%\n  add_recipe(reg_recipe) %>%\n  add_model(elastic_spec)\n\n# Fit all models\nridge_fit <- ridge_wf %>% fit(ames_train)\nlasso_fit <- lasso_wf %>% fit(ames_train)\nelastic_fit <- elastic_wf %>% fit(ames_train)\n\n# Compare number of non-zero coefficients\nn_features <- tibble(\n  Model = c(\"Ridge\", \"Lasso\", \"Elastic Net\"),\n  `Non-zero Coefficients` = c(\n    sum(tidy(ridge_fit)$estimate != 0),\n    sum(tidy(lasso_fit)$estimate != 0),\n    sum(tidy(elastic_fit)$estimate != 0)\n  ),\n  `Total Features` = c(\n    nrow(tidy(ridge_fit)),\n    nrow(tidy(lasso_fit)),\n    nrow(tidy(elastic_fit))\n  )\n)\n\nknitr::kable(n_features)\n```\n\n::: {.cell-output-display}\n\n\n|Model       | Non-zero Coefficients| Total Features|\n|:-----------|---------------------:|--------------:|\n|Ridge       |                   203|            210|\n|Lasso       |                    34|            210|\n|Elastic Net |                    52|            210|\n\n\n:::\n:::\n\n\nNotice how Lasso has fewer non-zero coefficients - it's performing feature selection automatically!\n\n## Regression Trees and Random Forests\n\nTree-based methods can capture non-linear relationships and interactions without explicitly specifying them.\n\n### How Regression Trees Work\n\nRegression trees recursively partition the feature space into rectangles, predicting the mean value in each region. The splitting criterion for regression is typically the reduction in RSS:\n\n$$\\text{RSS} = \\sum_{i \\in R_1} (y_i - \\hat{y}_{R_1})^2 + \\sum_{i \\in R_2} (y_i - \\hat{y}_{R_2})^2$$\n\nWhere $R_1$ and $R_2$ are the two regions created by the split.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit a simple regression tree\ntree_spec <- decision_tree(\n  cost_complexity = 0.01,\n  tree_depth = 4,\n  min_n = 20\n) %>%\n  set_engine(\"rpart\") %>%\n  set_mode(\"regression\")\n\ntree_fit <- tree_spec %>%\n  fit(Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames_train)\n\n# Visualize the tree\nif (require(rpart.plot, quietly = TRUE)) {\n  rpart.plot(tree_fit$fit, type = 4, extra = 1, roundint = FALSE)\n}\n\n# Create prediction surface\ngrid <- expand_grid(\n  Gr_Liv_Area = seq(min(ames_train$Gr_Liv_Area, na.rm = TRUE),\n                     max(ames_train$Gr_Liv_Area, na.rm = TRUE),\n                     length.out = 100),\n  Year_Built = seq(min(ames_train$Year_Built, na.rm = TRUE),\n                   max(ames_train$Year_Built, na.rm = TRUE),\n                   length.out = 100)\n)\n\ngrid_pred <- tree_fit %>%\n  predict(grid) %>%\n  bind_cols(grid)\n\n# Visualize prediction surface\nggplot(grid_pred, aes(x = Gr_Liv_Area, y = Year_Built, fill = .pred)) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  geom_point(data = sample_n(ames_train, 200), \n             aes(fill = NULL, color = Sale_Price), \n             size = 1, alpha = 0.5) +\n  scale_color_viridis_c() +\n  labs(\n    title = \"Regression Tree Prediction Surface\",\n    subtitle = \"Notice the rectangular regions - characteristic of tree-based methods\",\n    x = \"Living Area (sq ft)\",\n    y = \"Year Built\",\n    fill = \"Predicted\\nlog(Price)\",\n    color = \"Actual\\nlog(Price)\"\n  )\n```\n\n::: {.cell-output-display}\n![](15-regression_files/figure-html/unnamed-chunk-10-1.png){width=1152}\n:::\n:::\n\n\nThe rectangular regions are a key characteristic of tree-based methods. Each rectangle represents a leaf node in the tree, with all observations in that region receiving the same prediction.\n\n### Random Forests for Regression\n\nRandom Forests improve upon single trees by:\n1. Building many trees on bootstrap samples (bagging)\n2. Randomly selecting features at each split\n3. Averaging predictions across all trees\n\nThis reduces overfitting and improves generalization:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Random Forest specification\nrf_spec <- rand_forest(\n  trees = 500,\n  mtry = 10,\n  min_n = 5\n) %>%\n  set_engine(\"ranger\", importance = \"impurity\") %>%\n  set_mode(\"regression\")\n\n# Create workflow\nrf_workflow <- workflow() %>%\n  add_recipe(reg_recipe) %>%\n  add_model(rf_spec)\n\n# Fit with cross-validation for better evaluation\nset.seed(123)\names_folds <- vfold_cv(ames_train, v = 5)\n\nrf_cv <- rf_workflow %>%\n  fit_resamples(\n    resamples = ames_folds,\n    metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq, yardstick::mae),\n    control = control_resamples(save_pred = TRUE)\n  )\n\n# Show performance\ncollect_metrics(rf_cv) %>%\n  knitr::kable(digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|.metric |.estimator |   mean|  n| std_err|.config         |\n|:-------|:----------|------:|--:|-------:|:---------------|\n|mae     |standard   | 0.0433|  5|  0.0011|pre0_mod0_post0 |\n|rmse    |standard   | 0.0686|  5|  0.0032|pre0_mod0_post0 |\n|rsq     |standard   | 0.8656|  5|  0.0074|pre0_mod0_post0 |\n\n\n:::\n\n```{.r .cell-code}\n# Fit final model for feature importance\nrf_final <- rf_workflow %>%\n  fit(ames_train)\n\n# Extract and plot feature importance\nrf_importance <- rf_final %>%\n  extract_fit_parsnip() %>%\n  vip(num_features = 20)\n\nrf_importance +\n  labs(title = \"Top 20 Most Important Features in Random Forest\",\n       subtitle = \"Based on impurity reduction\")\n```\n\n::: {.cell-output-display}\n![](15-regression_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nRandom Forests often provide excellent predictive performance with minimal tuning, making them a go-to method for many regression problems.\n\n## Model Diagnostics and Validation\n\nProper model validation is crucial for ensuring your model will generalize well to new data.\n\n### Residual Analysis\n\nResidual analysis helps identify problems with model assumptions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get predictions from our linear model\nlm_pred <- lm_fit %>%\n  predict(ames_test) %>%\n  bind_cols(ames_test) %>%\n  mutate(\n    residual = Sale_Price - .pred,\n    std_residual = residual / sd(residual)\n  )\n\n# Create diagnostic plots\np1 <- ggplot(lm_pred, aes(x = .pred, y = residual)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"blue\") +\n  labs(\n    title = \"Residuals vs Fitted Values\",\n    subtitle = \"Look for patterns - should be random scatter around zero\",\n    x = \"Fitted Values\",\n    y = \"Residuals\"\n  )\n\np2 <- ggplot(lm_pred, aes(sample = std_residual)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  labs(\n    title = \"Q-Q Plot\",\n    subtitle = \"Check normality - points should follow the line\",\n    x = \"Theoretical Quantiles\",\n    y = \"Standardized Residuals\"\n  )\n\np3 <- ggplot(lm_pred, aes(x = .pred, y = sqrt(abs(std_residual)))) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"blue\") +\n  labs(\n    title = \"Scale-Location Plot\",\n    subtitle = \"Check homoscedasticity - spread should be constant\",\n    x = \"Fitted Values\",\n    y = \"âˆš|Standardized Residuals|\"\n  )\n\np4 <- ggplot(lm_pred, aes(x = residual)) +\n  geom_histogram(bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  geom_density(aes(y = after_stat(count)), color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Distribution of Residuals\",\n    subtitle = \"Should be approximately normal\",\n    x = \"Residual\",\n    y = \"Count\"\n  )\n\n(p1 + p2) / (p3 + p4)\n```\n\n::: {.cell-output-display}\n![](15-regression_files/figure-html/unnamed-chunk-12-1.png){width=1344}\n:::\n:::\n\n\nEach plot reveals different aspects:\n- **Pattern in residuals vs fitted**: Indicates non-linearity\n- **Deviation from Q-Q line**: Indicates non-normality\n- **Funnel shape in scale-location**: Indicates heteroscedasticity\n- **Skewed residual distribution**: May need transformation\n\n### Cross-Validation for Model Selection\n\nCross-validation provides unbiased estimates of model performance:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare multiple models using cross-validation\nmodels_list <- list(\n  \"Linear\" = lm_spec,\n  \"Ridge\" = ridge_spec,\n  \"Lasso\" = lasso_spec,\n  \"Random Forest\" = rf_spec,\n  \"Decision Tree\" = tree_spec\n)\n\n# Evaluate all models\ncv_results <- map_df(names(models_list), function(model_name) {\n  wf <- workflow() %>%\n    add_recipe(reg_recipe) %>%\n    add_model(models_list[[model_name]])\n  \n  fit_resamples(\n    wf,\n    resamples = ames_folds,\n    metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq, yardstick::mae)\n  ) %>%\n    collect_metrics() %>%\n    mutate(model = model_name)\n})\n\n# Visualize comparison\nggplot(cv_results, aes(x = model, y = mean, fill = model)) +\n  geom_col() +\n  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), \n                width = 0.2) +\n  facet_wrap(~.metric, scales = \"free_y\") +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"Model Comparison via 5-Fold Cross-Validation\",\n    subtitle = \"Error bars show standard error across folds\",\n    y = \"Metric Value\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](15-regression_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n## Advanced Topics\n\n### Dealing with Multicollinearity\n\nMulticollinearity occurs when predictors are highly correlated, making coefficient interpretation difficult:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check correlation among numeric predictors\nnumeric_features <- ames_train %>%\n  select(where(is.numeric)) %>%\n  select(-Sale_Price) %>%\n  drop_na()\n\ncor_matrix <- cor(numeric_features)\n\n# Find highly correlated pairs\nhigh_cor <- which(abs(cor_matrix) > 0.8 & cor_matrix != 1, arr.ind = TRUE)\nhigh_cor_pairs <- tibble(\n  var1 = rownames(cor_matrix)[high_cor[,1]],\n  var2 = colnames(cor_matrix)[high_cor[,2]],\n  correlation = cor_matrix[high_cor]\n) %>%\n  filter(var1 < var2) %>%  # Remove duplicates\n  arrange(desc(abs(correlation)))\n\nprint(\"Highly correlated variable pairs:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Highly correlated variable pairs:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(high_cor_pairs, 10) %>% knitr::kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|var1        |var2          | correlation|\n|:-----------|:-------------|-----------:|\n|Garage_Area |Garage_Cars   |       0.892|\n|Gr_Liv_Area |TotRms_AbvGrd |       0.809|\n\n\n:::\n\n```{.r .cell-code}\n# Visualize correlation matrix\ncorrplot(cor_matrix, method = \"color\", type = \"upper\", \n         order = \"hclust\", tl.cex = 0.6, tl.col = \"black\",\n         diag = FALSE, addCoef.col = \"black\", number.cex = 0.4)\n```\n\n::: {.cell-output-display}\n![](15-regression_files/figure-html/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\nHigh correlations between predictors can cause:\n- Unstable coefficient estimates\n- Large standard errors\n- Difficulty interpreting individual effects\n\nSolutions include:\n- Remove one of the correlated variables\n- Use PCA to create uncorrelated components\n- Apply regularization (Ridge regression handles multicollinearity well)\n\n### Handling Non-constant Variance (Heteroscedasticity)\n\nWhen variance changes with the fitted values, we can apply transformations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example with original scale prices (not log-transformed)\names_original <- ames %>%\n  select(Sale_Price, Gr_Liv_Area, Year_Built, Overall_Cond) %>%\n  drop_na()\n\n# Fit model on original scale\noriginal_fit <- lm(Sale_Price ~ ., data = ames_original)\n\n# Fit model on log scale\nlog_fit <- lm(log(Sale_Price) ~ ., data = ames_original)\n\n# Compare residual plots\norig_pred <- tibble(\n  fitted = fitted(original_fit),\n  residual = residuals(original_fit),\n  model = \"Original Scale\"\n)\n\nlog_pred <- tibble(\n  fitted = fitted(log_fit),\n  residual = residuals(log_fit),\n  model = \"Log Scale\"\n)\n\ncombined_pred <- bind_rows(orig_pred, log_pred)\n\nggplot(combined_pred, aes(x = fitted, y = residual)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"blue\") +\n  facet_wrap(~model, scales = \"free\") +\n  labs(\n    title = \"Effect of Log Transformation on Heteroscedasticity\",\n    subtitle = \"Log transformation often stabilizes variance\",\n    x = \"Fitted Values\",\n    y = \"Residuals\"\n  )\n```\n\n::: {.cell-output-display}\n![](15-regression_files/figure-html/unnamed-chunk-15-1.png){width=1152}\n:::\n:::\n\n\nThe log transformation has stabilized the variance - notice how the spread of residuals is more constant in the log scale model.\n\n## Practical Example: Complete Regression Pipeline\n\nLet's put everything together in a comprehensive example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a complete modeling pipeline\n# 1. Data preparation\nconcrete_clean <- concrete %>%\n  drop_na()\n\nconcrete_split <- initial_split(concrete_clean, prop = 0.8)\nconcrete_train <- training(concrete_split)\nconcrete_test <- testing(concrete_split)\n\n# 2. Exploratory data analysis\nconcrete_summary <- concrete_train %>%\n  pivot_longer(everything(), names_to = \"variable\", values_to = \"value\") %>%\n  group_by(variable) %>%\n  summarise(\n    mean = mean(value),\n    sd = sd(value),\n    min = min(value),\n    max = max(value)\n  )\n\nprint(\"Dataset summary:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Dataset summary:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nconcrete_summary %>% knitr::kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n|variable             |   mean|     sd|    min|     max|\n|:--------------------|------:|------:|------:|-------:|\n|age                  |  45.85|  64.08|   1.00|  365.00|\n|blast_furnace_slag   |  71.36|  84.29|   0.00|  359.40|\n|cement               | 283.40| 104.81| 102.00|  540.00|\n|coarse_aggregate     | 970.40|  76.68| 801.00| 1134.30|\n|compressive_strength |  35.56|  16.71|   2.33|   81.75|\n|fine_aggregate       | 773.44|  81.49| 594.00|  992.60|\n|fly_ash              |  54.42|  63.98|   0.00|  200.10|\n|superplasticizer     |   6.13|   5.86|   0.00|   32.20|\n|water                | 182.12|  21.78| 121.80|  247.00|\n\n\n:::\n\n```{.r .cell-code}\n# 3. Create recipe with feature engineering\nconcrete_recipe <- recipe(compressive_strength ~ ., data = concrete_train) %>%\n  step_normalize(all_predictors()) %>%\n  step_poly(age, degree = 2) %>%\n  step_interact(terms = ~ cement:water)\n\n# 4. Specify multiple models\nmodels <- list(\n  linear = linear_reg() %>% set_engine(\"lm\"),\n  ridge = linear_reg(penalty = tune(), mixture = 0) %>% set_engine(\"glmnet\"),\n  rf = rand_forest(mtry = tune(), trees = 500, min_n = tune()) %>% \n    set_engine(\"ranger\") %>% set_mode(\"regression\")\n)\n\n# 5. Create workflows\nworkflows <- map(models, function(model) {\n  workflow() %>%\n    add_recipe(concrete_recipe) %>%\n    add_model(model)\n})\n\n# 6. Tune hyperparameters for models that need it\nconcrete_folds <- vfold_cv(concrete_train, v = 5)\n\n# Tune Ridge\nridge_grid <- grid_regular(penalty(), levels = 10)\nridge_tune <- workflows$ridge %>%\n  tune_grid(\n    resamples = concrete_folds,\n    grid = ridge_grid,\n    metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq)\n  )\n\n# Tune Random Forest\nrf_grid <- grid_regular(\n  mtry(range = c(2, 7)),\n  min_n(),\n  levels = 5\n)\nrf_tune <- workflows$rf %>%\n  tune_grid(\n    resamples = concrete_folds,\n    grid = rf_grid,\n    metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq)\n  )\n\n# 7. Select best models\nbest_ridge <- select_best(ridge_tune, metric = \"rmse\")\nbest_rf <- select_best(rf_tune, metric = \"rmse\")\n\n# 8. Finalize workflows\nfinal_linear <- workflows$linear\nfinal_ridge <- workflows$ridge %>% finalize_workflow(best_ridge)\nfinal_rf <- workflows$rf %>% finalize_workflow(best_rf)\n\n# 9. Fit final models and evaluate\nfinal_fits <- list(\n  linear = final_linear %>% fit(concrete_train),\n  ridge = final_ridge %>% fit(concrete_train),\n  rf = final_rf %>% fit(concrete_train)\n)\n\n# 10. Make predictions and evaluate\ntest_results <- map_df(names(final_fits), function(model_name) {\n  final_fits[[model_name]] %>%\n    predict(concrete_test) %>%\n    bind_cols(concrete_test) %>%\n    metrics(truth = compressive_strength, estimate = .pred) %>%\n    mutate(model = model_name)\n})\n\n# Visualize final comparison\nggplot(test_results, aes(x = model, y = .estimate, fill = model)) +\n  geom_col() +\n  facet_wrap(~.metric, scales = \"free_y\") +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"Final Model Comparison on Test Set\",\n    subtitle = \"Concrete compressive strength prediction\",\n    y = \"Metric Value\"\n  ) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](15-regression_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n## Exercises\n\n### Exercise 1: Implement Regularization with Tuning\n\nBuild a regularized regression model with proper hyperparameter tuning:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Use the Ames dataset to predict Sale_Price\n# Create an elastic net model with tuned penalty and mixture\n\n# Simpler recipe for exercise\nelastic_recipe <- recipe(Sale_Price ~ Gr_Liv_Area + Year_Built + Overall_Cond + \n                        Neighborhood + Total_Bsmt_SF, data = ames_train) %>%\n  step_impute_median(all_numeric_predictors()) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_normalize(all_numeric_predictors())\n\n# Elastic net specification with tuning\nelastic_tune_spec <- linear_reg(\n  penalty = tune(),\n  mixture = tune()\n) %>%\n  set_engine(\"glmnet\") %>%\n  set_mode(\"regression\")\n\n# Workflow\nelastic_tune_wf <- workflow() %>%\n  add_recipe(elastic_recipe) %>%\n  add_model(elastic_tune_spec)\n\n# Tuning grid\nelastic_grid <- grid_regular(\n  penalty(range = c(-3, 0)),\n  mixture(range = c(0, 1)),\n  levels = c(10, 5)\n)\n\n# Create resamples\names_folds <- vfold_cv(ames_train, v = 5)\n\n# Tune the model\nelastic_tuned <- elastic_tune_wf %>%\n  tune_grid(\n    resamples = ames_folds,\n    grid = elastic_grid,\n    metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq, yardstick::mae)\n  )\n\n# Visualize tuning results\nautoplot(elastic_tuned) +\n  labs(title = \"Elastic Net Tuning Results\",\n       subtitle = \"Performance across different penalty and mixture values\")\n```\n\n::: {.cell-output-display}\n![](15-regression_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Select and fit best model\nbest_elastic <- select_best(elastic_tuned, metric = \"rmse\")\nfinal_elastic <- elastic_tune_wf %>%\n  finalize_workflow(best_elastic) %>%\n  fit(ames_train)\n\n# Evaluate on test set\nelastic_test_pred <- final_elastic %>%\n  predict(ames_test) %>%\n  bind_cols(ames_test)\n\nelastic_metrics <- elastic_test_pred %>%\n  metrics(truth = Sale_Price, estimate = .pred)\n\nprint(\"Best hyperparameters:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Best hyperparameters:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(best_elastic)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 3\n  penalty mixture .config         \n    <dbl>   <dbl> <chr>           \n1   0.001    0.25 pre0_mod02_post0\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"Test set performance:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Test set performance:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(elastic_metrics)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      0.0752\n2 rsq     standard      0.816 \n3 mae     standard      0.0479\n```\n\n\n:::\n:::\n\n\n### Exercise 2: Diagnose and Fix Model Problems\n\nIdentify and address issues in a regression model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Create a problematic model and fix it\n\n# Intentionally problematic model (using highly correlated predictors)\nproblem_data <- ames_train %>%\n  select(Sale_Price, Gr_Liv_Area, Total_Bsmt_SF, First_Flr_SF, \n         Second_Flr_SF, Garage_Area, Garage_Cars) %>%\n  drop_na()\n\n# Check correlations\ncor(problem_data[,-1]) %>%\n  corrplot(method = \"number\", type = \"upper\")\n```\n\n::: {.cell-output-display}\n![](15-regression_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Fit problematic model\nproblem_fit <- lm(Sale_Price ~ ., data = problem_data)\nsummary(problem_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Sale_Price ~ ., data = problem_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.05511 -0.03523  0.01104  0.05335  0.31904 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    4.740e+00  7.247e-03 654.021  < 2e-16 ***\nGr_Liv_Area   -5.079e-05  4.297e-05  -1.182   0.2373    \nTotal_Bsmt_SF  1.368e-04  7.725e-06  17.707  < 2e-16 ***\nFirst_Flr_SF   1.723e-04  4.409e-05   3.908 9.60e-05 ***\nSecond_Flr_SF  1.969e-04  4.341e-05   4.537 6.02e-06 ***\nGarage_Area    3.874e-05  2.156e-05   1.797   0.0725 .  \nGarage_Cars    7.311e-02  6.039e-03  12.106  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09484 on 2190 degrees of freedom\nMultiple R-squared:  0.7168,\tAdjusted R-squared:  0.716 \nF-statistic: 923.8 on 6 and 2190 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n# Note the high VIF (Variance Inflation Factor)\nif (require(car, quietly = TRUE)) {\n  vif_values <- car::vif(problem_fit)\n  print(\"Variance Inflation Factors:\")\n  print(vif_values)\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Variance Inflation Factors:\"\n  Gr_Liv_Area Total_Bsmt_SF  First_Flr_SF Second_Flr_SF   Garage_Area \n   112.172194      2.772377     73.222425     84.197910      5.207468 \n  Garage_Cars \n     5.107801 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Fix 1: Remove highly correlated variables\nfixed_data1 <- problem_data %>%\n  select(-Garage_Cars, -Second_Flr_SF)  # Remove redundant variables\n\nfixed_fit1 <- lm(Sale_Price ~ ., data = fixed_data1)\n\n# Fix 2: Use PCA\npca_recipe <- recipe(Sale_Price ~ ., data = problem_data) %>%\n  step_normalize(all_predictors()) %>%\n  step_pca(all_predictors(), num_comp = 4)\n\npca_fit <- workflow() %>%\n  add_recipe(pca_recipe) %>%\n  add_model(lm_spec) %>%\n  fit(problem_data)\n\n# Fix 3: Use Ridge regression\nridge_fix <- linear_reg(penalty = 0.1, mixture = 0) %>%\n  set_engine(\"glmnet\") %>%\n  fit(Sale_Price ~ ., data = problem_data)\n\n# Compare solutions\ncat(\"Original model R-squared:\", summary(problem_fit)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOriginal model R-squared: 0.7167839 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Fixed model R-squared:\", summary(fixed_fit1)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFixed model R-squared: 0.6943782 \n```\n\n\n:::\n:::\n\n\n### Exercise 3: Non-linear Relationships\n\nExplore and model non-linear relationships:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Create synthetic data with non-linear relationship\nset.seed(456)\nnonlinear_ex <- tibble(\n  x = seq(0, 10, length.out = 200),\n  y = 5 + 3*sin(x) + 0.5*x^2 - 0.05*x^3 + rnorm(200, sd = 1)\n)\n\n# Visualize the relationship\nggplot(nonlinear_ex, aes(x = x, y = y)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"blue\") +\n  labs(title = \"Non-linear Relationship to Model\")\n```\n\n::: {.cell-output-display}\n![](15-regression_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Compare different approaches\n# 1. Linear model (will underfit)\nlinear_ex <- lm(y ~ x, data = nonlinear_ex)\n\n# 2. Polynomial regression\npoly_ex <- lm(y ~ poly(x, 5), data = nonlinear_ex)\n\n# 3. Spline regression\nspline_ex <- lm(y ~ splines::ns(x, df = 5), data = nonlinear_ex)\n\n# 4. Random Forest\nrf_ex <- rand_forest(trees = 100) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"regression\") %>%\n  fit(y ~ x, data = nonlinear_ex)\n\n# Generate predictions\npredictions_ex <- nonlinear_ex %>%\n  mutate(\n    linear = predict(linear_ex, nonlinear_ex),\n    polynomial = predict(poly_ex, nonlinear_ex),\n    spline = predict(spline_ex, nonlinear_ex),\n    rf = predict(rf_ex, nonlinear_ex)$.pred\n  ) %>%\n  pivot_longer(c(linear, polynomial, spline, rf), \n               names_to = \"model\", values_to = \"prediction\")\n\n# Visualize all models\nggplot(predictions_ex, aes(x = x)) +\n  geom_point(aes(y = y), alpha = 0.2) +\n  geom_line(aes(y = prediction, color = model), linewidth = 1.2) +\n  facet_wrap(~model) +\n  scale_color_viridis_d() +\n  labs(\n    title = \"Comparison of Methods for Non-linear Relationships\",\n    subtitle = \"Different approaches to capturing non-linearity\"\n  ) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](15-regression_files/figure-html/unnamed-chunk-19-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Calculate performance\nperformance_ex <- predictions_ex %>%\n  group_by(model) %>%\n  summarise(\n    rmse = sqrt(mean((y - prediction)^2)),\n    mae = mean(abs(y - prediction)),\n    r_squared = cor(y, prediction)^2\n  )\n\nperformance_ex %>% knitr::kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|model      |  rmse|   mae| r_squared|\n|:----------|-----:|-----:|---------:|\n|linear     | 2.597| 2.011|     0.173|\n|polynomial | 1.074| 0.865|     0.858|\n|rf         | 0.582| 0.472|     0.959|\n|spline     | 0.980| 0.808|     0.882|\n\n\n:::\n:::\n\n\n## Summary\n\nIn this comprehensive chapter, you've learned:\n\nâœ… **Linear regression theory**: OLS, assumptions, diagnostics  \nâœ… **Polynomial and non-linear regression**: Capturing curved relationships  \nâœ… **Regularization techniques**: Ridge, Lasso, and Elastic Net  \nâœ… **Tree-based methods**: Decision trees and Random Forests  \nâœ… **Model validation**: Cross-validation, residual analysis  \nâœ… **Advanced topics**: Multicollinearity, heteroscedasticity  \nâœ… **Complete workflows**: From data preparation to final evaluation  \n\nKey takeaways:\n- Always check model assumptions before interpreting results\n- Use regularization when you have many predictors or multicollinearity\n- Tree-based methods are powerful for capturing non-linear patterns\n- Cross-validation is essential for honest model evaluation\n- Different problems require different approaches - no single best method\n\n## What's Next?\n\nIn [Chapter 16](16-ensemble-methods.Rmd), we'll explore ensemble methods in depth, learning how to combine multiple models for superior performance.\n\n## Additional Resources\n\n- [An Introduction to Statistical Learning](https://www.statlearning.com/) - Chapters 3, 6, 8\n- [The Elements of Statistical Learning](https://hastie.su.domains/ElemStatLearn/) - Chapters 3, 4, 15\n- [Applied Predictive Modeling](http://appliedpredictivemodeling.com/) - Chapters 6-8\n- [Regression and Other Stories](https://avehtari.github.io/ROS-Examples/) - Comprehensive regression guide\n- [Tidy Modeling with R](https://www.tmwr.org/) - Chapters 7-11\n",
    "supporting": [
      "15-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}