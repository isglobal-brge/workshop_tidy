{
  "hash": "9ce23cba776865e407bcacbd78a97552",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 9: Data Splitting and Resampling - The Foundation of Model Validation\"\nauthor: \"David Sarrat GonzÃ¡lez, Juan R GonzÃ¡lez\"\ndate: today\nformat:\n  html:\n    code-fold: false\n    code-tools: true\n---\n\n## Learning Objectives\n\nBy the end of this chapter, you will master:\n\n- The critical importance of proper data splitting\n- Train/test/validation splits and when to use them\n- Stratified sampling for balanced splits\n- Cross-validation techniques and theory\n- Bootstrap methods for uncertainty estimation\n- Time series splitting strategies\n- Nested resampling for unbiased evaluation\n- Best practices and common pitfalls\n\n::: {.callout-tip}\n## Download R Script\nYou can download the complete R code for this chapter:\n[ðŸ“¥ Download 09-data-splitting.R](R_scripts/09-data-splitting.R){.btn .btn-primary download=\"09-data-splitting.R\"}\n:::\n\n## Why Data Splitting Matters\n\nImagine you're studying for an exam. If you only practice with questions you've already seen and memorized the answers to, you might think you understand the material perfectly. But when you face new questions on the actual exam, you realize you've only memorized specific answers rather than understanding the concepts. This is exactly what happens in machine learning when we don't properly split our data.\n\n### The Fundamental Problem: Overfitting\n\nWhen we train a model on data and evaluate it on the same data, we get an overly optimistic estimate of performance. The model has essentially \"memorized\" the training data, including its noise and peculiarities.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(modeldata)\nlibrary(vip)\nlibrary(patchwork)\n\n# Set theme and seed\ntheme_set(theme_minimal())\nset.seed(123)\n\n# Demonstrate overfitting with a simple example\nn <- 100\nsimple_data <- tibble(\n  x = seq(0, 10, length.out = n),\n  y_true = 2 * x + 5,  # True relationship\n  y = y_true + rnorm(n, sd = 3)  # Add noise\n)\n\n# Fit increasingly complex models\nmodels <- list(\n  linear = lm(y ~ x, data = simple_data),\n  poly5 = lm(y ~ poly(x, 5), data = simple_data),\n  poly15 = lm(y ~ poly(x, 15), data = simple_data)\n)\n\n# Calculate training error (biased!)\ntraining_errors <- map_dbl(models, ~ sqrt(mean(residuals(.)^2)))\n\n# Generate new test data\ntest_data <- tibble(\n  x = seq(0, 10, length.out = 50),\n  y_true = 2 * x + 5,\n  y = y_true + rnorm(50, sd = 3)\n)\n\n# Calculate test error (unbiased)\ntest_errors <- map_dbl(models, ~ {\n  predictions <- predict(., newdata = test_data)\n  sqrt(mean((test_data$y - predictions)^2))\n})\n\n# Compare errors\nerror_comparison <- tibble(\n  Model = c(\"Linear\", \"Polynomial (5)\", \"Polynomial (15)\"),\n  `Training RMSE` = training_errors,\n  `Test RMSE` = test_errors,\n  `Overfit Amount` = test_errors - training_errors\n)\n\nknitr::kable(error_comparison, digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n|Model           | Training RMSE| Test RMSE| Overfit Amount|\n|:---------------|-------------:|---------:|--------------:|\n|Linear          |          2.72|      3.12|           0.40|\n|Polynomial (5)  |          2.68|      3.19|           0.51|\n|Polynomial (15) |          2.46|      3.22|           0.76|\n\n\n:::\n\n```{.r .cell-code}\n# Visualize the fits\nplot_data <- simple_data %>%\n  mutate(\n    linear = predict(models$linear),\n    poly5 = predict(models$poly5),\n    poly15 = predict(models$poly15)\n  )\n\nggplot(plot_data, aes(x = x)) +\n  geom_point(aes(y = y), alpha = 0.5) +\n  geom_line(aes(y = y_true), color = \"black\", linewidth = 1, linetype = \"dashed\") +\n  geom_line(aes(y = linear, color = \"Linear\"), linewidth = 1) +\n  geom_line(aes(y = poly5, color = \"Poly(5)\"), linewidth = 1) +\n  geom_line(aes(y = poly15, color = \"Poly(15)\"), linewidth = 1) +\n  scale_color_manual(values = c(\"Linear\" = \"blue\", \"Poly(5)\" = \"green\", \"Poly(15)\" = \"red\")) +\n  labs(\n    title = \"Model Complexity and Overfitting\",\n    subtitle = \"Complex models fit training data better but may generalize worse\",\n    x = \"X\", y = \"Y\", color = \"Model\"\n  )\n```\n\n::: {.cell-output-display}\n![](09-data-splitting_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nNotice how the complex polynomial model (degree 15) has lower training error but higher test error - classic overfitting! This is why we need proper data splitting strategies.\n\n## The Train/Test Split\n\nThe simplest approach is to split data into training and testing sets. The training set is used to fit the model, and the test set provides an unbiased estimate of performance on new data.\n\n### Basic Splitting with rsample\n\nThe `rsample` package provides powerful tools for data splitting:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the Ames housing data\ndata(ames)\n\n# Basic 75/25 split\names_split <- initial_split(ames, prop = 0.75)\n\n# Extract the datasets\names_train <- training(ames_split)\names_test <- testing(ames_split)\n\n# Check the sizes\nsplit_summary <- tibble(\n  Dataset = c(\"Original\", \"Training\", \"Testing\"),\n  `Number of Rows` = c(nrow(ames), nrow(ames_train), nrow(ames_test)),\n  Percentage = c(100, \n                 nrow(ames_train) / nrow(ames) * 100,\n                 nrow(ames_test) / nrow(ames) * 100)\n)\n\nknitr::kable(split_summary, digits = 1)\n```\n\n::: {.cell-output-display}\n\n\n|Dataset  | Number of Rows| Percentage|\n|:--------|--------------:|----------:|\n|Original |           2930|        100|\n|Training |           2197|         75|\n|Testing  |            733|         25|\n\n\n:::\n\n```{.r .cell-code}\n# The split object contains indices\names_split\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Training/Testing/Total>\n<2197/733/2930>\n```\n\n\n:::\n:::\n\n\nThe `initial_split()` function doesn't just randomly split the data - it's designed with several important features:\n- Reproducible with set.seed()\n- Maintains data structure\n- Can do stratified sampling\n- Preserves data types\n\n### Stratified Sampling\n\nWhen you have imbalanced classes or want to ensure representative splits, use stratified sampling:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a classification example with imbalanced classes\nset.seed(123)\nimbalanced_data <- tibble(\n  feature1 = rnorm(1000),\n  feature2 = rnorm(1000),\n  class = factor(c(rep(\"Common\", 900), rep(\"Rare\", 100)))\n)\n\n# Regular split (might not preserve class balance)\nregular_split <- initial_split(imbalanced_data, prop = 0.75)\nregular_train <- training(regular_split)\n\n# Stratified split (preserves class balance)\nstratified_split <- initial_split(imbalanced_data, prop = 0.75, strata = class)\nstratified_train <- training(stratified_split)\n\n# Compare class distributions\ndistribution_comparison <- bind_rows(\n  imbalanced_data %>% count(class) %>% mutate(Dataset = \"Original\"),\n  regular_train %>% count(class) %>% mutate(Dataset = \"Regular Split\"),\n  stratified_train %>% count(class) %>% mutate(Dataset = \"Stratified Split\")\n) %>%\n  group_by(Dataset) %>%\n  mutate(Percentage = n / sum(n) * 100)\n\nggplot(distribution_comparison, aes(x = Dataset, y = Percentage, fill = class)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Common\" = \"steelblue\", \"Rare\" = \"coral\")) +\n  labs(\n    title = \"Class Distribution: Regular vs Stratified Splitting\",\n    subtitle = \"Stratified sampling preserves class proportions\",\n    y = \"Percentage (%)\"\n  ) +\n  geom_hline(yintercept = c(10, 90), linetype = \"dashed\", alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](09-data-splitting_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nStratified sampling is crucial for:\n- Classification with imbalanced classes\n- Regression with skewed target distributions\n- Ensuring all groups are represented in both sets\n\n### Multi-level Splits: Train/Validation/Test\n\nSometimes we need three sets:\n- **Training**: For model fitting\n- **Validation**: For hyperparameter tuning\n- **Test**: For final, unbiased evaluation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create initial split\ninitial_split <- initial_split(ames, prop = 0.8)\ntrain_val <- training(initial_split)\ntest <- testing(initial_split)\n\n# Split training into train/validation\ntrain_val_split <- initial_split(train_val, prop = 0.75)\ntrain <- training(train_val_split)\nvalidation <- testing(train_val_split)\n\n# Summary of three-way split\nthree_way_summary <- tibble(\n  Dataset = c(\"Training\", \"Validation\", \"Test\", \"Total\"),\n  Rows = c(nrow(train), nrow(validation), nrow(test), nrow(ames)),\n  `Percentage of Total` = c(\n    nrow(train) / nrow(ames) * 100,\n    nrow(validation) / nrow(ames) * 100,\n    nrow(test) / nrow(ames) * 100,\n    100\n  )\n)\n\nknitr::kable(three_way_summary, digits = 1)\n```\n\n::: {.cell-output-display}\n\n\n|Dataset    | Rows| Percentage of Total|\n|:----------|----:|-------------------:|\n|Training   | 1758|                  60|\n|Validation |  586|                  20|\n|Test       |  586|                  20|\n|Total      | 2930|                 100|\n\n\n:::\n\n```{.r .cell-code}\n# Alternative: Use initial_validation_split (tidymodels 1.1.0+)\n# This creates all three splits at once\n# val_split <- initial_validation_split(ames, prop = c(0.6, 0.2))\n# This would give: 60% train, 20% validation, 20% test\n```\n:::\n\n\n## Cross-Validation: Making the Most of Your Data\n\nWhile train/test splits are simple, they have limitations:\n- Only use part of the data for training\n- Results can vary based on the specific split\n- May not be reliable for small datasets\n\nCross-validation addresses these issues by using multiple splits.\n\n### K-Fold Cross-Validation\n\nK-fold CV divides data into k equal parts (folds), trains on k-1 folds, and tests on the remaining fold. This process repeats k times.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create 5-fold cross-validation\names_cv <- vfold_cv(ames_train, v = 5)\names_cv\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#  5-fold cross-validation \n# A tibble: 5 Ã— 2\n  splits             id   \n  <list>             <chr>\n1 <split [1757/440]> Fold1\n2 <split [1757/440]> Fold2\n3 <split [1758/439]> Fold3\n4 <split [1758/439]> Fold4\n5 <split [1758/439]> Fold5\n```\n\n\n:::\n\n```{.r .cell-code}\n# Examine the structure\nfirst_fold <- ames_cv$splits[[1]]\nanalysis(first_fold) %>% nrow()  # Training data for this fold\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1757\n```\n\n\n:::\n\n```{.r .cell-code}\nassessment(first_fold) %>% nrow()  # Test data for this fold\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 440\n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize how data is split across folds\nfold_assignments <- map_df(1:5, function(fold) {\n  assessment_rows <- which(ames_cv$splits[[fold]]$in_id == 0)\n  ames_train %>%\n    mutate(\n      row_id = row_number(),\n      fold = fold,\n      in_assessment = row_id %in% assessment_rows\n    )\n}) %>%\n  filter(row_id <= 100)  # Show first 100 rows for visualization\n\nggplot(fold_assignments, aes(x = row_id, y = factor(fold), fill = in_assessment)) +\n  geom_tile(height = 0.8) +\n  scale_fill_manual(values = c(\"FALSE\" = \"steelblue\", \"TRUE\" = \"coral\"),\n                    labels = c(\"Training\", \"Testing\")) +\n  labs(\n    title = \"5-Fold Cross-Validation Data Assignment\",\n    subtitle = \"Each row is used for testing exactly once\",\n    x = \"Observation ID\", y = \"Fold\", fill = \"Role\"\n  )\n```\n\n::: {.cell-output-display}\n![](09-data-splitting_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nThe mathematics of cross-validation:\n- Each observation is used for testing exactly once\n- Each observation is used for training k-1 times\n- We get k performance estimates\n- Final estimate is the average across folds\n- Standard error provides uncertainty estimate\n\n### Repeated Cross-Validation\n\nFor more stable estimates, we can repeat the CV process with different random splits:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Repeated 5-fold CV (3 repeats = 15 total resamples)\names_repeated_cv <- vfold_cv(ames_train, v = 5, repeats = 3)\names_repeated_cv\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#  5-fold cross-validation repeated 3 times \n# A tibble: 15 Ã— 3\n   splits             id      id2  \n   <list>             <chr>   <chr>\n 1 <split [1757/440]> Repeat1 Fold1\n 2 <split [1757/440]> Repeat1 Fold2\n 3 <split [1758/439]> Repeat1 Fold3\n 4 <split [1758/439]> Repeat1 Fold4\n 5 <split [1758/439]> Repeat1 Fold5\n 6 <split [1757/440]> Repeat2 Fold1\n 7 <split [1757/440]> Repeat2 Fold2\n 8 <split [1758/439]> Repeat2 Fold3\n 9 <split [1758/439]> Repeat2 Fold4\n10 <split [1758/439]> Repeat2 Fold5\n11 <split [1757/440]> Repeat3 Fold1\n12 <split [1757/440]> Repeat3 Fold2\n13 <split [1758/439]> Repeat3 Fold3\n14 <split [1758/439]> Repeat3 Fold4\n15 <split [1758/439]> Repeat3 Fold5\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare variability: single vs repeated CV\n# Fit a simple model to demonstrate\nsimple_model <- linear_reg() %>% set_engine(\"lm\")\nsimple_recipe <- recipe(Sale_Price ~ Gr_Liv_Area + Overall_Cond, data = ames_train)\n\n# Single CV\nsingle_cv_results <- workflow() %>%\n  add_model(simple_model) %>%\n  add_recipe(simple_recipe) %>%\n  fit_resamples(vfold_cv(ames_train, v = 5)) %>%\n  collect_metrics()\n\n# Repeated CV\nrepeated_cv_results <- workflow() %>%\n  add_model(simple_model) %>%\n  add_recipe(simple_recipe) %>%\n  fit_resamples(ames_repeated_cv) %>%\n  collect_metrics()\n\ncomparison <- bind_rows(\n  single_cv_results %>% mutate(Method = \"Single 5-fold CV\"),\n  repeated_cv_results %>% mutate(Method = \"Repeated 5-fold CV (3x)\")\n)\n\nggplot(comparison, aes(x = Method, y = mean, fill = Method)) +\n  geom_col() +\n  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), width = 0.2) +\n  facet_wrap(~.metric, scales = \"free_y\") +\n  labs(\n    title = \"Single vs Repeated Cross-Validation\",\n    subtitle = \"Repeated CV has smaller standard errors (more stable estimates)\",\n    y = \"Metric Value\"\n  ) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](09-data-splitting_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n### Leave-One-Out Cross-Validation (LOOCV)\n\nLOOCV is a special case where k = n (number of observations):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a small dataset for LOOCV demonstration\nsmall_data <- ames_train %>% \n  slice_sample(n = 50)  # LOOCV is computationally expensive\n\n# Create LOOCV splits\nloocv_splits <- loo_cv(small_data)\nloocv_splits\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Leave-one-out cross-validation \n# A tibble: 50 Ã— 2\n   splits         id        \n   <list>         <chr>     \n 1 <split [49/1]> Resample1 \n 2 <split [49/1]> Resample2 \n 3 <split [49/1]> Resample3 \n 4 <split [49/1]> Resample4 \n 5 <split [49/1]> Resample5 \n 6 <split [49/1]> Resample6 \n 7 <split [49/1]> Resample7 \n 8 <split [49/1]> Resample8 \n 9 <split [49/1]> Resample9 \n10 <split [49/1]> Resample10\n# â„¹ 40 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare computational cost\ncv_comparison <- tibble(\n  Method = c(\"5-fold CV\", \"10-fold CV\", \"LOOCV\"),\n  `Number of Models` = c(5, 10, nrow(small_data)),\n  `Training Set Size` = c(\n    floor(nrow(small_data) * 4/5),\n    floor(nrow(small_data) * 9/10),\n    nrow(small_data) - 1\n  ),\n  `Test Set Size` = c(\n    floor(nrow(small_data) / 5),\n    floor(nrow(small_data) / 10),\n    1\n  )\n)\n\nknitr::kable(cv_comparison)\n```\n\n::: {.cell-output-display}\n\n\n|Method     | Number of Models| Training Set Size| Test Set Size|\n|:----------|----------------:|-----------------:|-------------:|\n|5-fold CV  |                5|                40|            10|\n|10-fold CV |               10|                45|             5|\n|LOOCV      |               50|                49|             1|\n\n\n:::\n:::\n\n\nLOOCV characteristics:\n- **Pros**: Uses maximum data for training, deterministic (no randomness)\n- **Cons**: Computationally expensive, high variance, can overfit to the dataset\n\n## Bootstrap Methods\n\nBootstrap resampling draws samples WITH replacement from the original data. This creates datasets of the same size but with some observations repeated and others omitted.\n\n### Understanding Bootstrap\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create bootstrap samples\names_boot <- bootstraps(ames_train, times = 25)\names_boot\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Bootstrap sampling \n# A tibble: 25 Ã— 2\n   splits             id         \n   <list>             <chr>      \n 1 <split [2197/804]> Bootstrap01\n 2 <split [2197/819]> Bootstrap02\n 3 <split [2197/822]> Bootstrap03\n 4 <split [2197/798]> Bootstrap04\n 5 <split [2197/814]> Bootstrap05\n 6 <split [2197/812]> Bootstrap06\n 7 <split [2197/794]> Bootstrap07\n 8 <split [2197/825]> Bootstrap08\n 9 <split [2197/819]> Bootstrap09\n10 <split [2197/806]> Bootstrap10\n# â„¹ 15 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\n# Examine one bootstrap sample\nfirst_boot <- ames_boot$splits[[1]]\n\n# In bootstrap, some observations appear multiple times\nboot_sample <- analysis(first_boot)\noriginal_ids <- 1:nrow(ames_train)\nsampled_ids <- as.integer(rownames(boot_sample))\n\n# Count frequency of observations\nid_frequency <- table(sampled_ids)\nfreq_summary <- tibble(\n  `Times Sampled` = 0:max(id_frequency),\n  Count = c(\n    sum(!(original_ids %in% sampled_ids)),  # Not sampled (0 times)\n    sapply(1:max(id_frequency), function(x) sum(id_frequency == x))\n  ),\n  Percentage = Count / nrow(ames_train) * 100\n)\n\nknitr::kable(freq_summary, digits = 1)\n```\n\n::: {.cell-output-display}\n\n\n| Times Sampled| Count| Percentage|\n|-------------:|-----:|----------:|\n|             0|     0|          0|\n|             1|  2197|        100|\n\n\n:::\n\n```{.r .cell-code}\n# Visualize bootstrap sampling\nset.seed(456)\nsample_viz <- tibble(\n  original_id = 1:20,\n  bootstrap_sample = sample(1:20, 20, replace = TRUE)\n) %>%\n  count(bootstrap_sample) %>%\n  complete(bootstrap_sample = 1:20, fill = list(n = 0))\n\nggplot(sample_viz, aes(x = bootstrap_sample, y = n)) +\n  geom_col(fill = \"steelblue\") +\n  geom_hline(yintercept = 1, linetype = \"dashed\", color = \"red\") +\n  scale_x_continuous(breaks = 1:20) +\n  labs(\n    title = \"Bootstrap Sampling Example (n=20)\",\n    subtitle = \"Some observations appear multiple times, others not at all\",\n    x = \"Original Observation ID\",\n    y = \"Times Selected\"\n  )\n```\n\n::: {.cell-output-display}\n![](09-data-splitting_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nThe mathematics of bootstrap:\n- Probability of being selected at least once: $1 - (1 - 1/n)^n \\approx 0.632$ as $n \\to \\infty$\n- About 37% of observations are not selected (out-of-bag)\n- Provides estimates of sampling distribution\n- Useful for confidence intervals and bias estimation\n\n### Out-of-Bag (OOB) Error Estimation\n\nBootstrap's unique property is that ~37% of data is not sampled, providing a natural test set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate OOB predictions for each bootstrap sample\noob_analysis <- map_df(1:25, function(i) {\n  boot_split <- ames_boot$splits[[i]]\n  \n  # Observations in this bootstrap sample\n  in_bag <- as.integer(rownames(analysis(boot_split)))\n  \n  # OOB observations\n  all_ids <- 1:nrow(ames_train)\n  oob_ids <- setdiff(all_ids, unique(in_bag))\n  \n  tibble(\n    bootstrap = i,\n    n_unique_in_bag = length(unique(in_bag)),\n    n_oob = length(oob_ids),\n    pct_oob = length(oob_ids) / length(all_ids) * 100\n  )\n})\n\n# Summary statistics\noob_summary <- oob_analysis %>%\n  summarise(\n    mean_pct_oob = mean(pct_oob),\n    sd_pct_oob = sd(pct_oob),\n    theoretical_oob = (1 - 1/exp(1)) * 100  # ~36.8%\n  )\n\nknitr::kable(oob_summary, digits = 1)\n```\n\n::: {.cell-output-display}\n\n\n| mean_pct_oob| sd_pct_oob| theoretical_oob|\n|------------:|----------:|---------------:|\n|            0|          0|            63.2|\n\n\n:::\n\n```{.r .cell-code}\n# Visualize OOB percentages\nggplot(oob_analysis, aes(x = pct_oob)) +\n  geom_histogram(bins = 15, fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(xintercept = oob_summary$theoretical_oob, \n             color = \"red\", linewidth = 1, linetype = \"dashed\") +\n  labs(\n    title = \"Out-of-Bag Percentage Across Bootstrap Samples\",\n    subtitle = \"Red line shows theoretical value (~36.8%)\",\n    x = \"OOB Percentage\",\n    y = \"Count\"\n  )\n```\n\n::: {.cell-output-display}\n![](09-data-splitting_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n## Time Series Splitting\n\nTime series data requires special handling because:\n- Observations are not independent\n- Future cannot be used to predict past\n- Temporal patterns must be preserved\n\n### Time Series Cross-Validation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create time series data\nset.seed(789)\nn_days <- 365\nts_data <- tibble(\n  date = seq(as.Date(\"2022-01-01\"), by = \"day\", length.out = n_days),\n  trend = seq(100, 200, length.out = n_days),\n  seasonal = 20 * sin(2 * pi * (1:n_days) / 7),  # Weekly pattern\n  noise = rnorm(n_days, sd = 10),\n  value = trend + seasonal + noise\n)\n\n# Time series split - expanding window\nts_initial <- 180  # Initial training size\nts_assess <- 30    # Assessment size\n\nts_splits <- sliding_period(\n  ts_data,\n  date,\n  period = \"month\",\n  lookback = 5,  # Use 6 months of data\n  assess_stop = 1  # Assess on next month\n)\n\n# Visualize time series CV\nsplit_viz <- map_df(1:min(5, length(ts_splits$splits)), function(i) {\n  split <- ts_splits$splits[[i]]\n  train_data <- analysis(split) %>% mutate(role = \"Training\", split_id = i)\n  test_data <- assessment(split) %>% mutate(role = \"Testing\", split_id = i)\n  bind_rows(train_data, test_data)\n})\n\nggplot(split_viz, aes(x = date, y = split_id, color = role)) +\n  geom_point(size = 0.5) +\n  scale_color_manual(values = c(\"Training\" = \"steelblue\", \"Testing\" = \"coral\")) +\n  labs(\n    title = \"Time Series Cross-Validation\",\n    subtitle = \"Training window slides forward, always predicting future\",\n    x = \"Date\", y = \"Split\", color = \"Role\"\n  ) +\n  theme(legend.position = \"top\")\n```\n\n::: {.cell-output-display}\n![](09-data-splitting_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nTime series splitting strategies:\n- **Expanding window**: Training set grows over time\n- **Sliding window**: Fixed-size training window\n- **Skip periods**: Gap between training and testing\n\n### Rolling Origin Evaluation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Rolling origin (expanding window)\nrolling_splits <- rolling_origin(\n  ts_data,\n  initial = 180,    # Start with 180 days\n  assess = 30,      # Assess next 30 days\n  skip = 30,        # Skip 30 days between splits\n  cumulative = TRUE # Expanding window\n)\n\n# Calculate how training size grows\nrolling_summary <- rolling_splits %>%\n  mutate(\n    train_size = map_int(splits, ~ nrow(analysis(.))),\n    test_size = map_int(splits, ~ nrow(assessment(.)))\n  ) %>%\n  select(id, train_size, test_size) %>%\n  mutate(split = row_number())\n\nggplot(rolling_summary, aes(x = split)) +\n  geom_line(aes(y = train_size, color = \"Training\"), linewidth = 1) +\n  geom_line(aes(y = test_size, color = \"Testing\"), linewidth = 1) +\n  scale_color_manual(values = c(\"Training\" = \"steelblue\", \"Testing\" = \"coral\")) +\n  labs(\n    title = \"Rolling Origin: Expanding Window\",\n    subtitle = \"Training set grows while test set remains constant\",\n    x = \"Split Number\", y = \"Number of Observations\",\n    color = \"Dataset\"\n  )\n```\n\n::: {.cell-output-display}\n![](09-data-splitting_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n## Nested Resampling\n\nWhen we need to tune hyperparameters AND get an unbiased performance estimate, we use nested resampling:\n- **Outer loop**: For performance estimation\n- **Inner loop**: For hyperparameter tuning\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create nested resampling\n# Outer: 5-fold CV for performance estimation\n# Inner: 5-fold CV for hyperparameter tuning\n\n# Outer resampling\nouter_cv <- vfold_cv(ames_train, v = 5)\n\n# For each outer fold, create inner resampling\nnested_cv <- map(outer_cv$splits, function(outer_split) {\n  # Training data for this outer fold\n  outer_train <- analysis(outer_split)\n  \n  # Create inner CV on the outer training data\n  inner_cv <- vfold_cv(outer_train, v = 5)\n  \n  list(\n    outer_split = outer_split,\n    inner_cv = inner_cv\n  )\n})\n\n# Visualize nested structure\nnested_viz <- tibble(\n  `Outer Fold` = 1:5,\n  `Outer Training Size` = map_int(outer_cv$splits, ~ nrow(analysis(.))),\n  `Outer Test Size` = map_int(outer_cv$splits, ~ nrow(assessment(.))),\n  `Inner Folds` = 5,\n  `Total Models per Outer Fold` = 5,\n  `Total Models Overall` = 25\n)\n\nknitr::kable(nested_viz)\n```\n\n::: {.cell-output-display}\n\n\n| Outer Fold| Outer Training Size| Outer Test Size| Inner Folds| Total Models per Outer Fold| Total Models Overall|\n|----------:|-------------------:|---------------:|-----------:|---------------------------:|--------------------:|\n|          1|                1757|             440|           5|                           5|                   25|\n|          2|                1757|             440|           5|                           5|                   25|\n|          3|                1758|             439|           5|                           5|                   25|\n|          4|                1758|             439|           5|                           5|                   25|\n|          5|                1758|             439|           5|                           5|                   25|\n\n\n:::\n\n```{.r .cell-code}\n# Conceptual diagram\ncat(\"\nNested Resampling Structure:\n============================\nFull Training Data\n  |\n  â”œâ”€â”€ Outer Fold 1\n  â”‚   â”œâ”€â”€ Outer Training (80%)\n  â”‚   â”‚   â”œâ”€â”€ Inner Fold 1: Train (64%) + Val (16%)\n  â”‚   â”‚   â”œâ”€â”€ Inner Fold 2: Train (64%) + Val (16%)\n  â”‚   â”‚   â”œâ”€â”€ Inner Fold 3: Train (64%) + Val (16%)\n  â”‚   â”‚   â”œâ”€â”€ Inner Fold 4: Train (64%) + Val (16%)\n  â”‚   â”‚   â””â”€â”€ Inner Fold 5: Train (64%) + Val (16%)\n  â”‚   â””â”€â”€ Outer Test (20%) - Never seen during tuning\n  â”‚\n  â”œâ”€â”€ Outer Fold 2\n  â”‚   â””â”€â”€ [Same structure]\n  ...\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nNested Resampling Structure:\n============================\nFull Training Data\n  |\n  â”œâ”€â”€ Outer Fold 1\n  â”‚   â”œâ”€â”€ Outer Training (80%)\n  â”‚   â”‚   â”œâ”€â”€ Inner Fold 1: Train (64%) + Val (16%)\n  â”‚   â”‚   â”œâ”€â”€ Inner Fold 2: Train (64%) + Val (16%)\n  â”‚   â”‚   â”œâ”€â”€ Inner Fold 3: Train (64%) + Val (16%)\n  â”‚   â”‚   â”œâ”€â”€ Inner Fold 4: Train (64%) + Val (16%)\n  â”‚   â”‚   â””â”€â”€ Inner Fold 5: Train (64%) + Val (16%)\n  â”‚   â””â”€â”€ Outer Test (20%) - Never seen during tuning\n  â”‚\n  â”œâ”€â”€ Outer Fold 2\n  â”‚   â””â”€â”€ [Same structure]\n  ...\n```\n\n\n:::\n:::\n\n\nNested resampling is crucial for:\n- Unbiased performance estimation when tuning\n- Comparing different modeling strategies\n- Understanding generalization performance\n\n## Validation Sets in Practice\n\nSometimes we want a single validation set for quick iterations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a validation set\nval_split <- initial_validation_split(ames, prop = c(0.6, 0.2))\n\n# Extract all three sets\ntrain_set <- training(val_split)\nval_set <- validation(val_split)\ntest_set <- testing(val_split)\n\n# Use validation set for model selection\nmodels_to_compare <- list(\n  simple = recipe(Sale_Price ~ Gr_Liv_Area + Overall_Cond, data = train_set) %>%\n    step_dummy(all_nominal_predictors()),\n  moderate = recipe(Sale_Price ~ Gr_Liv_Area + Overall_Cond + Year_Built + \n                   Neighborhood, data = train_set) %>%\n    step_dummy(all_nominal_predictors()),\n  complex = recipe(Sale_Price ~ Gr_Liv_Area + Overall_Cond + Year_Built + \n                  Neighborhood + Total_Bsmt_SF + First_Flr_SF, data = train_set) %>%\n    step_dummy(all_nominal_predictors())\n)\n\n# Fit models and evaluate on validation set\nlm_spec <- linear_reg() %>% set_engine(\"lm\")\n\nvalidation_results <- map_df(names(models_to_compare), function(model_name) {\n  recipe <- models_to_compare[[model_name]]\n  \n  wf <- workflow() %>%\n    add_recipe(recipe) %>%\n    add_model(lm_spec)\n  \n  # Fit on training\n  fit <- wf %>% fit(train_set)\n  \n  # Predict on validation\n  val_pred <- fit %>%\n    predict(val_set) %>%\n    bind_cols(val_set)\n  \n  # Calculate metrics\n  val_pred %>%\n    metrics(truth = Sale_Price, estimate = .pred) %>%\n    mutate(model = model_name)\n})\n\n# Select best model based on validation performance\nbest_model <- validation_results %>%\n  filter(.metric == \"rmse\") %>%\n  arrange(.estimate) %>%\n  slice(1) %>%\n  pull(model)\n\nprint(paste(\"Best model based on validation set:\", best_model))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Best model based on validation set: complex\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Final evaluation on test set (only done once!)\nfinal_recipe <- models_to_compare[[best_model]]\nfinal_fit <- workflow() %>%\n  add_recipe(final_recipe) %>%\n  add_model(lm_spec) %>%\n  fit(train_set)\n\ntest_pred <- final_fit %>%\n  predict(test_set) %>%\n  bind_cols(test_set)\n\nfinal_performance <- test_pred %>%\n  metrics(truth = Sale_Price, estimate = .pred)\n\nknitr::kable(final_performance, digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|.metric |.estimator | .estimate|\n|:-------|:----------|---------:|\n|rmse    |standard   | 38837.693|\n|rsq     |standard   |     0.797|\n|mae     |standard   | 24125.346|\n\n\n:::\n:::\n\n\n## Choosing the Right Resampling Strategy\n\nDifferent strategies for different situations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Decision guide\nstrategy_guide <- tibble(\n  Scenario = c(\n    \"Large dataset (n > 10,000)\",\n    \"Medium dataset (1,000 < n < 10,000)\",\n    \"Small dataset (n < 1,000)\",\n    \"Imbalanced classes\",\n    \"Time series data\",\n    \"Quick prototyping\",\n    \"Final evaluation\",\n    \"Hyperparameter tuning\",\n    \"Model comparison\",\n    \"Uncertainty estimation\"\n  ),\n  `Recommended Strategy` = c(\n    \"Simple train/test split or validation set\",\n    \"5 or 10-fold CV\",\n    \"Repeated CV or LOOCV\",\n    \"Stratified CV\",\n    \"Time series CV or rolling origin\",\n    \"Validation set\",\n    \"Held-out test set (touched only once)\",\n    \"Nested CV\",\n    \"Repeated CV\",\n    \"Bootstrap\"\n  ),\n  Reasoning = c(\n    \"Sufficient data for reliable estimates\",\n    \"Balance between bias and variance\",\n    \"Maximize training data usage\",\n    \"Preserve class proportions\",\n    \"Respect temporal ordering\",\n    \"Fast iteration and feedback\",\n    \"Unbiased final assessment\",\n    \"Avoid overfitting to validation set\",\n    \"Stable comparison metrics\",\n    \"Confidence intervals and distributions\"\n  )\n)\n\nknitr::kable(strategy_guide)\n```\n\n::: {.cell-output-display}\n\n\n|Scenario                            |Recommended Strategy                      |Reasoning                              |\n|:-----------------------------------|:-----------------------------------------|:--------------------------------------|\n|Large dataset (n > 10,000)          |Simple train/test split or validation set |Sufficient data for reliable estimates |\n|Medium dataset (1,000 < n < 10,000) |5 or 10-fold CV                           |Balance between bias and variance      |\n|Small dataset (n < 1,000)           |Repeated CV or LOOCV                      |Maximize training data usage           |\n|Imbalanced classes                  |Stratified CV                             |Preserve class proportions             |\n|Time series data                    |Time series CV or rolling origin          |Respect temporal ordering              |\n|Quick prototyping                   |Validation set                            |Fast iteration and feedback            |\n|Final evaluation                    |Held-out test set (touched only once)     |Unbiased final assessment              |\n|Hyperparameter tuning               |Nested CV                                 |Avoid overfitting to validation set    |\n|Model comparison                    |Repeated CV                               |Stable comparison metrics              |\n|Uncertainty estimation              |Bootstrap                                 |Confidence intervals and distributions |\n\n\n:::\n:::\n\n\n## Common Pitfalls and Best Practices\n\n### Pitfall 1: Data Leakage\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# WRONG: Preprocessing before splitting\nwrong_data <- ames %>%\n  mutate(\n    # This uses information from ALL data including test!\n    Gr_Liv_Area_scaled = scale(Gr_Liv_Area)[,1],\n    Sale_Price_log = log(Sale_Price)\n  )\n\nwrong_split <- initial_split(wrong_data)\nwrong_train <- training(wrong_split)\nwrong_test <- testing(wrong_split)\n\n# RIGHT: Preprocessing after splitting (using recipes)\nright_split <- initial_split(ames)\nright_train <- training(right_split)\nright_test <- testing(right_split)\n\nright_recipe <- recipe(Sale_Price ~ ., data = right_train) %>%\n  step_log(Sale_Price) %>%\n  step_normalize(all_numeric_predictors())\n\n# The recipe learns parameters only from training data\n```\n:::\n\n\n### Pitfall 2: Multiple Testing on Test Set\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# WRONG: Using test set multiple times\n# This is pseudocode - don't actually do this!\n# for (model in models) {\n#   performance <- evaluate(model, test_set)\n#   if (performance < best_performance) {\n#     adjust_model(model)\n#     # Testing again - overfitting to test set!\n#   }\n# }\n\n# RIGHT: Use validation set or CV for model selection\n# Test set only for final evaluation\n```\n:::\n\n\n### Pitfall 3: Improper Stratification\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create data with important subgroups\ngrouped_data <- ames %>%\n  mutate(\n    price_category = cut(Sale_Price, \n                        breaks = quantile(Sale_Price, c(0, 0.33, 0.67, 1)),\n                        labels = c(\"Low\", \"Medium\", \"High\"))\n  )\n\n# WRONG: Not stratifying on important variable\nwrong_split <- initial_split(grouped_data)\nwrong_train <- training(wrong_split)\n\n# RIGHT: Stratify to preserve distribution\nright_split <- initial_split(grouped_data, strata = price_category)\nright_train <- training(right_split)\n\n# Compare distributions\ncomparison <- bind_rows(\n  grouped_data %>% count(price_category) %>% mutate(Set = \"Original\"),\n  wrong_train %>% count(price_category) %>% mutate(Set = \"No Stratification\"),\n  right_train %>% count(price_category) %>% mutate(Set = \"With Stratification\")\n) %>%\n  group_by(Set) %>%\n  mutate(Percentage = n / sum(n) * 100)\n\nggplot(comparison, aes(x = price_category, y = Percentage, fill = Set)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Effect of Stratification on Distribution\",\n    subtitle = \"Stratification preserves the original distribution\",\n    x = \"Price Category\", y = \"Percentage\"\n  )\n```\n\n::: {.cell-output-display}\n![](09-data-splitting_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n## Exercises\n\n### Exercise 1: Implement Custom Resampling\n\nCreate a custom resampling strategy for grouped data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Data with natural groups (e.g., different stores)\nstore_data <- tibble(\n  store_id = rep(LETTERS[1:10], each = 100),\n  date = rep(seq(as.Date(\"2023-01-01\"), length.out = 100, by = \"day\"), 10),\n  sales = rnorm(1000, mean = rep(seq(100, 190, 10), each = 100), sd = 20)\n)\n\n# Custom group-based CV: leave-one-store-out\ngroup_splits <- group_vfold_cv(store_data, group = store_id)\n\n# Examine splits\nsplit_summary <- map_df(1:length(group_splits$splits), function(i) {\n  split <- group_splits$splits[[i]]\n  train <- analysis(split)\n  test <- assessment(split)\n  \n  tibble(\n    fold = i,\n    train_stores = n_distinct(train$store_id),\n    test_stores = n_distinct(test$store_id),\n    train_rows = nrow(train),\n    test_rows = nrow(test)\n  )\n})\n\nknitr::kable(split_summary)\n```\n\n::: {.cell-output-display}\n\n\n| fold| train_stores| test_stores| train_rows| test_rows|\n|----:|------------:|-----------:|----------:|---------:|\n|    1|            9|           1|        900|       100|\n|    2|            9|           1|        900|       100|\n|    3|            9|           1|        900|       100|\n|    4|            9|           1|        900|       100|\n|    5|            9|           1|        900|       100|\n|    6|            9|           1|        900|       100|\n|    7|            9|           1|        900|       100|\n|    8|            9|           1|        900|       100|\n|    9|            9|           1|        900|       100|\n|   10|            9|           1|        900|       100|\n\n\n:::\n:::\n\n\n### Exercise 2: Compare Resampling Strategies\n\nEvaluate different resampling methods on the same dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Use a subset of Ames data for speed\names_subset <- ames %>%\n  select(Sale_Price, Gr_Liv_Area, Overall_Cond, Year_Built, Lot_Area) %>%\n  slice_sample(n = 500)\n\n# Define resampling strategies\nstrategies <- list(\n  holdout = initial_split(ames_subset, prop = 0.75),\n  cv5 = vfold_cv(ames_subset, v = 5),\n  cv10 = vfold_cv(ames_subset, v = 10),\n  repeated_cv = vfold_cv(ames_subset, v = 5, repeats = 3),\n  bootstrap = bootstraps(ames_subset, times = 25)\n)\n\n# Simple model for comparison\nmodel_spec <- linear_reg() %>% set_engine(\"lm\")\nrecipe_spec <- recipe(Sale_Price ~ ., data = ames_subset)\n\n# Evaluate each strategy (except holdout which needs different handling)\nresults <- map_df(names(strategies)[-1], function(strategy_name) {\n  wf <- workflow() %>%\n    add_recipe(recipe_spec) %>%\n    add_model(model_spec)\n  \n  fit_resamples(wf, strategies[[strategy_name]]) %>%\n    collect_metrics() %>%\n    mutate(strategy = strategy_name)\n})\n\n# Visualize results\nggplot(results, aes(x = strategy, y = mean, fill = strategy)) +\n  geom_col() +\n  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), width = 0.2) +\n  facet_wrap(~.metric, scales = \"free_y\") +\n  labs(\n    title = \"Comparison of Resampling Strategies\",\n    subtitle = \"Error bars show standard error\",\n    x = \"Strategy\", y = \"Metric Value\"\n  ) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](09-data-splitting_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n### Exercise 3: Time Series Validation\n\nImplement proper time series validation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Generate time series data with trend and seasonality\nset.seed(123)\nts_exercise <- tibble(\n  date = seq(as.Date(\"2021-01-01\"), as.Date(\"2023-12-31\"), by = \"day\"),\n  day_of_week = lubridate::wday(date),\n  month = lubridate::month(date),\n  trend = seq(1000, 2000, length.out = length(date)),\n  seasonal = 100 * sin(2 * pi * as.numeric(date) / 365),\n  weekly = 50 * (day_of_week %in% c(1, 7)),  # Weekend effect\n  noise = rnorm(length(date), 0, 50),\n  sales = trend + seasonal + weekly + noise\n)\n\n# Create time-based splits\n# Initial training: 2 years, assess: 1 month, skip: 1 month\nts_splits <- rolling_origin(\n  ts_exercise,\n  initial = 730,  # 2 years\n  assess = 30,    # 1 month\n  skip = 30,      # Skip 1 month\n  cumulative = FALSE  # Sliding window\n)\n\n# Evaluate a simple model\nts_recipe <- recipe(sales ~ day_of_week + month + trend, data = ts_exercise)\nts_model <- linear_reg() %>% set_engine(\"lm\")\n\nts_workflow <- workflow() %>%\n  add_recipe(ts_recipe) %>%\n  add_model(ts_model)\n\n# Fit and evaluate\nts_results <- fit_resamples(\n  ts_workflow,\n  ts_splits,\n  metrics = yardstick::metric_set(yardstick::rmse, yardstick::mae, yardstick::rsq)\n)\n\n# Examine performance over time\nts_metrics <- ts_results %>%\n  collect_metrics() %>%\n  mutate(\n    split_num = rep(1:(n()/3), 3)\n  )\n\nggplot(ts_metrics, aes(x = split_num, y = mean, color = .metric)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 2) +\n  facet_wrap(~.metric, scales = \"free_y\") +\n  labs(\n    title = \"Model Performance Over Time\",\n    subtitle = \"Time series cross-validation results\",\n    x = \"Split Number (Time â†’)\", y = \"Metric Value\"\n  ) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](09-data-splitting_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n## Summary\n\nIn this comprehensive chapter, you've mastered:\n\nâœ… **Fundamental concepts**\n  - Why proper data splitting is critical\n  - The bias-variance tradeoff in evaluation\n  - Training, validation, and test sets\n\nâœ… **Splitting strategies**\n  - Simple holdout splits\n  - Stratified sampling for balance\n  - Multi-level splits for complex workflows\n\nâœ… **Cross-validation techniques**\n  - K-fold and repeated CV\n  - Leave-one-out CV\n  - Time series CV\n\nâœ… **Advanced methods**\n  - Bootstrap resampling\n  - Nested resampling for tuning\n  - Group-based splitting\n\nâœ… **Best practices**\n  - Avoiding data leakage\n  - Choosing appropriate strategies\n  - Proper use of test sets\n\nKey takeaways:\n- Never evaluate on training data\n- Choose resampling based on your data and goals\n- Stratify when you have imbalanced data\n- Respect temporal ordering in time series\n- Use nested CV for unbiased tuning\n- Touch the test set only once!\n\n## What's Next?\n\nIn [Chapter 10](10-feature-engineering.Rmd), we'll explore feature engineering with recipes, learning how to transform raw data into model-ready features.\n\n## Additional Resources\n\n- [rsample Documentation](https://rsample.tidymodels.org/)\n- [Cross-Validation: The Right and Wrong Way](https://stats.stackexchange.com/questions/65128/)\n- [Nested Resampling Tutorial](https://www.tidymodels.org/learn/work/nested-resampling/)\n- [Time Series Cross-Validation](https://otexts.com/fpp3/tscv.html)\n- [Bootstrap Methods and Their Application](https://www.cambridge.org/core/books/bootstrap-methods-and-their-application/)\n",
    "supporting": [
      "09-data-splitting_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}