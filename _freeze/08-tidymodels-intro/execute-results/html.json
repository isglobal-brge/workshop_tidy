{
  "hash": "a5a88395415d37bd3ff3e9a5ce38c089",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 8: Introduction to Tidymodels - Theory and Practice\"\nauthor: \"David Sarrat GonzÃ¡lez, Juan R GonzÃ¡lez\"\ndate: today\nformat:\n  html:\n    code-fold: false\n    code-tools: true\n---\n\n## Learning Objectives\n\nBy the end of this chapter, you will understand:\n\n- The philosophy and structure of tidymodels\n- Core machine learning concepts and theory\n- The bias-variance tradeoff\n- Overfitting and underfitting\n- Cross-validation theory\n- The tidymodels workflow\n- Key packages in the tidymodels ecosystem\n\n::: {.callout-tip}\n## Download R Script\nYou can download the complete R code for this chapter:\n[ğŸ“¥ Download 08-tidymodels-intro.R](R_scripts/08-tidymodels-intro.R){.btn .btn-primary download=\"08-tidymodels-intro.R\"}\n:::\n\n## Machine Learning Foundations\n\n### What is Machine Learning?\n\nMachine learning is the science of getting computers to learn patterns from data without being explicitly programmed. Instead of writing rules, we let algorithms discover patterns.\n\n**Key Concepts:**\n\n- **Supervised Learning**: Learning from labeled examples (y is known)\n  - Classification: Predicting categories\n  - Regression: Predicting continuous values\n  \n- **Unsupervised Learning**: Finding patterns without labels\n  - Clustering: Grouping similar observations\n  - Dimensionality reduction: Simplifying complex data\n\n- **Features (X)**: Input variables/predictors\n- **Target (y)**: Output variable we want to predict\n- **Training**: Process of learning patterns from data\n- **Inference**: Making predictions on new data\n\n### The Learning Process\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(modeldata)\nlibrary(vip)\nlibrary(corrplot)\n\n# Set theme\ntheme_set(theme_minimal())\n\n# For reproducibility\nset.seed(123)\n```\n:::\n\n\n### Mathematical Foundation\n\nIn supervised learning, we seek to find a function f that maps inputs X to outputs y:\n\n$$y = f(X) + \\epsilon$$\n\nWhere:\n- $f$ is the true underlying function\n- $\\epsilon$ is irreducible error (noise)\n\nOur goal is to estimate $\\hat{f}$ that minimizes prediction error:\n\n$$\\text{Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$$\n\n## The Bias-Variance Tradeoff\n\n### Understanding Bias and Variance\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Demonstrate bias-variance tradeoff\nn <- 100\nx <- seq(0, 10, length.out = n)\n\n# True function\ntrue_function <- function(x) sin(x) + 0.5 * x\n\n# Generate data with noise\ny <- true_function(x) + rnorm(n, sd = 0.5)\ndata_sim <- tibble(x = x, y = y, y_true = true_function(x))\n\n# Fit models of different complexity\nmodels <- list(\n  \"Underfit (High Bias)\" = lm(y ~ x, data = data_sim),\n  \"Good Fit\" = lm(y ~ poly(x, 3), data = data_sim),\n  \"Overfit (High Variance)\" = lm(y ~ poly(x, 15), data = data_sim)\n)\n\n# Generate predictions\npredictions <- map_df(names(models), function(model_name) {\n  model <- models[[model_name]]\n  tibble(\n    x = x,\n    y_true = true_function(x),\n    y_pred = predict(model),\n    model = model_name\n  )\n})\n\n# Visualize\nggplot() +\n  geom_point(data = data_sim, aes(x = x, y = y), alpha = 0.3) +\n  geom_line(data = data_sim, aes(x = x, y = y_true), \n            color = \"black\", linewidth = 1.5, linetype = \"dashed\") +\n  geom_line(data = predictions, aes(x = x, y = y_pred, color = model), \n            linewidth = 1.2) +\n  facet_wrap(~model, ncol = 3) +\n  scale_color_manual(values = c(\"red\", \"green\", \"blue\")) +\n  labs(\n    title = \"Bias-Variance Tradeoff Demonstration\",\n    subtitle = \"Black dashed line = true function, Points = observed data\",\n    x = \"X\",\n    y = \"Y\"\n  ) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](08-tidymodels-intro_files/figure-html/unnamed-chunk-2-1.png){width=1152}\n:::\n:::\n\n\n**Key Insights:**\n\n- **Bias**: Error from overly simplistic assumptions\n  - High bias = Underfitting\n  - Model misses relevant patterns\n  \n- **Variance**: Error from sensitivity to small fluctuations\n  - High variance = Overfitting  \n  - Model learns noise as patterns\n\n- **Goal**: Find the sweet spot balancing bias and variance\n\n## Overfitting and Underfitting\n\n### Detecting Overfitting\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create training and test sets\nset.seed(123)\ntrain_indices <- sample(1:n, size = 0.7 * n)\ntrain_data <- data_sim[train_indices, ]\ntest_data <- data_sim[-train_indices, ]\n\n# Fit models of increasing complexity\ncomplexity_range <- 1:12\ntrain_errors <- numeric(length(complexity_range))\ntest_errors <- numeric(length(complexity_range))\n\nfor (i in complexity_range) {\n  model <- lm(y ~ poly(x, i), data = train_data)\n  \n  train_pred <- predict(model, train_data)\n  test_pred <- predict(model, test_data)\n  \n  train_errors[i] <- mean((train_data$y - train_pred)^2)\n  test_errors[i] <- mean((test_data$y - test_pred)^2)\n}\n\n# Plot training vs test error\nerror_data <- tibble(\n  complexity = rep(complexity_range, 2),\n  error = c(train_errors, test_errors),\n  type = rep(c(\"Training\", \"Test\"), each = length(complexity_range))\n)\n\nggplot(error_data, aes(x = complexity, y = error, color = type)) +\n  geom_line(linewidth = 1.2) +\n  geom_point(size = 3) +\n  scale_color_manual(values = c(\"Training\" = \"blue\", \"Test\" = \"red\")) +\n  labs(\n    title = \"Training vs Test Error: Detecting Overfitting\",\n    subtitle = \"Test error increases while training error decreases = Overfitting\",\n    x = \"Model Complexity (Polynomial Degree)\",\n    y = \"Mean Squared Error\",\n    color = \"Dataset\"\n  ) +\n  geom_vline(xintercept = 3, linetype = \"dashed\", alpha = 0.5) +\n  annotate(\"text\", x = 3, y = max(test_errors) * 0.9, \n           label = \"Optimal\\nComplexity\", hjust = -0.1)\n```\n\n::: {.cell-output-display}\n![](08-tidymodels-intro_files/figure-html/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n## Introduction to Tidymodels\n\n### The Tidymodels Ecosystem\n\nTidymodels is a collection of packages for modeling and machine learning using tidyverse principles:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Core tidymodels packages\ntidymodels_packages <- c(\n  \"rsample\",     # Data splitting and resampling\n  \"parsnip\",     # Model specification\n  \"recipes\",     # Feature engineering\n  \"workflows\",   # Workflow management\n  \"tune\",        # Hyperparameter tuning\n  \"yardstick\",   # Model evaluation metrics\n  \"broom\",       # Tidy model outputs\n  \"dials\"        # Parameter tuning dials\n)\n\n# Display package info\ntibble(\n  Package = tidymodels_packages,\n  Purpose = c(\n    \"Data splitting, cross-validation, bootstrapping\",\n    \"Unified interface for model specification\",\n    \"Feature engineering and preprocessing\",\n    \"Combine preprocessing and modeling\",\n    \"Hyperparameter optimization\",\n    \"Performance metrics and evaluation\",\n    \"Convert model outputs to tidy format\",\n    \"Tools for creating tuning parameter sets\"\n  )\n) %>%\n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|Package   |Purpose                                         |\n|:---------|:-----------------------------------------------|\n|rsample   |Data splitting, cross-validation, bootstrapping |\n|parsnip   |Unified interface for model specification       |\n|recipes   |Feature engineering and preprocessing           |\n|workflows |Combine preprocessing and modeling              |\n|tune      |Hyperparameter optimization                     |\n|yardstick |Performance metrics and evaluation              |\n|broom     |Convert model outputs to tidy format            |\n|dials     |Tools for creating tuning parameter sets        |\n\n\n:::\n:::\n\n\n### Tidymodels Philosophy\n\n1. **Consistency**: Same interface across different models\n2. **Composability**: Modular components that work together\n3. **Reproducibility**: Clear, documented workflows\n4. **Best Practices**: Built-in safeguards against common mistakes\n\n## A Complete Tidymodels Workflow\n\nLet's build a complete machine learning workflow to predict penguin species:\n\n### 1. Data Exploration\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load and explore data\npenguins_clean <- penguins %>%\n  drop_na()\n\n# Basic exploration\nglimpse(penguins_clean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 333\nColumns: 8\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelâ€¦\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerseâ€¦\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.6â€¦\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.2â€¦\n$ flipper_length_mm <int> 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 18â€¦\n$ body_mass_g       <int> 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3800â€¦\n$ sex               <fct> male, female, female, female, male, female, male, feâ€¦\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007â€¦\n```\n\n\n:::\n\n```{.r .cell-code}\n# Class distribution\npenguins_clean %>%\n  count(species) %>%\n  mutate(prop = n / sum(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 Ã— 3\n  species       n  prop\n  <fct>     <int> <dbl>\n1 Adelie      146 0.438\n2 Chinstrap    68 0.204\n3 Gentoo      119 0.357\n```\n\n\n:::\n\n```{.r .cell-code}\n# Correlation matrix\npenguins_clean %>%\n  select(where(is.numeric)) %>%\n  cor() %>%\n  corrplot(method = \"color\", type = \"upper\", \n           order = \"hclust\", tl.cex = 0.8,\n           addCoef.col = \"black\", number.cex = 0.7)\n```\n\n::: {.cell-output-display}\n![](08-tidymodels-intro_files/figure-html/unnamed-chunk-5-1.png){width=1152}\n:::\n\n```{.r .cell-code}\n# Feature relationships\npenguins_clean %>%\n  select(species, bill_length_mm, bill_depth_mm, \n         flipper_length_mm, body_mass_g) %>%\n  pivot_longer(cols = -species, names_to = \"measurement\", values_to = \"value\") %>%\n  ggplot(aes(x = value, fill = species)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(~measurement, scales = \"free\") +\n  scale_fill_viridis_d() +\n  labs(title = \"Feature Distributions by Species\")\n```\n\n::: {.cell-output-display}\n![](08-tidymodels-intro_files/figure-html/unnamed-chunk-5-2.png){width=1152}\n:::\n:::\n\n\n### 2. Data Splitting\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Initial split (training vs testing)\nset.seed(123)\npenguin_split <- initial_split(penguins_clean, prop = 0.75, strata = species)\n\npenguin_train <- training(penguin_split)\npenguin_test <- testing(penguin_split)\n\n# Check split proportions\ntibble(\n  Dataset = c(\"Training\", \"Testing\"),\n  N = c(nrow(penguin_train), nrow(penguin_test)),\n  Proportion = c(nrow(penguin_train), nrow(penguin_test)) / nrow(penguins_clean)\n) %>%\n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|Dataset  |   N| Proportion|\n|:--------|---:|----------:|\n|Training | 249|  0.7477477|\n|Testing  |  84|  0.2522523|\n\n\n:::\n\n```{.r .cell-code}\n# Create cross-validation folds\npenguin_folds <- vfold_cv(penguin_train, v = 5, strata = species)\npenguin_folds\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#  5-fold cross-validation using stratification \n# A tibble: 5 Ã— 2\n  splits           id   \n  <list>           <chr>\n1 <split [198/51]> Fold1\n2 <split [199/50]> Fold2\n3 <split [199/50]> Fold3\n4 <split [199/50]> Fold4\n5 <split [201/48]> Fold5\n```\n\n\n:::\n:::\n\n\n### 3. Feature Engineering with Recipes\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a recipe\npenguin_recipe <- recipe(species ~ ., data = penguin_train) %>%\n  # Remove unnecessary variables\n  step_rm(year) %>%\n  # Convert factors to dummy variables\n  step_dummy(all_nominal_predictors()) %>%\n  # Normalize numeric predictors\n  step_normalize(all_numeric_predictors()) %>%\n  # Remove zero variance predictors\n  step_zv(all_predictors())\n\n# View the recipe\npenguin_recipe\n\n# Prepare and bake to see transformed data\npenguin_prep <- prep(penguin_recipe)\nbake(penguin_prep, new_data = penguin_train %>% head())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 Ã— 8\n  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g species\n           <dbl>         <dbl>             <dbl>       <dbl> <fct>  \n1         -0.683         0.393            -0.437      -1.21  Adelie \n2         -1.35          1.05             -0.578      -0.963 Adelie \n3         -0.942         0.293            -1.42       -0.751 Adelie \n4         -0.886         1.20             -0.437       0.520 Adelie \n5         -0.535         0.192            -1.35       -1.27  Adelie \n6         -0.997         2.00             -0.719      -0.539 Adelie \n# â„¹ 3 more variables: island_Dream <dbl>, island_Torgersen <dbl>,\n#   sex_male <dbl>\n```\n\n\n:::\n:::\n\n\n### 4. Model Specification\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Specify different models\n\n# Multinomial regression (for multiclass classification)\nmultinom_spec <- multinom_reg() %>%\n  set_engine(\"nnet\") %>%\n  set_mode(\"classification\")\n\n# Random forest\nrf_spec <- rand_forest(\n  trees = 100,\n  min_n = 5\n) %>%\n  set_engine(\"ranger\", importance = \"impurity\") %>%\n  set_mode(\"classification\")\n\n# Support vector machine\nsvm_spec <- svm_rbf(\n  cost = 1,\n  rbf_sigma = 0.01\n) %>%\n  set_engine(\"kernlab\") %>%\n  set_mode(\"classification\")\n\nprint(\"Model specifications created\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Model specifications created\"\n```\n\n\n:::\n:::\n\n\n### 5. Creating Workflows\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Combine recipe and model into workflows\nmultinom_workflow <- workflow() %>%\n  add_recipe(penguin_recipe) %>%\n  add_model(multinom_spec)\n\nrf_workflow <- workflow() %>%\n  add_recipe(penguin_recipe) %>%\n  add_model(rf_spec)\n\nsvm_workflow <- workflow() %>%\n  add_recipe(penguin_recipe) %>%\n  add_model(svm_spec)\n\nmultinom_workflow\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nâ•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nPreprocessor: Recipe\nModel: multinom_reg()\n\nâ”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n4 Recipe Steps\n\nâ€¢ step_rm()\nâ€¢ step_dummy()\nâ€¢ step_normalize()\nâ€¢ step_zv()\n\nâ”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nMultinomial Regression Model Specification (classification)\n\nComputational engine: nnet \n```\n\n\n:::\n:::\n\n\n### 6. Model Training and Evaluation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit models using cross-validation\n# For multiclass problems, we'll use accuracy and multiclass AUC\nmultinom_cv <- fit_resamples(\n  multinom_workflow,\n  resamples = penguin_folds,\n  metrics = metric_set(accuracy, roc_auc),\n  control = control_resamples(save_pred = TRUE)\n)\n\nrf_cv <- fit_resamples(\n  rf_workflow,\n  resamples = penguin_folds,\n  metrics = metric_set(accuracy, roc_auc),\n  control = control_resamples(save_pred = TRUE)\n)\n\n# Compare models\nmodel_comparison <- bind_rows(\n  collect_metrics(multinom_cv) %>% mutate(model = \"Multinomial Regression\"),\n  collect_metrics(rf_cv) %>% mutate(model = \"Random Forest\")\n)\n\n# Visualize comparison\nggplot(model_comparison, aes(x = model, y = mean, fill = model)) +\n  geom_col() +\n  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), width = 0.2) +\n  facet_wrap(~.metric, scales = \"free_y\") +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"Model Performance Comparison\",\n    subtitle = \"5-fold cross-validation results\",\n    y = \"Score\"\n  ) +\n  theme(legend.position = \"none\", axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](08-tidymodels-intro_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n### 7. Final Model Training\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Train final model on full training set\nfinal_model <- rf_workflow %>%\n  fit(penguin_train)\n\n# Make predictions on test set\npredictions <- final_model %>%\n  predict(penguin_test) %>%\n  bind_cols(penguin_test %>% select(species))\n\n# Confusion matrix\nconf_mat <- predictions %>%\n  conf_mat(truth = species, estimate = .pred_class)\n\nconf_mat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           Truth\nPrediction  Adelie Chinstrap Gentoo\n  Adelie        37         0      0\n  Chinstrap      0        17      0\n  Gentoo         0         0     30\n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize confusion matrix\nautoplot(conf_mat, type = \"heatmap\") +\n  scale_fill_gradient(low = \"white\", high = \"darkblue\") +\n  labs(title = \"Confusion Matrix - Random Forest\")\n```\n\n::: {.cell-output-display}\n![](08-tidymodels-intro_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n### 8. Model Interpretation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Feature importance\nfinal_rf <- final_model %>%\n  extract_fit_parsnip()\n\n# Variable importance plot\nif (require(vip, quietly = TRUE)) {\n  vip(final_rf, num_features = 10) +\n    labs(title = \"Feature Importance - Random Forest\")\n}\n```\n\n::: {.cell-output-display}\n![](08-tidymodels-intro_files/figure-html/unnamed-chunk-12-1.png){width=960}\n:::\n\n```{.r .cell-code}\n# Prediction probabilities\nprob_predictions <- final_model %>%\n  predict(penguin_test, type = \"prob\") %>%\n  bind_cols(penguin_test %>% select(species))\n\n# ROC curves for multiclass\nif (require(yardstick, quietly = TRUE)) {\n  roc_data <- prob_predictions %>%\n    roc_curve(truth = species, .pred_Adelie:.pred_Gentoo)\n  \n  autoplot(roc_data) +\n    labs(\n      title = \"ROC Curves by Species\",\n      subtitle = \"One-vs-All approach\"\n    )\n}\n```\n\n::: {.cell-output-display}\n![](08-tidymodels-intro_files/figure-html/unnamed-chunk-12-2.png){width=960}\n:::\n:::\n\n\n## Cross-Validation Theory\n\n### Why Cross-Validation?\n\nCross-validation helps us:\n1. Estimate model performance on unseen data\n2. Detect overfitting\n3. Compare different models fairly\n4. Make better use of limited data\n\n### Types of Cross-Validation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Demonstrate different CV strategies\nset.seed(123)\nsample_data <- tibble(\n  id = 1:100,\n  x = rnorm(100),\n  y = 2 * x + rnorm(100, sd = 0.5),\n  group = rep(1:10, each = 10),\n  time = rep(1:10, 10)\n)\n\n# Different CV strategies\ncv_strategies <- list(\n  \"5-Fold CV\" = vfold_cv(sample_data, v = 5),\n  \"10-Fold CV\" = vfold_cv(sample_data, v = 10),\n  \"Leave-One-Out CV\" = loo_cv(sample_data),\n  \"Bootstrap\" = bootstraps(sample_data, times = 5),\n  \"Group CV\" = group_vfold_cv(sample_data, group = group, v = 5)\n)\n\n# Visualize fold assignments\nfold_viz <- vfold_cv(sample_data, v = 5) %>%\n  mutate(fold_data = map(splits, analysis)) %>%\n  unnest(fold_data, names_sep = \"_\") %>%\n  select(obs_id = fold_data_id, Fold = id) %>%\n  distinct()\n\nggplot(fold_viz, aes(x = obs_id, y = 1, fill = Fold)) +\n  geom_tile(height = 0.8) +\n  scale_fill_viridis_d() +\n  labs(\n    title = \"5-Fold Cross-Validation: Data Assignment\",\n    subtitle = \"Each observation appears in exactly one test fold\",\n    x = \"Observation ID\",\n    y = \"\"\n  ) +\n  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())\n```\n\n::: {.cell-output-display}\n![](08-tidymodels-intro_files/figure-html/unnamed-chunk-13-1.png){width=1152}\n:::\n:::\n\n\n### Mathematical Foundation of CV\n\nFor k-fold cross-validation, the CV estimate of prediction error is:\n\n$$CV_{(k)} = \\frac{1}{k} \\sum_{i=1}^{k} MSE_i$$\n\nWhere $MSE_i$ is the mean squared error on fold $i$.\n\n## Model Selection Theory\n\n### Information Criteria\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Demonstrate AIC/BIC for model selection\nmodels_to_compare <- list(\n  \"Simple\" = lm(body_mass_g ~ bill_length_mm, data = penguin_train),\n  \"Moderate\" = lm(body_mass_g ~ bill_length_mm + flipper_length_mm, data = penguin_train),\n  \"Complex\" = lm(body_mass_g ~ bill_length_mm + flipper_length_mm + \n                 bill_depth_mm + island + sex, data = penguin_train),\n  \"Very Complex\" = lm(body_mass_g ~ .^2, data = penguin_train)  # All interactions\n)\n\nmodel_selection <- map_df(names(models_to_compare), function(name) {\n  model <- models_to_compare[[name]]\n  tibble(\n    Model = name,\n    Parameters = length(coef(model)),\n    AIC = AIC(model),\n    BIC = BIC(model),\n    `Adj RÂ²` = summary(model)$adj.r.squared\n  )\n})\n\nmodel_selection %>%\n  arrange(AIC) %>%\n  knitr::kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n|Model        | Parameters|     AIC|     BIC| Adj RÂ²|\n|:------------|----------:|-------:|-------:|------:|\n|Very Complex |         44| 3566.11| 3710.33|   0.88|\n|Complex      |          7| 3607.11| 3635.25|   0.84|\n|Moderate     |          3| 3699.53| 3713.59|   0.76|\n|Simple       |          2| 3948.30| 3958.85|   0.35|\n\n\n:::\n:::\n\n\n### Regularization Theory\n\nRegularization adds a penalty term to prevent overfitting:\n\n**Ridge Regression (L2):**\n$$\\min_{\\beta} \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij})^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2$$\n\n**Lasso Regression (L1):**\n$$\\min_{\\beta} \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij})^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Demonstrate regularization\nlibrary(glmnet)\n\n# Prepare data\nX <- model.matrix(body_mass_g ~ . - 1, data = penguin_train %>% select(-species, -year))\ny <- penguin_train$body_mass_g\n\n# Fit ridge and lasso\nridge_fit <- glmnet(X, y, alpha = 0)  # Ridge\nlasso_fit <- glmnet(X, y, alpha = 1)  # Lasso\n\n# Plot coefficient paths\npar(mfrow = c(1, 2))\nplot(ridge_fit, xvar = \"lambda\", main = \"Ridge Regression\")\nplot(lasso_fit, xvar = \"lambda\", main = \"Lasso Regression\")\n```\n\n::: {.cell-output-display}\n![](08-tidymodels-intro_files/figure-html/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n## Best Practices in Machine Learning\n\n### 1. Data Leakage Prevention\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# WRONG: Preprocessing before splitting\n# This leaks information from test set into training\nwrong_way <- penguins_clean %>%\n  mutate(bill_length_scaled = scale(bill_length_mm)[,1])  # Uses all data!\n\n# RIGHT: Preprocessing within training set only\nright_way <- recipe(species ~ ., data = penguin_train) %>%\n  step_normalize(all_numeric_predictors())  # Only uses training data\n```\n:::\n\n\n### 2. Proper Evaluation\n\nAlways use:\n- Separate test set (never touched during development)\n- Cross-validation for model selection\n- Appropriate metrics for your problem\n\n### 3. Feature Engineering Guidelines\n\n- Domain knowledge is crucial\n- Start simple, add complexity gradually\n- Validate feature importance\n- Watch for multicollinearity\n\n## Exercises\n\n### Exercise 1: Implement Cross-Validation\n\nCompare different CV strategies on the penguins dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\ncv_comparison <- tibble(\n  strategy = c(\"5-Fold\", \"10-Fold\", \"Bootstrap\", \"Monte Carlo\"),\n  cv_object = list(\n    vfold_cv(penguin_train, v = 5),\n    vfold_cv(penguin_train, v = 10),\n    bootstraps(penguin_train, times = 25),\n    mc_cv(penguin_train, prop = 0.75, times = 25)\n  )\n)\n\n# Fit a simple model with each CV strategy\nsimple_spec <- multinom_reg() %>%\n  set_engine(\"nnet\")\n\nsimple_recipe <- recipe(species ~ bill_length_mm + bill_depth_mm, \n                       data = penguin_train) %>%\n  step_normalize(all_predictors())\n\nsimple_workflow <- workflow() %>%\n  add_recipe(simple_recipe) %>%\n  add_model(simple_spec)\n\n# Compare results\ncv_results <- cv_comparison %>%\n  mutate(\n    fits = map(cv_object, ~ fit_resamples(simple_workflow, resamples = .)),\n    metrics = map(fits, collect_metrics)\n  ) %>%\n  unnest(metrics) %>%\n  filter(.metric == \"accuracy\") %>%\n  select(strategy, mean, std_err)\n\ncv_results %>%\n  ggplot(aes(x = strategy, y = mean)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), width = 0.2) +\n  labs(title = \"Cross-Validation Strategy Comparison\",\n       y = \"Accuracy\")\n```\n\n::: {.cell-output-display}\n![](08-tidymodels-intro_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n### Exercise 2: Bias-Variance Analysis\n\nCreate models with different complexity levels and analyze their bias-variance tradeoff:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Create polynomial features of different degrees\ncomplexity_levels <- 1:5\n\nmodel_fits <- map(complexity_levels, function(degree) {\n  recipe_poly <- recipe(body_mass_g ~ flipper_length_mm, data = penguin_train) %>%\n    step_poly(flipper_length_mm, degree = degree)\n  \n  lm_spec <- linear_reg() %>%\n    set_engine(\"lm\")\n  \n  workflow() %>%\n    add_recipe(recipe_poly) %>%\n    add_model(lm_spec) %>%\n    fit(penguin_train)\n})\n\n# Evaluate on training and test sets\nevaluation <- map_df(1:length(model_fits), function(i) {\n  model <- model_fits[[i]]\n  \n  train_pred <- predict(model, penguin_train)\n  test_pred <- predict(model, penguin_test)\n  \n  tibble(\n    complexity = complexity_levels[i],\n    train_rmse = rmse_vec(penguin_train$body_mass_g, train_pred$.pred),\n    test_rmse = rmse_vec(penguin_test$body_mass_g, test_pred$.pred)\n  )\n})\n\nevaluation %>%\n  pivot_longer(cols = c(train_rmse, test_rmse), \n               names_to = \"dataset\", values_to = \"rmse\") %>%\n  ggplot(aes(x = complexity, y = rmse, color = dataset)) +\n  geom_line(linewidth = 1.2) +\n  geom_point(size = 3) +\n  labs(title = \"Model Complexity vs Error\",\n       x = \"Polynomial Degree\",\n       y = \"RMSE\") +\n  scale_color_manual(values = c(\"train_rmse\" = \"blue\", \"test_rmse\" = \"red\"))\n```\n\n::: {.cell-output-display}\n![](08-tidymodels-intro_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n### Exercise 3: Build a Complete Pipeline\n\nCreate a complete tidymodels pipeline for a regression problem:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Predict penguin body mass\nmass_split <- initial_split(penguins_clean, prop = 0.8)\nmass_train <- training(mass_split)\nmass_test <- testing(mass_split)\n\n# Create recipe with feature engineering\nmass_recipe <- recipe(body_mass_g ~ ., data = mass_train) %>%\n  step_rm(year) %>%\n  step_impute_median(all_numeric_predictors()) %>%\n  step_impute_mode(all_nominal_predictors()) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_normalize(all_numeric_predictors()) %>%\n  step_interact(terms = ~ bill_length_mm:bill_depth_mm)\n\n# Specify model\nrf_reg_spec <- rand_forest(\n  trees = 200,\n  min_n = 10\n) %>%\n  set_engine(\"ranger\", importance = \"impurity\") %>%\n  set_mode(\"regression\")\n\n# Create workflow\nmass_workflow <- workflow() %>%\n  add_recipe(mass_recipe) %>%\n  add_model(rf_reg_spec)\n\n# Fit and evaluate\nmass_fit <- mass_workflow %>%\n  fit(mass_train)\n\n# Predictions\nmass_predictions <- mass_fit %>%\n  predict(mass_test) %>%\n  bind_cols(mass_test)\n\n# Evaluate\nmetrics <- mass_predictions %>%\n  metrics(truth = body_mass_g, estimate = .pred)\n\nprint(metrics)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard     309.   \n2 rsq     standard       0.843\n3 mae     standard     249.   \n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize predictions\nggplot(mass_predictions, aes(x = body_mass_g, y = .pred)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Predicted vs Actual Body Mass\",\n       x = \"Actual Mass (g)\",\n       y = \"Predicted Mass (g)\")\n```\n\n::: {.cell-output-display}\n![](08-tidymodels-intro_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n## Summary\n\nYou've learned the theoretical foundations and practical implementation of:\n\nâœ… Machine learning fundamentals and theory  \nâœ… Bias-variance tradeoff  \nâœ… Overfitting and underfitting concepts  \nâœ… Cross-validation theory and practice  \nâœ… The tidymodels ecosystem structure  \nâœ… Complete ML workflow implementation  \nâœ… Model selection and evaluation  \nâœ… Best practices in machine learning  \n\n## What's Next?\n\nIn [Chapter 9](09-data-splitting.Rmd), we'll dive deep into data splitting strategies and resampling techniques with rsample.\n\n## Additional Resources\n\n- [Tidymodels Documentation](https://www.tidymodels.org/)\n- [An Introduction to Statistical Learning](https://www.statlearning.com/)\n- [The Elements of Statistical Learning](https://hastie.su.domains/ElemStatLearn/)\n- [Tidy Modeling with R](https://www.tmwr.org/)\n- [Feature Engineering and Selection](http://www.feat.engineering/)\n",
    "supporting": [
      "08-tidymodels-intro_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}