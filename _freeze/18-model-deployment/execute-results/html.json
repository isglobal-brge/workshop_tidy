{
  "hash": "06f21089d30506bdb80ad2b590673f15",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 18: Model Deployment and Production - From Prototype to Product\"\nauthor: \"David Sarrat González, Juan R González\"\ndate: today\nformat:\n  html:\n    code-fold: false\n    code-tools: true\n---\n\n## Learning Objectives\n\nBy the end of this chapter, you will master:\n\n- The journey from development to production\n- Model serialization and versioning\n- Creating APIs with plumber\n- Building Shiny applications for model deployment\n- Docker containerization for R models\n- Model monitoring and maintenance\n- A/B testing and gradual rollouts\n- Best practices for production ML systems\n\n## The Production Challenge\n\nBuilding a great model is only the beginning. The real challenge lies in deploying it to production where it can deliver value. Studies show that 87% of data science projects never make it to production. This chapter will ensure your models are in the successful 13%.\n\nThe journey from Jupyter notebook to production system involves many considerations:\n- **Reliability**: Will it work 24/7?\n- **Scalability**: Can it handle production load?\n- **Maintainability**: Can others understand and update it?\n- **Monitoring**: How do we know if it's working correctly?\n- **Versioning**: How do we update without breaking things?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Attaching packages -------------------------------------- tidymodels 1.4.1 --\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nv broom        1.0.10     v recipes      1.3.1 \nv dials        1.4.2      v rsample      1.3.1 \nv dplyr        1.1.4      v tailor       0.1.0 \nv ggplot2      4.0.0      v tidyr        1.3.1 \nv infer        1.0.9      v tune         2.0.0 \nv modeldata    1.5.1      v workflows    1.3.0 \nv parsnip      1.3.3      v workflowsets 1.1.1 \nv purrr        1.1.0      v yardstick    1.3.2 \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx purrr::discard() masks scales::discard()\nx dplyr::filter()  masks stats::filter()\nx dplyr::lag()     masks stats::lag()\nx recipes::step()  masks stats::step()\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv forcats   1.0.0     v stringr   1.5.2\nv lubridate 1.9.4     v tibble    3.3.0\nv readr     2.1.5     \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx readr::col_factor() masks scales::col_factor()\nx purrr::discard()    masks scales::discard()\nx dplyr::filter()     masks stats::filter()\nx stringr::fixed()    masks recipes::fixed()\nx dplyr::lag()        masks stats::lag()\nx readr::spec()       masks yardstick::spec()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(plumber)\nlibrary(pins)\nlibrary(vetiver)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAdjuntando el paquete: 'vetiver'\n\nThe following object is masked from 'package:tune':\n\n    load_pkgs\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(jsonlite)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAdjuntando el paquete: 'jsonlite'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(httr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAdjuntando el paquete: 'httr'\n\nThe following object is masked from 'package:pins':\n\n    cache_info\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(shiny)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAdjuntando el paquete: 'shiny'\n\nThe following object is masked from 'package:jsonlite':\n\n    validate\n\nThe following object is masked from 'package:infer':\n\n    observe\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(shinydashboard)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAdjuntando el paquete: 'shinydashboard'\n\nThe following object is masked from 'package:graphics':\n\n    box\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(DT)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAdjuntando el paquete: 'DT'\n\nThe following objects are masked from 'package:shiny':\n\n    dataTableOutput, renderDataTable\n```\n\n\n:::\n\n```{.r .cell-code}\n# Set theme and seed\ntheme_set(theme_minimal())\nset.seed(123)\n\n# Load and prepare example model\ndata(ames)\names_split <- initial_split(ames, prop = 0.75, strata = Sale_Price)\names_train <- training(ames_split)\names_test <- testing(ames_split)\n\n# Create a production-ready model\n# Transform outcome before recipe\names_train_log <- ames_train %>%\n  mutate(Sale_Price = log(Sale_Price))\n\names_test_log <- ames_test %>%\n  mutate(Sale_Price = log(Sale_Price))\n\nproduction_recipe <- recipe(Sale_Price ~ Gr_Liv_Area + Overall_Cond + \n                          Year_Built + Neighborhood + Total_Bsmt_SF, \n                          data = ames_train_log) %>%\n  step_impute_median(all_numeric_predictors()) %>%\n  step_novel(all_nominal_predictors()) %>%  # Handle new categories\n  step_unknown(all_nominal_predictors()) %>%  # Handle missing categories\n  step_dummy(all_nominal_predictors()) %>%\n  step_normalize(all_numeric_predictors())\n\nproduction_model <- linear_reg(penalty = 0.01, mixture = 0.5) %>%\n  set_engine(\"glmnet\")\n\nproduction_workflow <- workflow() %>%\n  add_recipe(production_recipe) %>%\n  add_model(production_model)\n\n# Fit the model\nproduction_fit <- production_workflow %>%\n  fit(ames_train_log)\n\n# Evaluate\ntest_predictions <- predict(production_fit, ames_test_log) %>%\n  bind_cols(ames_test_log %>% select(Sale_Price))\n\ntest_metrics <- test_predictions %>%\n  metrics(Sale_Price, .pred)\n\ncat(\"Model Performance:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel Performance:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(test_metrics)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       0.147\n2 rsq     standard       0.870\n3 mae     standard       0.107\n```\n\n\n:::\n:::\n\n\n## Model Serialization and Storage\n\n### Saving Models Locally\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Method 1: Base R serialization\n# Create directory if it doesn't exist\ndir.create(\"models\", showWarnings = FALSE)\nsaveRDS(production_fit, \"models/ames_model_v1.rds\")\n\n# Load it back\nloaded_model <- readRDS(\"models/ames_model_v1.rds\")\n\n# Verify it works\ntest_pred <- predict(loaded_model, ames_test_log %>% slice(1:5))\nprint(test_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 1\n  .pred\n  <dbl>\n1  11.7\n2  11.9\n3  12.1\n4  12.1\n5  12.3\n```\n\n\n:::\n\n```{.r .cell-code}\n# Method 2: Using bundle package for better portability\nlibrary(bundle)\n\n# Bundle the model (includes necessary metadata)\nmodel_bundle <- bundle(production_fit)\nsaveRDS(model_bundle, \"models/ames_model_bundle_v1.rds\")\n\n# Unbundle when loading\nloaded_bundle <- readRDS(\"models/ames_model_bundle_v1.rds\")\nunbundled_model <- unbundle(loaded_bundle)\n```\n:::\n\n\n### Version Control with pins\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a board for model storage\nlibrary(pins)\n\n# Local board (can also use S3, Azure, etc.)\nmodel_board <- board_folder(\"models/pins\", versioned = TRUE)\n\n# Pin the model with metadata\nmodel_board %>%\n  pin_write(\n    production_fit,\n    name = \"ames_price_model\",\n    type = \"rds\",\n    title = \"Ames Housing Price Model\",\n    description = \"Elastic net model for predicting house prices\",\n    metadata = list(\n      metrics = test_metrics,\n      training_date = Sys.Date(),\n      features = c(\"Gr_Liv_Area\", \"Overall_Cond\", \"Year_Built\", \n                  \"Neighborhood\", \"Total_Bsmt_SF\"),\n      model_type = \"elastic_net\",\n      package_versions = sessionInfo()\n    )\n  )\n\n# List available versions\nmodel_board %>%\n  pin_versions(\"ames_price_model\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 3\n  version                created             hash \n  <chr>                  <dttm>              <chr>\n1 20250927T131955Z-95ee1 2025-09-27 15:19:55 95ee1\n2 20250927T143618Z-06c96 2025-09-27 16:36:18 06c96\n3 20251001T111314Z-b12c5 2025-10-01 13:13:14 b12c5\n4 20251001T112218Z-76001 2025-10-01 13:22:18 76001\n5 20251001T112538Z-3e8e8 2025-10-01 13:25:38 3e8e8\n```\n\n\n:::\n\n```{.r .cell-code}\n# Load specific version\nretrieved_model <- model_board %>%\n  pin_read(\"ames_price_model\")\n```\n:::\n\n\n### Using vetiver for Model Deployment\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(vetiver)\n\n# Create a vetiver model\nv <- vetiver_model(\n  production_fit,\n  \"ames_price_predictor\",\n  metadata = list(\n    description = \"Predicts house prices in Ames, Iowa\",\n    features = c(\"Gr_Liv_Area\", \"Overall_Cond\", \"Year_Built\", \n                \"Neighborhood\", \"Total_Bsmt_SF\"),\n    target = \"Sale_Price\"\n  )\n)\n\n# Store with version\nmodel_board %>%\n  vetiver_pin_write(v)\n\n# Create model card documentation\nmodel_card <- \"\n# Ames Housing Price Model\n\n## Model Details\n- **Type**: Elastic Net Regression\n- **Version**: 1.0.0\n- **Training Date**: 2024-01-01\n- **Author**: Data Science Team\n\n## Intended Use\nPredicts sale prices for residential properties in Ames, Iowa.\n\n## Training Data\n- **Source**: Ames Housing Dataset\n- **Size**: 2,930 properties\n- **Time Period**: 2006-2010\n\n## Performance Metrics\n- **RMSE**: $25,432\n- **R-squared**: 0.89\n- **MAE**: $18,234\n\n## Limitations\n- Only applies to Ames, Iowa market\n- May not generalize to luxury homes (>$500k)\n- Requires neighborhood information\n\n## Ethical Considerations\n- Model should not be used for discriminatory pricing\n- Regular audits for fairness across neighborhoods\n\"\n\ncat(model_card)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n# Ames Housing Price Model\n\n## Model Details\n- **Type**: Elastic Net Regression\n- **Version**: 1.0.0\n- **Training Date**: 2024-01-01\n- **Author**: Data Science Team\n\n## Intended Use\nPredicts sale prices for residential properties in Ames, Iowa.\n\n## Training Data\n- **Source**: Ames Housing Dataset\n- **Size**: 2,930 properties\n- **Time Period**: 2006-2010\n\n## Performance Metrics\n- **RMSE**: $25,432\n- **R-squared**: 0.89\n- **MAE**: $18,234\n\n## Limitations\n- Only applies to Ames, Iowa market\n- May not generalize to luxury homes (>$500k)\n- Requires neighborhood information\n\n## Ethical Considerations\n- Model should not be used for discriminatory pricing\n- Regular audits for fairness across neighborhoods\n```\n\n\n:::\n:::\n\n\n## Creating REST APIs with Plumber\n\n### Basic API Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create plumber API file\napi_code <- '\n#* @apiTitle Ames House Price Prediction API\n#* @apiDescription Predicts house prices using machine learning\n#* @apiVersion 1.0.0\n\nlibrary(tidymodels)\nlibrary(vetiver)\n\n# Load model\nmodel <- readRDS(\"models/ames_model_v1.rds\")\n\n#* Health check endpoint\n#* @get /health\nfunction() {\n  list(\n    status = \"healthy\",\n    timestamp = Sys.time(),\n    model_version = \"1.0.0\"\n  )\n}\n\n#* Predict house price\n#* @param Gr_Liv_Area:numeric Living area in square feet\n#* @param Overall_Cond:numeric Overall quality (1-10)\n#* @param Year_Built:numeric Year built\n#* @param Neighborhood:character Neighborhood name\n#* @param Total_Bsmt_SF:numeric Basement square feet\n#* @post /predict\nfunction(Gr_Liv_Area, Overall_Cond, Year_Built, Neighborhood, Total_Bsmt_SF) {\n  \n  # Input validation\n  if (is.na(as.numeric(Gr_Liv_Area))) {\n    stop(\"Gr_Liv_Area must be numeric\")\n  }\n  \n  # Create input data frame\n  input_data <- data.frame(\n    Gr_Liv_Area = as.numeric(Gr_Liv_Area),\n    Overall_Cond = as.numeric(Overall_Cond),\n    Year_Built = as.numeric(Year_Built),\n    Neighborhood = as.character(Neighborhood),\n    Total_Bsmt_SF = as.numeric(Total_Bsmt_SF)\n  )\n  \n  # Make prediction\n  prediction <- predict(model, input_data)\n  \n  # Return result\n  list(\n    predicted_price = prediction$.pred,\n    input = input_data,\n    model_version = \"1.0.0\",\n    timestamp = Sys.time()\n  )\n}\n\n#* Batch predictions\n#* @param data:character JSON string of multiple houses\n#* @post /predict_batch\nfunction(data) {\n  # Parse JSON\n  input_data <- jsonlite::fromJSON(data)\n  \n  # Make predictions\n  predictions <- predict(model, input_data)\n  \n  # Return results\n  list(\n    predictions = predictions$.pred,\n    n_predictions = nrow(predictions),\n    timestamp = Sys.time()\n  )\n}\n\n#* Model information\n#* @get /model_info\nfunction() {\n  list(\n    model_type = \"elastic_net\",\n    features = c(\"Gr_Liv_Area\", \"Overall_Cond\", \"Year_Built\", \n                \"Neighborhood\", \"Total_Bsmt_SF\"),\n    target = \"Sale_Price\",\n    training_date = \"2024-01-01\",\n    performance = list(\n      rmse = 25432,\n      r_squared = 0.89\n    )\n  )\n}\n'\n\n# Save API file\ndir.create(\"api\", showWarnings = FALSE)\nwriteLines(api_code, \"api/model_api.R\")\n\n# To run the API (don't run in notebook):\n# library(plumber)\n# pr(\"api/model_api.R\") %>%\n#   pr_run(port = 8000)\n```\n:::\n\n\n### Advanced API with Authentication and Logging\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadvanced_api <- '\n#* @apiTitle Production House Price API\n#* @apiDescription Enterprise-grade prediction service\n\nlibrary(tidymodels)\nlibrary(logger)\nlibrary(jose)\n\n# Setup logging\nlog_appender(appender_file(\"logs/api.log\"))\n\n# Load model and config\nmodel <- readRDS(\"models/ames_model_v1.rds\")\napi_keys <- readRDS(\"config/api_keys.rds\")  # In production, use env variables\n\n# Request counter for rate limiting\nrequest_counts <- new.env()\n\n#* @filter cors\ncors <- function(req, res) {\n  res$setHeader(\"Access-Control-Allow-Origin\", \"*\")\n  res$setHeader(\"Access-Control-Allow-Methods\", \"GET, POST\")\n  res$setHeader(\"Access-Control-Allow-Headers\", \"Content-Type, X-API-Key\")\n  plumber::forward()\n}\n\n#* @filter authenticate\nfunction(req, res) {\n  # Check API key\n  api_key <- req$HTTP_X_API_KEY\n  \n  if (is.null(api_key) || !api_key %in% api_keys) {\n    res$status <- 401\n    log_warn(\"Unauthorized access attempt\")\n    return(list(error = \"Invalid or missing API key\"))\n  }\n  \n  # Rate limiting\n  if (!exists(api_key, envir = request_counts)) {\n    assign(api_key, list(count = 1, reset_time = Sys.time() + 3600), \n           envir = request_counts)\n  } else {\n    rate_info <- get(api_key, envir = request_counts)\n    if (Sys.time() > rate_info$reset_time) {\n      rate_info <- list(count = 1, reset_time = Sys.time() + 3600)\n    } else if (rate_info$count > 100) {  # 100 requests per hour\n      res$status <- 429\n      return(list(error = \"Rate limit exceeded\"))\n    } else {\n      rate_info$count <- rate_info$count + 1\n    }\n    assign(api_key, rate_info, envir = request_counts)\n  }\n  \n  plumber::forward()\n}\n\n#* Predict with comprehensive logging\n#* @post /predict\nfunction(req, Gr_Liv_Area, Overall_Cond, Year_Built, Neighborhood, Total_Bsmt_SF) {\n  \n  request_id <- uuid::UUIDgenerate()\n  log_info(\"Prediction request\", request_id = request_id)\n  \n  tryCatch({\n    # Input validation\n    input_data <- data.frame(\n      Gr_Liv_Area = as.numeric(Gr_Liv_Area),\n      Overall_Cond = as.numeric(Overall_Cond),\n      Year_Built = as.numeric(Year_Built),\n      Neighborhood = as.character(Neighborhood),\n      Total_Bsmt_SF = as.numeric(Total_Bsmt_SF)\n    )\n    \n    # Data quality checks\n    if (input_data$Gr_Liv_Area < 0 || input_data$Gr_Liv_Area > 10000) {\n      stop(\"Gr_Liv_Area out of valid range\")\n    }\n    \n    if (input_data$Overall_Cond < 1 || input_data$Overall_Cond > 10) {\n      stop(\"Overall_Cond must be between 1 and 10\")\n    }\n    \n    # Make prediction\n    start_time <- Sys.time()\n    prediction <- predict(model, input_data)\n    inference_time <- as.numeric(Sys.time() - start_time, units = \"secs\")\n    \n    # Log successful prediction\n    log_info(\"Prediction successful\", \n             request_id = request_id,\n             predicted_value = prediction$.pred,\n             inference_time = inference_time)\n    \n    # Return result\n    list(\n      request_id = request_id,\n      predicted_price = prediction$.pred,\n      confidence_interval = c(\n        lower = prediction$.pred * 0.9,  # Simplified CI\n        upper = prediction$.pred * 1.1\n      ),\n      input = input_data,\n      inference_time_ms = round(inference_time * 1000, 2),\n      model_version = \"1.0.0\",\n      timestamp = Sys.time()\n    )\n    \n  }, error = function(e) {\n    log_error(\"Prediction failed\", \n              request_id = request_id, \n              error = e$message)\n    res$status <- 400\n    list(\n      request_id = request_id,\n      error = e$message\n    )\n  })\n}\n'\n\nwriteLines(advanced_api, \"api/advanced_api.R\")\n```\n:::\n\n\n## Building Shiny Applications\n\n### Basic Prediction App\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create Shiny app for model deployment\nshiny_app <- '\nlibrary(shiny)\nlibrary(shinydashboard)\nlibrary(tidymodels)\nlibrary(ggplot2)\nlibrary(DT)\n\n# Load model\nmodel <- readRDS(\"models/ames_model_v1.rds\")\n\n# UI\nui <- dashboardPage(\n  dashboardHeader(title = \"House Price Predictor\"),\n  \n  dashboardSidebar(\n    sidebarMenu(\n      menuItem(\"Predict\", tabName = \"predict\", icon = icon(\"calculator\")),\n      menuItem(\"Batch Upload\", tabName = \"batch\", icon = icon(\"upload\")),\n      menuItem(\"Model Info\", tabName = \"info\", icon = icon(\"info-circle\")),\n      menuItem(\"Performance\", tabName = \"performance\", icon = icon(\"chart-line\"))\n    )\n  ),\n  \n  dashboardBody(\n    tabItems(\n      # Prediction tab\n      tabItem(\n        tabName = \"predict\",\n        fluidRow(\n          box(\n            title = \"Input Features\",\n            status = \"primary\",\n            solidHeader = TRUE,\n            width = 6,\n            \n            numericInput(\"gr_liv_area\", \"Living Area (sq ft)\", \n                        value = 1500, min = 0, max = 10000),\n            \n            sliderInput(\"overall_qual\", \"Overall Quality\", \n                       min = 1, max = 10, value = 5),\n            \n            numericInput(\"year_built\", \"Year Built\", \n                        value = 2000, min = 1900, max = 2024),\n            \n            selectInput(\"neighborhood\", \"Neighborhood\",\n                       choices = unique(ames_train$Neighborhood),\n                       selected = \"NAmes\"),\n            \n            numericInput(\"total_bsmt_sf\", \"Basement Area (sq ft)\", \n                        value = 1000, min = 0, max = 5000),\n            \n            actionButton(\"predict\", \"Predict Price\", \n                        class = \"btn-primary btn-lg\")\n          ),\n          \n          box(\n            title = \"Prediction Result\",\n            status = \"success\",\n            solidHeader = TRUE,\n            width = 6,\n            \n            h2(textOutput(\"predicted_price\")),\n            \n            plotOutput(\"confidence_plot\", height = 200),\n            \n            br(),\n            \n            h4(\"Input Summary:\"),\n            tableOutput(\"input_summary\")\n          )\n        )\n      ),\n      \n      # Batch upload tab\n      tabItem(\n        tabName = \"batch\",\n        fluidRow(\n          box(\n            title = \"Upload CSV File\",\n            status = \"primary\",\n            solidHeader = TRUE,\n            width = 12,\n            \n            fileInput(\"file\", \"Choose CSV File\",\n                     accept = c(\".csv\")),\n            \n            actionButton(\"predict_batch\", \"Predict All\", \n                        class = \"btn-success\"),\n            \n            br(), br(),\n            \n            DTOutput(\"batch_results\")\n          )\n        )\n      ),\n      \n      # Model info tab\n      tabItem(\n        tabName = \"info\",\n        fluidRow(\n          box(\n            title = \"Model Information\",\n            status = \"info\",\n            solidHeader = TRUE,\n            width = 12,\n            \n            h3(\"Model Type: Elastic Net Regression\"),\n            \n            h4(\"Features Used:\"),\n            tags$ul(\n              tags$li(\"Living Area (Gr_Liv_Area)\"),\n              tags$li(\"Overall Quality (Overall_Cond)\"),\n              tags$li(\"Year Built (Year_Built)\"),\n              tags$li(\"Neighborhood\"),\n              tags$li(\"Basement Area (Total_Bsmt_SF)\")\n            ),\n            \n            h4(\"Model Performance:\"),\n            tags$ul(\n              tags$li(\"RMSE: $25,432\"),\n              tags$li(\"R-squared: 0.89\"),\n              tags$li(\"MAE: $18,234\")\n            ),\n            \n            h4(\"Training Information:\"),\n            tags$ul(\n              tags$li(\"Training Date: 2024-01-01\"),\n              tags$li(\"Training Samples: 2,197\"),\n              tags$li(\"Validation Method: 5-fold CV\")\n            )\n          )\n        )\n      ),\n      \n      # Performance monitoring tab\n      tabItem(\n        tabName = \"performance\",\n        fluidRow(\n          box(\n            title = \"Model Performance Monitoring\",\n            status = \"warning\",\n            solidHeader = TRUE,\n            width = 12,\n            \n            plotOutput(\"performance_plot\", height = 400),\n            \n            br(),\n            \n            h4(\"Recent Predictions:\"),\n            DTOutput(\"recent_predictions\")\n          )\n        )\n      )\n    )\n  )\n)\n\n# Server\nserver <- function(input, output, session) {\n  \n  # Store predictions for monitoring\n  predictions_log <- reactiveVal(data.frame())\n  \n  # Single prediction\n  observeEvent(input$predict, {\n    \n    # Create input data\n    input_data <- data.frame(\n      Gr_Liv_Area = input$gr_liv_area,\n      Overall_Cond = input$overall_qual,\n      Year_Built = input$year_built,\n      Neighborhood = input$neighborhood,\n      Total_Bsmt_SF = input$total_bsmt_sf\n    )\n    \n    # Make prediction\n    prediction <- predict(model, input_data)\n    \n    # Update predictions log\n    new_log <- rbind(\n      predictions_log(),\n      data.frame(\n        timestamp = Sys.time(),\n        predicted = prediction$.pred,\n        living_area = input$gr_liv_area,\n        quality = input$overall_qual\n      )\n    )\n    predictions_log(new_log)\n    \n    # Display prediction\n    output$predicted_price <- renderText({\n      paste0(\"$\", format(round(prediction$.pred), big.mark = \",\"))\n    })\n    \n    # Confidence interval plot\n    output$confidence_plot <- renderPlot({\n      ci_lower <- prediction$.pred * 0.9\n      ci_upper <- prediction$.pred * 1.1\n      \n      ggplot(data.frame(x = c(ci_lower, prediction$.pred, ci_upper))) +\n        geom_segment(aes(x = ci_lower, xend = ci_upper, y = 0, yend = 0),\n                    size = 2, color = \"steelblue\") +\n        geom_point(aes(x = prediction$.pred, y = 0), \n                  size = 5, color = \"darkblue\") +\n        scale_x_continuous(labels = scales::dollar) +\n        theme_minimal() +\n        theme(axis.text.y = element_blank(),\n              axis.title.y = element_blank()) +\n        labs(x = \"Predicted Price\",\n             title = \"90% Confidence Interval\")\n    })\n    \n    # Input summary\n    output$input_summary <- renderTable({\n      data.frame(\n        Feature = c(\"Living Area\", \"Quality\", \"Year Built\", \n                   \"Neighborhood\", \"Basement\"),\n        Value = c(\n          paste(input$gr_liv_area, \"sq ft\"),\n          paste(input$overall_qual, \"/ 10\"),\n          input$year_built,\n          input$neighborhood,\n          paste(input$total_bsmt_sf, \"sq ft\")\n        )\n      )\n    })\n  })\n  \n  # Batch predictions\n  observeEvent(input$predict_batch, {\n    req(input$file)\n    \n    # Read uploaded file\n    batch_data <- read.csv(input$file$datapath)\n    \n    # Make predictions\n    predictions <- predict(model, batch_data)\n    \n    # Combine with input\n    results <- cbind(batch_data, Predicted_Price = predictions$.pred)\n    \n    # Display results\n    output$batch_results <- renderDT({\n      datatable(results, options = list(pageLength = 10))\n    })\n  })\n  \n  # Performance monitoring\n  output$performance_plot <- renderPlot({\n    if (nrow(predictions_log()) > 0) {\n      ggplot(predictions_log(), aes(x = timestamp, y = predicted)) +\n        geom_line() +\n        geom_point() +\n        scale_y_continuous(labels = scales::dollar) +\n        theme_minimal() +\n        labs(title = \"Predictions Over Time\",\n             x = \"Time\",\n             y = \"Predicted Price\")\n    }\n  })\n  \n  output$recent_predictions <- renderDT({\n    if (nrow(predictions_log()) > 0) {\n      recent <- tail(predictions_log(), 10)\n      recent$predicted <- scales::dollar(round(recent$predicted))\n      datatable(recent, options = list(pageLength = 5))\n    }\n  })\n}\n\n# Run app\nshinyApp(ui = ui, server = server)\n'\n\n# Save Shiny app\ndir.create(\"apps\", showWarnings = FALSE)\nwriteLines(shiny_app, \"apps/prediction_app.R\")\n```\n:::\n\n\n## Docker Containerization\n\n### Creating a Dockerfile\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndockerfile_content <- '\n# Base R image\nFROM rocker/r-ver:4.3.0\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\\\n    libcurl4-openssl-dev \\\\\n    libssl-dev \\\\\n    libxml2-dev \\\\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install R packages\nRUN R -e \"install.packages(c(\\'tidymodels\\', \\'plumber\\', \\'jsonlite\\'), repos=\\'https://cloud.r-project.org/\\')\"\n\n# Create app directory\nWORKDIR /app\n\n# Copy model and API files\nCOPY models/ames_model_v1.rds /app/model.rds\nCOPY api/model_api.R /app/api.R\n\n# Expose port\nEXPOSE 8000\n\n# Run API\nCMD [\"R\", \"-e\", \"plumber::pr(\\'api.R\\') %>% plumber::pr_run(host=\\'0.0.0.0\\', port=8000)\"]\n'\n\ndir.create(\"docker\", showWarnings = FALSE)\nwriteLines(dockerfile_content, \"docker/Dockerfile\")\n\n# Docker compose for multi-container deployment\ndocker_compose <- '\nversion: \"3.8\"\n\nservices:\n  api:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      - MODEL_VERSION=1.0.0\n    volumes:\n      - ./models:/app/models\n      - ./logs:/app/logs\n    restart: unless-stopped\n    \n  shiny:\n    image: rocker/shiny:4.3.0\n    ports:\n      - \"3838:3838\"\n    volumes:\n      - ./apps:/srv/shiny-server/\n      - ./models:/srv/shiny-server/models\n    restart: unless-stopped\n    \n  nginx:\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf\n    depends_on:\n      - api\n      - shiny\n    restart: unless-stopped\n'\n\nwriteLines(docker_compose, \"docker/docker-compose.yml\")\n```\n:::\n\n\n## Model Monitoring and Maintenance\n\n### Performance Tracking\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create monitoring system\ncreate_monitoring_system <- function(model, production_data) {\n  \n  # Calculate current metrics\n  current_predictions <- predict(model, production_data)\n  current_metrics <- production_data %>%\n    bind_cols(current_predictions) %>%\n    metrics(truth = Sale_Price, estimate = .pred)\n  \n  # Compare with baseline\n  baseline_metrics <- tibble(\n    .metric = c(\"rmse\", \"rsq\", \"mae\"),\n    baseline = c(25432, 0.89, 18234)\n  )\n  \n  # Join metrics\n  comparison <- current_metrics %>%\n    left_join(baseline_metrics, by = \".metric\") %>%\n    mutate(\n      degradation = (.estimate - baseline) / baseline * 100,\n      alert = abs(degradation) > 10  # Alert if >10% degradation\n    )\n  \n  return(comparison)\n}\n\n# Simulate production data with drift\nproduction_sample <- ames_test_log %>%\n  mutate(\n    # Simulate data drift - keep as integer\n    Gr_Liv_Area = as.integer(Gr_Liv_Area * runif(n(), 0.9, 1.1)),\n    Year_Built = as.integer(Year_Built + sample(-5:5, n(), replace = TRUE))\n  )\n\nmonitoring_results <- create_monitoring_system(production_fit, production_sample)\nknitr::kable(monitoring_results, digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n|.metric |.estimator | .estimate| baseline| degradation|alert |\n|:-------|:----------|---------:|--------:|-----------:|:-----|\n|rmse    |standard   |      0.15| 25432.00|      -100.0|TRUE  |\n|rsq     |standard   |      0.86|     0.89|        -3.2|FALSE |\n|mae     |standard   |      0.11| 18234.00|      -100.0|TRUE  |\n\n\n:::\n\n```{.r .cell-code}\n# Visualize model performance over time\nsimulate_performance_timeline <- function(n_days = 30) {\n  timeline <- map_df(1:n_days, function(day) {\n    # Simulate daily performance with random variation\n    daily_rmse <- 25432 + rnorm(1, mean = day * 50, sd = 1000)  # Gradual degradation\n    daily_requests <- rpois(1, lambda = 1000)\n    \n    tibble(\n      date = Sys.Date() - n_days + day,\n      rmse = daily_rmse,\n      requests = daily_requests,\n      alert = daily_rmse > 28000  # Alert threshold\n    )\n  })\n  \n  return(timeline)\n}\n\nperformance_timeline <- simulate_performance_timeline()\n\n# Plot monitoring dashboard\np1 <- ggplot(performance_timeline, aes(x = date, y = rmse)) +\n  geom_line(linewidth = 1) +\n  geom_point(aes(color = alert), size = 2) +\n  geom_hline(yintercept = 28000, linetype = \"dashed\", color = \"red\") +\n  scale_color_manual(values = c(\"FALSE\" = \"steelblue\", \"TRUE\" = \"red\")) +\n  labs(title = \"Model RMSE Over Time\",\n       subtitle = \"Red line indicates alert threshold\",\n       y = \"RMSE\") +\n  theme(legend.position = \"none\")\n\np2 <- ggplot(performance_timeline, aes(x = date, y = requests)) +\n  geom_col(fill = \"steelblue\", alpha = 0.7) +\n  labs(title = \"Daily Request Volume\",\n       y = \"Requests\")\n\nlibrary(patchwork)\np1 / p2\n```\n\n::: {.cell-output-display}\n![](18-model-deployment_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n### Data Drift Detection\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Detect feature drift\ndetect_drift <- function(training_data, production_data, threshold = 0.1) {\n  \n  drift_results <- map_df(names(training_data), function(col) {\n    if (is.numeric(training_data[[col]])) {\n      # Kolmogorov-Smirnov test for continuous variables\n      ks_test <- ks.test(training_data[[col]], production_data[[col]])\n      \n      tibble(\n        feature = col,\n        test_type = \"KS\",\n        p_value = ks_test$p.value,\n        drift_detected = ks_test$p.value < threshold,\n        train_mean = mean(training_data[[col]], na.rm = TRUE),\n        prod_mean = mean(production_data[[col]], na.rm = TRUE),\n        mean_shift = abs(train_mean - prod_mean) / train_mean\n      )\n    } else {\n      # Chi-square test for categorical variables\n      train_prop <- table(training_data[[col]]) / nrow(training_data)\n      prod_prop <- table(production_data[[col]]) / nrow(production_data)\n      \n      # Ensure same categories\n      all_cats <- union(names(train_prop), names(prod_prop))\n      train_prop <- train_prop[all_cats]\n      prod_prop <- prod_prop[all_cats]\n      train_prop[is.na(train_prop)] <- 0\n      prod_prop[is.na(prod_prop)] <- 0\n      \n      chi_test <- chisq.test(rbind(train_prop, prod_prop))\n      \n      tibble(\n        feature = col,\n        test_type = \"Chi-square\",\n        p_value = chi_test$p.value,\n        drift_detected = chi_test$p.value < threshold,\n        train_mean = NA_real_,\n        prod_mean = NA_real_,\n        mean_shift = NA_real_\n      )\n    }\n  })\n  \n  return(drift_results)\n}\n\n# Test drift detection\ntraining_sample <- ames_train %>%\n  select(Gr_Liv_Area, Overall_Cond, Year_Built, Neighborhood, Total_Bsmt_SF) %>%\n  slice_sample(n = 200)\n\nproduction_sample_drift <- training_sample %>%\n  mutate(\n    Gr_Liv_Area = Gr_Liv_Area * 1.2,  # Introduce drift\n    Year_Built = Year_Built + 10       # Houses are newer\n  )\n\ndrift_analysis <- detect_drift(training_sample, production_sample_drift)\nknitr::kable(drift_analysis, digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|feature       |test_type  | p_value|drift_detected | train_mean| prod_mean| mean_shift|\n|:-------------|:----------|-------:|:--------------|----------:|---------:|----------:|\n|Gr_Liv_Area   |KS         |       0|TRUE           |   1496.695|  1796.034|      0.200|\n|Overall_Cond  |Chi-square |     NaN|NA             |         NA|        NA|         NA|\n|Year_Built    |KS         |       0|TRUE           |   1970.110|  1980.110|      0.005|\n|Neighborhood  |Chi-square |     NaN|NA             |         NA|        NA|         NA|\n|Total_Bsmt_SF |KS         |       1|FALSE          |   1008.070|  1008.070|      0.000|\n\n\n:::\n\n```{.r .cell-code}\n# Visualize drift\ndrift_viz <- training_sample %>%\n  mutate(dataset = \"Training\") %>%\n  bind_rows(production_sample_drift %>% mutate(dataset = \"Production\"))\n\nggplot(drift_viz, aes(x = Gr_Liv_Area, fill = dataset)) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Feature Drift: Living Area\",\n       subtitle = \"Distribution shift between training and production\")\n```\n\n::: {.cell-output-display}\n![](18-model-deployment_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n## A/B Testing and Gradual Rollouts\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Implement A/B testing framework\nab_test_framework <- function(model_a, model_b, test_data, split_ratio = 0.5) {\n  \n  n <- nrow(test_data)\n  \n  # Random assignment to groups\n  assignment <- sample(c(\"A\", \"B\"), n, replace = TRUE, \n                      prob = c(split_ratio, 1 - split_ratio))\n  \n  # Make predictions with each model\n  results <- test_data %>%\n    mutate(\n      group = assignment,\n      prediction = if_else(\n        group == \"A\",\n        predict(model_a, test_data)$.pred,\n        predict(model_b, test_data)$.pred\n      ),\n      error = abs(Sale_Price - prediction)\n    )\n  \n  # Calculate metrics per group\n  group_metrics <- results %>%\n    group_by(group) %>%\n    summarise(\n      n = n(),\n      rmse = sqrt(mean(error^2)),\n      mae = mean(error),\n      median_error = median(error),\n      .groups = \"drop\"\n    )\n  \n  # Statistical test\n  t_test <- t.test(error ~ group, data = results)\n  \n  return(list(\n    metrics = group_metrics,\n    test = t_test,\n    winner = if_else(t_test$p.value < 0.05,\n                    if_else(group_metrics$rmse[1] < group_metrics$rmse[2], \n                           \"Model A\", \"Model B\"),\n                    \"No significant difference\")\n  ))\n}\n\n# Create alternative model for testing\nalternative_model <- workflow() %>%\n  add_recipe(production_recipe) %>%\n  add_model(rand_forest(trees = 100) %>% \n           set_engine(\"ranger\") %>% \n           set_mode(\"regression\")) %>%\n  fit(ames_train_log)\n\n# Run A/B test\nab_results <- ab_test_framework(production_fit, alternative_model, ames_test_log)\n\ncat(\"A/B Test Results:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nA/B Test Results:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(ab_results$metrics)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 5\n  group     n  rmse   mae median_error\n  <chr> <int> <dbl> <dbl>        <dbl>\n1 A       368 0.152 0.109       0.0757\n2 B       365 0.151 0.109       0.0838\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nWinner:\", ab_results$winner, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nWinner: No significant difference \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"P-value:\", ab_results$test$p.value, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nP-value: 0.9844975 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Gradual rollout simulation\nsimulate_gradual_rollout <- function(days = 14) {\n  rollout_schedule <- tibble(\n    day = 1:days,\n    traffic_percentage = pmin(100, 5 * 1.5^(day - 1)),  # Exponential increase\n    errors_detected = rpois(days, lambda = 100 - traffic_percentage) / 10,\n    rollback = errors_detected > 5\n  )\n  \n  # Find rollback point if any\n  rollback_day <- which(rollout_schedule$rollback)[1]\n  if (!is.na(rollback_day)) {\n    rollout_schedule$traffic_percentage[rollback_day:days] <- 0\n  }\n  \n  return(rollout_schedule)\n}\n\nrollout <- simulate_gradual_rollout()\n\nggplot(rollout, aes(x = day)) +\n  geom_area(aes(y = traffic_percentage), fill = \"steelblue\", alpha = 0.7) +\n  geom_point(aes(y = errors_detected * 10), color = \"red\", size = 2) +\n  scale_y_continuous(\n    name = \"Traffic Percentage\",\n    sec.axis = sec_axis(~./10, name = \"Errors Detected\")\n  ) +\n  labs(title = \"Gradual Model Rollout\",\n       subtitle = \"Blue area shows traffic %, red points show errors\")\n```\n\n::: {.cell-output-display}\n![](18-model-deployment_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n## Best Practices Checklist\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Production readiness checklist\nproduction_checklist <- tibble(\n  Category = c(\n    rep(\"Model\", 4),\n    rep(\"Code\", 4),\n    rep(\"Infrastructure\", 4),\n    rep(\"Monitoring\", 4),\n    rep(\"Documentation\", 3)\n  ),\n  Item = c(\n    # Model\n    \"Model validated on holdout set\",\n    \"Handles missing values gracefully\",\n    \"Handles new categories\",\n    \"Performance meets requirements\",\n    \n    # Code\n    \"Code review completed\",\n    \"Unit tests written\",\n    \"Integration tests passed\",\n    \"Error handling implemented\",\n    \n    # Infrastructure\n    \"API endpoints defined\",\n    \"Load testing completed\",\n    \"Backup strategy in place\",\n    \"Rollback plan documented\",\n    \n    # Monitoring\n    \"Logging configured\",\n    \"Alerts set up\",\n    \"Performance metrics tracked\",\n    \"Data drift detection enabled\",\n    \n    # Documentation\n    \"API documentation complete\",\n    \"Model card created\",\n    \"Runbook available\"\n  ),\n  Status = c(\n    \"✅\", \"✅\", \"✅\", \"✅\",  # Model\n    \"✅\", \"✅\", \"⚠️\", \"✅\",  # Code\n    \"✅\", \"⚠️\", \"✅\", \"✅\",  # Infrastructure\n    \"✅\", \"✅\", \"⚠️\", \"❌\",  # Monitoring\n    \"✅\", \"✅\", \"⚠️\"        # Documentation\n  ),\n  Priority = c(\n    \"High\", \"High\", \"High\", \"High\",\n    \"High\", \"High\", \"Medium\", \"High\",\n    \"High\", \"Medium\", \"High\", \"High\",\n    \"High\", \"High\", \"Medium\", \"Low\",\n    \"Medium\", \"High\", \"Medium\"\n  )\n)\n\nknitr::kable(production_checklist)\n```\n\n::: {.cell-output-display}\n\n\n|Category       |Item                              |Status           |Priority |\n|:--------------|:---------------------------------|:----------------|:--------|\n|Model          |Model validated on holdout set    |<U+2705>         |High     |\n|Model          |Handles missing values gracefully |<U+2705>         |High     |\n|Model          |Handles new categories            |<U+2705>         |High     |\n|Model          |Performance meets requirements    |<U+2705>         |High     |\n|Code           |Code review completed             |<U+2705>         |High     |\n|Code           |Unit tests written                |<U+2705>         |High     |\n|Code           |Integration tests passed          |<U+26A0><U+FE0F> |Medium   |\n|Code           |Error handling implemented        |<U+2705>         |High     |\n|Infrastructure |API endpoints defined             |<U+2705>         |High     |\n|Infrastructure |Load testing completed            |<U+26A0><U+FE0F> |Medium   |\n|Infrastructure |Backup strategy in place          |<U+2705>         |High     |\n|Infrastructure |Rollback plan documented          |<U+2705>         |High     |\n|Monitoring     |Logging configured                |<U+2705>         |High     |\n|Monitoring     |Alerts set up                     |<U+2705>         |High     |\n|Monitoring     |Performance metrics tracked       |<U+26A0><U+FE0F> |Medium   |\n|Monitoring     |Data drift detection enabled      |<U+274C>         |Low      |\n|Documentation  |API documentation complete        |<U+2705>         |Medium   |\n|Documentation  |Model card created                |<U+2705>         |High     |\n|Documentation  |Runbook available                 |<U+26A0><U+FE0F> |Medium   |\n\n\n:::\n\n```{.r .cell-code}\n# Calculate readiness score\nreadiness_score <- sum(production_checklist$Status == \"✅\") / \n                  nrow(production_checklist) * 100\n\ncat(\"\\nProduction Readiness Score:\", round(readiness_score, 1), \"%\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nProduction Readiness Score: 73.7 %\n```\n\n\n:::\n\n```{.r .cell-code}\n# Priority action items\naction_items <- production_checklist %>%\n  filter(Status != \"✅\") %>%\n  arrange(match(Priority, c(\"High\", \"Medium\", \"Low\")))\n\ncat(\"\\nAction Items:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAction Items:\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(action_items)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 4\n  Category       Item                         Status           Priority\n  <chr>          <chr>                        <chr>            <chr>   \n1 Code           Integration tests passed     <U+26A0><U+FE0F> Medium  \n2 Infrastructure Load testing completed       <U+26A0><U+FE0F> Medium  \n3 Monitoring     Performance metrics tracked  <U+26A0><U+FE0F> Medium  \n4 Documentation  Runbook available            <U+26A0><U+FE0F> Medium  \n5 Monitoring     Data drift detection enabled <U+274C>         Low     \n```\n\n\n:::\n:::\n\n\n## Exercises\n\n### Exercise 1: Create a Model Registry\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Implement a model registry system\ncreate_model_registry <- function() {\n  \n  # Registry structure\n  registry <- list(\n    models = list(),\n    metadata = list(),\n    performance = list()\n  )\n  \n  # Register model function\n  register_model <- function(model, name, version, metrics, metadata = list()) {\n    model_id <- paste0(name, \"_v\", version)\n    \n    registry$models[[model_id]] <<- model\n    registry$metadata[[model_id]] <<- c(\n      list(\n        name = name,\n        version = version,\n        registration_date = Sys.Date()\n      ),\n      metadata\n    )\n    registry$performance[[model_id]] <<- metrics\n    \n    cat(\"Model\", model_id, \"registered successfully\\n\")\n  }\n  \n  # Get model function\n  get_model <- function(name, version = \"latest\") {\n    if (version == \"latest\") {\n      # Find latest version\n      all_versions <- grep(paste0(\"^\", name, \"_v\"), \n                          names(registry$models), value = TRUE)\n      if (length(all_versions) == 0) {\n        stop(\"Model not found\")\n      }\n      model_id <- all_versions[length(all_versions)]\n    } else {\n      model_id <- paste0(name, \"_v\", version)\n    }\n    \n    if (!model_id %in% names(registry$models)) {\n      stop(\"Model version not found\")\n    }\n    \n    return(list(\n      model = registry$models[[model_id]],\n      metadata = registry$metadata[[model_id]],\n      performance = registry$performance[[model_id]]\n    ))\n  }\n  \n  # Compare models function\n  compare_models <- function(name) {\n    all_versions <- grep(paste0(\"^\", name, \"_v\"), \n                        names(registry$models), value = TRUE)\n    \n    comparison <- map_df(all_versions, function(model_id) {\n      perf <- registry$performance[[model_id]]\n      meta <- registry$metadata[[model_id]]\n      \n      tibble(\n        version = meta$version,\n        date = meta$registration_date,\n        rmse = perf$rmse,\n        rsq = perf$rsq\n      )\n    })\n    \n    return(comparison)\n  }\n  \n  # Return registry functions\n  list(\n    register = register_model,\n    get = get_model,\n    compare = compare_models\n  )\n}\n\n# Use the registry\nregistry <- create_model_registry()\n\n# Register models\nregistry$register(\n  production_fit, \n  \"ames_predictor\", \n  \"1.0.0\",\n  list(rmse = 25432, rsq = 0.89),\n  list(algorithm = \"elastic_net\")\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel ames_predictor_v1.0.0 registered successfully\n```\n\n\n:::\n\n```{.r .cell-code}\nregistry$register(\n  alternative_model, \n  \"ames_predictor\", \n  \"1.1.0\",\n  list(rmse = 24800, rsq = 0.91),\n  list(algorithm = \"random_forest\")\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel ames_predictor_v1.1.0 registered successfully\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare versions\ncomparison <- registry$compare(\"ames_predictor\")\nprint(comparison)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 4\n  version date        rmse   rsq\n  <chr>   <date>     <dbl> <dbl>\n1 1.0.0   2025-10-01 25432  0.89\n2 1.1.0   2025-10-01 24800  0.91\n```\n\n\n:::\n:::\n\n\n### Exercise 2: Implement Model Retraining Pipeline\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Automated retraining pipeline\ncreate_retraining_pipeline <- function(current_model, retraining_threshold = 0.1) {\n  \n  # Monitor performance\n  monitor <- function(model, new_data) {\n    predictions <- predict(model, new_data)\n    metrics <- new_data %>%\n      bind_cols(predictions) %>%\n      metrics(truth = Sale_Price, estimate = .pred)\n    \n    return(metrics)\n  }\n  \n  # Check if retraining needed\n  should_retrain <- function(current_metrics, baseline_metrics) {\n    rmse_degradation <- (current_metrics$rmse - baseline_metrics$rmse) / \n                       baseline_metrics$rmse\n    \n    return(rmse_degradation > retraining_threshold)\n  }\n  \n  # Retrain model\n  retrain <- function(new_data) {\n    cat(\"Retraining model with\", nrow(new_data), \"samples\\n\")\n    \n    # Create new workflow\n    new_workflow <- workflow() %>%\n      add_recipe(production_recipe) %>%\n      add_model(production_model)\n    \n    # Fit on new data\n    new_fit <- new_workflow %>%\n      fit(new_data)\n    \n    # Validate on holdout\n    validation_split <- initial_split(new_data, prop = 0.8)\n    validation_metrics <- new_fit %>%\n      predict(testing(validation_split)) %>%\n      bind_cols(testing(validation_split)) %>%\n      metrics(truth = Sale_Price, estimate = .pred)\n    \n    return(list(\n      model = new_fit,\n      metrics = validation_metrics\n    ))\n  }\n  \n  # Pipeline execution\n  execute <- function(new_data) {\n    # Current performance\n    current_metrics <- monitor(current_model, new_data)\n    baseline_metrics <- list(rmse = 25432, rsq = 0.89)\n    \n    cat(\"Current RMSE:\", current_metrics %>% \n        filter(.metric == \"rmse\") %>% \n        pull(.estimate), \"\\n\")\n    \n    if (should_retrain(\n      list(rmse = current_metrics %>% \n             filter(.metric == \"rmse\") %>% \n             pull(.estimate)),\n      baseline_metrics)) {\n      \n      cat(\"Performance degradation detected. Initiating retraining...\\n\")\n      retrain_result <- retrain(new_data)\n      \n      cat(\"New model RMSE:\", retrain_result$metrics %>% \n          filter(.metric == \"rmse\") %>% \n          pull(.estimate), \"\\n\")\n      \n      return(retrain_result)\n    } else {\n      cat(\"Model performance acceptable. No retraining needed.\\n\")\n      return(NULL)\n    }\n  }\n  \n  return(execute)\n}\n\n# Test the pipeline\npipeline <- create_retraining_pipeline(production_fit)\n\n# Simulate degraded performance data\ndegraded_data <- ames_test_log %>%\n  mutate(Sale_Price = Sale_Price * runif(n(), 0.7, 1.3))  # Add noise\n\nresult <- pipeline(degraded_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCurrent RMSE: 2.037085 \nModel performance acceptable. No retraining needed.\n```\n\n\n:::\n:::\n\n\n### Exercise 3: Create Performance Dashboard\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Create monitoring dashboard data\ncreate_dashboard_data <- function(n_days = 30) {\n  \n  # Simulate daily metrics\n  daily_data <- map_df(1:n_days, function(day) {\n    tibble(\n      date = Sys.Date() - n_days + day,\n      \n      # Model metrics\n      predictions = rpois(1, 1000 + day * 10),\n      avg_response_time = rnorm(1, 50, 10),\n      error_rate = rbeta(1, 1, 100),\n      \n      # Business metrics  \n      conversion_rate = rbeta(1, 10, 90),\n      revenue_impact = rnorm(1, 10000, 2000),\n      \n      # System metrics\n      cpu_usage = rbeta(1, 20, 80),\n      memory_usage = rbeta(1, 30, 70),\n      \n      # Data quality\n      missing_features = rpois(1, 5),\n      out_of_range = rpois(1, 3)\n    )\n  })\n  \n  return(daily_data)\n}\n\ndashboard_data <- create_dashboard_data()\n\n# Create dashboard visualizations\np1 <- ggplot(dashboard_data, aes(x = date, y = predictions)) +\n  geom_line(color = \"steelblue\", linewidth = 1) +\n  geom_smooth(se = FALSE, color = \"red\") +\n  labs(title = \"Daily Predictions\", y = \"Count\")\n\np2 <- ggplot(dashboard_data, aes(x = date, y = avg_response_time)) +\n  geom_line(color = \"darkgreen\", linewidth = 1) +\n  geom_hline(yintercept = 100, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Response Time\", y = \"Milliseconds\")\n\np3 <- ggplot(dashboard_data, aes(x = date)) +\n  geom_ribbon(aes(ymin = 0, ymax = cpu_usage), fill = \"blue\", alpha = 0.3) +\n  geom_ribbon(aes(ymin = 0, ymax = memory_usage), fill = \"red\", alpha = 0.3) +\n  labs(title = \"Resource Usage\", y = \"Percentage\") +\n  scale_y_continuous(labels = scales::percent)\n\np4 <- ggplot(dashboard_data, aes(x = date, y = revenue_impact)) +\n  geom_col(fill = \"gold\", alpha = 0.7) +\n  scale_y_continuous(labels = scales::dollar) +\n  labs(title = \"Revenue Impact\", y = \"Daily Revenue\")\n\n(p1 + p2) / (p3 + p4)\n```\n\n::: {.cell-output-display}\n![](18-model-deployment_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Summary statistics\nsummary_stats <- dashboard_data %>%\n  summarise(\n    total_predictions = sum(predictions),\n    avg_response_time = mean(avg_response_time),\n    total_revenue = sum(revenue_impact),\n    avg_cpu = mean(cpu_usage),\n    total_errors = sum(missing_features + out_of_range)\n  )\n\ncat(\"\\nDashboard Summary (Last 30 Days):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nDashboard Summary (Last 30 Days):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Total Predictions:\", format(summary_stats$total_predictions, big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTotal Predictions: 34,662 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Avg Response Time:\", round(summary_stats$avg_response_time, 1), \"ms\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAvg Response Time: 46.7 ms\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Total Revenue Impact: $\", format(round(summary_stats$total_revenue), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTotal Revenue Impact: $ 290,824 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Average CPU Usage:\", round(summary_stats$avg_cpu * 100, 1), \"%\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAverage CPU Usage: 18.9 %\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Total Data Errors:\", summary_stats$total_errors, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTotal Data Errors: 222 \n```\n\n\n:::\n:::\n\n\n## Summary\n\nIn this comprehensive final chapter, you've mastered:\n\n✅ **Model deployment fundamentals**\n\n  - Serialization and versioning\n  - Model registries\n  - Production readiness\n\n✅ **API development**\n\n  - REST APIs with plumber\n  - Authentication and rate limiting\n  - Error handling and logging\n\n✅ **Application development**\n\n  - Shiny dashboards\n  - User interfaces\n  - Batch processing\n\n✅ **Containerization**\n\n  - Docker for R models\n  - Multi-container orchestration\n  - Environment consistency\n\n✅ **Monitoring and maintenance**\n\n  - Performance tracking\n  - Data drift detection\n  - Automated retraining\n\n✅ **Production best practices**\n\n  - A/B testing\n  - Gradual rollouts\n  - Production checklists\n\nKey takeaways:\n\n- Deployment is as important as model development\n- Monitor everything in production\n- Version control is crucial for models\n- Automate retraining pipelines\n- Plan for failure and rollback\n- Documentation is essential\n\n## Final Assessment\n\nCongratulations on completing Block 3! Before we conclude this comprehensive workshop, we recommend taking our **[Block 3 Assessment](quiz-block-3.qmd)** to test your mastery of advanced machine learning concepts including classification, regression, ensemble methods, unsupervised learning, and model deployment.\n\nThis final assessment will help consolidate your understanding of the sophisticated techniques you've learned and prepare you for real-world machine learning applications.\n\n## Course Conclusion\n\nCongratulations! You've completed a comprehensive journey through:\n\n- **Tidyverse** fundamentals and advanced techniques\n- **Tidymodels** framework for machine learning\n- **Production deployment** of ML systems\n\nYou now have the skills to:\n\n- Wrangle and visualize data efficiently\n- Build and evaluate machine learning models\n- Deploy models to production systems\n- Monitor and maintain ML applications\n\nRemember: The journey doesn't end here. Continue practicing, stay curious, and keep learning!\n\n## Additional Resources\n\n- [Engineering Production Machine Learning Systems](https://www.oreilly.com/library/view/building-machine-learning/9781492053187/)\n- [plumber Documentation](https://www.rplumber.io/)\n- [Shiny Documentation](https://shiny.rstudio.com/)\n- [Docker for Data Science](https://www.docker.com/blog/docker-for-data-science/)\n- [MLOps: Continuous Delivery for ML](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)\n",
    "supporting": [
      "18-model-deployment_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}