{
  "hash": "4d7557ad7c977fdf9b17b754cf318e42",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 12: Workflows and Model Evaluation - Building Reproducible ML Pipelines\"\ndate: today\nformat:\n  html:\n    code-fold: false\n    code-tools: true\n---\n\n## Learning Objectives\n\nBy the end of this chapter, you will master:\n\n- The workflow concept and its importance\n- Combining preprocessing and modeling\n- Workflow sets for comparing multiple approaches\n- Comprehensive model evaluation with yardstick\n- Custom metrics and metric sets\n- Visualizing model performance\n- Workflow extraction and modification\n- Best practices for reproducible ML pipelines\n\n## Why Workflows Matter\n\nImagine you're baking a complex cake. You wouldn't just throw ingredients together randomly - you'd follow a recipe that specifies the exact order of operations: mix dry ingredients, cream butter and sugar, combine wet and dry, bake at specific temperature. Machine learning is similar: the order and combination of steps matters immensely.\n\nA workflow in tidymodels bundles together:\n1. **Preprocessing** (recipes)\n2. **Model specification** (parsnip)\n3. **Post-processing** (if needed)\n\nThis bundling ensures:\n- **Reproducibility**: The exact same steps are applied to new data\n- **Prevention of data leakage**: Preprocessing parameters are learned only from training data\n- **Simplicity**: One object contains your entire modeling pipeline\n- **Flexibility**: Easy to swap components and compare approaches\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Attaching packages -------------------------------------- tidymodels 1.4.1 --\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nv broom        1.0.10     v recipes      1.3.1 \nv dials        1.4.2      v rsample      1.3.1 \nv dplyr        1.1.4      v tailor       0.1.0 \nv ggplot2      4.0.0      v tidyr        1.3.1 \nv infer        1.0.9      v tune         2.0.0 \nv modeldata    1.5.1      v workflows    1.3.0 \nv parsnip      1.3.3      v workflowsets 1.1.1 \nv purrr        1.1.0      v yardstick    1.3.2 \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx purrr::discard() masks scales::discard()\nx dplyr::filter()  masks stats::filter()\nx dplyr::lag()     masks stats::lag()\nx recipes::step()  masks stats::step()\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv forcats   1.0.0     v stringr   1.5.2\nv lubridate 1.9.4     v tibble    3.3.0\nv readr     2.1.5     \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx readr::col_factor() masks scales::col_factor()\nx purrr::discard()    masks scales::discard()\nx dplyr::filter()     masks stats::filter()\nx stringr::fixed()    masks recipes::fixed()\nx dplyr::lag()        masks stats::lag()\nx readr::spec()       masks yardstick::spec()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(modeldata)\nlibrary(vip)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAdjuntando el paquete: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(patchwork)\nlibrary(workflowsets)\nlibrary(probably)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAdjuntando el paquete: 'probably'\n\nThe following objects are masked from 'package:base':\n\n    as.factor, as.ordered\n```\n\n\n:::\n\n```{.r .cell-code}\n# Set theme and seed\ntheme_set(theme_minimal())\nset.seed(123)\n\n# Load example data\ndata(ames)\names_split <- initial_split(ames, prop = 0.75, strata = Sale_Price)\names_train <- training(ames_split)\names_test <- testing(ames_split)\n```\n:::\n\n\n## Building Your First Workflow\n\nLet's start by understanding the problem with not using workflows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# The WRONG way - manual preprocessing\n# This approach is error-prone and can lead to data leakage\n\n# Manual preprocessing on training data\names_train_processed <- ames_train %>%\n  mutate(\n    # Log transform - uses training data statistics\n    Sale_Price_log = log(Sale_Price),\n    # Scaling - WRONG! Uses all training data including validation folds\n    Gr_Liv_Area_scaled = scale(Gr_Liv_Area)[,1]\n  )\n\n# Now we need to remember these transformations for test data\n# And apply them consistently... but what were the scaling parameters?\n\n# The RIGHT way - using workflows\n# Step 1: Create a recipe\names_recipe <- recipe(Sale_Price ~ Gr_Liv_Area + Year_Built + Total_Bsmt_SF + \n                      Neighborhood, \n                      data = ames_train) %>%\n  step_normalize(all_numeric_predictors()) %>%\n  step_dummy(all_nominal_predictors())\n\n# Step 2: Create a model specification\nlm_spec <- linear_reg() %>%\n  set_engine(\"lm\")\n\n# Step 3: Combine into a workflow\nlm_workflow <- workflow() %>%\n  add_recipe(ames_recipe) %>%\n  add_model(lm_spec)\n\nprint(lm_workflow)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: linear_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n2 Recipe Steps\n\n* step_normalize()\n* step_dummy()\n\n-- Model -----------------------------------------------------------------------\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n\n\n:::\n:::\n\n\nThe workflow object now contains everything needed to go from raw data to predictions. This is incredibly powerful for maintaining consistency across training, validation, and deployment.\n\n## Workflow Components\n\n### Adding and Modifying Components\n\nWorkflows are modular - you can add, remove, or update components:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Start with an empty workflow\nbase_workflow <- workflow()\n\n# Add components step by step\nbase_workflow <- base_workflow %>%\n  add_recipe(ames_recipe)\n\nprint(\"After adding recipe:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"After adding recipe:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(base_workflow)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: None\n\n-- Preprocessor ----------------------------------------------------------------\n2 Recipe Steps\n\n* step_normalize()\n* step_dummy()\n```\n\n\n:::\n\n```{.r .cell-code}\nbase_workflow <- base_workflow %>%\n  add_model(lm_spec)\n\nprint(\"After adding model:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"After adding model:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(base_workflow)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: linear_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n2 Recipe Steps\n\n* step_normalize()\n* step_dummy()\n\n-- Model -----------------------------------------------------------------------\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n\n\n:::\n\n```{.r .cell-code}\n# You can also update components\nupdated_workflow <- lm_workflow %>%\n  update_model(\n    linear_reg(penalty = 0.01) %>%\n      set_engine(\"glmnet\")\n  )\n\nprint(\"After updating model:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"After updating model:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(updated_workflow)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: linear_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n2 Recipe Steps\n\n* step_normalize()\n* step_dummy()\n\n-- Model -----------------------------------------------------------------------\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 0.01\n\nComputational engine: glmnet \n```\n\n\n:::\n\n```{.r .cell-code}\n# Or remove components\nrecipe_only <- lm_workflow %>%\n  remove_model()\n\nprint(\"After removing model:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"After removing model:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(recipe_only)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: None\n\n-- Preprocessor ----------------------------------------------------------------\n2 Recipe Steps\n\n* step_normalize()\n* step_dummy()\n```\n\n\n:::\n:::\n\n\nThis modularity is essential for:\n- **Experimentation**: Quickly try different models with same preprocessing\n- **Model comparison**: Keep preprocessing constant while varying models\n- **Debugging**: Test components independently\n\n### Formula vs Recipe Interface\n\nWorkflows support two interfaces for specifying predictors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Method 1: Formula interface (simple, no preprocessing)\nformula_workflow <- workflow() %>%\n  add_formula(Sale_Price ~ Gr_Liv_Area + Overall_Cond) %>%\n  add_model(lm_spec)\n\n# Method 2: Recipe interface (complex preprocessing)\nrecipe_workflow <- workflow() %>%\n  add_recipe(ames_recipe) %>%\n  add_model(lm_spec)\n\n# Method 3: Variables interface (programmatic)\nvars_workflow <- workflow() %>%\n  add_variables(\n    outcomes = Sale_Price,\n    predictors = c(Gr_Liv_Area, Overall_Cond, Neighborhood)\n  ) %>%\n  add_model(lm_spec)\n\n# Compare the approaches\nprint(\"Formula approach:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Formula approach:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nformula_workflow\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n== Workflow ====================================================================\nPreprocessor: Formula\nModel: linear_reg()\n\n-- Preprocessor ----------------------------------------------------------------\nSale_Price ~ Gr_Liv_Area + Overall_Cond\n\n-- Model -----------------------------------------------------------------------\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"\\nRecipe approach:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"\\nRecipe approach:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nrecipe_workflow\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: linear_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n2 Recipe Steps\n\n* step_normalize()\n* step_dummy()\n\n-- Model -----------------------------------------------------------------------\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"\\nVariables approach:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"\\nVariables approach:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nvars_workflow\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n== Workflow ====================================================================\nPreprocessor: Variables\nModel: linear_reg()\n\n-- Preprocessor ----------------------------------------------------------------\nOutcomes: Sale_Price\nPredictors: c(Gr_Liv_Area, Overall_Cond, Neighborhood)\n\n-- Model -----------------------------------------------------------------------\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n\n\n:::\n:::\n\n\nChoose based on your needs:\n- **Formula**: Quick prototyping, simple models\n- **Recipe**: Complex preprocessing, feature engineering\n- **Variables**: Programmatic variable selection\n\n## Fitting and Predicting with Workflows\n\nThe workflow handles all the complexity of applying transformations consistently:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the workflow\nlm_fit <- lm_workflow %>%\n  fit(data = ames_train)\n\n# The fitted workflow contains:\n# 1. The prepared recipe (with learned parameters)\n# 2. The fitted model\nprint(lm_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n== Workflow [trained] ==========================================================\nPreprocessor: Recipe\nModel: linear_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n2 Recipe Steps\n\n* step_normalize()\n* step_dummy()\n\n-- Model -----------------------------------------------------------------------\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n                                         (Intercept)  \n                                              164586  \n                                         Gr_Liv_Area  \n                                               33020  \n                                          Year_Built  \n                                               16040  \n                                       Total_Bsmt_SF  \n                                               15285  \n                          Neighborhood_College_Creek  \n                                               18650  \n                               Neighborhood_Old_Town  \n                                               -1959  \n                                Neighborhood_Edwards  \n                                              -10541  \n                               Neighborhood_Somerset  \n                                               37339  \n                     Neighborhood_Northridge_Heights  \n                                               89923  \n                                Neighborhood_Gilbert  \n                                                9375  \n                                 Neighborhood_Sawyer  \n                                                2027  \n                         Neighborhood_Northwest_Ames  \n                                                3646  \n                            Neighborhood_Sawyer_West  \n                                                4397  \n                               Neighborhood_Mitchell  \n                                                1085  \n                              Neighborhood_Brookside  \n                                                6919  \n                               Neighborhood_Crawford  \n                                               44017  \n                 Neighborhood_Iowa_DOT_and_Rail_Road  \n                                               -9375  \n                             Neighborhood_Timberland  \n                                               44374  \n                             Neighborhood_Northridge  \n                                               74323  \n                            Neighborhood_Stone_Brook  \n                                              101480  \nNeighborhood_South_and_West_of_Iowa_State_University  \n                                               -8260  \n                            Neighborhood_Clear_Creek  \n                                               15883  \n                         Neighborhood_Meadow_Village  \n\n...\nand 20 more lines.\n```\n\n\n:::\n\n```{.r .cell-code}\n# Make predictions - automatically applies all preprocessing!\npredictions <- lm_fit %>%\n  predict(ames_test)\n\nhead(predictions)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 x 1\n    .pred\n    <dbl>\n1 114373.\n2 155891.\n3 191539.\n4 190392.\n5 270562.\n6 206107.\n```\n\n\n:::\n\n```{.r .cell-code}\n# Get multiple types of predictions\nall_predictions <- bind_cols(\n  ames_test %>% select(Sale_Price),\n  predict(lm_fit, ames_test),           # Point predictions\n  predict(lm_fit, ames_test, type = \"conf_int\")  # Confidence intervals for lm\n)\n\nhead(all_predictions)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 x 4\n  Sale_Price   .pred .pred_lower .pred_upper\n       <int>   <dbl>       <dbl>       <dbl>\n1     105000 114373.     110063.     118683.\n2     172000 155891.     151593.     160189.\n3     189900 191539.     184896.     198181.\n4     195500 190392.     183751.     197033.\n5     191500 270562.     258711.     282414.\n6     189000 206107.     199435.     212779.\n```\n\n\n:::\n\n```{.r .cell-code}\n# The workflow ensures consistency\n# These transformations are automatically applied:\n# 1. Log transform of Sale_Price (inverse transformed for predictions)\n# 2. Normalization of numeric predictors\n# 3. Dummy encoding of Neighborhood\n```\n:::\n\n\n## Workflow Sets: Comparing Multiple Approaches\n\nOne of the most powerful features is comparing multiple workflows systematically:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create multiple preprocessing recipes\nrecipe_simple <- recipe(Sale_Price ~ Gr_Liv_Area + Year_Built, \n                       data = ames_train)\n\nrecipe_normalized <- recipe(Sale_Price ~ Gr_Liv_Area + Year_Built + Total_Bsmt_SF, \n                           data = ames_train) %>%\n  step_normalize(all_numeric_predictors())\n\nrecipe_complex <- recipe(Sale_Price ~ Gr_Liv_Area + Year_Built + \n                         Neighborhood + Total_Bsmt_SF + Garage_Cars + \n                         First_Flr_SF + Full_Bath, data = ames_train) %>%\n  step_normalize(all_numeric_predictors()) %>%\n  step_other(all_nominal_predictors(), threshold = 0.05) %>%\n  step_dummy(all_nominal_predictors())\n\n# Create multiple model specifications\nmodels <- list(\n  lm = linear_reg() %>% set_engine(\"lm\"),\n  ridge = linear_reg(penalty = 0.1, mixture = 0) %>% set_engine(\"glmnet\"),\n  lasso = linear_reg(penalty = 0.1, mixture = 1) %>% set_engine(\"glmnet\"),\n  tree = decision_tree(tree_depth = 10) %>% \n    set_engine(\"rpart\") %>% \n    set_mode(\"regression\")\n)\n\n# Create workflow set\nworkflow_set <- workflow_set(\n  preproc = list(\n    simple = recipe_simple,\n    normalized = recipe_normalized\n  ),\n  models = models\n)\n\nprint(workflow_set)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A workflow set/tibble: 8 x 4\n  wflow_id         info             option    result    \n  <chr>            <list>           <list>    <list>    \n1 simple_lm        <tibble [1 x 4]> <opts[0]> <list [0]>\n2 simple_ridge     <tibble [1 x 4]> <opts[0]> <list [0]>\n3 simple_lasso     <tibble [1 x 4]> <opts[0]> <list [0]>\n4 simple_tree      <tibble [1 x 4]> <opts[0]> <list [0]>\n5 normalized_lm    <tibble [1 x 4]> <opts[0]> <list [0]>\n6 normalized_ridge <tibble [1 x 4]> <opts[0]> <list [0]>\n7 normalized_lasso <tibble [1 x 4]> <opts[0]> <list [0]>\n8 normalized_tree  <tibble [1 x 4]> <opts[0]> <list [0]>\n```\n\n\n:::\n\n```{.r .cell-code}\n# This creates 8 workflows (2 recipes × 4 models)!\n```\n:::\n\n\n### Evaluating Workflow Sets\n\nNow we can evaluate all workflows systematically:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create resamples for evaluation\names_folds <- vfold_cv(ames_train, v = 5, strata = Sale_Price)\n\n# Evaluate workflows individually to avoid errors\n# We'll evaluate just a subset for demonstration\nsimple_lm <- workflow() %>%\n  add_recipe(recipe_simple) %>%\n  add_model(linear_reg() %>% set_engine(\"lm\"))\n\nsimple_lm_results <- simple_lm %>%\n  fit_resamples(\n    resamples = ames_folds,\n    metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq, yardstick::mae)\n  )\n\n# Show metrics\ncollect_metrics(simple_lm_results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 x 6\n  .metric .estimator      mean     n   std_err .config        \n  <chr>   <chr>          <dbl> <int>     <dbl> <chr>          \n1 mae     standard   31681.        5  727.     pre0_mod0_post0\n2 rmse    standard   46911.        5 2520.     pre0_mod0_post0\n3 rsq     standard       0.655     5    0.0192 pre0_mod0_post0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize performance\nsimple_lm_results %>%\n  collect_metrics() %>%\n  ggplot(aes(x = .metric, y = mean)) +\n  geom_col(fill = \"steelblue\") +\n  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), width = 0.2) +\n  facet_wrap(~.metric, scales = \"free\") +\n  labs(title = \"Model Performance with Cross-Validation\",\n       subtitle = \"Simple linear model with 5-fold CV\")\n```\n\n::: {.cell-output-display}\n![](12-workflows-evaluation_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nThis systematic comparison helps identify:\n- Which preprocessing steps add value\n- Which models work best with your data\n- Interaction effects between preprocessing and models\n\n## Model Evaluation with yardstick\n\nThe yardstick package provides comprehensive metrics for model evaluation:\n\n### Regression Metrics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit our simple workflow\nbest_fit <- simple_lm %>%\n  fit(ames_train)\n\n# Get test predictions\ntest_predictions <- best_fit %>%\n  predict(ames_test) %>%\n  bind_cols(ames_test %>% select(Sale_Price))\n\n# Calculate multiple metrics\nregression_metrics <- metric_set(\n  rmse,      # Root Mean Squared Error\n  mae,       # Mean Absolute Error\n  mape,      # Mean Absolute Percentage Error\n  rsq,       # R-squared\n  ccc        # Concordance Correlation Coefficient\n)\n\ntest_performance <- test_predictions %>%\n  regression_metrics(truth = Sale_Price, estimate = .pred)\n\nprint(test_performance)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard   45366.   \n2 mae     standard   31162.   \n3 mape    standard      17.6  \n4 rsq     standard       0.690\n5 ccc     standard       0.804\n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize predictions vs actual\nggplot(test_predictions, aes(x = Sale_Price, y = .pred)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(\n    title = \"Predicted vs Actual Sale Prices\",\n    subtitle = paste(\"Test RMSE:\", round(test_performance$.estimate[1], 2)),\n    x = \"Actual Sale Price\",\n    y = \"Predicted Sale Price\"\n  ) +\n  coord_equal()\n```\n\n::: {.cell-output-display}\n![](12-workflows-evaluation_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n### Classification Metrics\n\nLet's also explore classification metrics:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a classification problem\names_class <- ames_train %>%\n  mutate(expensive = factor(if_else(Sale_Price > median(Sale_Price), \n                                    \"yes\", \"no\"))) %>%\n  select(-Sale_Price)\n\n# Simple classification workflow\nclass_recipe <- recipe(expensive ~ Gr_Liv_Area + Overall_Cond + Year_Built, \n                      data = ames_class) %>%\n  step_normalize(all_numeric_predictors())\n\nclass_spec <- logistic_reg() %>%\n  set_engine(\"glm\")\n\nclass_workflow <- workflow() %>%\n  add_recipe(class_recipe) %>%\n  add_model(class_spec)\n\n# Fit and predict\nclass_split <- initial_split(ames_class, strata = expensive)\nclass_train <- training(class_split)\nclass_test <- testing(class_split)\n\nclass_fit <- class_workflow %>%\n  fit(class_train)\n\nclass_predictions <- bind_cols(\n  class_test %>% select(expensive),\n  predict(class_fit, class_test),\n  predict(class_fit, class_test, type = \"prob\")\n)\n\n# Classification metrics\nclass_metrics <- metric_set(\n  accuracy,\n  precision,\n  recall,\n  f_meas,\n  roc_auc,\n  pr_auc\n)\n\nclass_performance <- class_predictions %>%\n  class_metrics(truth = expensive, estimate = .pred_class, .pred_yes)\n\nprint(class_performance)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 x 3\n  .metric   .estimator .estimate\n  <chr>     <chr>          <dbl>\n1 accuracy  binary        0.885 \n2 precision binary        0.902 \n3 recall    binary        0.865 \n4 f_meas    binary        0.883 \n5 roc_auc   binary        0.0380\n6 pr_auc    binary        0.310 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Confusion matrix\nconf_matrix <- class_predictions %>%\n  conf_mat(truth = expensive, estimate = .pred_class)\n\nautoplot(conf_matrix, type = \"heatmap\") +\n  labs(title = \"Confusion Matrix\")\n```\n\n::: {.cell-output-display}\n![](12-workflows-evaluation_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# ROC curve\nroc_curve_data <- class_predictions %>%\n  roc_curve(truth = expensive, .pred_yes)\n\nautoplot(roc_curve_data) +\n  labs(title = \"ROC Curve\") +\n  annotate(\"text\", x = 0.5, y = 0.5, \n           label = paste(\"AUC:\", round(class_performance$.estimate[5], 3)))\n```\n\n::: {.cell-output-display}\n![](12-workflows-evaluation_files/figure-html/unnamed-chunk-9-2.png){width=672}\n:::\n:::\n\n\n## Custom Metrics and Evaluation\n\nSometimes you need custom metrics for your specific problem:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a custom metric - Mean Absolute Percentage Error with threshold\nmape_vec_threshold <- function(truth, estimate, threshold = 0.2, na_rm = TRUE) {\n  errors <- abs((truth - estimate) / truth)\n  errors[errors > threshold] <- threshold  # Cap errors at threshold\n  mean(errors, na.rm = na_rm)\n}\n\n# Use the custom metric directly\ncustom_mape <- test_predictions %>%\n  summarise(\n    custom_mape = mape_vec_threshold(Sale_Price, .pred),\n    regular_mape = mean(abs((Sale_Price - .pred) / Sale_Price))\n  )\n\nprint(custom_mape)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 2\n  custom_mape regular_mape\n        <dbl>        <dbl>\n1       0.127        0.176\n```\n\n\n:::\n\n```{.r .cell-code}\n# Create a business-specific metric\n# For example: penalize underestimation more than overestimation\nasymmetric_loss <- function(truth, estimate, under_weight = 2) {\n  errors <- truth - estimate\n  result <- ifelse(errors > 0, \n                   under_weight * errors^2,  # Underestimation penalty\n                   errors^2)                  # Overestimation penalty\n  sqrt(mean(result))\n}\n\n# Apply custom metric\ntest_predictions %>%\n  mutate(\n    asymmetric_rmse = asymmetric_loss(Sale_Price, .pred)\n  ) %>%\n  summarise(\n    regular_rmse = rmse_vec(Sale_Price, .pred),\n    asymmetric_rmse = mean(asymmetric_rmse)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 2\n  regular_rmse asymmetric_rmse\n         <dbl>           <dbl>\n1       45366.          59141.\n```\n\n\n:::\n:::\n\n\n## Advanced Workflow Techniques\n\n### Extracting and Modifying Fitted Workflows\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract components from fitted workflow\nextracted_recipe <- lm_fit %>%\n  extract_recipe()\n\nextracted_model <- lm_fit %>%\n  extract_fit_parsnip()\n\n# Get preprocessing results\npreprocessed_data <- lm_fit %>%\n  extract_recipe() %>%\n  bake(new_data = ames_test)\n\nhead(preprocessed_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 x 32\n  Gr_Liv_Area Year_Built Total_Bsmt_SF Sale_Price Neighborhood_College_Creek\n        <dbl>      <dbl>         <dbl>      <int>                      <dbl>\n1      -1.18      -0.353        -0.364     105000                          0\n2      -0.337     -0.452         0.634     172000                          0\n3       0.247      0.836        -0.262     189900                          0\n4       0.198      0.869        -0.266     195500                          0\n5      -0.433      0.671         0.525     191500                          0\n6       0.588      0.902        -0.114     189000                          0\n# i 27 more variables: Neighborhood_Old_Town <dbl>, Neighborhood_Edwards <dbl>,\n#   Neighborhood_Somerset <dbl>, Neighborhood_Northridge_Heights <dbl>,\n#   Neighborhood_Gilbert <dbl>, Neighborhood_Sawyer <dbl>,\n#   Neighborhood_Northwest_Ames <dbl>, Neighborhood_Sawyer_West <dbl>,\n#   Neighborhood_Mitchell <dbl>, Neighborhood_Brookside <dbl>,\n#   Neighborhood_Crawford <dbl>, Neighborhood_Iowa_DOT_and_Rail_Road <dbl>,\n#   Neighborhood_Timberland <dbl>, Neighborhood_Northridge <dbl>, ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# Extract model coefficients\ncoefficients <- lm_fit %>%\n  extract_fit_parsnip() %>%\n  tidy()\n\nhead(coefficients)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 x 5\n  term                       estimate std.error statistic   p.value\n  <chr>                         <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)                 164586.     2214.    74.3   0        \n2 Gr_Liv_Area                  33020.     1003.    32.9   4.40e-193\n3 Year_Built                   16040.     1732.     9.26  4.64e- 20\n4 Total_Bsmt_SF                15285.     1029.    14.9   1.28e- 47\n5 Neighborhood_College_Creek   18650.     3982.     4.68  2.99e-  6\n6 Neighborhood_Old_Town        -1959.     4085.    -0.480 6.32e-  1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Variable importance (if applicable)\n# For models that support it\nrf_workflow <- workflow() %>%\n  add_recipe(recipe_simple) %>%\n  add_model(rand_forest() %>% set_engine(\"ranger\", importance = \"impurity\") %>% set_mode(\"regression\"))\n\nrf_fit <- rf_workflow %>%\n  fit(ames_train)\n\nrf_importance <- rf_fit %>%\n  extract_fit_parsnip() %>%\n  vip()\n\nprint(rf_importance)\n```\n\n::: {.cell-output-display}\n![](12-workflows-evaluation_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n### Workflow Finalization\n\nAfter tuning (covered in next chapter), you finalize workflows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example: Finalize a workflow with best parameters\n# This would typically come from tuning\nbest_params <- tibble(\n  penalty = 0.01,\n  mixture = 0.5\n)\n\n# Create tunable workflow\ntunable_spec <- linear_reg(\n  penalty = tune(),\n  mixture = tune()\n) %>%\n  set_engine(\"glmnet\")\n\ntunable_workflow <- workflow() %>%\n  add_recipe(recipe_normalized) %>%\n  add_model(tunable_spec)\n\n# Finalize with best parameters\nfinal_workflow <- tunable_workflow %>%\n  finalize_workflow(best_params)\n\nprint(final_workflow)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: linear_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n1 Recipe Step\n\n* step_normalize()\n\n-- Model -----------------------------------------------------------------------\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 0.01\n  mixture = 0.5\n\nComputational engine: glmnet \n```\n\n\n:::\n\n```{.r .cell-code}\n# Fit final model on all training data\nfinal_fit <- final_workflow %>%\n  fit(ames_train)\n\n# Last fit - train on all data, evaluate on test\nlast_fit_results <- final_workflow %>%\n  last_fit(split = ames_split, metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq))\n\n# Extract final metrics\nfinal_metrics <- last_fit_results %>%\n  collect_metrics()\n\nprint(final_metrics)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 4\n  .metric .estimator .estimate .config        \n  <chr>   <chr>          <dbl> <chr>          \n1 rmse    standard   40233.    pre0_mod0_post0\n2 rsq     standard       0.760 pre0_mod0_post0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Extract final model\nfinal_model <- last_fit_results %>%\n  extract_workflow()\n```\n:::\n\n\n## Evaluating Model Assumptions\n\nWorkflows make it easy to check model assumptions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Residual analysis for regression\nresidual_analysis <- test_predictions %>%\n  mutate(\n    residual = Sale_Price - .pred,\n    std_residual = residual / sd(residual),\n    abs_residual = abs(residual)\n  )\n\n# Residual plots\np1 <- ggplot(residual_analysis, aes(x = .pred, y = residual)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  labs(title = \"Residuals vs Fitted\", x = \"Fitted Values\", y = \"Residuals\")\n\np2 <- ggplot(residual_analysis, aes(sample = std_residual)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  labs(title = \"Q-Q Plot\", x = \"Theoretical Quantiles\", y = \"Standardized Residuals\")\n\np3 <- ggplot(residual_analysis, aes(x = .pred, y = sqrt(abs_residual))) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  labs(title = \"Scale-Location\", x = \"Fitted Values\", y = \"√|Residuals|\")\n\np4 <- ggplot(residual_analysis, aes(x = residual)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", alpha = 0.7) +\n  geom_density(aes(y = after_stat(count)), color = \"red\", linewidth = 1) +\n  labs(title = \"Residual Distribution\", x = \"Residuals\", y = \"Count\")\n\n# Combine plots\n(p1 + p2) / (p3 + p4) +\n  plot_annotation(title = \"Regression Diagnostics\")\n```\n\n::: {.cell-output-display}\n![](12-workflows-evaluation_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n## Probability Calibration\n\nFor classification, we often need well-calibrated probabilities:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calibration analysis\ncalibration_data <- class_predictions %>%\n  mutate(\n    prob_bin = cut(.pred_yes, breaks = seq(0, 1, 0.1), include.lowest = TRUE)\n  ) %>%\n  group_by(prob_bin) %>%\n  summarise(\n    mean_predicted = mean(.pred_yes),\n    fraction_positive = mean(expensive == \"yes\"),\n    n = n(),\n    .groups = \"drop\"\n  ) %>%\n  filter(n > 5)  # Remove bins with few observations\n\n# Calibration plot\nggplot(calibration_data, aes(x = mean_predicted, y = fraction_positive)) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  geom_point(aes(size = n), color = \"darkblue\") +\n  geom_line(color = \"darkblue\") +\n  scale_size_continuous(range = c(2, 8)) +\n  labs(\n    title = \"Probability Calibration Plot\",\n    subtitle = \"Well-calibrated models follow the diagonal\",\n    x = \"Mean Predicted Probability\",\n    y = \"Observed Frequency\",\n    size = \"Count\"\n  ) +\n  coord_equal() +\n  xlim(0, 1) + ylim(0, 1)\n```\n\n::: {.cell-output-display}\n![](12-workflows-evaluation_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Use probably package for calibration\nlibrary(probably)\n\n# Simple calibration visualization\n# We'll just show the calibration plot without the probably package functions\n# that are causing issues\n\n# Create a simple comparison\nggplot(calibration_data, aes(x = mean_predicted, y = fraction_positive)) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\", size = 1) +\n  geom_point(aes(size = n), color = \"darkblue\", alpha = 0.7) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"blue\") +\n  scale_size_continuous(range = c(2, 8)) +\n  labs(\n    title = \"Model Calibration Assessment\",\n    subtitle = \"Points should follow the red diagonal for perfect calibration\",\n    x = \"Mean Predicted Probability\",\n    y = \"Observed Frequency\",\n    size = \"Count\"\n  ) +\n  coord_equal() +\n  xlim(0, 1) + ylim(0, 1) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](12-workflows-evaluation_files/figure-html/unnamed-chunk-14-2.png){width=672}\n:::\n:::\n\n\n## Performance Visualization\n\nCreate comprehensive performance visualizations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# For regression: actual vs predicted with confidence bands\nprediction_plot <- test_predictions %>%\n  ggplot(aes(x = Sale_Price, y = .pred)) +\n  geom_hex(bins = 30) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"blue\") +\n  scale_fill_viridis_c() +\n  labs(\n    title = \"Prediction Accuracy\",\n    subtitle = paste(\"R² =\", round(rsq_vec(test_predictions$Sale_Price, \n                                          test_predictions$.pred), 3)),\n    x = \"Actual Price\",\n    y = \"Predicted Price\"\n  ) +\n  coord_equal()\n\n# Error distribution\nerror_plot <- test_predictions %>%\n  mutate(error = .pred - Sale_Price,\n         pct_error = error / Sale_Price * 100) %>%\n  ggplot(aes(x = pct_error)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Prediction Error Distribution\",\n    subtitle = \"Percentage error\",\n    x = \"Error (%)\",\n    y = \"Count\"\n  )\n\nprediction_plot + error_plot\n```\n\n::: {.cell-output-display}\n![](12-workflows-evaluation_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n## Best Practices for Workflows\n\n### 1. Always Use Workflows for Production\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Good practice: Complete workflow\nproduction_workflow <- workflow() %>%\n  add_recipe(\n    recipe(Sale_Price ~ ., data = ames_train) %>%\n      step_impute_median(all_numeric_predictors()) %>%\n      step_impute_mode(all_nominal_predictors()) %>%\n      step_normalize(all_numeric_predictors()) %>%\n      step_dummy(all_nominal_predictors())\n  ) %>%\n  add_model(\n    linear_reg() %>% set_engine(\"lm\")\n  )\n\n# This ensures all preprocessing is contained and reproducible\n```\n:::\n\n\n### 2. Version Control Your Workflows\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Save workflow for reproducibility\nsaveRDS(final_fit, \"models/final_workflow_v1.rds\")\n\n# Load and use later\n# loaded_workflow <- readRDS(\"models/final_workflow_v1.rds\")\n# new_predictions <- predict(loaded_workflow, new_data)\n```\n:::\n\n\n### 3. Document Your Choices\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create workflow with documentation\ndocumented_workflow <- workflow() %>%\n  add_recipe(\n    recipe(Sale_Price ~ ., data = ames_train) %>%\n      # Handle missing values before other steps\n      step_impute_median(all_numeric_predictors()) %>%\n      # Normalize for model stability\n      step_normalize(all_numeric_predictors()) %>%\n      # Create dummies for linear model\n      step_dummy(all_nominal_predictors())\n  ) %>%\n  add_model(\n    # Ridge regression to handle multicollinearity\n    linear_reg(penalty = 0.01, mixture = 0) %>%\n      set_engine(\"glmnet\")\n  )\n```\n:::\n\n\n## Exercises\n\n### Exercise 1: Build a Complete Evaluation Pipeline\n\nCreate a workflow and comprehensive evaluation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Create a complete evaluation pipeline\neval_recipe <- recipe(Sale_Price ~ Gr_Liv_Area + Year_Built + Overall_Cond + \n                      Neighborhood + Total_Bsmt_SF, data = ames_train) %>%\n  step_log(Sale_Price, skip = TRUE) %>%  # Skip for prediction\n  step_impute_median(all_numeric_predictors()) %>%\n  step_normalize(all_numeric_predictors()) %>%\n  step_dummy(all_nominal_predictors())\n\neval_spec <- linear_reg(penalty = 0.01, mixture = 0.5) %>%\n  set_engine(\"glmnet\")\n\neval_workflow <- workflow() %>%\n  add_recipe(eval_recipe) %>%\n  add_model(eval_spec)\n\n# Fit with cross-validation\neval_folds <- vfold_cv(ames_train, v = 10, strata = Sale_Price)\n\neval_results <- eval_workflow %>%\n  fit_resamples(\n    resamples = eval_folds,\n    metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq, yardstick::mae, yardstick::mape),\n    control = control_resamples(save_pred = TRUE)\n  )\n\n# Summarize performance\ncollect_metrics(eval_results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 x 6\n  .metric .estimator       mean     n      std_err .config        \n  <chr>   <chr>           <dbl> <int>        <dbl> <chr>          \n1 mae     standard   180453.       10  696.        pre0_mod0_post0\n2 mape    standard      100.0      10    0.0000425 pre0_mod0_post0\n3 rmse    standard   197162.       10 1400.        pre0_mod0_post0\n4 rsq     standard        0.764    10    0.0157    pre0_mod0_post0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Get predictions for visualization\neval_predictions <- collect_predictions(eval_results)\n\n# Visualize CV performance\nggplot(eval_predictions, aes(x = Sale_Price, y = .pred)) +\n  geom_point(alpha = 0.1) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\") +\n  facet_wrap(~id, ncol = 5) +\n  labs(title = \"Predictions Across CV Folds\")\n```\n\n::: {.cell-output-display}\n![](12-workflows-evaluation_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n### Exercise 2: Compare Preprocessing Strategies\n\nEvaluate different preprocessing approaches:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Define different preprocessing strategies\npreproc_minimal <- recipe(Sale_Price ~ Gr_Liv_Area + Year_Built, \n                         data = ames_train)\n\npreproc_standard <- recipe(Sale_Price ~ Gr_Liv_Area + Year_Built + Total_Bsmt_SF, \n                          data = ames_train) %>%\n  step_normalize(all_numeric_predictors())\n\npreproc_complex <- recipe(Sale_Price ~ Gr_Liv_Area + Year_Built + \n                         Neighborhood + Total_Bsmt_SF + Garage_Cars, data = ames_train) %>%\n  step_normalize(all_numeric_predictors()) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_interact(terms = ~ Gr_Liv_Area:Year_Built)\n\n# Create workflows\nstrategies <- list(\n  minimal = preproc_minimal,\n  standard = preproc_standard,\n  complex = preproc_complex\n)\n\n# Same model for all\nmodel_spec <- linear_reg() %>% set_engine(\"lm\")\n\n# Create and evaluate workflows\nstrategy_results <- map_df(names(strategies), function(strategy_name) {\n  wf <- workflow() %>%\n    add_recipe(strategies[[strategy_name]]) %>%\n    add_model(model_spec)\n  \n  # Fit and evaluate\n  fit_resamples(\n    wf,\n    resamples = vfold_cv(ames_train, v = 5),\n    metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq)\n  ) %>%\n    collect_metrics() %>%\n    mutate(strategy = strategy_name)\n})\n\n# Compare strategies\nggplot(strategy_results, aes(x = strategy, y = mean, fill = strategy)) +\n  geom_col() +\n  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), width = 0.2) +\n  facet_wrap(~.metric, scales = \"free_y\") +\n  labs(title = \"Preprocessing Strategy Comparison\") +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](12-workflows-evaluation_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n### Exercise 3: Custom Metrics for Business Goals\n\nCreate business-specific metrics:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Business scenario: Real estate company\n# - Overestimating is bad (disappointed customers)\n# - Underestimating by <5% is acceptable\n# - Underestimating by >5% is very bad (lost opportunity)\n\nbusiness_metric <- function(truth, estimate) {\n  pct_error <- (estimate - truth) / truth * 100\n  \n  penalties <- case_when(\n    pct_error > 0 ~ abs(pct_error) * 2,        # Overestimate penalty\n    pct_error > -5 ~ abs(pct_error) * 0.5,     # Small underestimate\n    TRUE ~ abs(pct_error) * 3                  # Large underestimate penalty\n  )\n  \n  mean(penalties)\n}\n\n# Apply to test predictions\ntest_predictions %>%\n  mutate(\n    business_score = business_metric(Sale_Price, .pred),\n    standard_mape = mape_vec(Sale_Price, .pred)\n  ) %>%\n  summarise(\n    mean_business_score = mean(business_score),\n    mean_standard_mape = mean(standard_mape)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 2\n  mean_business_score mean_standard_mape\n                <dbl>              <dbl>\n1                42.4               17.6\n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize business metric\ntest_predictions %>%\n  mutate(\n    pct_error = (.pred - Sale_Price) / Sale_Price * 100,\n    error_category = case_when(\n      pct_error > 0 ~ \"Overestimate\",\n      pct_error > -5 ~ \"Small Underestimate\",\n      TRUE ~ \"Large Underestimate\"\n    )\n  ) %>%\n  ggplot(aes(x = pct_error, fill = error_category)) +\n  geom_histogram(bins = 30, alpha = 0.7) +\n  scale_fill_manual(values = c(\n    \"Overestimate\" = \"red\",\n    \"Small Underestimate\" = \"yellow\",\n    \"Large Underestimate\" = \"darkred\"\n  )) +\n  labs(\n    title = \"Business Impact of Prediction Errors\",\n    x = \"Percentage Error\",\n    y = \"Count\",\n    fill = \"Error Category\"\n  )\n```\n\n::: {.cell-output-display}\n![](12-workflows-evaluation_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n## Summary\n\nIn this comprehensive chapter, you've mastered:\n\n✅ **Workflow fundamentals**\n  - Combining preprocessing and models\n  - Preventing data leakage\n  - Ensuring reproducibility\n\n✅ **Workflow components**\n  - Adding and updating recipes/models\n  - Formula vs recipe interfaces\n  - Modular design\n\n✅ **Workflow sets**\n  - Comparing multiple approaches\n  - Systematic evaluation\n  - Ranking and selection\n\n✅ **Model evaluation**\n  - Comprehensive metrics with yardstick\n  - Custom metrics for business needs\n  - Visualization techniques\n\n✅ **Advanced techniques**\n  - Extracting workflow components\n  - Probability calibration\n  - Model diagnostics\n\nKey takeaways:\n- Always use workflows for production models\n- Workflows prevent data leakage automatically\n- Workflow sets enable systematic comparison\n- Custom metrics align models with business goals\n- Proper evaluation is crucial for model trust\n\n## What's Next?\n\nIn [Chapter 13](13-hyperparameter-tuning.Rmd), we'll explore hyperparameter tuning to optimize model performance.\n\n## Additional Resources\n\n- [workflows Documentation](https://workflows.tidymodels.org/)\n- [workflowsets Documentation](https://workflowsets.tidymodels.org/)\n- [yardstick Documentation](https://yardstick.tidymodels.org/)\n- [Tidy Modeling with R - Workflows Chapter](https://www.tmwr.org/workflows.html)\n",
    "supporting": [
      "12-workflows-evaluation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}