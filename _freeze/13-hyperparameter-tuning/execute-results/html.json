{
  "hash": "1e94d22db06ba39c0eeb92000084bc32",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 13: Hyperparameter Tuning - Finding the Sweet Spot\"\nauthor: \"David Sarrat González, Juan R González\"\ndate: today\nformat:\n  html:\n    code-fold: false\n    code-tools: true\n---\n\n## Learning Objectives\n\nBy the end of this chapter, you will master:\n\n- Understanding hyperparameters vs parameters\n- Grid search and its variations\n- Random search strategies\n- Bayesian optimization with Gaussian processes\n- Racing methods for efficiency\n- Simulated annealing\n- Nested resampling for unbiased evaluation\n- Parallel processing for faster tuning\n- Best practices and common pitfalls\n\n## The Art and Science of Hyperparameter Tuning\n\nImagine you're a chef perfecting a recipe. The ingredients are your features, the cooking method is your algorithm, but what about the temperature, timing, and seasoning amounts? These are like hyperparameters - they control the learning process but aren't learned from the data directly.\n\nGetting hyperparameters right can mean the difference between a model that barely works and one that achieves state-of-the-art performance. Too conservative, and you underfit. Too aggressive, and you overfit. The sweet spot lies somewhere in between.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Attaching packages -------------------------------------- tidymodels 1.4.1 --\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nv broom        1.0.10     v recipes      1.3.1 \nv dials        1.4.2      v rsample      1.3.1 \nv dplyr        1.1.4      v tailor       0.1.0 \nv ggplot2      4.0.0      v tidyr        1.3.1 \nv infer        1.0.9      v tune         2.0.0 \nv modeldata    1.5.1      v workflows    1.3.0 \nv parsnip      1.3.3      v workflowsets 1.1.1 \nv purrr        1.1.0      v yardstick    1.3.2 \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx purrr::discard() masks scales::discard()\nx dplyr::filter()  masks stats::filter()\nx dplyr::lag()     masks stats::lag()\nx recipes::step()  masks stats::step()\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv forcats   1.0.0     v stringr   1.5.2\nv lubridate 1.9.4     v tibble    3.3.0\nv readr     2.1.5     \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx readr::col_factor() masks scales::col_factor()\nx purrr::discard()    masks scales::discard()\nx dplyr::filter()     masks stats::filter()\nx stringr::fixed()    masks recipes::fixed()\nx dplyr::lag()        masks stats::lag()\nx readr::spec()       masks yardstick::spec()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(modeldata)\nlibrary(vip)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAdjuntando el paquete: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(patchwork)\nlibrary(doParallel)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCargando paquete requerido: foreach\n\nAdjuntando el paquete: 'foreach'\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n\nCargando paquete requerido: iterators\nCargando paquete requerido: parallel\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(finetune)\nlibrary(dials)\n\n# Set theme and seed\ntheme_set(theme_minimal())\nset.seed(123)\n\n# Load and prepare data\ndata(ames)\names_split <- initial_split(ames, prop = 0.75, strata = Sale_Price)\names_train <- training(ames_split)\names_test <- testing(ames_split)\n\n# Create resamples for tuning\names_folds <- vfold_cv(ames_train, v = 5, strata = Sale_Price)\n```\n:::\n\n\n## Understanding Hyperparameters\n\nFirst, let's clarify the distinction:\n\n- **Parameters**: Learned from data (e.g., regression coefficients, neural network weights)\n- **Hyperparameters**: Set before training (e.g., learning rate, tree depth, penalty)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example: Linear regression with regularization\n# Parameters: The coefficient values (beta_0, beta_1, ..., beta_p)\n# Hyperparameters: penalty (lambda) and mixture (alpha)\n\n# Without tuning - we guess the hyperparameters\nfixed_spec <- linear_reg(\n  penalty = 0.01,  # Hyperparameter (guessed)\n  mixture = 0.5    # Hyperparameter (guessed)\n) %>%\n  set_engine(\"glmnet\")\n\n# With tuning - we mark them for optimization\ntunable_spec <- linear_reg(\n  penalty = tune(),  # To be optimized\n  mixture = tune()   # To be optimized\n) %>%\n  set_engine(\"glmnet\")\n\n# The model will learn the coefficients (parameters) during fitting\n# But we need to find the best penalty and mixture (hyperparameters) through tuning\n```\n:::\n\n\nDifferent models have different hyperparameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Decision tree hyperparameters\ntree_spec <- decision_tree(\n  cost_complexity = tune(),  # Pruning parameter\n  tree_depth = tune(),       # Maximum depth\n  min_n = tune()            # Minimum observations in node\n) %>%\n  set_engine(\"rpart\") %>%\n  set_mode(\"regression\")\n\n# Random forest hyperparameters\nrf_spec <- rand_forest(\n  mtry = tune(),    # Variables per split\n  trees = tune(),   # Number of trees\n  min_n = tune()    # Minimum node size\n) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"regression\")\n\n# XGBoost has many hyperparameters\nxgb_spec <- boost_tree(\n  trees = tune(),\n  tree_depth = tune(),\n  min_n = tune(),\n  loss_reduction = tune(),\n  sample_size = tune(),\n  mtry = tune(),\n  learn_rate = tune()\n) %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"regression\")\n\n# Each model type has its own set of tunable parameters\n```\n:::\n\n\n## Grid Search: The Systematic Approach\n\nGrid search evaluates all combinations of hyperparameter values in a predefined grid:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a simple preprocessing recipe\nsimple_recipe <- recipe(Sale_Price ~ Gr_Liv_Area + Overall_Cond + Year_Built + \n                       Neighborhood, data = ames_train) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_normalize(all_numeric_predictors())\n\n# Create a tunable elastic net model\nelastic_spec <- linear_reg(\n  penalty = tune(),\n  mixture = tune()\n) %>%\n  set_engine(\"glmnet\")\n\n# Combine into workflow\nelastic_workflow <- workflow() %>%\n  add_recipe(simple_recipe) %>%\n  add_model(elastic_spec)\n\n# Define the grid\nelastic_grid <- grid_regular(\n  penalty(range = c(-3, 0), trans = log10_trans()),  # 10^-3 to 10^0\n  mixture(range = c(0, 1)),                          # 0 (ridge) to 1 (lasso)\n  levels = c(10, 5)  # 10 penalty values, 5 mixture values = 50 combinations\n)\n\nprint(elastic_grid)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 50 x 2\n   penalty mixture\n     <dbl>   <dbl>\n 1 0.001         0\n 2 0.00215       0\n 3 0.00464       0\n 4 0.01          0\n 5 0.0215        0\n 6 0.0464        0\n 7 0.1           0\n 8 0.215         0\n 9 0.464         0\n10 1             0\n# i 40 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize the grid\nggplot(elastic_grid, aes(x = penalty, y = mixture)) +\n  geom_point(size = 3, color = \"steelblue\") +\n  scale_x_log10() +\n  labs(\n    title = \"Regular Grid for Elastic Net\",\n    subtitle = \"50 combinations to evaluate\",\n    x = \"Penalty (log scale)\",\n    y = \"Mixture (0=Ridge, 1=Lasso)\"\n  )\n```\n\n::: {.cell-output-display}\n![](13-hyperparameter-tuning_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n### Performing Grid Search\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tune with grid search\ngrid_results <- elastic_workflow %>%\n  tune_grid(\n    resamples = ames_folds,\n    grid = elastic_grid,\n    metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq),\n    control = control_grid(save_pred = TRUE, verbose = FALSE)\n  )\n\n# Examine results\ngrid_results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Tuning results\n# 5-fold cross-validation using stratification \n# A tibble: 5 x 5\n  splits             id    .metrics           .notes           .predictions\n  <list>             <chr> <list>             <list>           <list>      \n1 <split [1756/441]> Fold1 <tibble [100 x 6]> <tibble [1 x 4]> <tibble>    \n2 <split [1757/440]> Fold2 <tibble [100 x 6]> <tibble [1 x 4]> <tibble>    \n3 <split [1757/440]> Fold3 <tibble [100 x 6]> <tibble [1 x 4]> <tibble>    \n4 <split [1758/439]> Fold4 <tibble [100 x 6]> <tibble [1 x 4]> <tibble>    \n5 <split [1760/437]> Fold5 <tibble [100 x 6]> <tibble [1 x 4]> <tibble>    \n\nThere were issues with some computations:\n\n  - Warning(s) x4: !  The following columns have zero variance so scaling cannot be ...\n  - Warning(s) x1: !  The following columns have zero variance so scaling cannot be ...\n\nRun `show_notes(.Last.tune.result)` for more information.\n```\n\n\n:::\n\n```{.r .cell-code}\n# Best parameters\nbest_grid <- grid_results %>%\n  select_best(metric = \"rmse\")\n\nprint(best_grid)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 3\n  penalty mixture .config         \n    <dbl>   <dbl> <chr>           \n1   0.001    0.25 pre0_mod02_post0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize tuning results\nautoplot(grid_results) +\n  labs(title = \"Grid Search Results\")\n```\n\n::: {.cell-output-display}\n![](13-hyperparameter-tuning_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# More detailed visualization\ngrid_results %>%\n  collect_metrics() %>%\n  filter(.metric == \"rmse\") %>%\n  ggplot(aes(x = penalty, y = mean, color = factor(mixture))) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), \n                width = 0.01, alpha = 0.5) +\n  scale_x_log10() +\n  scale_color_viridis_d(name = \"Mixture\") +\n  labs(\n    title = \"RMSE Across Penalty Values\",\n    subtitle = \"Different lines represent different mixture values\",\n    x = \"Penalty (log scale)\",\n    y = \"RMSE\"\n  )\n```\n\n::: {.cell-output-display}\n![](13-hyperparameter-tuning_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n:::\n\n\nGrid search characteristics:\n- **Pros**: Systematic, reproducible, finds global optimum within grid\n- **Cons**: Computationally expensive, curse of dimensionality, may miss optimum between grid points\n\n## Random Search: The Efficient Explorer\n\nRandom search samples hyperparameter combinations randomly, often finding good solutions faster than grid search:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Random grid - same parameter space, random sampling\nrandom_grid <- grid_random(\n  penalty(range = c(-3, 0), trans = log10_trans()),\n  mixture(range = c(0, 1)),\n  size = 30  # Only 30 random combinations instead of 50 in regular grid\n)\n\n# Visualize random vs regular grid\ngrid_comparison <- bind_rows(\n  elastic_grid %>% mutate(type = \"Regular Grid (50 points)\"),\n  random_grid %>% mutate(type = \"Random Grid (30 points)\")\n)\n\nggplot(grid_comparison, aes(x = penalty, y = mixture, color = type)) +\n  geom_point(size = 2, alpha = 0.7) +\n  scale_x_log10() +\n  facet_wrap(~type) +\n  labs(\n    title = \"Grid Search vs Random Search\",\n    subtitle = \"Random search covers the space more efficiently\",\n    x = \"Penalty (log scale)\",\n    y = \"Mixture\"\n  ) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](13-hyperparameter-tuning_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Perform random search\nrandom_results <- elastic_workflow %>%\n  tune_grid(\n    resamples = ames_folds,\n    grid = random_grid,\n    metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq),\n    control = control_grid(save_pred = FALSE, verbose = FALSE)\n  )\n\n# Compare efficiency\ncomparison <- bind_rows(\n  grid_results %>% \n    show_best(metric = \"rmse\", n = 1) %>% \n    mutate(method = \"Grid (50 points)\"),\n  random_results %>% \n    show_best(metric = \"rmse\", n = 1) %>% \n    mutate(method = \"Random (30 points)\")\n) %>%\n  select(method, penalty, mixture, mean, std_err)\n\nknitr::kable(comparison, digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|method             | penalty| mixture|     mean|  std_err|\n|:------------------|-------:|-------:|--------:|--------:|\n|Grid (50 points)   |   0.001|  0.2500| 38655.44| 1413.248|\n|Random (30 points) |   0.009|  0.0377| 38646.32| 1412.018|\n\n\n:::\n:::\n\n\nThe mathematics behind why random search works:\n- For important hyperparameters, random search explores more unique values\n- Less affected by unimportant hyperparameters\n- Better coverage of continuous spaces\n\n## Latin Hypercube Sampling\n\nLatin Hypercube provides space-filling designs that are more uniform than random:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Latin hypercube grid\nlhs_grid <- grid_latin_hypercube(\n  penalty(range = c(-3, 0), trans = log10_trans()),\n  mixture(range = c(0, 1)),\n  size = 20  # Even fewer points\n)\n\n# Visualize all three approaches\nall_grids <- bind_rows(\n  elastic_grid %>% slice_sample(n = 20) %>% mutate(type = \"Regular (20 points)\"),\n  random_grid %>% slice_sample(n = 20) %>% mutate(type = \"Random (20 points)\"),\n  lhs_grid %>% mutate(type = \"Latin Hypercube (20 points)\")\n)\n\nggplot(all_grids, aes(x = penalty, y = mixture)) +\n  geom_point(size = 3, color = \"darkblue\", alpha = 0.7) +\n  scale_x_log10() +\n  facet_wrap(~type) +\n  labs(\n    title = \"Sampling Strategies Comparison\",\n    subtitle = \"Latin Hypercube provides better space coverage\",\n    x = \"Penalty (log scale)\",\n    y = \"Mixture\"\n  )\n```\n\n::: {.cell-output-display}\n![](13-hyperparameter-tuning_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n## Bayesian Optimization: The Smart Search\n\nBayesian optimization uses past results to intelligently choose the next points to evaluate:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# More complex model for Bayesian optimization demonstration\nrf_recipe <- recipe(Sale_Price ~ ., data = ames_train) %>%\n  step_impute_median(all_numeric_predictors()) %>%\n  step_normalize(all_numeric_predictors()) %>%\n  step_dummy(all_nominal_predictors())\n\nrf_spec <- rand_forest(\n  mtry = tune(),\n  min_n = tune(),\n  trees = 500  # Fix trees to reduce tuning time\n) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"regression\")\n\nrf_workflow <- workflow() %>%\n  add_recipe(rf_recipe) %>%\n  add_model(rf_spec)\n\n# Set up parameter bounds\nrf_params <- rf_workflow %>%\n  extract_parameter_set_dials() %>%\n  update(\n    mtry = mtry(c(5, 20)),      # Number of variables per split\n    min_n = min_n(c(5, 30))      # Minimum node size\n  )\n\n# Initial random search to seed Bayesian optimization\nset.seed(456)\ninitial_results <- rf_workflow %>%\n  tune_grid(\n    resamples = ames_folds,\n    grid = 5,  # Start with 5 random points\n    metrics = yardstick::metric_set(yardstick::rmse),\n    control = control_grid(verbose = FALSE)\n  )\n\n# Bayesian optimization\nbayes_results <- rf_workflow %>%\n  tune_bayes(\n    resamples = ames_folds,\n    initial = initial_results,  # Use initial results\n    iter = 10,                   # 10 more iterations\n    metrics = yardstick::metric_set(yardstick::rmse),\n    param_info = rf_params,\n    control = control_bayes(\n      verbose = FALSE,\n      no_improve = 5,  # Stop after 5 iterations without improvement\n      uncertain = 5    # Exploration vs exploitation trade-off\n    )\n  )\n\n# Visualize the optimization path\nautoplot(bayes_results, type = \"performance\") +\n  labs(\n    title = \"Bayesian Optimization Progress\",\n    subtitle = \"RMSE improves over iterations\"\n  )\n```\n\n::: {.cell-output-display}\n![](13-hyperparameter-tuning_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Show acquisition function behavior\nautoplot(bayes_results, type = \"parameters\") +\n  labs(\n    title = \"Parameter Space Exploration\",\n    subtitle = \"Bayesian optimization focuses on promising regions\"\n  )\n```\n\n::: {.cell-output-display}\n![](13-hyperparameter-tuning_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n:::\n\n\nHow Bayesian optimization works:\n1. **Surrogate model**: Gaussian process models the objective function\n2. **Acquisition function**: Balances exploration vs exploitation\n3. **Sequential design**: Each point is chosen based on all previous results\n4. **Efficiency**: Often finds good solutions with fewer evaluations\n\n## Racing Methods: Survival of the Fittest\n\nRacing methods eliminate poor performers early, focusing resources on promising candidates:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a larger initial grid\nrace_grid <- grid_latin_hypercube(\n  penalty(range = c(-4, 0), trans = log10_trans()),\n  mixture(range = c(0, 1)),\n  size = 50  # Start with many candidates\n)\n\n# Racing with ANOVA (finetune package)\nrace_results <- elastic_workflow %>%\n  tune_race_anova(\n    resamples = ames_folds,\n    grid = race_grid,\n    metrics = yardstick::metric_set(yardstick::rmse),\n    control = control_race(\n      verbose_elim = TRUE,  # Show elimination progress\n      burn_in = 3,          # Evaluate all on first 3 folds\n      num_ties = 5,         # Number of ties to break\n      alpha = 0.05          # Significance level for elimination\n    )\n  )\n\n# Visualize racing progress\nplot_race(race_results) +\n  labs(\n    title = \"Racing Method: Progressive Elimination\",\n    subtitle = \"Poor performers are eliminated early\"\n  )\n```\n\n::: {.cell-output-display}\n![](13-hyperparameter-tuning_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Compare efficiency\nracing_summary <- tibble(\n  Method = c(\"Full Grid (50×5 folds)\", \"Racing\"),\n  `Total Evaluations` = c(50 * 5, \n                         sum(!is.na(collect_metrics(race_results, \n                                                   summarize = FALSE)$.estimate))),\n  `Best RMSE` = c(\n    show_best(grid_results, metric = \"rmse\", n = 1)$mean,\n    show_best(race_results, metric = \"rmse\", n = 1)$mean\n  )\n)\n\nknitr::kable(racing_summary, digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|Method                        | Total Evaluations| Best RMSE|\n|:-----------------------------|-----------------:|---------:|\n|Full Grid (50<U+00D7>5 folds) |               250|  38655.44|\n|Racing                        |                15|  38648.39|\n\n\n:::\n:::\n\n\nRacing advantages:\n- Dramatically reduces computation time\n- Focuses on promising candidates\n- Statistical rigor in elimination decisions\n\n## Simulated Annealing: The Flexible Explorer\n\nSimulated annealing allows \"bad\" moves early on to escape local optima:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulated annealing for hyperparameter optimization\nsa_results <- elastic_workflow %>%\n  tune_sim_anneal(\n    resamples = ames_folds,\n    metrics = yardstick::metric_set(yardstick::rmse),\n    initial = 4,      # Start with 4 random points\n    iter = 25,        # 25 iterations\n    control = control_sim_anneal(\n      verbose = FALSE,\n      cooling_coef = 0.02,  # How fast temperature decreases\n      radius = c(0.05, 0.15)  # Search radius\n    )\n  )\n\n# Visualize annealing path\nautoplot(sa_results, type = \"performance\") +\n  labs(\n    title = \"Simulated Annealing Progress\",\n    subtitle = \"Allows temporary performance degradation to escape local optima\"\n  )\n```\n\n::: {.cell-output-display}\n![](13-hyperparameter-tuning_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Show parameter exploration\nsa_results %>%\n  collect_metrics(summarize = FALSE) %>%\n  ggplot(aes(x = penalty, y = mixture, color = .estimate)) +\n  geom_point(size = 3) +\n  geom_path(alpha = 0.3) +\n  scale_x_log10() +\n  scale_color_viridis_c(name = \"RMSE\", direction = -1) +\n  labs(\n    title = \"Simulated Annealing Search Path\",\n    subtitle = \"Exploration of parameter space over iterations\",\n    x = \"Penalty (log scale)\",\n    y = \"Mixture\"\n  )\n```\n\n::: {.cell-output-display}\n![](13-hyperparameter-tuning_files/figure-html/unnamed-chunk-10-2.png){width=672}\n:::\n:::\n\n\n## Nested Resampling for Unbiased Evaluation\n\nWhen tuning hyperparameters, we need nested resampling to get unbiased performance estimates:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Outer resampling for performance estimation\nouter_folds <- vfold_cv(ames_train, v = 3)  # Reduced for computation time\n\n# Function to tune and evaluate on one outer fold\ntune_and_evaluate <- function(split) {\n  # Training data for this outer fold\n  train_data <- analysis(split)\n  # Test data for this outer fold\n  test_data <- assessment(split)\n  \n  # Inner resampling for tuning\n  inner_folds <- vfold_cv(train_data, v = 5)\n  \n  # Create workflow with data from this fold\n  fold_recipe <- recipe(Sale_Price ~ Gr_Liv_Area + Overall_Cond + Year_Built, \n                       data = train_data) %>%\n    step_normalize(all_numeric_predictors())\n  \n  fold_workflow <- workflow() %>%\n    add_recipe(fold_recipe) %>%\n    add_model(elastic_spec)\n  \n  # Tune on inner folds\n  tune_results <- fold_workflow %>%\n    tune_grid(\n      resamples = inner_folds,\n      grid = 10,\n      metrics = yardstick::metric_set(yardstick::rmse)\n    )\n  \n  # Select best parameters\n  best_params <- select_best(tune_results, metric = \"rmse\")\n  \n  # Finalize workflow\n  final_workflow <- fold_workflow %>%\n    finalize_workflow(best_params)\n  \n  # Fit on full training data for this fold\n  final_fit <- final_workflow %>%\n    fit(train_data)\n  \n  # Evaluate on test data for this fold\n  test_pred <- predict(final_fit, test_data) %>%\n    bind_cols(test_data %>% select(Sale_Price))\n  \n  # Return metrics\n  test_pred %>%\n    metrics(Sale_Price, .pred) %>%\n    mutate(\n      penalty = best_params$penalty,\n      mixture = best_params$mixture\n    )\n}\n\n# Apply to all outer folds (this takes time!)\n# nested_results <- map_df(outer_folds$splits, tune_and_evaluate, .id = \"fold\")\n\n# For demonstration, show the concept\ncat(\"Nested Resampling Structure:\nOuter Fold 1:\n  - Inner CV: Tune hyperparameters\n  - Select best hyperparameters\n  - Train final model\n  - Evaluate on outer test set\nOuter Fold 2:\n  - (Repeat process independently)\nOuter Fold 3:\n  - (Repeat process independently)\n  \nFinal estimate: Average of outer fold performances\nThis gives unbiased estimate of generalization performance\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNested Resampling Structure:\nOuter Fold 1:\n  - Inner CV: Tune hyperparameters\n  - Select best hyperparameters\n  - Train final model\n  - Evaluate on outer test set\nOuter Fold 2:\n  - (Repeat process independently)\nOuter Fold 3:\n  - (Repeat process independently)\n  \nFinal estimate: Average of outer fold performances\nThis gives unbiased estimate of generalization performance\n```\n\n\n:::\n:::\n\n\n## Parallel Processing for Speed\n\nTuning can be computationally intensive. Parallel processing helps:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Setup parallel backend\ncores <- parallel::detectCores() - 1  # Leave one core free\ncl <- makePSOCKcluster(cores)\nregisterDoParallel(cl)\n\n# Check parallel backend\ncat(\"Parallel backend registered with\", cores, \"cores\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nParallel backend registered with 7 cores\n```\n\n\n:::\n\n```{.r .cell-code}\n# Time comparison (simplified example)\ntic <- Sys.time()\nsmall_grid <- grid_regular(\n  penalty(range = c(-2, 0)),\n  mixture(),\n  levels = c(5, 3)\n)\n\n# Parallel tuning\nparallel_results <- elastic_workflow %>%\n  tune_grid(\n    resamples = ames_folds,\n    grid = small_grid,\n    metrics = yardstick::metric_set(yardstick::rmse),\n    control = control_grid(parallel_over = \"resamples\")  # Parallelize over folds\n  )\n\nparallel_time <- Sys.time() - tic\n\n# Clean up\nstopCluster(cl)\nregisterDoSEQ()  # Return to sequential\n\ncat(\"Tuning completed in\", round(parallel_time, 2), units(parallel_time), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTuning completed in 2.69 secs \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"With sequential processing, this would take approximately\", \n    round(parallel_time * cores, 2), units(parallel_time), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWith sequential processing, this would take approximately 18.83 secs \n```\n\n\n:::\n:::\n\n\nParallelization strategies:\n- **Over resamples**: Each fold processed on different core\n- **Over models**: Each hyperparameter combination on different core\n- **Hybrid**: Both, depending on problem size\n\n## Advanced Tuning Strategies\n\n### Multi-Metric Optimization\n\nSometimes we need to optimize multiple metrics:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tune for multiple metrics\nmulti_metric_results <- elastic_workflow %>%\n  tune_grid(\n    resamples = ames_folds,\n    grid = 10,\n    metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq, yardstick::mae, yardstick::mape)\n  )\n\n# Different metrics might suggest different \"best\" parameters\nbest_rmse <- select_best(multi_metric_results, metric = \"rmse\")\nbest_rsq <- select_best(multi_metric_results, metric = \"rsq\")\nbest_mae <- select_best(multi_metric_results, metric = \"mae\")\n\ncomparison <- bind_rows(\n  best_rmse %>% mutate(optimized_for = \"RMSE\"),\n  best_rsq %>% mutate(optimized_for = \"R-squared\"),\n  best_mae %>% mutate(optimized_for = \"MAE\")\n)\n\nknitr::kable(comparison, digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n| penalty| mixture|.config          |optimized_for |\n|-------:|-------:|:----------------|:-------------|\n|       0|    0.05|pre0_mod03_post0 |RMSE          |\n|       0|    0.05|pre0_mod03_post0 |R-squared     |\n|       0|    0.05|pre0_mod03_post0 |MAE           |\n\n\n:::\n\n```{.r .cell-code}\n# Pareto frontier for multi-objective optimization\nmulti_metric_results %>%\n  collect_metrics() %>%\n  filter(.metric %in% c(\"rmse\", \"rsq\")) %>%\n  select(penalty, mixture, .metric, mean) %>%\n  pivot_wider(names_from = .metric, values_from = mean) %>%\n  ggplot(aes(x = rmse, y = rsq)) +\n  geom_point(size = 3, color = \"steelblue\") +\n  geom_point(data = . %>% filter(rmse == min(rmse)), \n             color = \"red\", size = 5, shape = 17) +\n  geom_point(data = . %>% filter(rsq == max(rsq)), \n             color = \"green\", size = 5, shape = 17) +\n  labs(\n    title = \"Multi-Metric Trade-offs\",\n    subtitle = \"Red: Best RMSE, Green: Best R²\",\n    x = \"RMSE (lower is better)\",\n    y = \"R² (higher is better)\"\n  )\n```\n\n::: {.cell-output-display}\n![](13-hyperparameter-tuning_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n### Adaptive Tuning Ranges\n\nAdjust parameter ranges based on initial results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Initial coarse search\ncoarse_grid <- grid_regular(\n  penalty(range = c(-4, 1)),\n  mixture(),\n  levels = c(6, 3)\n)\n\ncoarse_results <- elastic_workflow %>%\n  tune_grid(\n    resamples = ames_folds,\n    grid = coarse_grid,\n    metrics = yardstick::metric_set(yardstick::rmse)\n  )\n\n# Find promising region\nbest_coarse <- select_best(coarse_results, metric = \"rmse\")\n\n# Refined search around best region\nfine_grid <- grid_regular(\n  penalty(range = c(log10(best_coarse$penalty) - 0.5, \n                    log10(best_coarse$penalty) + 0.5)),\n  mixture(range = c(max(0, best_coarse$mixture - 0.2), \n                    min(1, best_coarse$mixture + 0.2))),\n  levels = c(10, 5)\n)\n\nfine_results <- elastic_workflow %>%\n  tune_grid(\n    resamples = ames_folds,\n    grid = fine_grid,\n    metrics = yardstick::metric_set(yardstick::rmse)\n  )\n\n# Visualize coarse vs fine search\nsearch_comparison <- bind_rows(\n  coarse_results %>% \n    collect_metrics() %>% \n    mutate(search = \"Coarse\"),\n  fine_results %>% \n    collect_metrics() %>% \n    mutate(search = \"Fine\")\n)\n\nggplot(search_comparison, aes(x = penalty, y = mean, color = search)) +\n  geom_point(size = 2, alpha = 0.7) +\n  scale_x_log10() +\n  facet_wrap(~mixture, labeller = label_both) +\n  labs(\n    title = \"Adaptive Search Strategy\",\n    subtitle = \"Fine search focuses on promising region\",\n    x = \"Penalty (log scale)\",\n    y = \"RMSE\"\n  )\n```\n\n::: {.cell-output-display}\n![](13-hyperparameter-tuning_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n## Finalizing and Evaluating\n\nAfter tuning, finalize your workflow:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Select best overall parameters\nbest_params <- select_best(fine_results, metric = \"rmse\")\n\n# Finalize workflow\nfinal_workflow <- elastic_workflow %>%\n  finalize_workflow(best_params)\n\nprint(final_workflow)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n== Workflow ====================================================================\nPreprocessor: Recipe\nModel: linear_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n2 Recipe Steps\n\n* step_dummy()\n* step_normalize()\n\n-- Model -----------------------------------------------------------------------\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 31.6227766016838\n  mixture = 1\n\nComputational engine: glmnet \n```\n\n\n:::\n\n```{.r .cell-code}\n# Fit on all training data\nfinal_fit <- final_workflow %>%\n  fit(ames_train)\n\n# Evaluate on test set\ntest_pred <- predict(final_fit, ames_test) %>%\n  bind_cols(ames_test %>% select(Sale_Price))\n\ntest_metrics <- test_pred %>%\n  metrics(Sale_Price, .pred)\n\nknitr::kable(test_metrics, digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|.metric |.estimator |  .estimate|\n|:-------|:----------|----------:|\n|rmse    |standard   | 37674.7825|\n|rsq     |standard   |     0.7856|\n|mae     |standard   | 24116.8959|\n\n\n:::\n\n```{.r .cell-code}\n# Visualize final model performance\nggplot(test_pred, aes(x = Sale_Price, y = .pred)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(\n    title = \"Final Model Performance\",\n    subtitle = paste(\"Test RMSE:\", round(test_metrics$.estimate[1], 2)),\n    x = \"Actual Sale Price\",\n    y = \"Predicted Sale Price\"\n  ) +\n  coord_equal()\n```\n\n::: {.cell-output-display}\n![](13-hyperparameter-tuning_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n## Best Practices\n\n### 1. Start Simple, Add Complexity\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Start with few hyperparameters\nsimple_tuning <- linear_reg(penalty = tune()) %>%\n  set_engine(\"glmnet\")\n\n# Then add more if needed\ncomplex_tuning <- linear_reg(\n  penalty = tune(),\n  mixture = tune()\n) %>%\n  set_engine(\"glmnet\")\n```\n:::\n\n\n### 2. Use Appropriate Search Strategies\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntuning_guide <- tibble(\n  Scenario = c(\n    \"Few hyperparameters (≤3)\",\n    \"Many hyperparameters (>5)\",\n    \"Expensive models\",\n    \"Quick exploration\",\n    \"Final optimization\",\n    \"Limited budget\"\n  ),\n  `Recommended Method` = c(\n    \"Grid search\",\n    \"Random search or Bayesian\",\n    \"Bayesian optimization\",\n    \"Random search\",\n    \"Bayesian or fine grid\",\n    \"Racing methods\"\n  ),\n  Reasoning = c(\n    \"Can afford exhaustive search\",\n    \"Grid search becomes prohibitive\",\n    \"Minimize number of evaluations\",\n    \"Good coverage quickly\",\n    \"Find true optimum\",\n    \"Eliminate bad candidates early\"\n  )\n)\n\nknitr::kable(tuning_guide)\n```\n\n::: {.cell-output-display}\n\n\n|Scenario                        |Recommended Method        |Reasoning                       |\n|:-------------------------------|:-------------------------|:-------------------------------|\n|Few hyperparameters (<U+2264>3) |Grid search               |Can afford exhaustive search    |\n|Many hyperparameters (>5)       |Random search or Bayesian |Grid search becomes prohibitive |\n|Expensive models                |Bayesian optimization     |Minimize number of evaluations  |\n|Quick exploration               |Random search             |Good coverage quickly           |\n|Final optimization              |Bayesian or fine grid     |Find true optimum               |\n|Limited budget                  |Racing methods            |Eliminate bad candidates early  |\n\n\n:::\n:::\n\n\n### 3. Monitor for Overfitting\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check for overfitting during tuning\ntuning_diagnostics <- fine_results %>%\n  collect_metrics(summarize = FALSE) %>%\n  group_by(penalty, mixture) %>%\n  summarise(\n    mean_rmse = mean(.estimate),\n    sd_rmse = sd(.estimate),\n    cv_variation = sd_rmse / mean_rmse,\n    .groups = \"drop\"\n  )\n\n# High CV variation might indicate overfitting\nggplot(tuning_diagnostics, aes(x = penalty, y = cv_variation, color = factor(mixture))) +\n  geom_point(size = 2) +\n  geom_line() +\n  scale_x_log10() +\n  labs(\n    title = \"Cross-Validation Stability\",\n    subtitle = \"High variation suggests potential overfitting\",\n    x = \"Penalty (log scale)\",\n    y = \"CV Coefficient of Variation\",\n    color = \"Mixture\"\n  )\n```\n\n::: {.cell-output-display}\n![](13-hyperparameter-tuning_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n## Exercises\n\n### Exercise 1: Compare Search Strategies\n\nCompare different search strategies on the same problem:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Define a tunable XGBoost model\nxgb_spec <- boost_tree(\n  trees = 300,\n  tree_depth = tune(),\n  learn_rate = tune(),\n  min_n = tune()\n) %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"regression\")\n\nxgb_workflow <- workflow() %>%\n  add_recipe(simple_recipe) %>%\n  add_model(xgb_spec)\n\n# Compare strategies (using smaller grids for speed)\n# 1. Regular grid\nregular_time <- system.time({\n  regular_xgb <- xgb_workflow %>%\n    tune_grid(\n      resamples = ames_folds,\n      grid = grid_regular(\n        tree_depth(c(3, 10)),\n        learn_rate(c(-3, -1)),\n        min_n(c(5, 20)),\n        levels = c(3, 3, 3)  # 27 combinations\n      ),\n      metrics = yardstick::metric_set(yardstick::rmse)\n    )\n})\n\n# 2. Random search\nrandom_time <- system.time({\n  random_xgb <- xgb_workflow %>%\n    tune_grid(\n      resamples = ames_folds,\n      grid = 15,  # 15 random combinations\n      metrics = yardstick::metric_set(yardstick::rmse)\n    )\n})\n\n# 3. Latin hypercube\nlhs_time <- system.time({\n  lhs_xgb <- xgb_workflow %>%\n    tune_grid(\n      resamples = ames_folds,\n      grid = grid_latin_hypercube(\n        tree_depth(c(3, 10)),\n        learn_rate(c(-3, -1)),\n        min_n(c(5, 20)),\n        size = 15\n      ),\n      metrics = yardstick::metric_set(yardstick::rmse)\n    )\n})\n\n# Compare results\nstrategy_comparison <- bind_rows(\n  show_best(regular_xgb, metric = \"rmse\", n = 1) %>% \n    mutate(strategy = \"Regular Grid\", time = regular_time[3], points = 27),\n  show_best(random_xgb, metric = \"rmse\", n = 1) %>% \n    mutate(strategy = \"Random\", time = random_time[3], points = 15),\n  show_best(lhs_xgb, metric = \"rmse\", n = 1) %>% \n    mutate(strategy = \"Latin Hypercube\", time = lhs_time[3], points = 15)\n) %>%\n  select(strategy, points, time, mean, tree_depth, learn_rate, min_n)\n\nknitr::kable(strategy_comparison, digits = 4)\n```\n\n::: {.cell-output-display}\n\n\n|strategy        | points|   time|     mean| tree_depth| learn_rate| min_n|\n|:---------------|------:|------:|--------:|----------:|----------:|-----:|\n|Regular Grid    |     27| 84.911| 33501.18|          3|     0.1000|    12|\n|Random          |     15| 54.714| 34230.75|          4|     0.2096|    12|\n|Latin Hypercube |     15| 49.750| 33799.37|          7|     0.0628|    12|\n\n\n:::\n:::\n\n\n### Exercise 2: Implement Custom Tuning\n\nCreate a custom tuning strategy:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Implement iterative refinement strategy\niterative_tuning <- function(workflow, resamples, n_iterations = 3) {\n  results <- list()\n  \n  # Start with coarse grid\n  current_range_tree <- c(3, 15)\n  current_range_rate <- c(-3, -0.5)\n  \n  for (i in 1:n_iterations) {\n    # Create grid for this iteration\n    current_grid <- grid_regular(\n      tree_depth(current_range_tree),\n      learn_rate(current_range_rate, trans = log10_trans()),\n      levels = c(5, 5)\n    )\n    \n    # Tune\n    iter_results <- workflow %>%\n      tune_grid(\n        resamples = resamples,\n        grid = current_grid,\n        metrics = yardstick::metric_set(yardstick::rmse)\n      )\n    \n    results[[i]] <- iter_results\n    \n    # Get best parameters\n    best <- select_best(iter_results, metric = \"rmse\")\n    \n    # Refine ranges for next iteration (zoom in by 50%)\n    tree_width <- diff(current_range_tree) * 0.25\n    rate_width <- diff(current_range_rate) * 0.25\n    \n    current_range_tree <- c(\n      max(3, best$tree_depth - tree_width),\n      min(15, best$tree_depth + tree_width)\n    )\n    \n    current_range_rate <- c(\n      max(-3, log10(best$learn_rate) - rate_width),\n      min(-0.5, log10(best$learn_rate) + rate_width)\n    )\n    \n    cat(\"Iteration\", i, \"- Best RMSE:\", best$mean, \"\\n\")\n  }\n  \n  return(results)\n}\n\n# Apply custom strategy (simplified for demonstration)\nsimple_xgb_workflow <- workflow() %>%\n  add_recipe(recipe(Sale_Price ~ Gr_Liv_Area + Overall_Cond, data = ames_train)) %>%\n  add_model(\n    boost_tree(trees = 100, tree_depth = tune(), learn_rate = tune()) %>%\n      set_engine(\"xgboost\") %>%\n      set_mode(\"regression\")\n  )\n\n# custom_results <- iterative_tuning(simple_xgb_workflow, ames_folds, n_iterations = 2)\n```\n:::\n\n\n### Exercise 3: Multi-Objective Optimization\n\nBalance multiple objectives:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your solution\n# Tune for both performance and model complexity\ncomplexity_workflow <- workflow() %>%\n  add_recipe(simple_recipe) %>%\n  add_model(\n    rand_forest(mtry = tune(), min_n = tune(), trees = tune()) %>%\n      set_engine(\"ranger\") %>%\n      set_mode(\"regression\")\n  )\n\n# Create grid\ncomplexity_grid <- grid_latin_hypercube(\n  mtry(c(2, 10)),\n  min_n(c(5, 40)),\n  trees(c(100, 1000)),\n  size = 30\n)\n\n# Tune\ncomplexity_results <- complexity_workflow %>%\n  tune_grid(\n    resamples = ames_folds,\n    grid = complexity_grid,\n    metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq)\n  )\n\n# Add complexity metric (total trees × average tree size estimate)\ncomplexity_metrics <- complexity_results %>%\n  collect_metrics() %>%\n  filter(.metric == \"rmse\") %>%\n  mutate(\n    complexity = trees * (1 / min_n) * mtry,  # Rough complexity measure\n    performance = -mean,  # Negative RMSE (higher is better)\n    # Pareto optimal if no other point has both better performance AND lower complexity\n    pareto = TRUE  # Simplified - would need proper calculation\n  )\n\n# Visualize trade-off\nggplot(complexity_metrics, aes(x = complexity, y = performance)) +\n  geom_point(aes(color = trees), size = 3) +\n  scale_color_viridis_c() +\n  labs(\n    title = \"Performance vs Complexity Trade-off\",\n    subtitle = \"Balancing model accuracy and computational cost\",\n    x = \"Model Complexity (arbitrary units)\",\n    y = \"Performance (-RMSE)\"\n  )\n```\n\n::: {.cell-output-display}\n![](13-hyperparameter-tuning_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Select based on custom criterion\nbest_balanced <- complexity_metrics %>%\n  mutate(\n    score = performance - 0.0001 * complexity  # Custom weighting\n  ) %>%\n  arrange(desc(score)) %>%\n  slice(1)\n\ncat(\"Best balanced model:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBest balanced model:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Trees:\", best_balanced$trees, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrees: 574 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"mtry:\", best_balanced$mtry, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nmtry: 10 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"min_n:\", best_balanced$min_n, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nmin_n: 15 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"RMSE:\", -best_balanced$performance, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRMSE: 35013.85 \n```\n\n\n:::\n:::\n\n\n## Summary\n\nIn this comprehensive chapter, you've mastered:\n\n✅ **Hyperparameter fundamentals**\n  - Difference from parameters\n  - Impact on model performance\n  - Model-specific hyperparameters\n\n✅ **Search strategies**\n  - Grid search for exhaustive exploration\n  - Random search for efficiency\n  - Latin hypercube for space-filling\n\n✅ **Advanced methods**\n  - Bayesian optimization for intelligent search\n  - Racing for early stopping\n  - Simulated annealing for flexibility\n\n✅ **Practical considerations**\n  - Nested resampling for unbiased evaluation\n  - Parallel processing for speed\n  - Multi-metric optimization\n\n✅ **Best practices**\n  - Choosing appropriate strategies\n  - Monitoring for overfitting\n  - Iterative refinement\n\nKey takeaways:\n- No single best tuning method - choose based on context\n- Start coarse, refine gradually\n- Use parallel processing for speed\n- Always validate on held-out data\n- Consider computational budget\n- Balance exploration and exploitation\n\n## What's Next?\n\nIn [Chapter 16](16-ensemble-methods.Rmd), we'll explore ensemble methods that combine multiple models for superior performance.\n\n## Additional Resources\n\n- [tune Documentation](https://tune.tidymodels.org/)\n- [finetune Documentation](https://finetune.tidymodels.org/)\n- [dials Documentation](https://dials.tidymodels.org/)\n- [Hyperparameter Optimization Review](https://arxiv.org/abs/1502.02127)\n- [Practical Bayesian Optimization](https://arxiv.org/abs/1206.2944)\n",
    "supporting": [
      "13-hyperparameter-tuning_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}