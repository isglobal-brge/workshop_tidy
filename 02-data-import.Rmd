---
title: "Chapter 2: Data Import and Export with readr and Beyond"
author: "Workshop Materials"
date: today
format:
  html:
    code-fold: false
    code-tools: true
---

## Learning Objectives

By the end of this chapter, you will:

- Import CSV, TSV, and delimited files with readr
- Handle Excel files with readxl
- Work with JSON and XML data
- Connect to databases
- Export data in various formats
- Handle common import problems (encoding, data types, missing values)
- Work with large files efficiently

## Setup

```{r}
#| message: true
library(tidyverse)  # Includes readr
library(readxl)     # For Excel files
library(jsonlite)   # For JSON files
library(DBI)        # For databases
library(RSQLite)    # For SQLite
library(haven)      # For SPSS, Stata, SAS files

# Create a temporary directory for our examples
temp_dir <- tempdir()
```

## Reading CSV Files with readr

### Basic CSV Import

```{r}
# Create a sample CSV file
sample_data <- tibble(
  id = 1:5,
  name = c("Alice", "Bob", "Charlie", "Diana", "Eve"),
  age = c(25, 30, 35, 28, 33),
  salary = c(50000, 65000, 80000, 55000, 70000),
  hire_date = c("2020-01-15", "2019-03-22", "2018-07-01", "2021-02-10", "2019-11-30")
)

# Write to CSV
csv_file <- file.path(temp_dir, "employees.csv")
write_csv(sample_data, csv_file)

# Read it back
employees <- read_csv(csv_file)
employees

# Compare with base R read.csv
employees_base <- read.csv(csv_file)
str(employees)      # readr version - note the data types
str(employees_base) # base R version - different types!
```

### Specifying Column Types

```{r}
# Explicit column types
employees_typed <- read_csv(
  csv_file,
  col_types = cols(
    id = col_integer(),
    name = col_character(),
    age = col_double(),
    salary = col_number(),
    hire_date = col_date(format = "%Y-%m-%d")
  )
)

employees_typed
```

### Handling Messy Data

```{r}
# Create a messy CSV
messy_csv <- "
Name,Age,Income,Start Date
Alice,25,$50,000,15/01/2020
Bob,30,$65,000,22/03/2019
Charlie,35,$80,000,01/07/2018
#Diana,28,$55,000,10/02/2021
Eve,NA,$70,000,30/11/2019
"

# Write to file
messy_file <- file.path(temp_dir, "messy_data.csv")
writeLines(messy_csv, messy_file)

# Read with various options
clean_data <- read_csv(
  messy_file,
  skip = 1,  # Skip the first line
  comment = "#",  # Treat lines starting with # as comments
  na = c("", "NA", "N/A", "missing"),  # Define NA values
  col_names = c("name", "age", "income", "start_date"),
  col_types = cols(
    name = col_character(),
    age = col_double(),
    income = col_character(),  # Keep as character for now
    start_date = col_character()
  )
)

# Clean the income column
clean_data <- clean_data %>%
  mutate(
    income = parse_number(income),  # Remove $ and , from numbers
    start_date = parse_date(start_date, format = "%d/%m/%Y")
  )

clean_data
```

## Reading Other Delimited Files

### TSV and Custom Delimiters

```{r}
# Tab-separated values
tsv_data <- "name\tage\tcity
Alice\t25\tNew York
Bob\t30\tLos Angeles
Charlie\t35\tChicago"

tsv_file <- file.path(temp_dir, "data.tsv")
writeLines(tsv_data, tsv_file)

# Read TSV
read_tsv(tsv_file)

# Custom delimiter (pipe-separated)
pipe_data <- "name|age|city
Alice|25|New York
Bob|30|Los Angeles"

pipe_file <- file.path(temp_dir, "data.txt")
writeLines(pipe_data, pipe_file)

read_delim(pipe_file, delim = "|")
```

### Fixed Width Files

```{r}
# Fixed width format
fwf_data <- "Alice    25   50000
Bob      30   65000
Charlie  35   80000"

fwf_file <- file.path(temp_dir, "fixed_width.txt")
writeLines(fwf_data, fwf_file)

# Read with column positions
read_fwf(
  fwf_file,
  col_positions = fwf_widths(
    widths = c(9, 5, 7),
    col_names = c("name", "age", "salary")
  )
)
```

## Excel Files with readxl

```{r}
# Create an Excel file with multiple sheets
excel_file <- file.path(temp_dir, "company_data.xlsx")

# Create sample data
sales_data <- tibble(
  month = month.name[1:6],
  revenue = c(100000, 120000, 115000, 130000, 125000, 140000),
  costs = c(80000, 85000, 82000, 90000, 88000, 95000)
)

employee_data <- tibble(
  department = c("Sales", "Marketing", "IT", "HR"),
  headcount = c(25, 15, 20, 10),
  avg_salary = c(60000, 55000, 75000, 50000)
)

# Write to Excel (requires writexl package)
if (require(writexl, quietly = TRUE)) {
  write_xlsx(
    list(
      Sales = sales_data,
      Employees = employee_data
    ),
    excel_file
  )
  
  # Read from Excel
  # List all sheets
  excel_sheets(excel_file)
  
  # Read specific sheet
  sales <- read_excel(excel_file, sheet = "Sales")
  sales
  
  # Read with specific range
  partial_data <- read_excel(
    excel_file, 
    sheet = "Sales",
    range = "A1:B4"  # Read only first 3 rows and 2 columns
  )
  partial_data
  
  # Read all sheets at once
  all_sheets <- excel_sheets(excel_file) %>%
    set_names() %>%
    map(~ read_excel(excel_file, sheet = .))
  
  all_sheets
}
```

## JSON Data

```{r}
# Create JSON data
json_data <- list(
  employees = list(
    list(
      id = 1,
      name = "Alice",
      skills = c("R", "Python", "SQL"),
      projects = list(
        list(name = "Project A", status = "completed"),
        list(name = "Project B", status = "in progress")
      )
    ),
    list(
      id = 2,
      name = "Bob",
      skills = c("JavaScript", "HTML", "CSS"),
      projects = list(
        list(name = "Project C", status = "completed")
      )
    )
  )
)

# Convert to JSON string
json_string <- toJSON(json_data, pretty = TRUE, auto_unbox = TRUE)
cat(json_string)

# Write to file
json_file <- file.path(temp_dir, "employees.json")
write(json_string, json_file)

# Read JSON
json_imported <- fromJSON(json_file)
str(json_imported)

# Flatten nested structure
employees_flat <- json_imported$employees %>%
  as_tibble() %>%
  mutate(
    skills = map_chr(skills, ~ paste(., collapse = ", ")),
    num_projects = map_int(projects, length)
  ) %>%
  select(-projects)  # Remove nested column for simplicity

employees_flat
```

## Database Connections

```{r}
# Create a SQLite database
db_file <- file.path(temp_dir, "company.db")
con <- dbConnect(RSQLite::SQLite(), db_file)

# Write data to database
dbWriteTable(con, "employees", sample_data, overwrite = TRUE)
dbWriteTable(con, "sales", sales_data, overwrite = TRUE)

# List tables
dbListTables(con)

# Query data using SQL
result <- dbGetQuery(
  con,
  "SELECT * FROM employees WHERE salary > 60000"
)
result

# Using dplyr with databases
employees_db <- tbl(con, "employees")

# dplyr operations are translated to SQL
employees_db %>%
  filter(salary > 60000) %>%
  select(name, salary) %>%
  arrange(desc(salary)) %>%
  show_query()  # Show the SQL query

# Collect results
high_earners <- employees_db %>%
  filter(salary > 60000) %>%
  collect()  # Bring data into R

high_earners

# Close connection
dbDisconnect(con)
```

## Working with APIs and Web Data

```{r}
# Example: Reading data from a URL
# Using a public dataset
url <- "https://raw.githubusercontent.com/tidyverse/ggplot2/main/data-raw/diamonds.csv"

# Direct read from URL
diamonds_web <- read_csv(url, n_max = 1000)  # Read first 1000 rows
glimpse(diamonds_web)

# Download first, then read (useful for large files)
local_file <- file.path(temp_dir, "diamonds.csv")
download.file(url, local_file, quiet = TRUE)
diamonds_local <- read_csv(local_file, n_max = 1000)
```

## Statistical Software Files (SPSS, Stata, SAS)

```{r}
# Create example data
survey_data <- tibble(
  respondent_id = 1:10,
  age_group = factor(c(1, 2, 1, 3, 2, 2, 1, 3, 2, 1),
                     labels = c("18-30", "31-50", "51+")),
  satisfaction = c(4, 5, 3, 4, 5, 2, 3, 4, 5, 4),
  comments = c(rep("Good service", 5), rep("Needs improvement", 5))
)

# Save as different formats
sav_file <- file.path(temp_dir, "survey.sav")
dta_file <- file.path(temp_dir, "survey.dta")

write_sav(survey_data, sav_file)
write_dta(survey_data, dta_file)

# Read them back
spss_data <- read_sav(sav_file)
stata_data <- read_dta(dta_file)

glimpse(spss_data)
```

## Handling Large Files

```{r}
# Create a large CSV file
large_data <- tibble(
  id = 1:100000,
  value1 = rnorm(100000),
  value2 = runif(100000),
  category = sample(LETTERS[1:5], 100000, replace = TRUE)
)

large_file <- file.path(temp_dir, "large_data.csv")
write_csv(large_data, large_file)

# Read only first few rows to check structure
preview <- read_csv(large_file, n_max = 5)
preview

# Read specific columns
subset_data <- read_csv(
  large_file,
  col_select = c(id, category, value1)
)
glimpse(subset_data)

# Read in chunks
process_chunk <- function(data, pos) {
  # Process each chunk
  summarise(data,
    chunk = pos,
    mean_value1 = mean(value1),
    n = n()
  )
}

# Read and process in chunks
chunked_results <- read_csv_chunked(
  large_file,
  callback = DataFrameCallback$new(process_chunk),
  chunk_size = 10000
)

chunked_results
```

## Data Export

### Writing CSV and TSV

```{r}
# Prepare data for export
export_data <- penguins %>%
  drop_na() %>%
  group_by(species, island) %>%
  summarise(
    count = n(),
    avg_mass = mean(body_mass_g),
    .groups = "drop"
  )

# Write CSV
write_csv(export_data, file.path(temp_dir, "penguin_summary.csv"))

# Write TSV
write_tsv(export_data, file.path(temp_dir, "penguin_summary.tsv"))

# Write with custom settings
write_csv2(export_data, file.path(temp_dir, "penguin_summary_eu.csv"))  # European format (semicolon separator)

# Write Excel
if (require(writexl, quietly = TRUE)) {
  write_xlsx(export_data, file.path(temp_dir, "penguin_summary.xlsx"))
}
```

### Advanced Export Options

```{r}
# Create a complex dataset
complex_data <- tibble(
  id = 1:5,
  name = c("Alice", "Bob", "Charlie", "Diana", "Eve"),
  scores = list(
    c(85, 90, 88),
    c(92, 88, 95),
    c(78, 85, 82),
    c(90, 92, 94),
    c(88, 86, 90)
  ),
  metadata = c(
    '{"department": "Sales", "level": 2}',
    '{"department": "IT", "level": 3}',
    '{"department": "Marketing", "level": 1}',
    '{"department": "Sales", "level": 3}',
    '{"department": "IT", "level": 2}'
  )
)

# Export to JSON
json_export <- toJSON(complex_data, pretty = TRUE)
write(json_export, file.path(temp_dir, "complex_data.json"))

# Export to RDS (R's native format - preserves all data types)
saveRDS(complex_data, file.path(temp_dir, "complex_data.rds"))

# Read it back
restored_data <- readRDS(file.path(temp_dir, "complex_data.rds"))
identical(complex_data, restored_data)  # TRUE - perfect preservation
```

## Common Import Problems and Solutions

### Problem 1: Encoding Issues

```{r}
# Create file with special characters
special_text <- "Café,Zürich,São Paulo,北京\n25,30,35,40"
special_file <- file.path(temp_dir, "special_chars.csv")

# Write with UTF-8 encoding
writeLines(special_text, special_file, useBytes = TRUE)

# Read with encoding specification
read_csv(special_file, locale = locale(encoding = "UTF-8"))
```

### Problem 2: Inconsistent Data Types

```{r}
# Mixed types in a column
mixed_csv <- "id,value
1,100
2,200
3,NA
4,missing
5,300"

mixed_file <- file.path(temp_dir, "mixed.csv")
writeLines(mixed_csv, mixed_file)

# Read with proper NA handling
read_csv(
  mixed_file,
  na = c("NA", "missing", ""),
  col_types = cols(
    id = col_integer(),
    value = col_double()
  )
)
```

### Problem 3: Date/Time Formats

```{r}
# Various date formats
dates_csv <- "event,date
Meeting,2023-01-15
Conference,15/01/2023
Workshop,Jan 15 2023
Seminar,15-Jan-23"

dates_file <- file.path(temp_dir, "dates.csv")
writeLines(dates_csv, dates_file)

# Read and parse dates
dates_data <- read_csv(dates_file, col_types = cols(date = col_character()))

# Parse different formats
dates_parsed <- dates_data %>%
  mutate(
    parsed_date = case_when(
      str_detect(date, "^\\d{4}-\\d{2}-\\d{2}$") ~ parse_date(date, "%Y-%m-%d"),
      str_detect(date, "^\\d{2}/\\d{2}/\\d{4}$") ~ parse_date(date, "%d/%m/%Y"),
      str_detect(date, "^[A-Za-z]+ \\d+ \\d{4}$") ~ parse_date(date, "%b %d %Y"),
      str_detect(date, "^\\d{2}-[A-Za-z]+-\\d{2}$") ~ parse_date(date, "%d-%b-%y"),
      TRUE ~ NA_Date_
    )
  )

dates_parsed
```

## Exercises

### Exercise 1: Multi-format Import

Create a function that can read data from CSV, Excel, or JSON based on file extension:

```{r}
read_any <- function(file_path) {
  ext <- tools::file_ext(file_path)
  
  data <- switch(
    tolower(ext),
    "csv" = read_csv(file_path, show_col_types = FALSE),
    "tsv" = read_tsv(file_path, show_col_types = FALSE),
    "xlsx" = read_excel(file_path),
    "xls" = read_excel(file_path),
    "json" = fromJSON(file_path) %>% as_tibble(),
    stop(paste("Unsupported file type:", ext))
  )
  
  return(data)
}

# Test the function
test_files <- c(
  file.path(temp_dir, "employees.csv"),
  file.path(temp_dir, "data.tsv")
)

for (file in test_files) {
  if (file.exists(file)) {
    cat("Reading:", basename(file), "\n")
    print(read_any(file))
    cat("\n")
  }
}
```

### Exercise 2: Data Cleaning Pipeline

Clean and import this messy dataset:

```{r}
messy_sales <- "
Date,Product,,,Price,Quantity
2023-01-15,Widget A,,,29.99,10
2023/01/16,Widget B,,,39.99,missing
17-01-2023,Widget C,,,49.99,5
2023-01-18,Widget A,,,29.99,
,Widget B,,,39.99,8
"

# Write to file
sales_file <- file.path(temp_dir, "messy_sales.csv")
writeLines(messy_sales, sales_file)

# Your cleaning pipeline here
cleaned_sales <- read_csv(
  sales_file,
  skip_empty_rows = TRUE,
  na = c("", "missing", "NA")
) %>%
  select(Date, Product, Price, Quantity) %>%
  filter(!is.na(Date) & !is.na(Product)) %>%
  mutate(
    Date = case_when(
      str_detect(Date, "^\\d{4}-\\d{2}-\\d{2}$") ~ parse_date(Date, "%Y-%m-%d"),
      str_detect(Date, "^\\d{4}/\\d{2}/\\d{2}$") ~ parse_date(Date, "%Y/%m/%d"),
      str_detect(Date, "^\\d{2}-\\d{2}-\\d{4}$") ~ parse_date(Date, "%d-%m-%Y"),
      TRUE ~ NA_Date_
    ),
    Quantity = replace_na(Quantity, 0),
    Total = Price * Quantity
  )

cleaned_sales
```

### Exercise 3: Database Operations

Create a mini database with related tables and perform joins:

```{r}
# Create database
db <- dbConnect(RSQLite::SQLite(), ":memory:")

# Create tables
customers <- tibble(
  customer_id = 1:5,
  name = c("Alice", "Bob", "Charlie", "Diana", "Eve"),
  city = c("NYC", "LA", "Chicago", "Houston", "Phoenix")
)

orders <- tibble(
  order_id = 1:8,
  customer_id = c(1, 2, 1, 3, 4, 2, 5, 1),
  amount = c(100, 200, 150, 300, 250, 175, 400, 225),
  order_date = seq(as.Date("2023-01-01"), by = "week", length.out = 8)
)

# Write to database
dbWriteTable(db, "customers", customers)
dbWriteTable(db, "orders", orders)

# Query with join
result <- dbGetQuery(db, "
  SELECT c.name, c.city, COUNT(o.order_id) as num_orders, SUM(o.amount) as total_spent
  FROM customers c
  LEFT JOIN orders o ON c.customer_id = o.customer_id
  GROUP BY c.customer_id, c.name, c.city
  ORDER BY total_spent DESC
")

result

# Using dplyr
customers_tbl <- tbl(db, "customers")
orders_tbl <- tbl(db, "orders")

summary_tbl <- customers_tbl %>%
  left_join(orders_tbl, by = "customer_id") %>%
  group_by(customer_id, name, city) %>%
  summarise(
    num_orders = n(),
    total_spent = sum(amount, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  arrange(desc(total_spent)) %>%
  collect()

summary_tbl

dbDisconnect(db)
```

### Exercise 4: Batch Processing

Process multiple files in a directory:

```{r}
# Create multiple CSV files
for (i in 1:3) {
  data <- tibble(
    file_id = i,
    values = rnorm(100, mean = i * 10, sd = 2),
    category = sample(c("A", "B", "C"), 100, replace = TRUE)
  )
  write_csv(data, file.path(temp_dir, paste0("batch_", i, ".csv")))
}

# Read and combine all files
batch_files <- list.files(temp_dir, pattern = "^batch_.*\\.csv$", full.names = TRUE)

combined_data <- batch_files %>%
  map_dfr(~ read_csv(., show_col_types = FALSE)) %>%
  group_by(file_id, category) %>%
  summarise(
    mean_value = mean(values),
    sd_value = sd(values),
    n = n(),
    .groups = "drop"
  )

combined_data
```

## Summary

In this chapter, you learned:

✅ Reading various file formats (CSV, Excel, JSON, databases)  
✅ Handling messy and inconsistent data  
✅ Working with large files efficiently  
✅ Exporting data in multiple formats  
✅ Solving common import problems  
✅ Connecting to and querying databases  

## What's Next?

In [Chapter 3](03-data-wrangling.qmd), we'll dive deep into data manipulation with dplyr, learning to filter, select, mutate, and summarize data efficiently.

## Additional Resources

- [readr Documentation](https://readr.tidyverse.org/)
- [readxl Documentation](https://readxl.tidyverse.org/)
- [DBI Documentation](https://dbi.r-dbi.org/)
- [Data Import Cheat Sheet](https://github.com/rstudio/cheatsheets/blob/main/data-import.pdf)
